full_code
"@Test
public void testAddRemoveRenewAction() throws IOException, InterruptedException {
    TestFileSystem tfs = new TestFileSystem();
    renewer.addRenewAction(tfs);
    for (int i = 0; i < 60; i++) {
        Thread.sleep(RENEW_CYCLE);
        if (tfs.testToken.renewCount > 0) {
            renewer.removeRenewAction(tfs);
            break;
        }
    }
    assertTrue(""Token not renewed even after 1 minute"", tfs.testToken.renewCount > 0);
    assertTrue(""Token not removed"", tfs.testToken.renewCount < MAX_RENEWALS);
    assertTrue(""Token not cancelled"", tfs.testToken.cancelled);
}"
"@Test
public void testToMetricResponse() throws Exception {
    String subscriptionId = ""12345"";
    long ts = 1000L;
    List<Tag> tags = new ArrayList<>();
    tags.add(new Tag(""tag1"", ""value1""));
    Metric m = Metric.newBuilder().name(""sys.cpu.user"").value(ts, 2.0).tags(tags).tag(VISIBILITY_TAG, ""(a&b)|(c&d)"").build();
    String json = JsonUtil.getObjectMapper().writeValueAsString(MetricResponse.fromMetric(m, subscriptionId));
    String expected = ""{\""metric\"":\""sys.cpu.user\"",\""timestamp\"":1000,\""value\"":2.0,\""tags\"":[{\""tag1\"":\""value1\""},{\""viz\"":\""(a&b)|(c&d)\""}],\""subscriptionId\"":\""12345\"",\""complete\"":false}"";
    Assert.assertEquals(expected, json);
}"
"@Test
public void testToFile() throws Exception {
    item = new PathData(""."", conf);
    assertEquals(new File(testDir.toString()), item.toFile());
    item = new PathData(""d1/f1"", conf);
    assertEquals(new File(testDir + ""/d1/f1""), item.toFile());
    item = new PathData(testDir + ""/d1/f1"", conf);
    assertEquals(new File(testDir + ""/d1/f1""), item.toFile());
}"
"@Test
public void testRemoveContext() throws IOException {
    String dir = buildBufferDir(ROOT, 0);
    String contextCfgItemName = ""application_1340842292563_0004.app.cache.dirs"";
    conf.set(contextCfgItemName, dir);
    LocalDirAllocator localDirAllocator = new LocalDirAllocator(contextCfgItemName);
    localDirAllocator.getLocalPathForWrite(""p1/x"", SMALL_FILE_SIZE, conf);
    assertTrue(LocalDirAllocator.isContextValid(contextCfgItemName));
    LocalDirAllocator.removeContext(contextCfgItemName);
    assertFalse(LocalDirAllocator.isContextValid(contextCfgItemName));
}"
"@Test
public void testSimple() throws Exception {
    Configuration conf = new Configuration();
    MyResourceManager rm = new MyResourceManager(conf);
    rm.start();
    DrainDispatcher dispatcher = ((DrainDispatcher) (rm.getRMContext().getDispatcher()));
    RMApp app = rm.submitApp(1024);
    dispatcher.await();
    MockNM amNodeManager = rm.registerNode(""amNM:1234"", 2048);
    amNodeManager.nodeHeartbeat(true);
    dispatcher.await();
    ApplicationAttemptId appAttemptId = app.getCurrentAppAttempt().getAppAttemptId();
    rm.sendAMLaunched(appAttemptId);
    dispatcher.await();
    JobId jobId = MRBuilderUtils.newJobId(appAttemptId.getApplicationId(), 0);
    Job mockJob = mock(Job.class);
    when(mockJob.getReport()).thenReturn(MRBuilderUtils.newJobReport(jobId, ""job"", ""user"", RUNNING, 0, 0, 0, 0, 0, 0, ""jobfile""));
    MyContainerAllocator allocator = new MyContainerAllocator(rm, conf, appAttemptId, mockJob);
    MockNM nodeManager1 = rm.registerNode(""h1:1234"", 10240);
    MockNM nodeManager2 = rm.registerNode(""h2:1234"", 10240);
    MockNM nodeManager3 = rm.registerNode(""h3:1234"", 10240);
    dispatcher.await();
    ContainerRequestEvent event1 = createReq(jobId, 1, 1024, new String[]{ ""h1"" });
    allocator.sendRequest(event1);
    ContainerRequestEvent event2 = createReq(jobId, 2, 1024, new String[]{ ""h2"" });
    allocator.sendRequest(event2);
    List<TaskAttemptContainerAssignedEvent> assigned = allocator.schedule();
    dispatcher.await();
    Assert.assertEquals(""No of assignments must be 0"", 0, assigned.size());
    ContainerRequestEvent event3 = createReq(jobId, 3, 1024, new String[]{ ""h3"" });
    allocator.sendRequest(event3);
    assigned = allocator.schedule();
    dispatcher.await();
    Assert.assertEquals(""No of assignments must be 0"", 0, assigned.size());
    nodeManager1.nodeHeartbeat(true);
    nodeManager2.nodeHeartbeat(true);
    nodeManager3.nodeHeartbeat(true);
    dispatcher.await();
    assigned = allocator.schedule();
    dispatcher.await();
    checkAssignments(new ContainerRequestEvent[]{ event1, event2, event3 }, assigned, false);
}"
"@Test
public void test_multimap() throws Exception {
    Map<String, Integer> map = ImmutableMap.of(""a"", 1, ""b"", 1, ""c"", 2);
    SetMultimap<String, Integer> multimap = Multimaps.forMap(map);
    Multimap<Integer, String> inverse = Multimaps.invertFrom(multimap, HashMultimap.<Integer, String>create());
    String json = JSON.toJSONString(inverse);
    assertEquals(""{1:[\""a\"",\""b\""],2:[\""c\""]}"", json);
}"
"@Test
public void testWorkflowForkFailure() throws Exception {
    Assert.assertEquals(200, deploy(WorkflowFailureInForkApp.class).getStatusLine().getStatusCode());
    Id.Application appId = Application.from(DEFAULT, NAME);
    Id.Workflow workflowId = Workflow.from(appId, NAME);
    Id.Program firstMRId = Program.from(appId, MAPREDUCE, FIRST_MAPREDUCE_NAME);
    Id.Program secondMRId = Program.from(appId, MAPREDUCE, SECOND_MAPREDUCE_NAME);
    String outputPath = new File(tmpFolder.newFolder(), ""output"").getAbsolutePath();
    File fileToSync = new File(tmpFolder.newFolder() + ""/sync.file"");
    File fileToWait = new File(tmpFolder.newFolder() + ""/wait.file"");
    startProgram(workflowId, ImmutableMap.of(""inputPath"", createInput(""testWorkflowForkFailureInput""), ""outputPath"", outputPath, ""sync.file"", fileToSync.getAbsolutePath(), ""wait.file"", fileToWait.getAbsolutePath(), (""mapreduce."" + WorkflowFailureInForkApp.SECOND_MAPREDUCE_NAME) + "".throw.exception"", ""true""));
    waitState(workflowId, RUNNING.name());
    waitState(workflowId, STOPPED.name());
    verifyProgramRuns(workflowId, ""failed"");
    List<RunRecord> mapReduceProgramRuns = getProgramRuns(firstMRId, KILLED.name());
    Assert.assertEquals(1, mapReduceProgramRuns.size());
    mapReduceProgramRuns = getProgramRuns(secondMRId, FAILED.name());
    Assert.assertEquals(1, mapReduceProgramRuns.size());
}"
"@Test
public void extraSweepersGiveUpAfterFailingToAcquireEnoughTimes() throws InterruptedException {
    int shards = 16;
    int sweepers = 4;
    int threads = shards / (sweepers / 2);
    TimelockService stickyLockService = createStickyLockService();
    createAndInitializeSweepersAndWaitForOneBackgroundIteration(sweepers, shards, threads, stickyLockService);
    ArgumentCaptor<LockRequest> captor = ArgumentCaptor.forClass(LockRequest.class);
    verify(stickyLockService, atLeast(shards * (shards / threads + 1) / 2 + shards * (threads * sweepers - shards)));
    verify(stickyLockService, atMost(shards * ((threads + 1) * sweepers - shards) - sweepers * (sweepers - 1) / 2));
    Set<String> requestedLockIds = captor.getAllValues().stream()
    .map(LockRequest::getLockDescriptors)
    .map(Iterables::getOnlyElement)
    .map(LockDescriptor::getLockIdAsString)
    .collect(Collectors.toSet());
    Set<String> expectedLockIds = IntStream.range(0, shards).boxed()
    .map(ShardAndStrategy::conservative)
    .map(ShardAndStrategy::toText)
    .collect(Collectors.toSet());
    assertThat(requestedLockIds).hasSameElementsAs(expectedLockIds);
}"
"@Test
public void testMultiReaderIsAbleToSeekWithTimeOnMiddleOfTopic() throws Exception {
    final String topicName = ""persistent"";
    final int numOfMessage = 10;
    final int halfMessages = numOfMessage / 2;
    admin.topics().createPartitionedTopic(topicName, 3);
    Producer<byte[]> producer = pulsarClient.newProducer().topic(topicName).create();
    long l = System.currentTimeMillis();
    for (int i = 0; i < numOfMessage; i++) {
        producer.send(String.format(""msg num %d"", i).getBytes());
    }
    Reader<byte[]> reader = pulsarClient.newReader().topic(topicName).startMessageId(earliest).create();
    int plusTime = (halfMessages + 1) * 100;
    reader.seek(l + plusTime);
    Set<String> messageSet = Sets.newHashSet();
    for (int i = halfMessages + 1; i < numOfMessage; i++) {
        Message<byte[]> message = reader.readNext();
        String receivedMessage = new String(message.getData());
        Assert.assertTrue(messageSet.add(receivedMessage), ""Received duplicate message "" + receivedMessage);
    }
    reader.close();
    producer.close();
}"
"@Test
public void testAddLocation() throws Exception {
    DataSize maxResponseSize = DataSize.of(10, MEGABYTE);
    MockExchangeRequestProcessor processor = new MockExchangeRequestProcessor(maxResponseSize);
    TaskId task1 = new TaskId(new StageId(""query"", 1), 0, 0);
    TaskId task2 = new TaskId(new StageId(""query"", 1), 1, 0);
    TaskId task3 = new TaskId(new StageId(""query"", 1), 2, 0);
    URI location1 = URI.create(""http:www.example1.com"");
    URI location2 = URI.create(""http:www.example2.com"");
    URI location3 = URI.create(""http:www.example3.com"");
    processor.addPage(location1, createSerializedPage(1));
    processor.addPage(location1, createSerializedPage(2));
    TestingExchangeClientBuffer buffer = new TestingExchangeClientBuffer(DataSize.of(1, MEGABYTE));
    @SuppressWarnings(""resource"")
    ExchangeClient exchangeClient = new ExchangeClient(""localhost"", DataIntegrityVerification.ABORT, buffer, maxResponseSize, 1, new Duration(1, TimeUnit.MINUTES), true, new TestingHttpClient(processor, scheduler), scheduler, new SimpleLocalMemoryContext(newSimpleAggregatedMemoryContext(), ""test""), pageBufferClientCallbackExecutor, ( taskId, failure) -> {
    });
    assertThat(buffer.getAllTasks()).isEmpty();
    assertThat(buffer.getPages().asMap()).isEmpty();
    assertThat(buffer.getFinishedTasks()).isEmpty();
    assertThat(buffer.getFailedTasks().asMap()).isEmpty();
    assertFalse(buffer.isNoMoreTasks());
    exchangeClient.addLocation(task1, location1);
    assertThat(buffer.getAllTasks()).containsExactly(task1);
    assertTaskIsNotFinished(buffer, task1);
    processor.setComplete(location1);
    buffer.whenTaskFinished(task1).get(10, SECONDS);
    assertThat(buffer.getPages().get(task1)).hasSize(2);
    assertThat(buffer.getFinishedTasks()).containsExactly(task1);
    exchangeClient.addLocation(task2, location2);
    assertThat(buffer.getAllTasks()).containsExactlyInAnyOrder(task1, task2);
    assertTaskIsNotFinished(buffer, task2);
    processor.setComplete(location2);
    buffer.whenTaskFinished(task2).get(10, SECONDS);
    assertThat(buffer.getFinishedTasks()).containsExactlyInAnyOrder(task1, task2);
    assertThat(buffer.getPages().get(task2)).hasSize(0);
    exchangeClient.addLocation(task3, location3);
    assertThat(buffer.getAllTasks()).containsExactlyInAnyOrder(task1, task2, task3);
    assertTaskIsNotFinished(buffer, task3);
    exchangeClient.noMoreLocations();
    assertTrue(buffer.isNoMoreTasks());
    assertThat(buffer.getAllTasks()).containsExactlyInAnyOrder(task1, task2, task3);
    assertTaskIsNotFinished(buffer, task3);
    exchangeClient.close();
    assertEventually(() -> assertEquals(exchangeClient.getStatus().getPageBufferClientStatuses().get(0).getHttpRequestState(), ""not scheduled"", ""httpRequestState""));
    assertEventually(() -> assertEquals(exchangeClient.getStatus().getPageBufferClientStatuses().get(1).getHttpRequestState(), ""not scheduled"", ""httpRequestState""));
    assertEventually(() -> assertEquals(exchangeClient.getStatus().getPageBufferClientStatuses().get(2).getHttpRequestState(), ""not scheduled"", ""httpRequestState""));
    assertThat(buffer.getFinishedTasks()).containsExactlyInAnyOrder(task1, task2, task3);
    assertThat(buffer.getFailedTasks().asMap()).isEmpty();
    assertTrue(exchangeClient.isFinished());
}"
"@Test
public void testDelegationTokenSecretManager() throws Exception {
    DelegationTokenSecretManager dtSecretManager = cluster.getNameNode().getNamesystem().getDelegationTokenSecretManager();
    Token<DelegationTokenIdentifier> token = generateDelegationToken(""SomeUser"", ""JobTracker"");
    try {
        dtSecretManager.renewToken(token, ""FakeRenewer"");
        Assert.fail(""should have failed"");
    } catch (AccessControlException ace) {
    }
    dtSecretManager.renewToken(token, ""JobTracker"");
    DelegationTokenIdentifier identifier = new DelegationTokenIdentifier();
    byte[] tokenId = token.getIdentifier();
    identifier.readFields(new DataInputStream(new ByteArrayInputStream(tokenId)));
    Assert.assertTrue(null != dtSecretManager.retrievePassword(identifier));
    LOG.info(""Sleep to expire the token"");
    Thread.sleep(6000);
    try {
        dtSecretManager.retrievePassword(identifier);
        Assert.fail(""Token should have expired"");
    } catch (InvalidToken e) {
    }
    dtSecretManager.renewToken(token, ""JobTracker"");
    LOG.info(""Sleep beyond the max lifetime"");
    Thread.sleep(5000);
    try {
        dtSecretManager.renewToken(token, ""JobTracker"");
        Assert.fail(""should have been expired"");
    } catch (InvalidToken it) {
    }
}"
"@Test
public void testReadSkip() throws Exception {
    FileSystem fs = cluster.getFileSystem();
    long tStart = System.currentTimeMillis();
    bench.getConf().setLong(""test.io.skip.size"", 1);
    bench.randomReadTest(fs);
    long execTime = System.currentTimeMillis() - tStart;
    bench.analyzeResult(fs, TestType.TEST_TYPE_READ_SKIP, execTime);
}"
"@Test
public void testReacquireLocksAfterSessionLost() throws Exception {
    @Cleanup
    MetadataStoreExtended store = MetadataStoreExtended.create(zks.getConnectionString(), MetadataStoreConfig.builder().sessionTimeoutMillis(2000).build());
    BlockingQueue<SessionEvent> sessionEvents = new LinkedBlockingQueue<>();
    store.registerSessionListener(sessionEvents::add);
    @Cleanup
    CoordinationService coordinationService = new CoordinationServiceImpl(store);
    @Cleanup
    LockManager<String> lm1 = coordinationService.getLockManager(String.class);
    String path = newKey();
    ResourceLock<String> lock = lm1.acquireLock(path, ""value-1"").join();
    zks.expireSession(((ZKMetadataStore) (store)).getZkSessionId());
    SessionEvent e = sessionEvents.poll(5, TimeUnit.SECONDS);
    assertEquals(e, ConnectionLost);
    e = sessionEvents.poll(10, TimeUnit.SECONDS);
    assertEquals(e, SessionLost);
    e = sessionEvents.poll(10, TimeUnit.SECONDS);
    assertEquals(e, Reconnected);
    e = sessionEvents.poll(10, TimeUnit.SECONDS);
    assertEquals(e, SessionReestablished);
    Awaitility.await().untilAsserted(() -> {
        assertFalse(lock.getLockExpiredFuture().isDone());
    });
    assertTrue(store.get(path).join().isPresent());
}"
"@Test
public void serializeWithTruncateArrayTest() throws IOException {
    final ResponseContext ctx = ResponseContext.createEmpty();
    ctx.put(UNCOVERED_INTERVALS, Arrays.asList(interval(1), interval(2), interval(3), interval(4), interval(5), interval(6)));
    ctx.put(EXTN_STRING_KEY, Strings.repeat(""x"", INTERVAL_LEN * 7));
    final DefaultObjectMapper objectMapper = new DefaultObjectMapper();
    final String fullString = objectMapper.writeValueAsString(ctx.getDelegate());
    final ResponseContext.SerializationResult res1 = ctx.serializeWith(objectMapper, Integer.MAX_VALUE);
    Assert.assertEquals(fullString, res1.getResult());
    final int maxLen = ((((INTERVAL_LEN * 4) + UNCOVERED_INTERVALS.getName().length()) + 4) + TRUNCATED.getName().length()) + 6;
    final ResponseContext.SerializationResult res2 = ctx.serializeWith(objectMapper, maxLen);
    final ResponseContext ctxCopy = ResponseContext.createEmpty();
    ctxCopy.put(UNCOVERED_INTERVALS, Arrays.asList(interval(1), interval(2), interval(3)));
    ctxCopy.put(TRUNCATED, true);
    Assert.assertEquals(ctxCopy.getDelegate(), deserializeContext(res2.getResult(), objectMapper));
}"
"@Test
public void shouldTogglePrepareForBulkLoadDuringRestoreCalls() throws Exception {
    final List<KeyValue<byte[], byte[]>> entries = new ArrayList<>();
    entries.add(new KeyValue<>(""1"".getBytes(""UTF-8""), ""a"".getBytes(""UTF-8"")));
    entries.add(new KeyValue<>(""2"".getBytes(""UTF-8""), ""b"".getBytes(""UTF-8"")));
    entries.add(new KeyValue<>(""3"".getBytes(""UTF-8""), ""c"".getBytes(""UTF-8"")));
    final AtomicReference<Exception> conditionNotMet = new AtomicReference<>();
    final AtomicInteger conditionCheckCount = new AtomicInteger();
    Thread conditionCheckThread = new Thread(new Runnable() {
        @Override
        public void run() {
            assertRocksDBTurnsOnBulkLoading(conditionCheckCount, conditionNotMet);
            assertRockDBTurnsOffBulkLoad(conditionCheckCount, conditionNotMet);
        }
    });
    subject.init(context, subject);
    conditionCheckThread.start();
    context.restore(subject.name(), entries);
    conditionCheckThread.join(2000);
    assertTrue(conditionNotMet.get() == null);
    assertTrue(conditionCheckCount.get() == 2);
}"
"@Test
public void testConsumerBacklogEvictionTimeQuotaWithEmptyLedger() throws Exception {
    assertEquals(admin.namespaces().getBacklogQuotaMap(""prop/ns-quota""), Maps.newHashMap());
    admin.namespaces().setBacklogQuota(""prop/ns-quota"", BacklogQuota.builder().limitTime(TIME_TO_CHECK_BACKLOG_QUOTA).retentionPolicy(consumer_backlog_eviction).build(), message_age);
    PulsarClient client = PulsarClient.builder().serviceUrl(adminUrl.toString()).statsInterval(0, TimeUnit.SECONDS).build();
    final String topic = ""persistent"";
    final String subName = ""c1"";
    Consumer<byte[]> consumer = client.newConsumer().topic(topic).subscriptionName(subName).subscribe();
    Producer<byte[]> producer = createProducer(client, topic);
    producer.send(new byte[1024]);
    consumer.receive();
    admin.topics().unload(topic);
    PersistentTopicInternalStats internalStats = admin.topics().getInternalStats(topic);
    assertEquals(internalStats.ledgers.size(), 2);
    assertEquals(internalStats.ledgers.get(1).entries, 0);
    TopicStats stats = admin.topics().getStats(topic);
    assertEquals(stats.getSubscriptions().get(subName).getMsgBacklog(), 1);
    TimeUnit.SECONDS.sleep(TIME_TO_CHECK_BACKLOG_QUOTA);
    Awaitility.await().pollInterval(Duration.ofSeconds(1)).atMost(Duration.ofSeconds(TIME_TO_CHECK_BACKLOG_QUOTA)).untilAsserted(() -> {
        rolloverStats();
        PersistentTopicInternalStats latestInternalStats = admin.topics().getInternalStats(topic);
        assertEquals(latestInternalStats.ledgers.size(), 2);
        assertEquals(latestInternalStats.ledgers.get(1).entries, 0);
        TopicStats latestStats = admin.topics().getStats(topic);
        assertEquals(latestStats.getSubscriptions().get(subName).getMsgBacklog(), 0);
    });
    client.close();
}"
"@Test
public void testTopicLevelInActiveTopicApi() throws Exception {
    super.resetConfig();
    conf.setSystemTopicEnabled(true);
    conf.setTopicLevelPoliciesEnabled(true);
    super.baseSetup();
    Thread.sleep(2000);
    final String topicName = ""persistent://prop/ns-abc/testMaxInactiveDuration-"" + UUID.randomUUID().toString();
    admin.topics().createPartitionedTopic(topicName, 3);
    InactiveTopicPolicies inactiveTopicPolicies = admin.topics().getInactiveTopicPolicies(topicName);
    assertNull(inactiveTopicPolicies);
    InactiveTopicPolicies policies = new InactiveTopicPolicies();
    policies.setDeleteWhileInactive(true);
    policies.setInactiveTopicDeleteMode(InactiveTopicDeleteMode.delete_when_no_subscriptions);
    policies.setMaxInactiveDurationSeconds(10);
    admin.topics().setInactiveTopicPolicies(topicName, policies);
    for (int i = 0; i < 50; i++) {
        if (admin.topics().getInactiveTopicPolicies(topicName) != null) {
            break;
        }
        Thread.sleep(100);
    }
    assertEquals(admin.topics().getInactiveTopicPolicies(topicName), policies);
    admin.topics().removeInactiveTopicPolicies(topicName);
    for (int i = 0; i < 50; i++) {
        if (admin.topics().getInactiveTopicPolicies(topicName) == null) {
            break;
        }
        Thread.sleep(100);
    }
    assertNull(admin.topics().getInactiveTopicPolicies(topicName));
    super.internalCleanup();
}"
"@Test
public void testReadBackward() throws Exception {
    FileSystem fs = cluster.getFileSystem();
    long tStart = System.currentTimeMillis();
    bench.getConf().setLong(""test.io.skip.size"", -DEFAULT_BUFFER_SIZE);
    bench.randomReadTest(fs);
    long execTime = System.currentTimeMillis() - tStart;
    bench.analyzeResult(fs, TestType.TEST_TYPE_READ_BACKWARD, execTime);
}"
"@Test
public void testReadRandom() throws Exception {
    FileSystem fs = cluster.getFileSystem();
    long tStart = System.currentTimeMillis();
    bench.getConf().setLong(""test.io.skip.size"", 0);
    bench.randomReadTest(fs);
    long execTime = System.currentTimeMillis() - tStart;
    bench.analyzeResult(fs, TestType.TEST_TYPE_READ_RANDOM, execTime);
}"
"@Test
public void testWithRevisions() {
    Country de = new Country();
    de.code = ""de"";
    de.name = ""Deutschland"";
    countryRepository.save(de);
    de.name = ""Germany"";
    countryRepository.save(de);
    Revisions<Integer, Country> revisions = countryRepository.findRevisions(de.id);
    assertThat(revisions).hasSize(2);
    Iterator<Revision<Integer, Country>> iterator = revisions.iterator();
    Integer firstRevisionNumber = iterator.next().getRevisionNumber().get();
    Integer secondRevisionNumber = iterator.next().getRevisionNumber().get();
    assertThat(countryRepository.findRevision(de.id, firstRevisionNumber).get().getEntity().name)
    .isEqualTo(""Deutschland"");
    assertThat(countryRepository.findRevision(de.id, secondRevisionNumber).get().getEntity().name).isEqualTo(""Germany"");
}"
"@Test
public void testCollectorContextWithKeyword() throws Exception {
    ValidationResult validationResult = validate(""{\""test-property1\"":\""sample1\"",\""test-property2\"":\""sample2\""}"");
    Assertions.assertEquals(0, validationResult.getValidationMessages().size());
    List<String> contextValues = ((List<String>) (validationResult.getCollectorContext().get(SAMPLE_COLLECTOR)));
    Assertions.assertEquals(0, validationResult.getValidationMessages().size());
    Assertions.assertEquals(2, contextValues.size());
    Assertions.assertEquals(contextValues.get(0), ""actual_value_added_to_context1"");
    Assertions.assertEquals(contextValues.get(1), ""actual_value_added_to_context2"");
}"
"@Test
public void testEviction() throws Exception {
    final int CAPACITY = 3;
    PeerCache cache = PeerCache.getInstance(CAPACITY, 100000);
    DatanodeID dnIds[] = new DatanodeID[CAPACITY + 1];
    FakePeer peers[] = new FakePeer[CAPACITY + 1];
    for (int i = 0; i < dnIds.length; ++i) {
        dnIds[i] = new DatanodeID(""192.168.0.1"",
        ""fakehostname_"" + i, ""fake_storage_id_"" + i,
        100, 101, 102);
        peers[i] = new FakePeer(dnIds[i], false);
    }
    for (int i = 0; i < CAPACITY; ++i) {
        cache.put(dnIds[i], peers[i]);
    }
    assertEquals(CAPACITY, cache.size());
    cache.put(dnIds[CAPACITY], peers[CAPACITY]);
    assertEquals(CAPACITY, cache.size());
    assertSame(null, cache.get(dnIds[0], false));
    for (int i = 1; i < CAPACITY; ++i) {
        Peer peer = cache.get(dnIds[i], false);
        assertSame(peers[i], peer);
        assertTrue(!peer.isClosed());
        peer.close();
    }
    assertEquals(1, cache.size());
    cache.close();
}"
"@Test
public void testLogicalTypePreviewRun(Engine engine) throws Exception {
    PreviewManager previewManager = getPreviewManager();
    String sourceTableName = ""singleInput"";
    String sinkTableName = ""singleOutput"";
    Schema schema = Schema.recordOf(
    ""testRecord"",
    Schema.Field.of(""name"", Schema.of(Schema.Type.STRING)),
    Schema.Field.of(""date"", Schema.of(Schema.LogicalType.DATE)),
    Schema.Field.of(""ts"", Schema.of(Schema.LogicalType.TIMESTAMP_MILLIS))
    );
    ETLBatchConfig etlConfig = ETLBatchConfig.builder()
    .addStage(new ETLStage(""source"", MockSource.getPlugin(sourceTableName, schema)))
    .addStage(new ETLStage(""transform"", IdentityTransform.getPlugin()))
    .addStage(new ETLStage(""sink"", MockSink.getPlugin(sinkTableName)))
    .addConnection(""source"", ""transform"")
    .addConnection(""transform"", ""sink"")
    .setEngine(engine)
    .setNumOfRecordsPreview(100)
    .build();
    PreviewConfig previewConfig = new PreviewConfig(SmartWorkflow.NAME, ProgramType.WORKFLOW,
    Collections.<String, String>emptyMap(), 10);
    addDatasetInstance(Table.class.getName(), sourceTableName,
    DatasetProperties.of(ImmutableMap.of(""schema"", schema.toString())));
    DataSetManager<Table> inputManager = getDataset(NamespaceId.DEFAULT.dataset(sourceTableName));
    ZonedDateTime expectedMillis = ZonedDateTime.of(2018, 11, 11, 11, 11, 11, 123 * 1000 * 1000,
    ZoneId.ofOffset(""UTC"", ZoneOffset.UTC));
    StructuredRecord recordSamuel = StructuredRecord.builder(schema).set(""name"", ""samuel"")
    .setDate(""date"", LocalDate.of(2002, 11, 18)).setTimestamp(""ts"", expectedMillis).build();
    StructuredRecord recordBob = StructuredRecord.builder(schema).set(""name"", ""bob"")
    .setDate(""date"", LocalDate.of(2003, 11, 18)).setTimestamp(""ts"", expectedMillis).build();
    MockSource.writeInput(inputManager, ImmutableList.of(recordSamuel, recordBob));
    AppRequest<ETLBatchConfig> appRequest = new AppRequest<>(APP_ARTIFACT_RANGE, etlConfig, previewConfig);
    ApplicationId previewId = previewManager.start(NamespaceId.DEFAULT, appRequest);
    Tasks.waitFor(PreviewStatus.Status.COMPLETED, new Callable<PreviewStatus.Status>() {
        @Override
        public PreviewStatus.Status call() throws Exception {
            PreviewStatus status = previewManager.getStatus(previewId);
            return status == null ? null : status.getStatus();
        }
    }, 5, TimeUnit.MINUTES);
    checkPreviewStore(previewManager, previewId, ""source"", 2);
    List<JsonElement> data = previewManager.getData(previewId, ""source"").get(DATA_TRACER_PROPERTY);
    StructuredRecord actualRecordSamuel = GSON.fromJson(data.get(0), StructuredRecord.class);
    Assert.assertEquals(actualRecordSamuel.get(""date""), ""2002-11-18"");
    Assert.assertEquals(actualRecordSamuel.get(""ts""), ""2018-11-11T11:11:11.123Z[UTC]"");
    StructuredRecord actualRecordBob = GSON.fromJson(data.get(1), StructuredRecord.class);
    Assert.assertEquals(actualRecordBob.get(""date""), ""2003-11-18"");
    Assert.assertEquals(actualRecordBob.get(""ts""), ""2018-11-11T11:11:11.123Z[UTC]"");
    checkPreviewStore(previewManager, previewId, ""transform"", 2);
    checkPreviewStore(previewManager, previewId, ""sink"", 2);
    validateMetric(2, previewId, ""source.records.in"", previewManager);
    validateMetric(2, previewId, ""source.records.out"", previewManager);
    validateMetric(2, previewId, ""transform.records.in"", previewManager);
    validateMetric(2, previewId, ""transform.records.out"", previewManager);
    validateMetric(2, previewId, ""sink.records.out"", previewManager);
    validateMetric(2, previewId, ""sink.records.in"", previewManager);
    DataSetManager<Table> sinkManager = getDataset(sinkTableName);
    Assert.assertNull(sinkManager.get());
    deleteDatasetInstance(NamespaceId.DEFAULT.dataset(sourceTableName));
    Assert.assertNotNull(previewManager.getRunId(previewId));
}"
"@Test
public void testHftpDefaultPorts() throws IOException {
    resetFileSystem();
    Configuration conf = new Configuration();
    URI uri = URI.create();
    HftpFileSystem fs = ((HftpFileSystem) (FileSystem.get(uri, conf)));
    assertEquals(DFS_NAMENODE_HTTP_PORT_DEFAULT, fs.getDefaultPort());
    assertEquals(DFS_NAMENODE_HTTPS_PORT_DEFAULT, fs.getDefaultSecurePort());
    assertEquals(uri, fs.getUri());
    assertEquals(""127.0.0.1:"" + DFSConfigKeys.DFS_NAMENODE_HTTPS_PORT_DEFAULT, fs.getCanonicalServiceName());
}"
"@Test
public void testFlowNoConflictsWithClients() throws Exception {
    startComputation(0, stopFlag0);
    if (!tcpDiscovery())
    return;
    startComputation(1, stopFlag1);
    startComputation(2, stopFlag2);
    startComputation(3, stopFlag3);
    startComputation(4, stopFlag4);
    final Set<Integer> deafClientObservedIds = new ConcurrentHashSet<>();
    startListening(5, true, deafClientObservedIds);
    final Set<Integer> regClientObservedIds = new ConcurrentHashSet<>();
    startListening(6, false, regClientObservedIds);
    START_LATCH.countDown();
    Thread killer = new Thread(new ServerNodeKiller());
    Thread resurrection = new Thread(new ServerNodeResurrection());
    killer.setName(""node-killer-thread"");
    killer.start();
    resurrection.setName(""node-resurrection-thread"");
    resurrection.start();
    while (!updatesQueue.isEmpty())
    Thread.sleep(1000);
    killer.interrupt();
    resurrection.interrupt();
}"
"@Test
public void incrementUpperLimitIfOneMinuteElapsedSinceLastUpdate() throws InterruptedException {
    Clock clock = mock(Clock.class);
    when(clock.getTimeMillis()).thenReturn(0L, TWO_MINUTES_IN_MILLIS, 2 * TWO_MINUTES_IN_MILLIS, 3 * TWO_MINUTES_IN_MILLIS);
    TimestampBoundStore timestampBoundStore = initialTimestampBoundStore();
    PersistentTimestampService persistentTimestampService = PersistentTimestampService.create(timestampBoundStore, clock);
    persistentTimestampService.getFreshTimestamp();
    Thread.sleep(10);
    persistentTimestampService.getFreshTimestamp();
    Thread.sleep(10);
    verify(timestampBoundStore, atLeast(2)).storeUpperLimit(anyLong());
}"
"@Test
public void testExpiredRequestAllocationOnAnyHost() throws Exception {
    MockClusterResourceManager spyManager = spy(new MockClusterResourceManager(callback, state));
    ContainerManager spyContainerManager = spy(new ContainerManager(containerPlacementMetadataStore, state, spyManager, true, false, mock(LocalityManager.class), faultDomainManager, config));
    spyAllocator = Mockito.spy(new ContainerAllocator(spyManager, config, state, true, spyContainerManager));
    spyAllocator.requestResources(new HashMap<String, String>() {
        {
            put(""0"", ""hostname-0"");
            put(""1"", ""hostname-1"");
        }
    });
    spyAllocatorThread = new Thread(spyAllocator);
    spyAllocatorThread.start();
    Thread.sleep(1000);
    assertTrue(state.preferredHostRequests.get() == 2);
    assertTrue(state.expiredPreferredHostRequests.get() == 2);
    verify(spyContainerManager, times(1)).handleExpiredRequest(eq(""0""), eq(""hostname-0""), any(SamzaResourceRequest.class), any(ContainerAllocator.class), any(ResourceRequestState.class));
    verify(spyContainerManager, times(1)).handleExpiredRequest(eq(""1""), eq(""hostname-1""), any(SamzaResourceRequest.class), any(ContainerAllocator.class), any(ResourceRequestState.class));
    ArgumentCaptor<SamzaResourceRequest> cancelledRequestCaptor = ArgumentCaptor.forClass(SamzaResourceRequest.class);
    verify(spyManager, atLeast(2)).cancelResourceRequest(cancelledRequestCaptor.capture());
    assertTrue(cancelledRequestCaptor.getAllValues().stream().map(( resourceRequest) -> resourceRequest.getPreferredHost()).collect(Collectors.toSet()).size() > 2);
    assertTrue(state.matchedResourceRequests.get() == 0);
    assertTrue(state.anyHostRequests.get() > 2);
    spyAllocator.stop();
}"
"@Test
public void testTokenExpiry() throws Exception {
    ClockMock clock = ClockMock.frozen();
    TokenService tokenService = createTokenService(tokenServiceEnabledSettings, clock);
    Authentication authentication = new Authentication(new User(""joe"", ""admin""), new RealmRef(""native_realm"", ""native"", ""node1""), null);
    PlainActionFuture<Tuple<UserToken, String>> tokenFuture = new PlainActionFuture<>();
    tokenService.createUserToken(authentication, authentication, tokenFuture, Collections.emptyMap(), true);
    final UserToken token = tokenFuture.get().v1();
    mockGetTokenFromId(token);
    mockCheckTokenInvalidationFromId(token);
    authentication = token.getAuthentication();
    ThreadContext requestContext = new ThreadContext(Settings.EMPTY);
    storeTokenHeader(requestContext, tokenService.getUserTokenString(token));
    try (ThreadContext.StoredContext ignore = requestContext.newStoredContext(true)) {
        PlainActionFuture<UserToken> future = new PlainActionFuture<>();
        tokenService.getAndValidateToken(requestContext, future);
        assertAuthenticationEquals(authentication, future.get().getAuthentication());
    }
    final TimeValue defaultExpiration = TokenService.TOKEN_EXPIRATION.get(Settings.EMPTY);
    final int fastForwardAmount = randomIntBetween(1, Math.toIntExact(defaultExpiration.getSeconds()) - 5);
    try (ThreadContext.StoredContext ignore = requestContext.newStoredContext(true)) {
        clock.fastForwardSeconds(Math.toIntExact(defaultExpiration.getSeconds()) - fastForwardAmount);
        clock.rewind(TimeValue.timeValueNanos(clock.instant().getNano()));
        PlainActionFuture<UserToken> future = new PlainActionFuture<>();
        tokenService.getAndValidateToken(requestContext, future);
        assertAuthenticationEquals(authentication, future.get().getAuthentication());
    }
    assertSettingDeprecationsAndWarnings(new Setting[] { TokenService.BWC_ENABLED });
}"
"@Test
public void testUnqualifiedUriContents() throws Exception {
    dirString = ""d1"";
    item = new PathData(dirString, conf);
    PathData[] items = item.getDirectoryContents();
    assertEquals(sortedString(""d1/f1"", ""d1/f1.1"", ""d1/f2""), sortedString(items));
}"
"@Test
public void testTransactionMetaStoreAssignAndFailover() throws IOException, InterruptedException {
    int transactionMetaStoreCount = 0;
    for (PulsarService pulsarService : pulsarServices) {
        transactionMetaStoreCount += pulsarService.getTransactionMetadataStoreService().getStores().size();
    }
    Assert.assertEquals(transactionMetaStoreCount, 16);
    PulsarService crashedMetaStore = null;
    for (int i = pulsarServices.length - 1; i >= 0; i--) {
        if (pulsarServices[i].getTransactionMetadataStoreService().getStores().size() > 0) {
            crashedMetaStore = pulsarServices[i];
            break;
        }
    }
    Assert.assertNotNull(crashedMetaStore);
    List<PulsarService> services = new ArrayList<>(pulsarServices.length - 1);
    for (PulsarService pulsarService : pulsarServices) {
        if (pulsarService != crashedMetaStore) {
            services.add(pulsarService);
        }
    }
    pulsarServices = new PulsarService[pulsarServices.length - 1];
    for (int i = 0; i < services.size(); i++) {
        pulsarServices[i] = services.get(i);
    }
    crashedMetaStore.close();
    Thread.sleep(3000);
    transactionMetaStoreCount = 0;
    for (PulsarService pulsarService : pulsarServices) {
        transactionMetaStoreCount += pulsarService.getTransactionMetadataStoreService().getStores().size();
    }
    Assert.assertEquals(transactionMetaStoreCount, 16);
    transactionCoordinatorClient.close();
}"
"@Test
public void testRemoveFirstConsumer() throws Exception {
    this.conf.setSubscriptionKeySharedEnable(true);
    String topic = ""testReadAheadWhenAddingConsumers-"" + UUID.randomUUID();
    @Cleanup
    Producer<Integer> producer = createProducer(topic, false);
    @Cleanup
    Consumer<Integer> c1 = pulsarClient.newConsumer(INT32).topic(topic).subscriptionName(""key_shared"").subscriptionType(Key_Shared).receiverQueueSize(10).consumerName(""c1"").subscribe();
    for (int i = 0; i < 10; i++) {
        producer.newMessage().key(String.valueOf(random.nextInt(NUMBER_OF_KEYS))).value(i).send();
    }
    @Cleanup
    Consumer<Integer> c2 = pulsarClient.newConsumer(INT32).topic(topic).subscriptionName(""key_shared"").subscriptionType(Key_Shared).receiverQueueSize(10).consumerName(""c2"").subscribe();
    for (int i = 10; i < 20; i++) {
        producer.newMessage().key(String.valueOf(random.nextInt(NUMBER_OF_KEYS))).value(i).send();
    }
    assertNull(c2.receive(100, TimeUnit.MILLISECONDS));
    c1.close();
    for (int i = 0; i < 20; i++) {
        Message<Integer> msg = c2.receive();
        assertEquals(msg.getValue().intValue(), i);
        c2.acknowledge(msg);
    }
}"
"@Test
public void testSimpleConsumerEventsWithoutPartition() throws Exception {
    final String topicName = ""persistent"";
    final String subName = ""sub1"";
    final int numMsgs = 100;
    TestConsumerStateEventListener listener1 = new TestConsumerStateEventListener();
    TestConsumerStateEventListener listener2 = new TestConsumerStateEventListener();
    ConsumerBuilder<byte[]> consumerBuilder = pulsarClient.newConsumer().topic(topicName).subscriptionName(subName).acknowledgmentGroupTime(0, TimeUnit.SECONDS).subscriptionType(Failover);
    ConsumerBuilder<byte[]> consumerBulder1 = consumerBuilder.clone().consumerName(""1"").consumerEventListener(listener1).acknowledgmentGroupTime(0, TimeUnit.SECONDS);
    Consumer<byte[]> consumer1 = consumerBulder1.subscribe();
    Consumer<byte[]> consumer2 = consumerBuilder.clone().consumerName(""2"").consumerEventListener(listener2).subscribe();
    verifyConsumerActive(listener1, -1);
    verifyConsumerInactive(listener2, -1);
    PersistentTopic topicRef = ((PersistentTopic) (pulsar.getBrokerService().getTopicReference(topicName).get()));
    PersistentSubscription subRef = topicRef.getSubscription(subName);
    assertNotNull(topicRef);
    assertNotNull(subRef);
    assertTrue(subRef.getDispatcher().isConsumerConnected());
    assertEquals(subRef.getDispatcher().getType(), Failover);
    List<CompletableFuture<MessageId>> futures = Lists.newArrayListWithCapacity(numMsgs);
    Producer<byte[]> producer = pulsarClient.newProducer().topic(topicName).enableBatching(false).messageRoutingMode(SinglePartition).create();
    for (int i = 0; i < numMsgs; i++) {
        String message = ""my-message-"" + i;
        futures.add(producer.sendAsync(message.getBytes()));
    }
    FutureUtil.waitForAll(futures).get();
    futures.clear();
    rolloverPerIntervalStats();
    assertEquals(subRef.getNumberOfEntriesInBacklog(), numMsgs);
    Thread.sleep(ASYNC_EVENT_COMPLETION_WAIT);
    Message<byte[]> msg = null;
    Assert.assertNull(consumer2.receive(1, TimeUnit.SECONDS));
    for (int i = 0; i < numMsgs; i++) {
        msg = consumer1.receive(1, TimeUnit.SECONDS);
        Assert.assertNotNull(msg);
        Assert.assertEquals(new String(msg.getData()), ""my-message-"" + i);
        consumer1.acknowledge(msg);
    }
    rolloverPerIntervalStats();
    Thread.sleep(ASYNC_EVENT_COMPLETION_WAIT);
    assertEquals(subRef.getNumberOfEntriesInBacklog(), 0);
    for (int i = 0; i < numMsgs; i++) {
        String message = ""my-message-"" + i;
        futures.add(producer.sendAsync(message.getBytes()));
    }
    FutureUtil.waitForAll(futures).get();
    futures.clear();
    for (int i = 0; i < 5; i++) {
        msg = consumer1.receive(1, TimeUnit.SECONDS);
        Assert.assertNotNull(msg);
        Assert.assertEquals(new String(msg.getData()), ""my-message-"" + i);
        consumer1.acknowledge(msg);
    }
    for (int i = 5; i < 10; i++) {
        msg = consumer1.receive(1, TimeUnit.SECONDS);
        Assert.assertNotNull(msg);
        Assert.assertEquals(new String(msg.getData()), ""my-message-"" + i);
    }
    consumer1.close();
    Thread.sleep(CONSUMER_ADD_OR_REMOVE_WAIT_TIME);
    verifyConsumerActive(listener2, -1);
    verifyConsumerNotReceiveAnyStateChanges(listener1);
    for (int i = 5; i < numMsgs; i++) {
        msg = consumer2.receive(1, TimeUnit.SECONDS);
        Assert.assertNotNull(msg);
        Assert.assertEquals(new String(msg.getData()), ""my-message-"" + i);
        consumer2.acknowledge(msg);
    }
    Assert.assertNull(consumer2.receive(1, TimeUnit.SECONDS));
    rolloverPerIntervalStats();
    Thread.sleep(ASYNC_EVENT_COMPLETION_WAIT);
    assertEquals(subRef.getNumberOfEntriesInBacklog(), 0);
    for (int i = 0; i < numMsgs; i++) {
        String message = ""my-message-"" + i;
        futures.add(producer.sendAsync(message.getBytes()));
    }
    FutureUtil.waitForAll(futures).get();
    futures.clear();
    for (int i = 0; i < 5; i++) {
        msg = consumer2.receive(1, TimeUnit.SECONDS);
        Assert.assertNotNull(msg);
        Assert.assertEquals(new String(msg.getData()), ""my-message-"" + i);
        consumer2.acknowledge(msg);
    }
    consumer1 = consumerBulder1.subscribe();
    Thread.sleep(CONSUMER_ADD_OR_REMOVE_WAIT_TIME);
    for (int i = 5; i < numMsgs; i++) {
        msg = consumer1.receive(1, TimeUnit.SECONDS);
        Assert.assertNotNull(msg);
        Assert.assertEquals(new String(msg.getData()), ""my-message-"" + i);
        consumer1.acknowledge(msg);
    }
    Assert.assertNull(consumer1.receive(1, TimeUnit.SECONDS));
    rolloverPerIntervalStats();
    Thread.sleep(ASYNC_EVENT_COMPLETION_WAIT);
    assertEquals(subRef.getNumberOfEntriesInBacklog(), 0);
    for (int i = 0; i < numMsgs; i++) {
        String message = ""my-message-"" + i;
        futures.add(producer.sendAsync(message.getBytes()));
    }
    FutureUtil.waitForAll(futures).get();
    futures.clear();
    for (int i = 0; i < 5; i++) {
        msg = consumer1.receive(1, TimeUnit.SECONDS);
        Assert.assertNotNull(msg);
        Assert.assertEquals(new String(msg.getData()), ""my-message-"" + i);
        consumer1.acknowledge(msg);
    }
    TestConsumerStateEventListener listener3 = new TestConsumerStateEventListener();
    Consumer<byte[]> consumer3 = consumerBuilder.clone().consumerName(""3"").consumerEventListener(listener3).subscribe();
    Thread.sleep(CONSUMER_ADD_OR_REMOVE_WAIT_TIME);
    verifyConsumerInactive(listener3, -1);
    Assert.assertNull(consumer3.receive(1, TimeUnit.SECONDS));
    for (int i = 5; i < numMsgs; i++) {
        msg = consumer1.receive(1, TimeUnit.SECONDS);
        Assert.assertNotNull(msg);
        Assert.assertEquals(new String(msg.getData()), ""my-message-"" + i);
        consumer1.acknowledge(msg);
    }
    rolloverPerIntervalStats();
    Thread.sleep(ASYNC_EVENT_COMPLETION_WAIT);
    assertEquals(subRef.getNumberOfEntriesInBacklog(), 0);
    try {
        consumer1.unsubscribe();
        fail(""should fail"");
    } catch (PulsarClientException e) {
    }
    consumer1.close();
    Thread.sleep(CONSUMER_ADD_OR_REMOVE_WAIT_TIME);
    consumer2.close();
    Thread.sleep(CONSUMER_ADD_OR_REMOVE_WAIT_TIME);
    try {
        consumer3.unsubscribe();
    } catch (PulsarClientException e) {
        fail(""Should not fail"", e);
    }
    Thread.sleep(ASYNC_EVENT_COMPLETION_WAIT);
    subRef = topicRef.getSubscription(subName);
    assertNull(subRef);
    producer.close();
    consumer3.close();
    admin.topics().delete(topicName);
}"
"@Test
public void testGeneratedBlock() throws Exception {
    LOG.info(""Test testGeneratedBlock started."");
    long blockSize = 8192L;
    int stripeLength = 3;
    mySetup(stripeLength, -1);
    Path file1 = new Path(""/user/dhruba/raidtest/file1"");
    Path destPath = new Path(""/destraid/user/dhruba/raidtest"");
    long crc1 = TestRaidDfs.createTestFile(fileSys, file1, 1, 7, blockSize);
    long file1Len = fileSys.getFileStatus(file1).getLen();
    LOG.info(""Test testGeneratedBlock created test files"");
    Configuration localConf = new Configuration(conf);
    localConf.set(RAID_LOCATION_KEY, ""/destraid"");
    localConf.setInt(""raid.blockfix.interval"", 1000);
    localConf.setLong(""raid.blockfix.filespertask"", 2L);
    try {
        cnode = RaidNode.createRaidNode(null, localConf);
        TestRaidDfs.waitForFileRaided(LOG, fileSys, file1, destPath);
        cnode.stop();
        cnode.join();
        FileStatus srcStat = fileSys.getFileStatus(file1);
        DistributedFileSystem dfs = ((DistributedFileSystem) (fileSys));
        LocatedBlocks locs = RaidDFSUtil.getBlockLocations(dfs, file1.toUri().getPath(), 0, srcStat.getLen());
        String[] corruptFiles = RaidDFSUtil.getCorruptFiles(conf);
        assertEquals(corruptFiles.length, 0);
        assertEquals(0, cnode.blockFixer.filesFixed());
        corruptBlock(locs.get(0).getBlock().getBlockName());
        reportCorruptBlocks(dfs, file1, new int[]{ 0 }, blockSize);
        corruptFiles = RaidDFSUtil.getCorruptFiles(conf);
        assertEquals(corruptFiles.length, 1);
        assertEquals(corruptFiles[0], file1.toUri().getPath());
        cnode = RaidNode.createRaidNode(null, localConf);
        long start = System.currentTimeMillis();
        while ((cnode.blockFixer.filesFixed() < 1) && ((System.currentTimeMillis() - start) < 120000)) {
            LOG.info(""Test testGeneratedBlock waiting for files to be fixed."");
            Thread.sleep(1000);
        }
        assertEquals(1, cnode.blockFixer.filesFixed());
        cnode.stop();
        cnode.join();
        cnode = null;
        dfs = getDFS(conf, dfs);
        assertTrue(TestRaidDfs.validateFile(dfs, file1, file1Len, crc1));
        locs = RaidDFSUtil.getBlockLocations(dfs, file1.toUri().getPath(), 0, srcStat.getLen());
        corruptBlock(locs.get(0).getBlock().getBlockName());
        reportCorruptBlocks(dfs, file1, new int[]{ 0 }, blockSize);
        try {
            Thread.sleep(5 * 1000);
        } catch (InterruptedException ignore) {
        }
        try {
            TestRaidDfs.validateFile(dfs, file1, file1Len, crc1);
            fail(""Expected exception not thrown"");
        } catch (ChecksumException ce) {
        } catch (BlockMissingException bme) {
        }
    } catch (Exception e) {
        LOG.info((""Test testGeneratedBlock Exception "" + e) + StringUtils.stringifyException(e));
        throw e;
    } finally {
        myTearDown();
    }
    LOG.info(""Test testGeneratedBlock completed."");
}"
"@Test
public void testQueryTimeout()
throws Exception
{
    try (Connection connection = createConnection(""blackhole"", ""blackhole"");
    Statement statement = connection.createStatement()) {
        statement.executeUpdate(""CREATE TABLE test_query_timeout (key BIGINT) "" +
        ""WITH ("" +
        ""   split_count = 1, "" +
        ""   pages_per_split = 1, "" +
        ""   rows_per_page = 1, "" +
        ""   page_processing_delay = '1m'"" +
        "")"");
    }
    CountDownLatch queryFinished = new CountDownLatch(1);
    AtomicReference<Throwable> queryFailure = new AtomicReference<>();
    executorService.submit(() -> {
        try (Connection connection = createConnection(""blackhole"", ""default"");
        Statement statement = connection.createStatement()) {
            statement.setQueryTimeout(1);
            try (ResultSet resultSet = statement.executeQuery(""SELECT * FROM test_query_timeout"")) {
                try {
                    resultSet.next();
                }
                catch (SQLException t) {
                    queryFailure.set(t);
                }
                finally {
                    queryFinished.countDown();
                }
            }
        }
        return null;
    });
    assertTrue(queryFinished.await(2, SECONDS));
    assertNotNull(queryFailure.get());
    assertContains(queryFailure.get().getMessage(), ""Query exceeded maximum time limit of 1.00s"");
    try (Connection connection = createConnection(""blackhole"", ""blackhole"");
    Statement statement = connection.createStatement()) {
        statement.executeUpdate(""DROP TABLE test_query_timeout"");
    }
}"
"@Test
public void testStartProgramWithDisabledRuntimeArgs() throws Exception {
    ProfileId profileId = new NamespaceId(TEST_NAMESPACE1).profile(""MyProfile"");
    Profile profile = new Profile(""MyProfile"", Profile.NATIVE.getLabel(), Profile.NATIVE.getDescription(),Profile.NATIVE.getScope(), Profile.NATIVE.getProvisioner());
    putProfile(profileId, profile, 200);
    disableProfile(profileId, 200);
    deploy(AppWithWorkflow.class, 200, Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE1);
    ProgramId programId = new NamespaceId(TEST_NAMESPACE1).app(APP_WITH_WORKFLOW_APP_ID).workflow(APP_WITH_WORKFLOW_WORKFLOW_NAME);
    Assert.assertEquals(STOPPED, getProgramStatus(programId));
    startProgram(programId, Collections.singletonMap(SystemArguments.PROFILE_NAME, profileId.getScopedName()), 409);
    Assert.assertEquals(STOPPED, getProgramStatus(programId));
    startProgram(programId, Collections.singletonMap(SystemArguments.PROFILE_NAME, ProfileId.NATIVE.getScopedName()),200);
    waitState(programId, STOPPED);
}"
"@Test
public void testConnectedComponents() {
    System.out.println(""graph is "" + graph.toString());
    List<Set<Integer>> ccs = graph.getConnectedComponents();
    for (Set<Integer> cc : ccs) {
        System.out.println(""Connected component: "" + cc);
    }
    assertEquals(ccs.size(), 4);
    assertEquals(CollectionUtils.sorted(ccs.get(0)), Arrays.asList(1, 2, 3, 4));
}"
"@Test
public void testVersion2ClientVersion2Server() throws Exception {
    ProtocolSignature.resetCache();
    TestImpl2 impl = new TestImpl2();
    server = new RPC.Builder(conf).setProtocol(TestProtocol2.class).setInstance(impl).setBindAddress(ADDRESS).setPort(0).setNumHandlers(2).setVerbose(false).build();
    server.addProtocol(RPC_WRITABLE, TestProtocol0.class, impl);
    server.start();
    addr = NetUtils.getConnectAddress(server);
    Version2Client client = new Version2Client();
    client.ping();
    assertEquals(""hello"", client.echo(""hello""));
    assertEquals(-3, client.echo(3));
}"
"@Test
public void testDelegationTokenWithRealUser() throws IOException {
    UserGroupInformation ugi = UserGroupInformation.createRemoteUser(REAL_USER);
    final UserGroupInformation proxyUgi = UserGroupInformation.createProxyUserForTesting(PROXY_USER, ugi, GROUP_NAMES);
    try {
        Token<?>[] tokens = proxyUgi.doAs(new PrivilegedExceptionAction<Token<?>[]>() {
            @Override
            public Token<?>[] run() throws IOException {
                return cluster.getFileSystem().addDelegationTokens(""RenewerUser"", null);
            }
        });
        DelegationTokenIdentifier identifier = new DelegationTokenIdentifier();
        byte[] tokenId = tokens[0].getIdentifier();
        identifier.readFields(new DataInputStream(new ByteArrayInputStream(tokenId)));
        Assert.assertEquals(identifier.getUser().getUserName(), PROXY_USER);
        Assert.assertEquals(identifier.getUser().getRealUser().getUserName(), REAL_USER);
    } catch (InterruptedException e) {
    }
}"
"@Test
public void testDuplicateConcurrentSubscribeCommand() throws Exception {
    resetChannel();
    setChannelConnected();
    CompletableFuture<Topic> delayFuture = new CompletableFuture<>();
    doReturn(delayFuture).when(brokerService).getOrCreateTopic(any(String.class));
    ByteBuf clientCommand =
    Commands.newSubscribe(successTopicName, successSubName, 1, 1, Exclusive, 0, ""test"", 0);
    channel.writeInbound(clientCommand);
    clientCommand =
    Commands.newSubscribe(successTopicName, successSubName, 1, 1, Exclusive, 0, ""test"", 0);
    channel.writeInbound(clientCommand);
    Object response = getResponse();
    assertTrue(response instanceof CommandError, ""Response is not CommandError but "" + response);
    CommandError error = ((CommandError) (response));
    assertEquals(error.getError(), ServiceNotReady);
    channel.finish();
}"
"@Test
public void testWithStringAndConfForBuggyPath() throws Exception {
    dirString = ""file"" ;
    testDir = new Path(dirString);
    item = new PathData(dirString, conf);
    assertEquals(""file:/tmp"", testDir.toString());
    checkPathData();
}"
"@Test
public void testModTime() throws IOException {
    Configuration conf = new Configuration();
    MiniDFSCluster cluster = new MiniDFSCluster(conf, numDatanodes, true, null);
    cluster.waitActive();
    InetSocketAddress addr = new InetSocketAddress(""localhost"", cluster.getNameNodePort());
    DFSClient client = new DFSClient(addr, conf);
    DatanodeInfo[] info = client.datanodeReport(LIVE);
    assertEquals(""Number of Datanodes "", numDatanodes, info.length);
    FileSystem fileSys = cluster.getFileSystem();
    int replicas = numDatanodes - 1;
    assertTrue(fileSys instanceof DistributedFileSystem);
    try {
        System.out.println(""Creating testdir1 and testdir1/test1.dat."");
        Path dir1 = new Path(""testdir1"");
        Path file1 = new Path(dir1, ""test1.dat"");
        writeFile(fileSys, file1, replicas);
        FileStatus stat = fileSys.getFileStatus(file1);
        long mtime1 = stat.getModificationTime();
        assertTrue(mtime1 != 0);
        stat = fileSys.getFileStatus(dir1);
        long mdir1 = stat.getModificationTime();
        System.out.println(""Creating testdir1/test2.dat."");
        Path file2 = new Path(dir1, ""test2.dat"");
        writeFile(fileSys, file2, replicas);
        stat = fileSys.getFileStatus(file2);
        stat = fileSys.getFileStatus(dir1);
        assertTrue(stat.getModificationTime() >= mdir1);
        mdir1 = stat.getModificationTime();
        Path dir2 = new Path(""testdir2/"").makeQualified(fileSys);
        System.out.println(""Creating testdir2 "" + dir2);
        assertTrue(fileSys.mkdirs(dir2));
        stat = fileSys.getFileStatus(dir2);
        long mdir2 = stat.getModificationTime();
        Path newfile = new Path(dir2, ""testnew.dat"");
        System.out.println(((""Moving "" + file1) + "" to "") + newfile);
        fileSys.rename(file1, newfile);
        stat = fileSys.getFileStatus(newfile);
        assertTrue(stat.getModificationTime() == mtime1);
        stat = fileSys.getFileStatus(dir1);
        assertTrue(stat.getModificationTime() != mdir1);
        mdir1 = stat.getModificationTime();
        stat = fileSys.getFileStatus(dir2);
        assertTrue(stat.getModificationTime() != mdir2);
        mdir2 = stat.getModificationTime();
        System.out.println(""Deleting testdir2/testnew.dat."");
        assertTrue(fileSys.delete(newfile, true));
        stat = fileSys.getFileStatus(dir1);
        assertTrue(stat.getModificationTime() == mdir1);
        stat = fileSys.getFileStatus(dir2);
        assertTrue(stat.getModificationTime() != mdir2);
        mdir2 = stat.getModificationTime();
        cleanupFile(fileSys, file2);
        cleanupFile(fileSys, dir1);
        cleanupFile(fileSys, dir2);
    } catch (IOException e) {
        info = client.datanodeReport(ALL);
        printDatanodeReport(info);
        throw e;
    } finally {
        fileSys.close();
        cluster.shutdown();
    }
}"
"@Test
public void testWithDirStringAndConf() throws Exception {
    dirString = ""d1"";
    item = new PathData(dirString, conf);
    checkPathData();
    dirString = ""d1/"";
    item = new PathData(dirString, conf);
    checkPathData();
}"
"@Test
void nullAndObjectValuesInMap() {
    Map<String, Object> queryParams = new HashMap<>();
    queryParams.put(""foo"", null);
    queryParams.put(""baz"", ""qux"");
    Unirest.get(GET).queryString(queryParams).asObject(RequestCapture.class).getBody().assertParam(""foo"", """").assertParam(""baz"", ""qux"").assertQueryString(""foo&baz=qux"");
}"
"@Test
public class Test {
    public void testPendingAndInvalidate() throws Exception {
        final Configuration CONF = new HdfsConfiguration();
        MiniDFSCluster cluster = new MiniDFSCluster.Builder(CONF).numDataNodes(DATANODE_COUNT).build();
        cluster.waitActive();
        FSNamesystem namesystem = cluster.getNamesystem();
        BlockManager bm = namesystem.getBlockManager();
        DistributedFileSystem fs = cluster.getFileSystem();
        try {
            Path filePath = new Path(""/tmp.txt"");
            DFSTestUtil.createFile(fs, filePath, 1024, (short) 3, 0L);
            for (DataNode dn : cluster.getDataNodes()) {
                DataNodeTestUtils.setHeartbeatsDisabledForTests(dn, true);
            }
            LocatedBlock block = NameNodeAdapter.getBlockLocations(
            cluster.getNameNode(), filePath.toString(), 0, 1).get(0);
            cluster.getNamesystem().writeLock();
            try {
                bm.findAndMarkBlockAsCorrupt(block.getBlock(), block.getLocations()[0],
                ""STORAGE_ID"", ""TEST"");
            } finally {
                cluster.getNamesystem().writeUnlock();
            }
            BlockManagerTestUtil.computeAllPendingWork(bm);
            BlockManagerTestUtil.updateState(bm);
            assertEquals(bm.getPendingReconstructionBlocksCount(), 1L);
            BlockInfo storedBlock = bm.getStoredBlock(block.getBlock().getLocalBlock());
            assertEquals(bm.pendingReconstruction.getNumReplicas(storedBlock), 2);
            fs.delete(filePath, true);
            int retries = 10;
            long pendingNum = bm.getPendingReconstructionBlocksCount();
            while (pendingNum != 0 && retries-- > 0) {
                Thread.sleep(1000);
                BlockManagerTestUtil.updateState(bm);
                pendingNum = bm.getPendingReconstructionBlocksCount();
            }
            assertEquals(pendingNum, 0L);
        } finally {
            cluster.shutdown();
        }
    }
}"
"@Test
public void testWorkflowClient() throws Exception {
    String outputPath = new File(tmpFolder.newFolder(), ""output"").getAbsolutePath();
    Map<String, String> runtimeArgs = ImmutableMap.of(""inputPath"", createInput(""input""),
    ""outputPath"", outputPath);
    Id.Workflow workflowId = Id.Workflow.from(appId, AppWithWorkflow.SampleWorkflow.NAME);
    programClient.start(workflowId, false, runtimeArgs);
    programClient.waitForStatus(workflowId, ""STOPPED"", 60, TimeUnit.SECONDS);
    List<RunRecord> workflowRuns = programClient.getProgramRuns(workflowId, ProgramRunStatus.COMPLETED.name(), 0,
    Long.MAX_VALUE, 10);
    Assert.assertEquals(1, workflowRuns.size());
    Id.Run workflowRunId = new Id.Run(workflowId, workflowRuns.get(0).getPid());
    try {
        workflowClient.getWorkflowToken(new Id.Run(Id.Workflow.from(appId, ""random""), workflowRunId.getId()));
        Assert.fail(""Should not find a workflow token for a non-existing workflow"");
    } catch (NotFoundException expected) {
    }
    try {
        workflowClient.getWorkflowToken(new Id.Run(workflowId, RunIds.generate().getId()));
        Assert.fail(""Should not find a workflow token for a random run id"");
    } catch (NotFoundException expected) {
    }
    WorkflowTokenDetail workflowToken = workflowClient.getWorkflowToken(workflowRunId);
    Assert.assertEquals(3, workflowToken.getTokenData().size());
    workflowToken = workflowClient.getWorkflowToken(workflowRunId, WorkflowToken.Scope.SYSTEM);
    Assert.assertTrue(workflowToken.getTokenData().size() > 0);
    workflowToken = workflowClient.getWorkflowToken(workflowRunId, ""start_time"");
    Map<String, List<WorkflowTokenDetail.NodeValueDetail>> tokenData = workflowToken.getTokenData();
    Assert.assertEquals(AppWithWorkflow.WordCountMapReduce.NAME, tokenData.get(""start_time"").get(0).getNode());
    Assert.assertTrue(Long.parseLong(tokenData.get(""start_time"").get(0).getValue()) < System.currentTimeMillis());
    workflowToken = workflowClient.getWorkflowToken(workflowRunId, WorkflowToken.Scope.USER, ""action_type"");
    tokenData = workflowToken.getTokenData();
    Assert.assertEquals(AppWithWorkflow.WordCountMapReduce.NAME, tokenData.get(""action_type"").get(0).getNode());
    Assert.assertEquals(""MapReduce"", tokenData.get(""action_type"").get(0).getValue());
    String nodeName = AppWithWorkflow.SampleWorkflow.firstActionName;
    WorkflowTokenNodeDetail workflowTokenAtNode =
    workflowClient.getWorkflowTokenAtNode(workflowRunId, nodeName);
    Assert.assertEquals(AppWithWorkflow.DummyAction.TOKEN_VALUE,
    workflowTokenAtNode.getTokenDataAtNode().get(AppWithWorkflow.DummyAction.TOKEN_KEY));
    workflowTokenAtNode = workflowClient.getWorkflowTokenAtNode(workflowRunId, nodeName, WorkflowToken.Scope.SYSTEM);
    Assert.assertEquals(0, workflowTokenAtNode.getTokenDataAtNode().size());
    workflowTokenAtNode = workflowClient.getWorkflowTokenAtNode(workflowRunId, nodeName,
    AppWithWorkflow.DummyAction.TOKEN_KEY);
    Assert.assertEquals(AppWithWorkflow.DummyAction.TOKEN_VALUE,
    workflowTokenAtNode.getTokenDataAtNode().get(AppWithWorkflow.DummyAction.TOKEN_KEY));
    String reduceOutputRecordsCounter = ""org.apache.hadoop.mapreduce.TaskCounter.REDUCE_OUTPUT_RECORDS"";
    workflowTokenAtNode = workflowClient.getWorkflowTokenAtNode(workflowRunId, AppWithWorkflow.WordCountMapReduce.NAME,
    WorkflowToken.Scope.SYSTEM, reduceOutputRecordsCounter);
    Assert.assertEquals(6, Integer.parseInt(workflowTokenAtNode.getTokenDataAtNode().get(reduceOutputRecordsCounter)));
}"
"@Test
public void testRemoveSuperColumn() throws IOException, ExecutionException, InterruptedException {
    Table table = Table.open(""Table1"");
    ColumnFamilyStore store = table.getColumnFamilyStore(""Super1"");
    RowMutation rm;
    rm = new RowMutation(""Table1"", ""key1"");
    rm.add(""Super1:SC1:Column1"", ""asdf"".getBytes(), 0);
    rm.apply();
    store.forceBlockingFlush();
    rm = new RowMutation(""Table1"", ""key1"");
    rm.delete(""Super1:SC1"", 1);
    rm.apply();
    List<ColumnFamily> families = store.getColumnFamilies(""key1"", ""Super1"", new IdentityFilter());
    assert families.get(0).getAllColumns().first().getMarkedForDeleteAt() == 1;
    assert !families.get(1).getAllColumns().first().isMarkedForDelete();
    ColumnFamily resolved = ColumnFamily.resolve(families);
    assert resolved.getAllColumns().first().getMarkedForDeleteAt() == 1;
    Collection<IColumn> subColumns = resolved.getAllColumns().first().getSubColumns();
    assert subColumns.size() == 1;
    assert subColumns.iterator().next().timestamp() == 0;
    assertNull(ColumnFamilyStore.removeDeleted(resolved, Integer.MAX_VALUE));
}"
"@Test
public void giteeSample() throws Exception {
    Map<String, Object> value = new ObjectMapper().readValue(new ClassPathResource(""pathsamples/gitee.json"").getInputStream(), new TypeReference<Map<String, Object>>() {});
    this.headers.set(""x-git-oschina-event"", ""Push Hook"");
    PropertyPathNotification extracted = this.extractor.extract(this.headers, value);
    assertThat(extracted).isNotNull();
    assertThat(extracted.getPaths()[0]).isEqualTo(""d.txt"");
}"
"@Test
public void testMinAllowedValue() {
    long millis = _validMinTime;
    DateTime dateTime = new DateTime(millis, DateTimeZone.UTC);
    LocalDateTime localDateTime = dateTime.toLocalDateTime();
    int year = localDateTime.getYear();
    int month = localDateTime.getMonthOfYear();
    int day = localDateTime.getDayOfMonth();
    Assert.assertEquals(year, 1971);
    Assert.assertEquals(month, 1);
    Assert.assertEquals(day, 1);
}"
"@Test
public void testWorkerInstances() throws Exception {
    ApplicationManager applicationManager = deployApplication(testSpace, AppUsingGetServiceURL.class);
    WorkerManager workerManager = applicationManager.getWorkerManager(PINGING_WORKER).start();
    workerManager.waitForStatus(true);
    workerInstancesCheck(workerManager, 5);
    workerManager.setInstances(10);
    workerInstancesCheck(workerManager, 10);
    workerManager.setInstances(2);
    workerInstancesCheck(workerManager, 2);
    workerManager.setInstances(2);
    workerInstancesCheck(workerManager, 2);
    WorkerManager lifecycleWorkerManager = applicationManager.getWorkerManager(LIFECYCLE_WORKER).start();
    lifecycleWorkerManager.waitForStatus(true);
    lifecycleWorkerManager.setInstances(5);
    workerInstancesCheck(lifecycleWorkerManager, 5);
    for (int i = 0; i < 5; i++) {
        kvTableKeyCheck(testSpace, WORKER_INSTANCES_DATASET, Bytes.toBytes(String.format(""init.%d"", i)));
    }
    lifecycleWorkerManager.stop();
    lifecycleWorkerManager.waitForStatus(false);
    if (workerManager.isRunning()) {
        workerManager.stop();
    }
    workerManager.waitForStatus(false);
    workerInstancesCheck(lifecycleWorkerManager, 5);
    workerInstancesCheck(workerManager, 2);
    assertWorkerDatasetWrites(Bytes.toBytes(""init""), Bytes.stopKeyForPrefix(Bytes.toBytes(""init.2"")), 3, 3);
    assertWorkerDatasetWrites(Bytes.toBytes(""init.3""), Bytes.stopKeyForPrefix(Bytes.toBytes(""init"")), 2, 5);
    byte[] startRow = Bytes.toBytes(""stop"");
    assertWorkerDatasetWrites(startRow, Bytes.stopKeyForPrefix(startRow), 5, 5);
}"
"@Test
public void notifyAboutChangesConcurrently() {
    final int numberOfThreads = 100;
    final TestSubscriber<Changes> testSubscriber = new TestSubscriber<Changes>();
    final Set<String> tables = new HashSet<String>();
    final List<Changes> expectedChanges = new ArrayList<Changes>();
    for (int i = 0; i < numberOfThreads; i++) {
        final String table = ""test_table"" + i;
        tables.add(table);
        expectedChanges.add(Changes.newInstance(table));
    }
    storIOSQLite.observeChanges(LATEST).subscribe(testSubscriber);
    final CountDownLatch startAllThreadsLock = new CountDownLatch(1);
    for (int i = 0; i < numberOfThreads; i++) {
        final int finalI = i;
        new Thread(new Runnable() {
            @Override
            public void run() {
                try {
                    startAllThreadsLock.await();
                } catch (InterruptedException e) {
                    throw new RuntimeException(e);
                }
                storIOSQLite.lowLevel().notifyAboutChanges(Changes.newInstance(""test_table"" + finalI));
            }
        }).start();
    }
    startAllThreadsLock.countDown();
    final long startTime = SystemClock.elapsedRealtime();
    while ((testSubscriber.valueCount() != tables.size()) && ((SystemClock.elapsedRealtime() - startTime) < 20000)) {
        Thread.yield();
    }
    testSubscriber.assertNoErrors();
    testSubscriber.assertValueCount(expectedChanges.size());
    assertThat(expectedChanges.containsAll(testSubscriber.values())).isTrue();
}"
"@Test
public void testCorrectRebalancingCurrentlyRentingPartitions() throws Exception {
    IgniteEx ignite = ((IgniteEx) (startGrids(3)));
    ignite.cluster().active(true);
    final int keysCnt = SF.applyLB(300000, 10000);
    try (final IgniteDataStreamer<Integer, Integer> ds = ignite.dataStreamer(CACHE_NAME)) {
        log.info(""Writing initial data..."");
        ds.allowOverwrite(true);
        for (int k = 1; k <= keysCnt; k++) {
            ds.addData(k, k);
            if ((k % 10000) == 0) {
                log.info((""Written "" + k) + "" entities."");
            }
        }
        log.info(""Writing initial data finished."");
    }
    startGrid(3);
    resetBaselineTopology();
    stopGrid(3);
    resetBaselineTopology();
    stopGrid(1);
    startGrid(1);
    awaitPartitionMapExchange();
    for (int k = 1; k <= keysCnt; k++) {
        Integer val = ((Integer) (ignite.cache(CACHE_NAME).get(k)));
        Assert.assertNotNull((""Value for "" + k) + "" is null"", val);
        Assert.assertEquals(((""Check failed for "" + k) + "" = "") + val, k, ((int) (val)));
    }
}"
"@Test
public void testBrokerSelectionForAntiAffinityGroup() throws Exception {
    final String broker1 = primaryHost;
    final String broker2 = secondaryHost;
    final String cluster = pulsar1.getConfiguration().getClusterName();
    final String tenant = ""tenant-"" + UUID.randomUUID().toString();
    final String namespace1 = ((tenant + ""/"") + cluster) + ""/ns1"";
    final String namespace2 = ((tenant + ""/"") + cluster) + ""/ns2"";
    final String namespaceAntiAffinityGroup = ""group"";
    FailureDomain domain1 = new FailureDomain();
    domain1.brokers = Sets.newHashSet(broker1);
    admin1.clusters().createFailureDomain(cluster, ""domain1"", domain1);
    FailureDomain domain2 = new FailureDomain();
    domain2.brokers = Sets.newHashSet(broker2);
    admin1.clusters().createFailureDomain(cluster, ""domain2"", domain2);
    admin1.tenants().createTenant(tenant, new TenantInfo(null, Sets.newHashSet(cluster)));
    admin1.namespaces().createNamespace(namespace1);
    admin1.namespaces().createNamespace(namespace2);
    admin1.namespaces().setNamespaceAntiAffinityGroup(namespace1, namespaceAntiAffinityGroup);
    admin1.namespaces().setNamespaceAntiAffinityGroup(namespace2, namespaceAntiAffinityGroup);
    for (int i = 0; i < 5; i++) {
        if ((!isLoadManagerUpdatedDomainCache(primaryLoadManager)) || (!isLoadManagerUpdatedDomainCache(secondaryLoadManager))) {
            Thread.sleep(200);
        } else {
            break;
        }
    }
    assertTrue(isLoadManagerUpdatedDomainCache(primaryLoadManager));
    assertTrue(isLoadManagerUpdatedDomainCache(secondaryLoadManager));
    ServiceUnitId serviceUnit1 = makeBundle(tenant, cluster, ""ns1"");
    String selectedBroker1 = primaryLoadManager.selectBrokerForAssignment(serviceUnit1).get();
    ServiceUnitId serviceUnit2 = makeBundle(tenant, cluster, ""ns2"");
    String selectedBroker2 = primaryLoadManager.selectBrokerForAssignment(serviceUnit2).get();
    assertNotEquals(selectedBroker1, selectedBroker2);
}"
"@Test
public void testQualifiedUriContents() throws Exception {
    dirString = fs.makeQualified(new Path(""d1"")).toString();
    item = new PathData(dirString, conf);
    PathData[] items = item.getDirectoryContents();
    assertEquals(sortedString(dirString + ""/f1"", dirString + ""/f1.1"", dirString + ""/f2""), sortedString(items));
}"
"@Test
public void testSubscribeRate() throws Exception {
    SubscribeRate subscribeRate = new SubscribeRate(1, 5);
    String namespace = ""my-tenants/my-namespace"";
    admin.tenants().createTenant(""my-tenants"", new TenantInfoImpl(Sets.newHashSet(), Sets.newHashSet(testLocalCluster)));
    admin.namespaces().createNamespace(namespace, Sets.newHashSet(testLocalCluster));
    admin.namespaces().setSubscribeRate(namespace, subscribeRate);
    assertEquals(subscribeRate, admin.namespaces().getSubscribeRate(namespace));
    String topicName = ((""persistent""));
    admin.topics().createPartitionedTopic(topicName, 2);
    pulsar.getConfiguration().setAuthorizationEnabled(false);
    Consumer<?> consumer = pulsarClient.newConsumer().topic(topicName).subscriptionType(Shared).subscriptionName(""subscribe-rate"").subscribe();
    assertTrue(consumer.isConnected());
    pulsarClient.updateServiceUrl(lookupUrl.toString());
    Awaitility.await().untilAsserted(() -> assertFalse(consumer.isConnected()));
    Thread.sleep(6000L);
    pulsarClient.updateServiceUrl(lookupUrl.toString());
    assertTrue(consumer.isConnected());
    subscribeRate = new SubscribeRate(0, 10);
    admin.namespaces().setSubscribeRate(namespace, subscribeRate);
    pulsarClient.updateServiceUrl(lookupUrl.toString());
    Awaitility.await().untilAsserted(() -> assertTrue(consumer.isConnected()));
    pulsar.getConfiguration().setAuthorizationEnabled(true);
    admin.topics().deletePartitionedTopic(topicName, true);
    admin.namespaces().deleteNamespace(namespace);
    admin.tenants().deleteTenant(""my-tenants"");
}"
"@Test
void writesAndReadsCustomFieldsConvertedClass() {
    List<Object> converters = new ArrayList<>();
    converters.add(BigDecimalToStringConverter.INSTANCE);
    converters.add(StringToBigDecimalConverter.INSTANCE);
    CustomConversions customConversions = new CouchbaseCustomConversions(converters);
    converter.setCustomConversions(customConversions);
    converter.afterPropertiesSet();
    ((CouchbaseMappingContext) (converter.getMappingContext())).setSimpleTypeHolder(customConversions.getSimpleTypeHolder());
    CouchbaseDocument converted = new CouchbaseDocument();
    final String valueStr = ""12.345"";
    final BigDecimal value = new BigDecimal(valueStr);
    final String value2Str = ""0.6789"";
    final BigDecimal value2 = new BigDecimal(value2Str);
    List<BigDecimal> listOfValues = new ArrayList<>();
    listOfValues.add(value);
    listOfValues.add(value2);
    Map<String, BigDecimal> mapOfValues = new HashMap<>();
    mapOfValues.put(""val1"", value);
    mapOfValues.put(""val2"", value2);
    CustomFieldsEntity entity = new CustomFieldsEntity(value, listOfValues, mapOfValues);
    converter.write(entity, converted);
    CouchbaseDocument source = new CouchbaseDocument();
    source.put(""_class"", CustomFieldsEntity.class.getName());
    source.put(""decimalValue"", valueStr);
    CouchbaseList listOfValuesDoc = new CouchbaseList();
    listOfValuesDoc.put(valueStr);
    listOfValuesDoc.put(value2Str);
    source.put(""listOfDecimalValues"", listOfValuesDoc);
    CouchbaseDocument mapOfValuesDoc = new CouchbaseDocument();
    mapOfValuesDoc.put(""val1"", valueStr);
    mapOfValuesDoc.put(""val2"", value2Str);
    source.put(""mapOfDecimalValues"", mapOfValuesDoc);
    assertThat(valueStr).isEqualTo(((CouchbaseList) (converted.getContent().get(""listOfDecimalValues""))).get(0));
    assertThat(value2Str).isEqualTo(((CouchbaseList) (converted.getContent().get(""listOfDecimalValues""))).get(1));
    assertThat(converted.export().toString()).isEqualTo(source.export().toString());
    CustomFieldsEntity readConverted = converter.read(CustomFieldsEntity.class, source);
    assertThat(readConverted.value).isEqualTo(value);
    assertThat(readConverted.listOfValues.get(0)).isEqualTo(listOfValues.get(0));
    assertThat(readConverted.listOfValues.get(1)).isEqualTo(listOfValues.get(1));
    assertThat(readConverted.mapOfValues.get(""val1"")).isEqualTo(mapOfValues.get(""val1""));
    assertThat(readConverted.mapOfValues.get(""val2"")).isEqualTo(mapOfValues.get(""val2""));
}"
"@Test
public void testTopicLevelInactivePolicyUpdateAndClean() throws Exception {
    super.resetConfig();
    conf.setSystemTopicEnabled(true);
    conf.setTopicLevelPoliciesEnabled(true);
    conf.setBrokerDeleteInactiveTopicsEnabled(true);
    conf.setBrokerDeleteInactiveTopicsMaxInactiveDurationSeconds(1000);
    conf.setBrokerDeleteInactiveTopicsMode(delete_when_no_subscriptions);
    InactiveTopicPolicies defaultPolicy = new InactiveTopicPolicies(InactiveTopicDeleteMode.delete_when_no_subscriptions, 1000, true);
    super.baseSetup();
    Thread.sleep(2000);
    final String namespace = ""prop/ns-abc"";
    final String topic = ""persistent"";
    final String topic2 = ""persistent"";
    final String topic3 = ""persistent"";
    List<String> topics = Arrays.asList(topic, topic2, topic3);
    for (String tp : topics) {
        admin.topics().createNonPartitionedTopic(tp);
    }
    InactiveTopicPolicies inactiveTopicPolicies = new InactiveTopicPolicies(InactiveTopicDeleteMode.delete_when_no_subscriptions, 1, true);
    admin.topics().setInactiveTopicPolicies(topic, inactiveTopicPolicies);
    inactiveTopicPolicies.setInactiveTopicDeleteMode(delete_when_subscriptions_caught_up);
    admin.topics().setInactiveTopicPolicies(topic2, inactiveTopicPolicies);
    inactiveTopicPolicies.setInactiveTopicDeleteMode(delete_when_no_subscriptions);
    admin.topics().setInactiveTopicPolicies(topic3, inactiveTopicPolicies);
    for (int i = 0; i < 50; i++) {
        if (admin.topics().getInactiveTopicPolicies(topic) != null) {
            break;
        }
        Thread.sleep(100);
    }
    InactiveTopicPolicies policies = ((PersistentTopic) (pulsar.getBrokerService().getTopic(topic, false).get().get())).inactiveTopicPolicies;
    Assert.assertTrue(policies.isDeleteWhileInactive());
    assertEquals(policies.getInactiveTopicDeleteMode(), delete_when_no_subscriptions);
    assertEquals(policies.getMaxInactiveDurationSeconds(), 1);
    assertEquals(policies, admin.topics().getInactiveTopicPolicies(topic));
    admin.topics().removeInactiveTopicPolicies(topic);
    for (int i = 0; i < 50; i++) {
        if (admin.topics().getInactiveTopicPolicies(topic) == null) {
            break;
        }
        Thread.sleep(100);
    }
    assertEquals(((PersistentTopic) (pulsar.getBrokerService().getTopic(topic, false).get().get())).inactiveTopicPolicies, defaultPolicy);
    policies = ((PersistentTopic) (pulsar.getBrokerService().getTopic(topic2, false).get().get())).inactiveTopicPolicies;
    Assert.assertTrue(policies.isDeleteWhileInactive());
    assertEquals(policies.getInactiveTopicDeleteMode(), delete_when_subscriptions_caught_up);
    assertEquals(policies.getMaxInactiveDurationSeconds(), 1);
    assertEquals(policies, admin.topics().getInactiveTopicPolicies(topic2));
    inactiveTopicPolicies.setMaxInactiveDurationSeconds(999);
    admin.namespaces().setInactiveTopicPolicies(namespace, inactiveTopicPolicies);
    Thread.sleep(1000);
    admin.topics().removeInactiveTopicPolicies(topic2);
    for (int i = 0; i < 50; i++) {
        if (admin.topics().getInactiveTopicPolicies(topic2) == null) {
            break;
        }
        Thread.sleep(100);
    }
    InactiveTopicPolicies nsPolicies = ((PersistentTopic) (pulsar.getBrokerService().getTopic(topic2, false).get().get())).inactiveTopicPolicies;
    assertEquals(nsPolicies.getMaxInactiveDurationSeconds(), 999);
    super.internalCleanup();
}"
"@Test
public void testMaximumRolloverTime() throws Exception {
    ManagedLedgerConfig conf = new ManagedLedgerConfig();
    conf.setMaxEntriesPerLedger(5);
    conf.setMinimumRolloverTime(1, SECONDS);
    conf.setMaximumRolloverTime(1, SECONDS);
    ManagedLedgerImpl ledger = ((ManagedLedgerImpl) (factory.open(""my_test_maxtime_ledger"", conf)));
    ledger.openCursor(""c1"");
    ledger.addEntry(""data"".getBytes());
    ledger.addEntry(""data"".getBytes());
    assertEquals(ledger.getLedgersInfoAsList().size(), 1);
    Thread.sleep(2000);
    ledger.addEntry(""data"".getBytes());
    ledger.addEntry(""data"".getBytes());
    assertEquals(ledger.getLedgersInfoAsList().size(), 2);
}"
"@Test
public void testBrokerDiscoveryRoundRobin() throws Exception {
    addBrokerToZk(5);
    String prevUrl = null;
    for (int i = 0; i < 10; i++) {
        String current = service.getDiscoveryProvider().nextBroker().getPulsarServiceUrl();
        assertNotEquals(prevUrl, current);
        prevUrl = current;
    }
}"
"@Test
public void testAutomaticStartStop() throws Exception {
    final TestRunnable task = new TestRunnable(500);
    e.execute(task);
    Thread thread = e.thread;
    assertThat(thread, is(not(nullValue())));
    assertThat(thread.isAlive(), is(true));
    Thread.sleep(1500);
    assertThat(thread.isAlive(), is(false));
    assertThat(task.ran.get(), is(true));
    task.ran.set(false);
    e.execute(task);
    assertThat(e.thread, not(sameInstance(thread)));
    thread = e.thread;
    Thread.sleep(1500);
    assertThat(thread.isAlive(), is(false));
    assertThat(task.ran.get(), is(true));
}"
"@Test
public void testPerTopicStats() throws Exception {
    String randSeed = randomName(16);
    System.out.println(""The randSeed of testPerTopicStats() is: "" + randSeed);
    Producer<byte[]> p1 = pulsarClient.newProducer().topic(""persistent://my-property/use/"" + randSeed + ""/my-topic1"").create();
    Producer<byte[]> p2 = pulsarClient.newProducer().topic(""persistent://my-property/use/"" + randSeed + ""/my-topic2"").create();
    for (int i = 0; i < 10; i++) {
        String message = ""my-message-"" + i;
        p1.send(message.getBytes());
        p2.send(message.getBytes());
    }
    ByteArrayOutputStream statsOut = new ByteArrayOutputStream();
    PrometheusMetricsGenerator.generate(pulsar, true, false, statsOut);
    String metricsStr = new String(statsOut.toByteArray());
    Multimap<String, Metric> metrics = parseMetrics(metricsStr);
    metrics.entries().forEach(e -> {
        System.out.println(e.getKey() + "": "" + e.getValue());
    });
    List<Metric> cm = (List<Metric>) metrics.get(""pulsar_storage_write_latency_le_1"");
    List<Metric> matchingMetrics = cm.stream().filter(t -> t.tags.containsValue(""my-property/use/"" + randSeed)).collect(Collectors.toList());
    int positionOfTopic1;
    int positionOfTopic2;
    if(cm.get(0).tags.get(""topic"").equals(""persistent://my-property/use/"" + randSeed + ""/my-topic1"")) {
        positionOfTopic1 = 0;
        positionOfTopic2 = 1;
    } else {
        positionOfTopic2 = 0;
        positionOfTopic1 = 1;
    }
    matchingMetrics = cm.stream().filter(t -> t.tags.containsValue(""my-property/use/"" + randSeed)).collect(Collectors.toList());
    if(matchingMetrics.size() > 2){
        System.out.println(""matchingMetrics.size() > 2 in testPerTopicStats(). First check. Debug entries: "");
        matchingMetrics.forEach(t -> t.tags.entrySet().forEach(kv -> System.out.println(kv.getKey() + "":""  + kv.getValue())));
    }
    assertEquals(matchingMetrics.size(), 2);
    assertEquals(matchingMetrics.get(positionOfTopic2).tags.get(""topic""), ""persistent://my-property/use/"" + randSeed + ""/my-topic2"");
    assertEquals(matchingMetrics.get(positionOfTopic2).tags.get(""namespace""), ""my-property/use/"" + randSeed);
    assertEquals(matchingMetrics.get(positionOfTopic1).tags.get(""topic""), ""persistent://my-property/use/"" + randSeed + ""/my-topic1"");
    assertEquals(matchingMetrics.get(positionOfTopic1).tags.get(""namespace""), ""my-property/use/"" + randSeed);
    cm = (List<Metric>) metrics.get(""pulsar_producers_count"");
    if(cm.get(1).tags.get(""topic"").equals(""persistent://my-property/use/"" + randSeed + ""/my-topic1"")) {
        positionOfTopic1 = 1;
        positionOfTopic2 = 2;
    } else {
        positionOfTopic2 = 1;
        positionOfTopic1 = 2;
    }
    matchingMetrics = cm.stream().filter(t -> t.tags.containsValue(""my-property/use/"" + randSeed)).collect(Collectors.toList());
    if(matchingMetrics.size() > 2){
        System.out.println(""matchingMetrics.size() > 2 in testPerTopicStats(). Second check. Debug entries: "");
        matchingMetrics.forEach(t -> t.tags.entrySet().forEach(kv -> System.out.println(kv.getKey() + "":""  + kv.getValue())));
    }
    assertEquals(matchingMetrics.size(), 2);
    assertEquals(matchingMetrics.get(positionOfTopic2).tags.get(""topic""), ""persistent://my-property/use/"" + randSeed + ""/my-topic2"");
    assertEquals(matchingMetrics.get(positionOfTopic2).tags.get(""namespace""), ""my-property/use/"" + randSeed);
    assertEquals(matchingMetrics.get(positionOfTopic1).tags.get(""topic""), ""persistent://my-property/use/"" + randSeed + ""/my-topic1"");
    assertEquals(matchingMetrics.get(positionOfTopic1).tags.get(""namespace""), ""my-property/use/"" + randSeed);
    cm = (List<Metric>) metrics.get(""topic_load_times_count"");
    if(cm.size() > 1){
        System.out.println(""matchingMetrics.size() > 2 in testPerTopicStats(). Third check. Debug entries: "");
        cm.forEach(t -> t.tags.entrySet().forEach(kv -> System.out.println(kv.getKey() + "":""  + kv.getValue())));
    }
    assertEquals(cm.size(), 1);
    assertEquals(cm.get(0).tags.get(""cluster""), ""test"");
    cm = (List<Metric>) metrics.get(""pulsar_in_bytes_total"");
    if(cm.get(0).tags.get(""topic"").equals(""persistent://my-property/use/"" + randSeed + ""/my-topic1"")) {
        positionOfTopic1 = 0;
        positionOfTopic2 = 1;
    } else {
        positionOfTopic2 = 0;
        positionOfTopic1 = 1;
    }
    matchingMetrics = cm.stream().filter(t -> t.tags.containsValue(""my-property/use/"" + randSeed)).collect(Collectors.toList());
    if(matchingMetrics.size() > 2){
        System.out.println(""matchingMetrics.size() > 2 in testPerTopicStats(). Fourth check. Debug entries: "");
        matchingMetrics.forEach(t -> t.tags.entrySet().forEach(kv -> System.out.println(kv.getKey() + "":""  + kv.getValue())));
    }
    assertEquals(matchingMetrics.size(), 2);
    assertEquals(matchingMetrics.get(positionOfTopic2).tags.get(""topic""), ""persistent://my-property/use/"" + randSeed + ""/my-topic2"");
    assertEquals(matchingMetrics.get(positionOfTopic2).tags.get(""namespace""), ""my-property/use/"" + randSeed);
    assertEquals(matchingMetrics.get(positionOfTopic1).tags.get(""topic""), ""persistent://my-property/use/"" + randSeed + ""/my-topic1"");
    assertEquals(matchingMetrics.get(positionOfTopic1).tags.get(""namespace""), ""my-property/use/"" + randSeed);
    cm = (List<Metric>) metrics.get(""pulsar_in_messages_total"");
    if(cm.get(0).tags.get(""topic"").equals(""persistent://my-property/use/"" + randSeed + ""/my-topic1"")) {
        positionOfTopic1 = 0;
        positionOfTopic2 = 1;
    } else {
        positionOfTopic2 = 0;
        positionOfTopic1 = 1;
    }
    matchingMetrics = cm.stream().filter(t -> t.tags.containsValue(""my-property/use/"" + randSeed)).collect(Collectors.toList());
    if(matchingMetrics.size() > 2){
        System.out.println(""matchingMetrics.size() > 2 in testPerTopicStats(). Fifth check. Debug entries: "");
        matchingMetrics.forEach(t -> t.tags.entrySet().forEach(kv -> System.out.println(kv.getKey() + "":""  + kv.getValue())));
    }
    assertEquals(matchingMetrics.size(), 2);
    assertEquals(matchingMetrics.get(positionOfTopic2).tags.get(""topic""), ""persistent://my-property/use/"" + randSeed + ""/my-topic2"");
    assertEquals(matchingMetrics.get(positionOfTopic2).tags.get(""namespace""), ""my-property/use/"" + randSeed);
    assertEquals(matchingMetrics.get(positionOfTopic1).tags.get(""topic""), ""persistent://my-property/use/"" + randSeed + ""/my-topic1"");
    assertEquals(matchingMetrics.get(positionOfTopic1).tags.get(""namespace""), ""my-property/use/"" + randSeed);
    p1.close();
    p2.close();
}"
"@Test
public void testCwdContents() throws Exception {
    dirString = Path.CUR_DIR;
    item = new PathData(dirString, conf);
    PathData[] items = item.getDirectoryContents();
    assertEquals(sortedString(""d1"", ""d2""), sortedString(items));
}"
"@Test
public void testCancelDeprovision() throws Exception {
    ProvisionerInfo provisionerInfo = new MockProvisioner.PropertyBuilder().waitDelete(1, TimeUnit.MINUTES).build();
    TaskFields taskFields = testProvision(ProvisioningOp.Status.CREATED, provisionerInfo);
    Runnable task = Transactionals.execute(transactional, dsContext -> {
        return provisioningService.deprovision(taskFields.programRunId, dsContext);
    });
    task.run();
    Assert.assertTrue(provisioningService.cancelDeprovisionTask(taskFields.programRunId).isPresent());
    ProvisioningTaskKey taskKey = new ProvisioningTaskKey(taskFields.programRunId, ProvisioningOp.Type.DEPROVISION);
    waitForExpectedProvisioningState(taskKey, ProvisioningOp.Status.CANCELLED);
}"
"@Test
public void testTimeWindows()
{
    Long tstamp1 = 1451001601000L;
    Long tstamp2 = 1451088001000L;
    Long lowHour = 1451001600000L;
    assertTrue(getWindowBoundsInMillis(TimeUnit.HOURS, 1, tstamp1).left.compareTo(lowHour) == 0);
    assertTrue(getWindowBoundsInMillis(TimeUnit.MINUTES, 1, tstamp1).left.compareTo(lowHour) == 0);
    assertTrue(getWindowBoundsInMillis(TimeUnit.DAYS, 1, tstamp1).left.compareTo(lowHour) == 0 );
    assertTrue(getWindowBoundsInMillis(TimeUnit.DAYS, 2, tstamp2).left.compareTo(lowHour) == 0);
    return;
}"
"@Test
public void testGracefulClose() throws Exception {
    int maxReceiveCountAfterClose = 0;
    for (int i = 6; i <= 100 && maxReceiveCountAfterClose < 5; i++) {
        int receiveCount = 0;
        KafkaChannel channel = createConnectionWithPendingReceives(i);
        selector.poll(1000);
        assertEquals(1, selector.completedReceives().size());
        server.closeConnections();
        while (selector.disconnected().isEmpty()) {
            selector.poll(1);
            receiveCount += selector.completedReceives().size();
            assertTrue(""Too many completed receives in one poll"", selector.completedReceives().size() <= 1);
        }
        assertEquals(channel.id(), selector.disconnected().keySet().iterator().next());
        maxReceiveCountAfterClose = Math.max(maxReceiveCountAfterClose, receiveCount);
    }
    assertTrue(""Too few receives after close: "" + maxReceiveCountAfterClose, maxReceiveCountAfterClose >= 5);
}"
"@Test
public void testAsyncFunction() throws Exception {
    InstanceConfig instanceConfig = new InstanceConfig();
    Function<String, CompletableFuture<String>> function = (input, context) -> {
        log.info(""input string: {}"", input);
        CompletableFuture<String> result  = new CompletableFuture<>();
        Executors.newCachedThreadPool().submit(() -> {
            try {
                Thread.sleep(500);
                result.complete(String.format(""%s-lambda"", input));
            } catch (Exception e) {
                result.completeExceptionally(e);
            }
        });
        return result;
    };
    JavaInstance instance = new JavaInstance(
    mock(ContextImpl.class),
    function,
    instanceConfig);
    String testString = ""ABC123"";
    CompletableFuture<JavaExecutionResult> result = instance.handleMessage(mock(Record.class), testString);
    assertNotNull(result.get().getResult());
    assertEquals(new String(testString + ""-lambda""), result.get().getResult());
    instance.close();
}"
"@Test
public void testHftpCustomDefaultPorts() throws IOException {
    resetFileSystem();
    Configuration conf = new Configuration();
    conf.setInt(""dfs.http.port"", 123);
    conf.setInt(""dfs.https.port"", 456);
    URI uri = URI.create();
    HftpFileSystem fs = ((HftpFileSystem) (FileSystem.get(uri, conf)));
    assertEquals(123, fs.getDefaultPort());
    assertEquals(456, fs.getDefaultSecurePort());
    assertEquals(uri, fs.getUri());
    assertEquals(""127.0.0.1:456"", fs.getCanonicalServiceName());
}"
"@Test
public void testLedgerReachMaximumRolloverTime() throws Exception {
    ManagedLedgerConfig config = new ManagedLedgerConfig();
    config.setMinimumRolloverTime(1, TimeUnit.MILLISECONDS);
    config.setMaximumRolloverTime(1, TimeUnit.SECONDS);
    ManagedLedger ml = factory.open(""ledger-reach-maximum-rollover-time"", config);
    long firstLedgerId = ml.addEntry(""test"".getBytes()).getLedgerId();
    Awaitility.await()
    .atMost(1100, TimeUnit.MILLISECONDS)
    .pollInterval(100, TimeUnit.MILLISECONDS)
    .until(() -> firstLedgerId != ml.addEntry(""test"".getBytes()).getLedgerId());
}"
"@Test
public void testSubscriber() throws InterruptedException, ExecutionException, TimeoutException {
    LineageWriter lineageWriter = getInjector().getInstance(MessagingLineageWriter.class);
    ProgramRunId run1 = service1.run(RunIds.generate());
    lineageWriter.addAccess(run1, dataset1, AccessType.READ);
    lineageWriter.addAccess(run1, dataset2, AccessType.WRITE);
    LineageStoreReader lineageReader = getInjector().getInstance(LineageStoreReader.class);
    ProgramRunId run1 = service1.run(RunIds.generate());
    Set<NamespacedEntityId> entities = lineageReader.getEntitiesForRun(run1);
    Assert.assertTrue(entities.isEmpty());
    LineageWriter lineageWriter = getInjector().getInstance(MessagingLineageWriter.class);
    lineageWriter.addAccess(run1, dataset1, AccessType.READ);
    lineageWriter.addAccess(run1, dataset2, AccessType.WRITE);
    FieldLineageWriter fieldLineageWriter = getInjector().getInstance(MessagingLineageWriter.class);
    ProgramRunId spark1Run1 = spark1.run(RunIds.generate(100));
    ReadOperation read = new ReadOperation(""read"", ""some read"", EndPoint.of(""ns"", ""endpoint1""), ""offset"", ""body"");
    TransformOperation parse = new TransformOperation(""parse"", ""parse body"",
    Collections.singletonList(InputField.of(""read"", ""body"")),
    ""name"", ""address"");
    WriteOperation write = new WriteOperation(""write"", ""write data"", EndPoint.of(""ns"", ""endpoint2""),
    Arrays.asList(InputField.of(""read"", ""offset""),
    InputField.of(""parse"", ""name""),
    InputField.of(""parse"", ""address"")));
    List<Operation> operations = new ArrayList<>();
    operations.add(read);
    operations.add(write);
    operations.add(parse);
    FieldLineageInfo info1 = new FieldLineageInfo(operations);
    fieldLineageWriter.write(spark1Run1, info1);
    ProgramRunId spark1Run2 = spark1.run(RunIds.generate(200));
    fieldLineageWriter.write(spark1Run2, info1);
    List<Operation> operations2 = new ArrayList<>();
    operations2.add(read);
    operations2.add(parse);
    TransformOperation normalize = new TransformOperation(""normalize"", ""normalize address"",
    Collections.singletonList(InputField.of(""parse"", ""address"")),
    ""address"");
    operations2.add(normalize);
    WriteOperation anotherWrite = new WriteOperation(""anotherwrite"", ""write data"", EndPoint.of(""ns"", ""endpoint2""),
    Arrays.asList(InputField.of(""read"", ""offset""),
    InputField.of(""parse"", ""name""),
    InputField.of(""normalize"", ""address"")));
    operations2.add(anotherWrite);
    FieldLineageInfo info2 = new FieldLineageInfo(operations2);
    ProgramRunId spark1Run3 = spark1.run(RunIds.generate(300));
    fieldLineageWriter.write(spark1Run3, info2);
    UsageWriter usageWriter = getInjector().getInstance(MessagingUsageWriter.class);
    usageWriter.register(spark1, dataset1);
    usageWriter.registerAll(Collections.singleton(spark1), dataset3);
    Set<NamespacedEntityId> expectedLineage = new HashSet<>(Arrays.asList(run1.getParent(), dataset1, dataset2));
    Tasks.waitFor(true, () -> expectedLineage.equals(lineageReader.getEntitiesForRun(run1)),
    10, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);
    Assert.assertTrue(lineageReader.getRelations(spark1, 0L, Long.MAX_VALUE, x -> true).isEmpty());
    FieldLineageReader fieldLineageReader = getInjector().getInstance(FieldLineageReader.class);
    Set<Operation> expectedOperations = new HashSet<>();
    expectedOperations.add(read);
    expectedOperations.add(anotherWrite);
    List<ProgramRunOperations> expected = new ArrayList<>();
    expected.add(new ProgramRunOperations(Collections.singleton(spark1Run3), expectedOperations));
    expectedOperations = new HashSet<>();
    expectedOperations.add(read);
    expectedOperations.add(write);
    expected.add(new ProgramRunOperations(new HashSet<>(Arrays.asList(spark1Run1, spark1Run2)),
    expectedOperations));
    EndPointField endPointField = new EndPointField(EndPoint.of(""ns"", ""endpoint2""), ""offset"");
    Tasks.waitFor(expected, () -> fieldLineageReader.getIncomingOperations(endPointField, 1L, Long.MAX_VALUE - 1),
    10, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);
    Set<EntityId> expectedUsage = new HashSet<>(Arrays.asList(dataset1, dataset3));
    UsageRegistry usageRegistry = getInjector().getInstance(UsageRegistry.class);
    Tasks.waitFor(true, () -> expectedUsage.equals(usageRegistry.getDatasets(spark1)),
    10, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);
}"
"@Test
public void test_6() throws Exception {
    JSONObject jsonObject = new JSONObject();
    jsonObject.put(""val"", new Character[]{  });
    jsonObject.put(""cls"", Number.class);
    jsonObject.put(""nums"", new Number[]{  });
    ByteArrayOutputStream bytesOut = new ByteArrayOutputStream();
    ObjectOutputStream objOut = new ObjectOutputStream(bytesOut);
    objOut.writeObject(jsonObject);
    objOut.flush();
    byte[] bytes = bytesOut.toByteArray();
    ByteArrayInputStream bytesIn = new ByteArrayInputStream(bytes);
    ObjectInputStream objIn = new ObjectInputStream(bytesIn);
    Object obj = objIn.readObject();
    assertEquals(JSONObject.class, obj.getClass());
    assertEquals(jsonObject.toJSONString(), JSON.toJSONString(obj));
}"
"@Test
public void testNotAllowNullSchema() {
    AvroSchema<Foo> avroSchema = AvroSchema.of(SchemaDefinition.<Foo>builder().withPojo(Foo.class).withAlwaysAllowNull(false).build());
    assertEquals(avroSchema.getSchemaInfo().getType(), AVRO);
    Schema.Parser parser = new Schema.Parser();
    String schemaJson = new String(avroSchema.getSchemaInfo().getSchema());
    assertEquals(schemaJson, SCHEMA_AVRO_NOT_ALLOW_NULL);
    Schema schema = parser.parse(schemaJson);
    for (String fieldName : FOO_FIELDS) {
        Schema.Field field = schema.getField(fieldName);
        Assert.assertNotNull(field);
        if (field.name().equals(""field4"")) {
            Assert.assertNotNull(field.schema().getTypes().get(1).getField(""field1""));
        }
        if (field.name().equals(""fieldUnableNull"")) {
            Assert.assertNotNull(field.schema().getType());
        }
    }
}"
"@Test
public void test_for_issue() throws Exception {
    DubboResponse resp = new DubboResponse();
    JSONObject obj = new JSONObject();
    obj.put(""key1"", ""value1"");
    obj.put(""key2"", ""value2"");
    resp.setData(obj);
    String str = JSON.toJSONString(resp);
    System.out.println(str);
    DubboResponse resp1 = JSON.parseObject(str, DubboResponse.class);
    assertEquals(str, JSON.toJSONString(resp1));
    JSONArray arr = new JSONArray();
    arr.add(""key1"");
    arr.add(""key2"");
    resp.setData(arr);
    String str2 = JSON.toJSONString(resp);
    System.out.println(str2);
    DubboResponse resp2 = JSON.parseObject(str2, DubboResponse.class);
    assertEquals(str2, JSON.toJSONString(resp2));
}"
"@Test
public void testCloseReason() throws Exception {
    MessageEndpoint.reset();
    Session session = deployment.connectToServer(AnnotatedClientEndpoint.class, new URI(""ws://"" + DefaultServer.getHostAddress(""default"") + "":"" + DefaultServer.getHostPort(""default"") + ""/ws/chat/Bob""));
    Assert.assertEquals(""hi Bob (protocol=foo)"", AnnotatedClientEndpoint.message());
    session.close(new CloseReason(CloseReason.CloseCodes.VIOLATED_POLICY, ""Foo!""));
    Assert.assertEquals(""CLOSED"", AnnotatedClientEndpoint.message());
    CloseReason cr = MessageEndpoint.getReason();
    Assert.assertEquals(CloseReason.CloseCodes.VIOLATED_POLICY.getCode(), cr.getCloseCode().getCode());
    Assert.assertEquals(""Foo!"", cr.getReasonPhrase());
}"
"@Test
public void testReplicatorProducerName() throws Exception {
    log.info(""--- Starting ReplicatorTest::testReplicatorProducerName ---"");
    final String topicName = BrokerTestUtil.newUniqueName(""persistent"");
    final TopicName dest = TopicName.get(topicName);
    @Cleanup
    MessageProducer producer1 = new MessageProducer(url1, dest);
    Awaitility.await().untilAsserted(() -> {
        assertTrue(pulsar2.getBrokerService().getTopicReference(topicName).isPresent());
    });
    Optional<Topic> topic = pulsar2.getBrokerService().getTopicReference(topicName);
    assertTrue(topic.isPresent());
    Set<String> remoteClusters = topic.get().getProducers().values().stream().map(Producer::getRemoteCluster).collect(Collectors.toSet());
    assertTrue(remoteClusters.contains(""r1""));
}"
"@Test
public void test_date() throws Exception {
    Date date1 = JSON.parseObject(""{\""gmtCreate\"":\""2018-09-12\""}"", VO.class).getGmtCreate();
    assertNotNull(date1);
    Date date2 = JSON.parseObject(""{\""gmtCreate\"":\""2018-09-12T15:10:19+00:00\""}"", VO.class).getGmtCreate();
    Date date3 = JSON.parseObject(""{\""gmtCreate\"":\""2018-09-12T15:10:19Z\""}"", VO.class).getGmtCreate();
    Date date4 = JSON.parseObject(""{\""gmtCreate\"":\""20180912T151019Z\""}"", VO.class).getGmtCreate();
    Date date5 = JSON.parseObject(""{\""gmtCreate\"":\""2018-09-12T15:10:19Z\""}"", VO.class).getGmtCreate();
    Date date6 = JSON.parseObject(""{\""gmtCreate\"":\""20180912\""}"", VO.class).getGmtCreate();
    long delta_2_1 = date2.getTime() - date1.getTime();
    assertEquals(83419000, delta_2_1);
    long delta_3_1 = date3.getTime() - date1.getTime();
    assertEquals(83419000, delta_3_1);
    long delta_4_3 = date4.getTime() - date3.getTime();
    assertEquals(0, delta_4_3);
    long delta_5_4 = date5.getTime() - date4.getTime();
    assertEquals(0, delta_5_4);
    long delta_6_1 = date6.getTime() - date1.getTime();
    assertEquals(0, delta_6_1);
}"
"@Test
public void testBuildDTServiceName() {
    assertEquals(""127.0.0.1:123"", SecurityUtil.buildDTServiceName(URI.create()));
    assertEquals(""127.0.0.1:123"", SecurityUtil.buildDTServiceName(URI.create()));
    assertEquals(""127.0.0.1:123"", SecurityUtil.buildDTServiceName(URI.create()));
    assertEquals(""127.0.0.1:123"", SecurityUtil.buildDTServiceName(URI.create()));
}"
"@Test
public void test() throws Exception {
    captureErr();
    new Thread(this::startServer).start();
    Ignite client = startGrid(getConfiguration(""client"").setClientMode(true));
    IgniteServices services = client.services();
    SimpleService srvc = services.serviceProxy(""service"", SimpleService.class, false);
    Thread.sleep(1000);
    srvc.isWorking();
    assertFalse(getErr().contains(""Cache is not configured:""));
}"
"@Test
public void testSetrepIncWithUnderReplicatedBlocks() throws Exception {
    Configuration conf = new HdfsConfiguration();
    final short REPLICATION_FACTOR = 2;
    final String FILE_NAME = ""/testFile"";
    final Path FILE_PATH = new Path(FILE_NAME);
    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).numDataNodes(REPLICATION_FACTOR + 1).build();
    try {
        final FileSystem fs = cluster.getFileSystem();
        DFSTestUtil.createFile(fs, FILE_PATH, 1L, REPLICATION_FACTOR, 1L);
        DFSTestUtil.waitReplication(fs, FILE_PATH, REPLICATION_FACTOR);
        final BlockManager bm = cluster.getNamesystem().getBlockManager();
        ExtendedBlock b = DFSTestUtil.getFirstBlock(fs, FILE_PATH);
        DatanodeDescriptor dn = bm.blocksMap.nodeIterator(b.getLocalBlock()).next();
        bm.addToInvalidates(b.getLocalBlock(), dn);
        bm.blocksMap.removeNode(b.getLocalBlock(), dn);
        FsShell shell = new FsShell(conf);
        assertEquals(0, shell.run(new String[]{ ""-setrep"", ""-w"", Integer.toString(1 + REPLICATION_FACTOR), FILE_NAME }));
    } finally {
        cluster.shutdown();
    }
}"
"@Test
public void test_for_issue() throws Exception {
    ParserConfig config = new ParserConfig();
    String json = ""{\""k\"":1,\""v\"":\""A\""}"";
    {
        Map.Entry entry = JSON.parseObject(json, Map.Entry.class, config);
        assertEquals(""v"", entry.getKey());
        assertEquals(""A"", entry.getValue());
    }
    config.putDeserializer(Map.Entry.class, new ObjectDeserializer() {
        public <T> T deserialze(DefaultJSONParser parser, Type type, Object fieldName) {
            JSONObject object = parser.parseObject();
            Object k = object.get(""k"");
            Object v = object.get(""v"");
            return ((T) (Collections.singletonMap(k, v).entrySet().iterator().next()));
        }
        public int getFastMatchToken() {
            return 0;
        }
    });
    Map.Entry entry = JSON.parseObject(json, Map.Entry.class, config);
    assertEquals(1, entry.getKey());
    assertEquals(""A"", entry.getValue());
}"
"@Test
public void testWebHdfsDoAs() throws Exception {
    LOG.info(""START: testWebHdfsDoAs()"");
    ((Log4JLogger) (LOG)).getLogger().setLevel(ALL);
    ((Log4JLogger) (LOG)).getLogger().setLevel(ALL);
    final UserGroupInformation ugi = UserGroupInformation.createRemoteUser(REAL_USER);
    LOG.info(""ugi.getShortUserName()="" + ugi.getShortUserName());
    final WebHdfsFileSystem webhdfs = WebHdfsTestUtil.getWebHdfsFileSystemAs(ugi, config);
    final Path root = new Path(""/"");
    cluster.getFileSystem().setPermission(root, new FsPermission(((short) (0777))));
    {
        final URL url = WebHdfsTestUtil.toUrl(webhdfs, GETHOMEDIRECTORY, root, new DoAsParam(PROXY_USER));
        final HttpURLConnection conn = ((HttpURLConnection) (url.openConnection()));
        final Map<?, ?> m = WebHdfsTestUtil.connectAndGetJson(conn, SC_OK);
        conn.disconnect();
        final Object responsePath = m.get(Path.class.getSimpleName());
        LOG.info(""responsePath="" + responsePath);
        Assert.assertEquals(""/user/"" + PROXY_USER, responsePath);
    }
    {
        final URL url = WebHdfsTestUtil.toUrl(webhdfs, GETHOMEDIRECTORY, root, new DoAsParam(PROXY_USER) {
            @Override
            public String getName() {
                return ""DOas"";
            }
        });
        final HttpURLConnection conn = ((HttpURLConnection) (url.openConnection()));
        final Map<?, ?> m = WebHdfsTestUtil.connectAndGetJson(conn, SC_OK);
        conn.disconnect();
        final Object responsePath = m.get(Path.class.getSimpleName());
        LOG.info(""responsePath="" + responsePath);
        Assert.assertEquals(""/user/"" + PROXY_USER, responsePath);
    }
    final Path f = new Path(""/testWebHdfsDoAs/a.txt"");
    {
        final PutOpParam.Op op = Op.CREATE;
        final URL url = WebHdfsTestUtil.toUrl(webhdfs, op, f, new DoAsParam(PROXY_USER));
        HttpURLConnection conn = ((HttpURLConnection) (url.openConnection()));
        conn = WebHdfsTestUtil.twoStepWrite(webhdfs, op, conn);
        final FSDataOutputStream out = WebHdfsTestUtil.write(webhdfs, op, conn, 4096);
        out.write(""Hello, webhdfs user!"".getBytes());
        out.close();
        final FileStatus status = webhdfs.getFileStatus(f);
        LOG.info(""status.getOwner()="" + status.getOwner());
        Assert.assertEquals(PROXY_USER, status.getOwner());
    }
    {
        final PostOpParam.Op op = Op.APPEND;
        final URL url = WebHdfsTestUtil.toUrl(webhdfs, op, f, new DoAsParam(PROXY_USER));
        HttpURLConnection conn = ((HttpURLConnection) (url.openConnection()));
        conn = WebHdfsTestUtil.twoStepWrite(webhdfs, op, conn);
        final FSDataOutputStream out = WebHdfsTestUtil.write(webhdfs, op, conn, 4096);
        out.write(""\nHello again!"".getBytes());
        out.close();
        final FileStatus status = webhdfs.getFileStatus(f);
        LOG.info(""status.getOwner()="" + status.getOwner());
        LOG.info(""status.getLen()  ="" + status.getLen());
        Assert.assertEquals(PROXY_USER, status.getOwner());
    }
}"
"@Test
public void testInitFirstVerifyCallBacks() throws Exception {
    DefaultMetricsSystem.shutdown();
    new ConfigBuilder().add(""*.period"", 8).add(""test.sink.test.class"", TestSink.class.getName()).add(""test.*.source.filter.exclude"", ""s0"").add(""test.source.s1.metric.filter.exclude"", ""X*"").add(""test.sink.sink1.metric.filter.exclude"", ""Y*"").add(""test.sink.sink2.metric.filter.exclude"", ""Y*"").save(TestMetricsConfig.getTestFilename(""hadoop-metrics2-test""));
    MetricsSystemImpl ms = new MetricsSystemImpl(""Test"");
    ms.start();
    ms.register(""s0"", ""s0 desc"", new TestSource(""s0rec""));
    TestSource s1 = ms.register(""s1"", ""s1 desc"", new TestSource(""s1rec""));
    s1.c1.incr();
    s1.xxx.incr();
    s1.g1.set(2);
    s1.yyy.incr(2);
    s1.s1.add(0);
    MetricsSink sink1 = mock(MetricsSink.class);
    MetricsSink sink2 = mock(MetricsSink.class);
    ms.registerSink(""sink1"", ""sink1 desc"", sink1);
    ms.registerSink(""sink2"", ""sink2 desc"", sink2);
    ms.publishMetricsNow();
    try {
        verify(sink1, timeout(200).times(2)).putMetrics(r1.capture());
        verify(sink2, timeout(200).times(2)).putMetrics(r2.capture());
    } finally {
        ms.stop();
        ms.shutdown();
    }
    List<MetricsRecord> mr1 = r1.getAllValues();
    List<MetricsRecord> mr2 = r2.getAllValues();
    checkMetricsRecords(mr1);
    assertEquals(""output"", mr1, mr2);
}"
"@Test
public void testWorkflowTokenPut() throws Exception {
    Assert.assertEquals(200, deploy(WorkflowTokenTestPutApp.class).getStatusLine().getStatusCode());
    Id.Application appId = Id.Application.from(Id.Namespace.DEFAULT, WorkflowTokenTestPutApp.NAME);
    Id.Workflow workflowId = Id.Workflow.from(appId, WorkflowTokenTestPutApp.WorkflowTokenTestPut.NAME);
    Id.Program mapReduceId = Id.Program.from(appId, ProgramType.MAPREDUCE, WorkflowTokenTestPutApp.RecordCounter.NAME);
    Id.Program sparkId = Id.Program.from(appId, ProgramType.SPARK, WorkflowTokenTestPutApp.SparkTestApp.NAME);
    String outputPath = new File(tmpFolder.newFolder(), ""output"").getAbsolutePath();
    startProgram(workflowId, ImmutableMap.of(""inputPath"", createInputForRecordVerification(""firstInput""),
    ""outputPath"", outputPath, ""put.in.mapper.initialize"", ""true""));
    waitState(workflowId, ProgramRunStatus.RUNNING.name());
    waitState(workflowId, ""STOPPED"");
    List<RunRecord> workflowProgramRuns = getProgramRuns(workflowId, ProgramRunStatus.FAILED.name());
    Assert.assertEquals(1, workflowProgramRuns.size());
    List<RunRecord> mapReduceProgramRuns = getProgramRuns(mapReduceId, ProgramRunStatus.FAILED.name());
    Assert.assertEquals(1, mapReduceProgramRuns.size());
    outputPath = new File(tmpFolder.newFolder(), ""output"").getAbsolutePath();
    startProgram(workflowId, ImmutableMap.of(""inputPath"", createInputForRecordVerification(""secondInput""),
    ""outputPath"", outputPath, ""put.in.map"", ""true""));
    waitState(workflowId, ProgramRunStatus.RUNNING.name());
    waitState(workflowId, ""STOPPED"");
    workflowProgramRuns = getProgramRuns(workflowId, ProgramRunStatus.FAILED.name());
    Assert.assertEquals(2, workflowProgramRuns.size());
    mapReduceProgramRuns = getProgramRuns(mapReduceId, ProgramRunStatus.FAILED.name());
    Assert.assertEquals(2, mapReduceProgramRuns.size());
    outputPath = new File(tmpFolder.newFolder(), ""output"").getAbsolutePath();
    startProgram(workflowId, ImmutableMap.of(""inputPath"", createInputForRecordVerification(""thirdInput""),
    ""outputPath"", outputPath, ""put.in.reducer.initialize"", ""true""));
    waitState(workflowId, ProgramRunStatus.RUNNING.name());
    waitState(workflowId, ""STOPPED"");
    workflowProgramRuns = getProgramRuns(workflowId, ProgramRunStatus.FAILED.name());
    Assert.assertEquals(3, workflowProgramRuns.size());
    mapReduceProgramRuns = getProgramRuns(mapReduceId, ProgramRunStatus.FAILED.name());
    Assert.assertEquals(3, mapReduceProgramRuns.size());
    outputPath = new File(tmpFolder.newFolder(), ""output"").getAbsolutePath();
    startProgram(workflowId, ImmutableMap.of(""inputPath"", createInputForRecordVerification(""fourthInput""),
    ""outputPath"", outputPath, ""put.in.reduce"", ""true""));
    waitState(workflowId, ProgramRunStatus.RUNNING.name());
    waitState(workflowId, ""STOPPED"");
    workflowProgramRuns = getProgramRuns(workflowId, ProgramRunStatus.FAILED.name());
    Assert.assertEquals(4, workflowProgramRuns.size());
    mapReduceProgramRuns = getProgramRuns(mapReduceId, ProgramRunStatus.FAILED.name());
    Assert.assertEquals(4, mapReduceProgramRuns.size());
    outputPath = new File(tmpFolder.newFolder(), ""output"").getAbsolutePath();
    startProgram(workflowId, ImmutableMap.of(""inputPath"", createInputForRecordVerification(""fifthInput""),
    ""outputPath"", outputPath, ""closurePutToken"", ""true""));
    waitState(workflowId, ProgramRunStatus.RUNNING.name());
    waitState(workflowId, ""STOPPED"");
    workflowProgramRuns = getProgramRuns(workflowId, ProgramRunStatus.FAILED.name());
    Assert.assertEquals(5, workflowProgramRuns.size());
    mapReduceProgramRuns = getProgramRuns(mapReduceId, ProgramRunStatus.COMPLETED.name());
    Assert.assertEquals(1, mapReduceProgramRuns.size());
    List<RunRecord> sparkProgramRuns = getProgramRuns(sparkId, ProgramRunStatus.FAILED.name());
    Assert.assertEquals(1, sparkProgramRuns.size());
    outputPath = new File(tmpFolder.newFolder(), ""output"").getAbsolutePath();
    startProgram(workflowId, ImmutableMap.of(""inputPath"", createInputForRecordVerification(""sixthInput""),
    ""outputPath"", outputPath));
    waitState(workflowId, ProgramRunStatus.RUNNING.name());
    waitState(workflowId, ""STOPPED"");
    workflowProgramRuns = getProgramRuns(workflowId, ProgramRunStatus.COMPLETED.name());
    Assert.assertEquals(1, workflowProgramRuns.size());
    workflowProgramRuns = getProgramRuns(sparkId, ProgramRunStatus.COMPLETED.name());
    Assert.assertEquals(1, workflowProgramRuns.size());
}"
"@Test
public void test_1() throws Exception {
    V1 entity = new V1();
    String text = JSON.toJSONString(entity, SortField);
    System.out.println(text);
    Assert.assertEquals(""{\""f1\"":0,\""f2\"":0,\""f3\"":0,\""f4\"":0,\""f5\"":0}"", text);
    JSONObject object = JSON.parseObject(text);
    text = JSON.toJSONString(object, SortField);
    Assert.assertEquals(""{\""f1\"":0,\""f2\"":0,\""f3\"":0,\""f4\"":0,\""f5\"":0}"", text);
}"
"@Test
public void testWithMismatchingPending() throws Throwable
{
    try(Cluster cluster = init(Cluster.build(2).withConfig(config -> config.with(GOSSIP).with(NETWORK)).start()))
    {
        cluster.schemaChange(""create table "" + KEYSPACE + "".tbl (id int primary key, t int)"");
        insert(cluster.coordinator(1), 0, 100);
        cluster.forEach((node) -> node.flush(KEYSPACE));
        cluster.get(1).callOnInstance(repair(options(false)));
        insert(cluster.coordinator(1), 100, 100);
        cluster.forEach((node) -> node.flush(KEYSPACE));
        cluster.forEach((node) -> node.runOnInstance(() -> {
            ColumnFamilyStore cfs = Keyspace.open(KEYSPACE).getColumnFamilyStore(""tbl"");
            FBUtilities.waitOnFutures(CompactionManager.instance.submitBackground(cfs));
            cfs.disableAutoCompaction();
        }));
        cluster.get(1).callOnInstance(repair(options(false)));
        cluster.get(1).runOnInstance(() -> {
            ColumnFamilyStore cfs = Keyspace.open(KEYSPACE).getColumnFamilyStore(""tbl"");
            cfs.enableAutoCompaction();
            FBUtilities.waitOnFutures(CompactionManager.instance.submitBackground(cfs));
        });
        RepairResult rs = cluster.get(1).callOnInstance(repair(options(true)));
        assertTrue(rs.success);
        assertFalse(rs.wasInconsistent);
    }
}"
"@Test
@Test(timeOut = 10000)
public void producerSendAsync() throws PulsarClientException {
    String key = ""producerSendAsync"";
    final String topicName = ""persistent://prop/cluster/namespace/topic-"" + key;
    final String subscriptionName = ""my-subscription-"" + key;
    final String messagePredicate = ""my-message-"" + key + ""-"";
    final int numberOfMessages = 30;
    Producer<byte[]> producer = pulsarClient.newProducer().topic(topicName)
    .enableBatching(false)
    .messageRoutingMode(MessageRoutingMode.SinglePartition)
    .create();
    Consumer<byte[]> consumer = pulsarClient.newConsumer().topic(topicName).subscriptionName(subscriptionName)
    .subscribe();
    Set<MessageId> messageIds = new HashSet<>();
    List<Future<MessageId>> futures = new ArrayList<>();
    for (int i = 0; i < numberOfMessages; i++) {
        String message = messagePredicate + i;
        futures.add(producer.sendAsync(message.getBytes()));
    }
    MessageIdImpl previousMessageId = null;
    for (Future<MessageId> f : futures) {
        try {
            MessageIdImpl currentMessageId = (MessageIdImpl) f.get();
            if (previousMessageId != null) {
                Assert.assertTrue(currentMessageId.compareTo(previousMessageId) > 0,
                ""Message Ids should be in ascending order"");
            }
            messageIds.add(currentMessageId);
            previousMessageId = currentMessageId;
        } catch (Exception e) {
            Assert.fail(""Failed to publish message, Exception: "" + e.getMessage());
        }
    }
    log.info(""Message IDs = "" + messageIds);
    Assert.assertEquals(messageIds.size(), numberOfMessages, ""Not all messages published successfully"");
    for (int i = 0; i < numberOfMessages; i++) {
        Message<byte[]> message = consumer.receive();
        Assert.assertEquals(new String(message.getData()), messagePredicate + i);
        MessageId messageId = message.getMessageId();
        Assert.assertTrue(messageIds.remove(messageId), ""Failed to receive message"");
    }
    log.info(""Message IDs = "" + messageIds);
    Assert.assertEquals(messageIds.size(), 0, ""Not all messages received successfully"");
    consumer.unsubscribe();
}"
"@Test
public void testAuthenticationFromMultipleThreadsWithCachedToken()
{
    ExecutorService executor = newCachedThreadPool(daemonThreadsNamed(this.getClass().getName() + ""%n""));
    MockTokenPoller tokenPoller = new MockTokenPoller()
    .withResult(URI.create(""http://token.uri""), successful(new Token(""valid-token"")));
    MockRedirectHandler redirectHandler = new MockRedirectHandler()
    .sleepOnRedirect(Duration.ofMillis(10));
    ExternalAuthenticator authenticator = new ExternalAuthenticator(redirectHandler, tokenPoller, KnownToken.memoryCached(), Duration.ofSeconds(1));
    List<Future<Request>> requests = times(
    4, () -> authenticator.authenticate(null, getUnauthorizedResponse(""Bearer x_token_server=\""http://token.uri\"", x_redirect_server=\""http://redirect.uri\"""")))
    .map(executor::submit)
    .collect(toImmutableList());
    ConcurrentRequestAssertion assertion = new ConcurrentRequestAssertion(requests);
    assertion.requests()
    .extracting(Request::headers)
    .extracting(headers -> headers.get(AUTHORIZATION))
    .containsOnly(""Bearer valid-token"");
    assertion.assertThatNoExceptionsHasBeenThrown();
    assertThat(redirectHandler.getRedirectionCount()).isEqualTo(1);
}"
"@Test
public void testChecksumReconnection() throws Exception {
    final String topicName = ""persistent"";
    ProducerImpl<byte[]> prod = ((ProducerImpl<byte[]>) (pulsarClient.newProducer().topic(topicName).enableBatching(false).messageRoutingMode(SinglePartition).create()));
    ProducerImpl<byte[]> producer = spy(prod);
    doReturn(producer.brokerChecksumSupportedVersion() + 1).when(producer).brokerChecksumSupportedVersion();
    doAnswer(( invocationOnMock) -> prod.getState()).when(producer).getState();
    doAnswer(( invocationOnMock) -> prod.getClientCnx()).when(producer).getClientCnx();
    doAnswer(( invocationOnMock) -> prod.cnx()).when(producer).cnx();
    Consumer<byte[]> consumer = pulsarClient.newConsumer().topic(topicName).subscriptionName(""my-sub"").subscribe();
    stopBroker();
    ((PulsarClientImpl) (pulsarClient)).timer().stop();
    ClientCnx mockClientCnx = spy(new ClientCnx(new ClientConfigurationData(), ((PulsarClientImpl) (pulsarClient)).eventLoopGroup()));
    doReturn(producer.brokerChecksumSupportedVersion() - 1).when(mockClientCnx).getRemoteEndpointProtocolVersion();
    prod.setClientCnx(mockClientCnx);
    CompletableFuture<MessageId> future1 = producer.sendAsync(""message-1"".getBytes());
    byte[] a2 = ""message-2"".getBytes();
    TypedMessageBuilder<byte[]> msg2 = producer.newMessage().value(a2);
    CompletableFuture<MessageId> future2 = msg2.sendAsync();
    ((TypedMessageBuilderImpl<byte[]>) (msg2)).getContent().put(a2.length - 1, ((byte) ('3')));
    prod.setClientCnx(null);
    startBroker();
    prod.grabCnx();
    try {
        future1.get(10, TimeUnit.SECONDS);
        future2.get(10, TimeUnit.SECONDS);
    } catch (Exception e) {
        e.printStackTrace();
        fail(""Broker shouldn't verify checksum for corrupted message and it shouldn't fail"");
    }
    ((ConsumerImpl<byte[]>) (consumer)).grabCnx();
    Message<byte[]> msg = consumer.receive(1, TimeUnit.SECONDS);
    assertEquals(new String(msg.getData()), ""message-1"");
    msg = consumer.receive(1, TimeUnit.SECONDS);
    assertEquals(new String(msg.getData()), ""message-3"");
}"
"@Test
public void testContinuousScheduling() throws Exception {
    FairScheduler fs = new FairScheduler();
    Configuration conf = createConfiguration();
    conf.setBoolean(CONTINUOUS_SCHEDULING_ENABLED, true);
    fs.reinitialize(conf, resourceManager.getRMContext());
    Assert.assertTrue(""Continuous scheduling should be enabled."", fs.isContinuousSchedulingEnabled());
    RMNode node1 = MockNodes.newNodeInfo(1, Resources.createResource(8 * 1024, 8), 1, ""127.0.0.1"");
    NodeAddedSchedulerEvent nodeEvent1 = new NodeAddedSchedulerEvent(node1);
    fs.handle(nodeEvent1);
    Assert.assertEquals(fs.getClusterCapacity().getMemory(), 8 * 1024);
    Assert.assertEquals(fs.getClusterCapacity().getVirtualCores(), 8);
    ApplicationAttemptId appAttemptId = createAppAttemptId(this.APP_ID++, this.ATTEMPT_ID++);
    fs.addApplication(appAttemptId, ""queue11"", ""user11"");
    List<ResourceRequest> ask = new ArrayList<ResourceRequest>();
    ResourceRequest request = createResourceRequest(1024, 1, ANY, 1, 1, true);
    ask.add(request);
    fs.allocate(appAttemptId, ask, new ArrayList<ContainerId>(), null, null);
    Thread.sleep(fs.getConf().getContinuousSchedulingSleepMs() + 500);
    Resource consumption = fs.applications.get(appAttemptId).getCurrentConsumption();
    Assert.assertEquals(1024, consumption.getMemory());
    Assert.assertEquals(1, consumption.getVirtualCores());
}"
"@Test
public void testInterruptsOnLFSRead() throws Exception {
    final Ignite ignite = startGrid();
    ignite.active(true);
    final int valLen = 8192;
    final byte[] payload = new byte[valLen];
    final int maxKey = 10000;
    Thread[] workers = new Thread[THREADS_CNT];
    final IgniteCache<Object, Object> cache = ignite.cache(CACHE_NAME);
    for (int i = 0; i < maxKey; i++) {
        cache.put(i, payload);
    }
    final AtomicReference<Throwable> fail = new AtomicReference<>();
    Runnable clo = new Runnable() {
        @Override
        public void run() {
            cache.get(ThreadLocalRandom.current().nextInt(maxKey / 5));
        }
    };
    for (int i = 0; i < workers.length; i++) {
        workers[i] = new Thread(clo);
        workers[i].setName(""reader-"" + i);
        workers[i].setUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler() {
            @Override
            public void uncaughtException(Thread t, Throwable e) {
                fail.compareAndSet(null, e);
            }
        });
    }
    for (Thread worker : workers) {
        worker.start();
    }
    for (int i = 0; i < (workers.length / 2); i++) {
        workers[i].interrupt();
    }
    Thread.sleep(3000);
    stop = true;
    for (Thread worker : workers) {
        worker.join();
    }
    Throwable t = fail.get();
    assertNull(t);
    int verifiedKeys = 0;
    for (int i = 0; i < maxKey; i++) {
        byte[] val = ((byte[]) (cache.get(i)));
        if (val != null) {
            assertEquals(""Illegal length"", valLen, val.length);
            verifiedKeys++;
        }
    }
}"
"@Test
public void shouldTerminateWhenFutureIsCancelled() throws InterruptedException {
    GracefulExecutorServicesShutdown shutdown = GracefulExecutorServicesShutdown.initiate();
    shutdown.timeout(Duration.ofMillis(15000));
    ExecutorService executorService = mock(ExecutorService.class);
    when(executorService.isShutdown()).thenReturn(true);
    AtomicBoolean terminated = new AtomicBoolean();
    AtomicBoolean awaitTerminationInterrupted = new AtomicBoolean();
    when(executorService.isTerminated()).thenAnswer(invocation -> terminated.get());
    when(executorService.awaitTermination(anyLong(), any())).thenAnswer(invocation  -> {
        long timeout = invocation.getArgument(0);
        TimeUnit unit = invocation.getArgument(1);
        try {
            Thread.sleep(unit.toMillis(timeout));
        } catch (InterruptedException e) {
            awaitTerminationInterrupted.set(true);
            Thread.currentThread().interrupt();
            throw e;
        }
        throw new IllegalStateException(""Thread.sleep should have been interrupted"");
    });
    when(executorService.shutdownNow()).thenAnswer(invocation -> {
        terminated.set(true);
        return null;
    });
    shutdown.shutdown(executorService);
    CompletableFuture<Void> future = shutdown.handle();
    future.cancel(false);
    Awaitility.await().untilAsserted(() -> assertTrue(awaitTerminationInterrupted.get(),
    ""awaitTermination should have been interrupted""));
    verify(executorService, times(1)).awaitTermination(anyLong(), any());
    verify(executorService, times(1)).shutdownNow();
}"
"@Test
public void testForceMetadataRefreshForPatternSubscriptionDuringRebalance() {
    final String consumerId = ""consumer"";
    subscriptions.subscribe(Pattern.compile("".*""), rebalanceListener);
    client.updateMetadata(TestUtils.metadataUpdateWith(1, singletonMap(topic1, 1)));
    assertEquals(singleton(topic1), subscriptions.subscription());
    client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));
    coordinator.ensureCoordinatorReady(time.timer(Long.MAX_VALUE));
    client.prepareMetadataUpdate(metadataResponse);
    client.prepareResponse(joinGroupFollowerResponse(1, consumerId, ""leader"", Errors.NONE));
    client.prepareResponse(new MockClient.RequestMatcher() {
        @Override
        public boolean matches(AbstractRequest body) {
            SyncGroupRequest sync = (SyncGroupRequest) body;
            return sync.memberId().equals(consumerId) &&
            sync.generationId() == 1 &&
            sync.groupAssignment().isEmpty();
        }
    }, syncGroupResponse(singletonList(t1p), Errors.NONE));
    partitionAssignor.prepare(singletonMap(consumerId, singletonList(t1p)));
    coordinator.poll(time.timer(Long.MAX_VALUE));
    final Set<String> updatedSubscriptionSet = new HashSet<>(Arrays.asList(topic1, topic2));
    assertEquals(updatedSubscriptionSet, subscriptions.subscription());
    metadata.requestUpdate();
    client.poll(Long.MAX_VALUE, time.milliseconds());
    assertFalse(coordinator.rejoinNeededOrPending());
}"
"@Test
public void testTrackMetadata_rowMarkerDelete() throws Throwable
{
    createTable(""CREATE TABLE %s (a int, PRIMARY KEY (a))"");
    ColumnFamilyStore cfs = Keyspace.open(keyspace()).getColumnFamilyStore(currentTable());
    execute(""DELETE FROM %s USING TIMESTAMP 9999 WHERE a=1"");
    cfs.forceBlockingFlush();
    assertEquals(1, cfs.getLiveSSTables().size());
    StatsMetadata metadata = cfs.getLiveSSTables().iterator().next().getSSTableMetadata();
    assertEquals(9999, metadata.minTimestamp);
    assertEquals(9999, metadata.maxTimestamp);
    assertEquals(System.currentTimeMillis()/1000, metadata.maxLocalDeletionTime, 5);
    cfs.forceMajorCompaction();
    StatsMetadata metadata2 = cfs.getLiveSSTables().iterator().next().getSSTableMetadata();
    assertEquals(metadata.maxLocalDeletionTime, metadata2.maxLocalDeletionTime);
    assertEquals(metadata.minTimestamp, metadata2.minTimestamp);
    assertEquals(metadata.maxTimestamp, metadata2.maxTimestamp);
}"
"@Test
public void testTrackMetadata_rowTombstone() throws Throwable
{
    createTable(""CREATE TABLE %s (a int, b int, c text, PRIMARY KEY (a, b))"");
    ColumnFamilyStore cfs = Keyspace.open(keyspace()).getColumnFamilyStore(currentTable());
    execute(""DELETE FROM %s USING TIMESTAMP 9999 WHERE a = 1"");
    cfs.forceBlockingFlush();
    assertEquals(1, cfs.getLiveSSTables().size());
    StatsMetadata metadata = cfs.getLiveSSTables().iterator().next().getSSTableMetadata();
    assertEquals(9999, metadata.minTimestamp);
    assertEquals(9999, metadata.maxTimestamp);
    assertEquals(System.currentTimeMillis()/1000, metadata.maxLocalDeletionTime, 5);
    assertEquals(nowInSec(), metadata.maxLocalDeletionTime, DELTA);
    cfs.forceMajorCompaction();
    StatsMetadata metadata2 = cfs.getLiveSSTables().iterator().next().getSSTableMetadata();
    assertEquals(metadata.maxLocalDeletionTime, metadata2.maxLocalDeletionTime);
    assertEquals(metadata.minTimestamp, metadata2.minTimestamp);
    assertEquals(metadata.maxTimestamp, metadata2.maxTimestamp);
}"
"@Test
public void testSetName() throws Exception {
    Configuration conf = new Configuration();
    WritableName.setName(SimpleWritable.class, testName);
    Class<?> test = WritableName.getClass(testName, conf);
    assertTrue(test.equals(SimpleWritable.class));
}"
"@Test
public void testStartStop() {
    final KafkaStream<String, String> kafkaStream = PowerMock.createStrictMock(KafkaStream.class);
    final ConsumerIterator<String, String> consumerIterator = PowerMock.createStrictMock(ConsumerIterator.class);
    final ConsumerConnector consumerConnector = PowerMock.createStrictMock(ConsumerConnector.class);
    EasyMock.expect(consumerConnector.createMessageStreamsByFilter(EasyMock.anyObject(TopicFilter.class), EasyMock.anyInt(), EasyMock.eq(DEFAULT_STRING_DECODER), EasyMock.eq(DEFAULT_STRING_DECODER))).andReturn(ImmutableList.of(kafkaStream)).once();
    EasyMock.expect(kafkaStream.iterator()).andReturn(consumerIterator).anyTimes();
    EasyMock.expect(consumerIterator.hasNext()).andAnswer(getBlockingAnswer()).anyTimes();
    EasyMock.expect(cacheManager.createCache()).andReturn(cacheHandler).once();
    EasyMock.expect(cacheHandler.getCache()).andReturn(new ConcurrentHashMap<String, String>()).once();
    cacheHandler.close();
    EasyMock.expectLastCall();
    final AtomicBoolean threadWasInterrupted = new AtomicBoolean(false);
    consumerConnector.shutdown();
    EasyMock.expectLastCall().andAnswer(new IAnswer<Object>() {
        @Override
        public Object answer() {
            threadWasInterrupted.set(Thread.currentThread().isInterrupted());
            return null;
        }
    }).times(2);
    PowerMock.replay(cacheManager, cacheHandler, kafkaStream, consumerConnector, consumerIterator);
    final KafkaLookupExtractorFactory factory = new KafkaLookupExtractorFactory(cacheManager, TOPIC, ImmutableMap.of(""zookeeper.connect"", ""localhost""), 10000L, false) {
        @Override
        ConsumerConnector buildConnector(Properties properties) {
            return consumerConnector;
        }
    };
    Assert.assertTrue(factory.start());
    Assert.assertTrue(factory.close());
    Assert.assertTrue(factory.getFuture().isDone());
    Assert.assertFalse(threadWasInterrupted.get());
    PowerMock.verify(cacheManager, cacheHandler);
}"
"@Test
public void testAddAndRetrieve() throws Exception {
    PeerCache cache = PeerCache.getInstance(3, 100000);
    DatanodeID dnId = new DatanodeID(""192.168.0.1"",
    ""fakehostname"", ""fake_storage_id"",
    100, 101, 102);
    FakePeer peer = new FakePeer(dnId, false);
    cache.put(dnId, peer);
    assertTrue(!peer.isClosed());
    assertEquals(1, cache.size());
    assertEquals(peer, cache.get(dnId, false));
    assertEquals(0, cache.size());
    cache.close();
}"
"@Test
public void testSystemMetadataRetrieval() throws Exception {
    appClient.deploy(DEFAULT, createAppJarFile(AllProgramsApp.class));
    Id.Stream streamId = Stream.from(DEFAULT, STREAM_NAME);
    Set<String> streamSystemTags = getTags(streamId, SYSTEM);
    Assert.assertEquals(ImmutableSet.of(STREAM_NAME), streamSystemTags);
    Map<String, String> streamSystemProperties = getProperties(streamId, SYSTEM);
    final String creationTime = ""creation-time"";
    String description = ""description"";
    String schema = ""schema"";
    String ttl = ""ttl"";
    Assert.assertTrue(""Expected creation time to exist but it does not"", streamSystemProperties.containsKey(creationTime));
    long createTime = Long.parseLong(streamSystemProperties.get(creationTime));
    Assert.assertTrue(""Stream create time should be within the last hour - "" + createTime, createTime > (System.currentTimeMillis() - TimeUnit.HOURS.toMillis(1)));
    Assert.assertEquals(ImmutableMap.of(schema, Schema.recordOf(""stringBody"", Field.of(""body"", Schema.of(STRING))).toString(), ttl, String.valueOf(Long.MAX_VALUE), description, ""test stream"", creationTime, String.valueOf(createTime)), streamSystemProperties);
    long newTtl = 100000L;
    streamClient.setStreamProperties(streamId, new StreamProperties(newTtl, null, null));
    streamSystemProperties = getProperties(streamId, SYSTEM);
    Assert.assertEquals(ImmutableMap.of(schema, Schema.recordOf(""stringBody"", Field.of(""body"", Schema.of(STRING))).toString(), ttl, String.valueOf(newTtl * 1000), description, ""test stream"", creationTime, String.valueOf(createTime)), streamSystemProperties);
    Set<MetadataRecord> streamSystemMetadata = getMetadata(streamId, SYSTEM);
    Assert.assertEquals(ImmutableSet.of(new MetadataRecord(streamId, MetadataScope.SYSTEM, streamSystemProperties, streamSystemTags)), streamSystemMetadata);
    Id.Stream.View view = View.from(streamId, ""view"");
    Schema viewSchema = Schema.recordOf(""record"", Field.of(""viewBody"", Schema.nullableOf(Schema.of(BYTES))));
    streamViewClient.createOrUpdate(view, new ViewSpecification(new FormatSpecification(""format"", viewSchema)));
    Set<String> viewSystemTags = getTags(view, SYSTEM);
    Assert.assertEquals(ImmutableSet.of(""view"", STREAM_NAME), viewSystemTags);
    Map<String, String> viewSystemProperties = getProperties(view, SYSTEM);
    Assert.assertEquals(viewSchema.toString(), viewSystemProperties.get(schema));
    ImmutableSet<String> viewUserTags = ImmutableSet.of(""viewTag"");
    addTags(view, viewUserTags);
    Assert.assertEquals(ImmutableSet.of(new MetadataRecord(view, MetadataScope.USER, ImmutableMap.<String, String>of(), viewUserTags), new MetadataRecord(view, MetadataScope.SYSTEM, viewSystemProperties, viewSystemTags)), getMetadata(view));
    Id.DatasetInstance datasetInstance = DatasetInstance.from(DEFAULT, DATASET_NAME);
    Set<String> dsSystemTags = getTags(datasetInstance, SYSTEM);
    Assert.assertEquals(ImmutableSet.of(DATASET_NAME, BATCH_TAG, EXPLORE_TAG), dsSystemTags);
    Map<String, String> dsSystemProperties = getProperties(datasetInstance, SYSTEM);
    Assert.assertTrue(""Expected creation time to exist but it does not"", dsSystemProperties.containsKey(creationTime));
    createTime = Long.parseLong(dsSystemProperties.get(creationTime));
    Assert.assertTrue(""Dataset create time should be within the last hour - "" + createTime, createTime > (System.currentTimeMillis() - TimeUnit.HOURS.toMillis(1)));
    Assert.assertEquals(ImmutableMap.of(""type"", KeyValueTable.class.getName(), description, ""test dataset"", creationTime, String.valueOf(createTime)), dsSystemProperties);
    datasetClient.update(datasetInstance, ImmutableMap.of(PROPERTY_TTL, ""100000""));
    dsSystemProperties = getProperties(datasetInstance, SYSTEM);
    Assert.assertEquals(ImmutableMap.of(""type"", KeyValueTable.class.getName(), description, ""test dataset"", ttl, ""100000"", creationTime, String.valueOf(createTime)), dsSystemProperties);
    Id.Artifact artifactId = getArtifactId();
    Assert.assertEquals(ImmutableSet.of(new MetadataRecord(artifactId, MetadataScope.SYSTEM, ImmutableMap.<String, String>of(), ImmutableSet.of(AllProgramsApp.class.getSimpleName()))), getMetadata(artifactId, SYSTEM));
    Id.Application app = Application.from(DEFAULT, NAME);
    Assert.assertEquals(ImmutableMap.builder().put((FLOW.getPrettyName() + MetadataDataset.KEYVALUE_SEPARATOR) + NoOpFlow.NAME, NAME).put((MAPREDUCE.getPrettyName() + MetadataDataset.KEYVALUE_SEPARATOR) + NoOpMR.NAME, NAME).put((MAPREDUCE.getPrettyName() + MetadataDataset.KEYVALUE_SEPARATOR) + NoOpMR2.NAME, NAME).put((SERVICE.getPrettyName() + MetadataDataset.KEYVALUE_SEPARATOR) + NoOpService.NAME, NAME).put((SPARK.getPrettyName() + MetadataDataset.KEYVALUE_SEPARATOR) + NoOpSpark.NAME, NAME).put((WORKER.getPrettyName() + MetadataDataset.KEYVALUE_SEPARATOR) + NoOpWorker.NAME, NAME).put((WORKFLOW.getPrettyName() + MetadataDataset.KEYVALUE_SEPARATOR) + NoOpWorkflow.NAME, NAME).put((""schedule"" + MetadataDataset.KEYVALUE_SEPARATOR) + AllProgramsApp.SCHEDULE_NAME, (AllProgramsApp.SCHEDULE_NAME + MetadataDataset.KEYVALUE_SEPARATOR) + AllProgramsApp.SCHEDULE_DESCRIPTION).build(), getProperties(app, SYSTEM));
    Assert.assertEquals(ImmutableSet.of(AllProgramsApp.class.getSimpleName(), NAME), getTags(app, SYSTEM));
    assertProgramSystemMetadata(Program.from(app, FLOW, NAME), ""Realtime"");
    assertProgramSystemMetadata(Program.from(app, WORKER, NAME), ""Realtime"");
    assertProgramSystemMetadata(Program.from(app, SERVICE, NAME), ""Realtime"");
    assertProgramSystemMetadata(Program.from(app, MAPREDUCE, NAME), ""Batch"");
    assertProgramSystemMetadata(Program.from(app, SPARK, NAME), ""Batch"");
    assertProgramSystemMetadata(Program.from(app, WORKFLOW, NAME), ""Batch"");
}"
"@Test
public void testRead() throws Exception {
    FileSystem fs = cluster.getFileSystem();
    long tStart = System.currentTimeMillis();
    bench.readTest(fs);
    long execTime = System.currentTimeMillis() - tStart;
    bench.analyzeResult(fs, TestType.TEST_TYPE_READ, execTime);
}"
"@Test
void writesAndReadsClassContainingCustomConvertedObjects() {
    List<Object> converters = new ArrayList<>();
    converters.add(BigDecimalToStringConverter.INSTANCE);
    converters.add(StringToBigDecimalConverter.INSTANCE);
    CustomConversions customConversions = new CouchbaseCustomConversions(converters);
    converter.setCustomConversions(customConversions);
    converter.afterPropertiesSet();
    ((CouchbaseMappingContext) (converter.getMappingContext())).setSimpleTypeHolder(customConversions.getSimpleTypeHolder());
    CouchbaseDocument converted = new CouchbaseDocument();
    final String weightStr = ""12.34"";
    final BigDecimal weight = new BigDecimal(weightStr);
    final CustomObject addy = new CustomObject(weight);
    List<CustomObject> listOfObjects = new ArrayList<>();
    listOfObjects.add(addy);
    Map<String, CustomObject> mapOfObjects = new HashMap<>();
    mapOfObjects.put(""obj0"", addy);
    mapOfObjects.put(""obj1"", addy);
    CustomObjectEntity entity = new CustomObjectEntity(addy, listOfObjects, mapOfObjects);
    converter.write(entity, converted);
    CouchbaseDocument source = new CouchbaseDocument();
    source.put(""_class"", CustomObjectEntity.class.getName());
    CouchbaseDocument objectDoc = new CouchbaseDocument();
    objectDoc.put(""weight"", weightStr);
    source.put(""object"", objectDoc);
    CouchbaseList listOfObjectsDoc = new CouchbaseList();
    listOfObjectsDoc.put(objectDoc);
    source.put(""listOfObjects"", listOfObjectsDoc);
    CouchbaseDocument mapOfObjectsDoc = new CouchbaseDocument();
    mapOfObjectsDoc.put(""obj0"", objectDoc);
    mapOfObjectsDoc.put(""obj1"", objectDoc);
    source.put(""mapOfObjects"", mapOfObjectsDoc);
    assertThat(converted.export().toString()).isEqualTo(source.export().toString());
    CustomObjectEntity readConverted = converter.read(CustomObjectEntity.class, source);
    assertThat(readConverted.object.weight).isEqualTo(addy.weight);
    assertThat(readConverted.listOfObjects.get(0).weight).isEqualTo(listOfObjects.get(0).weight);
    assertThat(readConverted.mapOfObjects.get(""obj0"").weight).isEqualTo(mapOfObjects.get(""obj0"").weight);
    assertThat(readConverted.mapOfObjects.get(""obj1"").weight).isEqualTo(mapOfObjects.get(""obj1"").weight);
}"
"@Test
public void testHftpCustomUriPortWithDefaultPorts() throws IOException {
    resetFileSystem();
    Configuration conf = new Configuration();
    URI uri = URI.create() ;
    HftpFileSystem fs = ((HftpFileSystem) (FileSystem.get(uri, conf)));
    assertEquals(DFS_NAMENODE_HTTP_PORT_DEFAULT, fs.getDefaultPort());
    assertEquals(DFS_NAMENODE_HTTPS_PORT_DEFAULT, fs.getDefaultSecurePort());
    assertEquals(uri, fs.getUri());
    assertEquals(""127.0.0.1:"" + DFSConfigKeys.DFS_NAMENODE_HTTPS_PORT_DEFAULT, fs.getCanonicalServiceName());
}"
"@Test
public void testAppWithServices() throws Exception {
    ApplicationManager applicationManager = deployApplication(AppWithServices.class);
    LOG.info(""Deployed."");
    ServiceManager serviceManager = applicationManager.getServiceManager(AppWithServices.SERVICE_NAME).start();
    serviceManager.waitForStatus(true);
    LOG.info(""Service Started"");
    URL serviceURL = serviceManager.getServiceURL(15, TimeUnit.SECONDS);
    Assert.assertNotNull(serviceURL);
    URL url = new URL(serviceURL, ""ping2"");
    HttpRequest request = HttpRequest.get(url).build();
    HttpResponse response = HttpRequests.execute(request);
    Assert.assertEquals(200, response.getResponseCode());
    url = new URL(serviceURL, ""failure"");
    request = HttpRequest.get(url).build();
    response = HttpRequests.execute(request);
    Assert.assertEquals(500, response.getResponseCode());
    Assert.assertTrue(response.getResponseBodyAsString().contains(""Exception""));
    url = new URL(serviceURL, ""verifyClassLoader"");
    request = HttpRequest.get(url).build();
    response = HttpRequests.execute(request);
    Assert.assertEquals(200, response.getResponseCode());
    RuntimeMetrics serviceMetrics = serviceManager.getMetrics();
    serviceMetrics.waitForinput(3, 5, TimeUnit.SECONDS);
    Assert.assertEquals(3, serviceMetrics.getInput());
    Assert.assertEquals(2, serviceMetrics.getProcessed());
    Assert.assertEquals(1, serviceMetrics.getException());
    RuntimeMetrics handlerMetrics = getMetricsManager().getServiceHandlerMetrics(Id.Namespace.DEFAULT.getId(),
    AppWithServices.APP_NAME,
    AppWithServices.SERVICE_NAME,
    AppWithServices.SERVICE_NAME);
    handlerMetrics.waitForinput(3, 5, TimeUnit.SECONDS);
    Assert.assertEquals(3, handlerMetrics.getInput());
    Assert.assertEquals(2, handlerMetrics.getProcessed());
    Assert.assertEquals(1, handlerMetrics.getException());
    LOG.info(""DatasetUpdateService Started"");
    Map<String, String> args
    = ImmutableMap.of(AppWithServices.WRITE_VALUE_RUN_KEY, AppWithServices.DATASET_TEST_VALUE,
    AppWithServices.WRITE_VALUE_STOP_KEY, AppWithServices.DATASET_TEST_VALUE_STOP);
    ServiceManager datasetWorkerServiceManager = applicationManager
    .getServiceManager(AppWithServices.DATASET_WORKER_SERVICE_NAME).start(args);
    WorkerManager datasetWorker =
    applicationManager.getWorkerManager(AppWithServices.DATASET_UPDATE_WORKER).start(args);
    datasetWorkerServiceManager.waitForStatus(true);
    ServiceManager noopManager = applicationManager.getServiceManager(""NoOpService"").start();
    serviceManager.waitForStatus(true, 2, 1);
    String result = callServiceGet(noopManager.getServiceURL(), ""ping/"" + AppWithServices.DATASET_TEST_KEY);
    String decodedResult = new Gson().fromJson(result, String.class);
    Assert.assertEquals(AppWithServices.DATASET_TEST_VALUE, decodedResult);
    handlerMetrics = getMetricsManager().getServiceHandlerMetrics(Id.Namespace.DEFAULT.getId(),
    AppWithServices.APP_NAME,
    ""NoOpService"",
    ""NoOpHandler"");
    handlerMetrics.waitForinput(1, 5, TimeUnit.SECONDS);
    Assert.assertEquals(1, handlerMetrics.getInput());
    Assert.assertEquals(1, handlerMetrics.getProcessed());
    Assert.assertEquals(0, handlerMetrics.getException());
    String path = String.format(""discover/%s/%s"",
    AppWithServices.APP_NAME, AppWithServices.DATASET_WORKER_SERVICE_NAME);
    url = new URL(serviceURL, path);
    request = HttpRequest.get(url).build();
    response = HttpRequests.execute(request);
    Assert.assertEquals(200, response.getResponseCode());
    datasetWorker.stop();
    datasetWorkerServiceManager.stop();
    datasetWorkerServiceManager.waitForStatus(false);
    LOG.info(""DatasetUpdateService Stopped"");
    serviceManager.stop();
    serviceManager.waitForStatus(false);
    LOG.info(""ServerService Stopped"");
    result = callServiceGet(noopManager.getServiceURL(), ""ping/"" + AppWithServices.DATASET_TEST_KEY_STOP);
    decodedResult = new Gson().fromJson(result, String.class);
    Assert.assertEquals(AppWithServices.DATASET_TEST_VALUE_STOP, decodedResult);
    result = callServiceGet(noopManager.getServiceURL(), ""ping/"" + AppWithServices.DATASET_TEST_KEY_STOP_2);
    decodedResult = new Gson().fromJson(result, String.class);
    Assert.assertEquals(AppWithServices.DATASET_TEST_VALUE_STOP_2, decodedResult);
}"
"@Test
public void testAdditionalModelTypeAnnotationsSemiColon() throws Exception {
    OpenAPI openAPI = TestUtils.createOpenAPI();
    final AbstractJavaCodegen codegen = new P_AbstractJavaCodegen();
    codegen.additionalProperties().put(ADDITIONAL_MODEL_TYPE_ANNOTATIONS, ""@Foo;@Bar"");
    codegen.processOpts();
    codegen.preprocessOpenAPI(openAPI);
    final List<String> additionalModelTypeAnnotations = new ArrayList<String>();
    additionalModelTypeAnnotations.add(""@Foo"");
    additionalModelTypeAnnotations.add(""@Bar"");
    Assert.assertEquals(codegen.getAdditionalModelTypeAnnotations(), additionalModelTypeAnnotations);
}"
"@Test
public void testNoTestClassesInDeploymentAssembly()
throws InvocationTargetException, CoreException {
    CreateAppEngineWtpProject creator = new CreateAppEngineStandardWtpProject(config, adaptable);
    creator.execute(monitor);
    ProjectUtils.waitForProjects(project);
    assertNoTestClassesInDeploymentAssembly();
}
private void assertNoTestClassesInDeploymentAssembly() throws CoreException {
    StructureEdit core = StructureEdit.getStructureEditForRead(project);
    WorkbenchComponent component = core.getComponent();
    assertNotNull(component);
    boolean seenMainSourcePath = false;
    List<ComponentResource> resources = component.getResources();
    for (ComponentResource resource : resources) {
        assertFalse(containsSegment(resource.getSourcePath(), ""test""));
        if (resource.getSourcePath().equals(new Path(""/src/main/java""))
        && resource.getRuntimePath().equals(new Path(""/WEB-INF/classes""))) {
            seenMainSourcePath = true;
        }
    }
    assertTrue(seenMainSourcePath);
}"
"@Test
public void testingUserServiceGaugeMetrics() throws Exception {
    MetricsCollector collector =
    collectionService.getCollector(getUserServiceContext(Constants.DEFAULT_NAMESPACE, ""WordCount"", ""CounterService"",
    ""CountRunnable""));
    collector.increment(""gmetric"", 1);
    collector.gauge(""gmetric"", 10);
    collector.increment(""gmetric"", 1);
    TimeUnit.SECONDS.sleep(1);
    collector.gauge(""gmetric"", 10);
    TimeUnit.SECONDS.sleep(2);
    String runnableRequest =
    ""/system/apps/WordCount/services/CounterService/runnables/CountRunnable/gmetric?aggregate=true"";
    String serviceRequest =
    ""/system/apps/WordCount/services/CounterService/gmetric?aggregate=true"";
    testSingleMetric(runnableRequest, 10);
    testSingleMetric(serviceRequest, 10);
}"
"@Test
public void testExpiry() throws Exception {
    final int CAPACITY = 3;
    final int EXPIRY_PERIOD = 10;
    PeerCache cache = PeerCache.getInstance(CAPACITY, EXPIRY_PERIOD);
    DatanodeID dnIds[] = new DatanodeID[CAPACITY];
    FakePeer peers[] = new FakePeer[CAPACITY];
    for (int i = 0; i < CAPACITY; ++i) {
        dnIds[i] = new DatanodeID(""192.168.0.1"",
        ""fakehostname_"" + i, ""fake_storage_id"",
        100, 101, 102);
        peers[i] = new FakePeer(dnIds[i], false);
    }
    for (int i = 0; i < CAPACITY; ++i) {
        cache.put(dnIds[i], peers[i]);
    }
    Thread.sleep(EXPIRY_PERIOD * 50);
    assertEquals(0, cache.size());
    for (int i = 0; i < CAPACITY; ++i) {
        assertTrue(peers[i].isClosed());
    }
    Thread.sleep(EXPIRY_PERIOD * 50);
    cache.close();
}"
"@Test
public void testValidate_badXml() throws IOException, CoreException {
    XmlValidator validator = new XmlValidator();
    validator.setHelper(new AppEngineWebXmlValidator());
    IFile file = createBogusProjectFile();
    byte[] badXml = BAD_XML.getBytes(StandardCharsets.UTF_8);
    validator.validate(file, badXml);
    IMarker[] emptyMarkers =
    ProjectUtils.waitUntilNoMarkersFound(file, PROBLEM, true, DEPTH_ZERO);
    ArrayAssertions.assertIsEmpty(emptyMarkers);
}"
"@Test
public void testBuildTokenServiceSockAddr() {
    assertEquals(""127.0.0.1:123"", SecurityUtil.buildTokenService(new InetSocketAddress(""LocalHost"", 123)).toString());
    assertEquals(""127.0.0.1:123"", SecurityUtil.buildTokenService(new InetSocketAddress(""127.0.0.1"", 123)).toString());
    assertEquals(""127.0.0.1:123"", SecurityUtil.buildTokenService(NetUtils.createSocketAddr(""127.0.0.1"", 123)).toString());
}"
"@Test
public void testWrite() throws Exception {
    FileSystem fs = cluster.getFileSystem();
    long tStart = System.currentTimeMillis();
    bench.writeTest(fs);
    long execTime = System.currentTimeMillis() - tStart;
    bench.analyzeResult(fs, TestType.TEST_TYPE_WRITE, execTime);
}"
"@Test
public void socketTest() throws Exception {
    URI consumeUri = URI.create(CONSUME_URI);
    URI produceUri = URI.create(PRODUCE_URI);
    WebSocketClient consumeClient = new WebSocketClient();
    SimpleConsumerSocket consumeSocket = new SimpleConsumerSocket();
    WebSocketClient produceClient = new WebSocketClient();
    SimpleProducerSocket produceSocket = new SimpleProducerSocket();
    try {
        consumeClient.start();
        ClientUpgradeRequest consumeRequest = new ClientUpgradeRequest();
        Future<Session> consumerFuture = consumeClient.connect(consumeSocket, consumeUri, consumeRequest);
        log.info(""Connecting to : {}"", consumeUri);
        ClientUpgradeRequest produceRequest = new ClientUpgradeRequest();
        produceClient.start();
        Future<Session> producerFuture = produceClient.connect(produceSocket, produceUri, produceRequest);
        Thread.sleep(1000);
        Assert.assertTrue(consumerFuture.get().isOpen());
        Assert.assertTrue(producerFuture.get().isOpen());
        consumeSocket.awaitClose(1, TimeUnit.SECONDS);
        produceSocket.awaitClose(1, TimeUnit.SECONDS);
        Assert.assertTrue(produceSocket.getBuffer().size() > 0);
        Assert.assertEquals(produceSocket.getBuffer(), consumeSocket.getBuffer());
    } finally {
        try {
            consumeClient.stop();
            produceClient.stop();
        } catch (Exception e) {
            log.error(e.getMessage());
        }
    }
}"
"  @Test(expected = IllegalArgumentException.class)
  public void testConstructor1() throws IOException {
    new OffsetRange(0, 0);
  }
"
"  @Test(expected = IllegalArgumentException.class)
  public void testConstructor2() throws IOException {
    new OffsetRange(-1, 0);
  }
"
"  @Test(expected = IllegalArgumentException.class)
  public void testConstructor3() throws IOException {
    new OffsetRange(-3, -1);
  }
"
"  @Test(expected = IllegalArgumentException.class)
  public void testConstructor4() throws IOException {
    new OffsetRange(-3, 100);
  }
"
"  @Test
  public void testCompare() throws IOException {
    OffsetRange r1 = new OffsetRange(0, 1);
    OffsetRange r2 = new OffsetRange(1, 3);
    OffsetRange r3 = new OffsetRange(1, 3);
    OffsetRange r4 = new OffsetRange(3, 4);

    assertEquals(0, OffsetRange.ReverseComparatorOnMin.compare(r2, r3));
    assertEquals(0, OffsetRange.ReverseComparatorOnMin.compare(r2, r2));
    assertTrue(OffsetRange.ReverseComparatorOnMin.compare(r2, r1) < 0);
    assertTrue(OffsetRange.ReverseComparatorOnMin.compare(r2, r4) > 0);
  }
"
"  @Test
  public void testReaddirBasic() throws IOException {
    // Get inodeId of /tmp
    HdfsFileStatus status = nn.getRpcServer().getFileInfo(testdir);
    long dirId = status.getFileId();
    int namenodeId = Nfs3Utils.getNamenodeId(config);

    // Create related part of the XDR request
    XDR xdr_req = new XDR();
    FileHandle handle = new FileHandle(dirId, namenodeId);
    handle.serialize(xdr_req);
    xdr_req.writeLongAsHyper(0); // cookie
    xdr_req.writeLongAsHyper(0); // verifier
    xdr_req.writeInt(100); // count

    READDIR3Response response = nfsd.readdir(xdr_req.asReadOnlyWrap(),
        securityHandler, new InetSocketAddress(""localhost"", 1234));
    List<Entry3> dirents = response.getDirList().getEntries();
    assertTrue(dirents.size() == 5); // inculding dot, dotdot

    // Test start listing from f2
    status = nn.getRpcServer().getFileInfo(testdir + ""/f2"");
    long f2Id = status.getFileId();

    // Create related part of the XDR request
    xdr_req = new XDR();
    handle = new FileHandle(dirId, namenodeId);
    handle.serialize(xdr_req);
    xdr_req.writeLongAsHyper(f2Id); // cookie
    xdr_req.writeLongAsHyper(0); // verifier
    xdr_req.writeInt(100); // count

    response = nfsd.readdir(xdr_req.asReadOnlyWrap(), securityHandler,
        new InetSocketAddress(""localhost"", 1234));
    dirents = response.getDirList().getEntries();
    assertTrue(dirents.size() == 1);
    Entry3 entry = dirents.get(0);
    assertTrue(entry.getName().equals(""f3""));

    // When the cookie is deleted, list starts over no including dot, dotdot
    hdfs.delete(new Path(testdir + ""/f2""), false);

    response = nfsd.readdir(xdr_req.asReadOnlyWrap(), securityHandler,
        new InetSocketAddress(""localhost"", 1234));
    dirents = response.getDirList().getEntries();
    assertTrue(dirents.size() == 2); // No dot, dotdot
  }
"
"  @Test
  public void testReaddirPlus() throws IOException {
    // Get inodeId of /tmp
    HdfsFileStatus status = nn.getRpcServer().getFileInfo(testdir);
    long dirId = status.getFileId();
    int namenodeId = Nfs3Utils.getNamenodeId(config);
    
    // Create related part of the XDR request
    XDR xdr_req = new XDR();
    FileHandle handle = new FileHandle(dirId, namenodeId);
    handle.serialize(xdr_req);
    xdr_req.writeLongAsHyper(0); // cookie
    xdr_req.writeLongAsHyper(0); // verifier
    xdr_req.writeInt(100); // dirCount
    xdr_req.writeInt(1000); // maxCount

    READDIRPLUS3Response responsePlus = nfsd.readdirplus(xdr_req
        .asReadOnlyWrap(), securityHandler, new InetSocketAddress(""localhost"",
        1234));
    List<EntryPlus3> direntPlus = responsePlus.getDirListPlus().getEntries();
    assertTrue(direntPlus.size() == 5); // including dot, dotdot

    // Test start listing from f2
    status = nn.getRpcServer().getFileInfo(testdir + ""/f2"");
    long f2Id = status.getFileId();

    // Create related part of the XDR request
    xdr_req = new XDR();
    handle = new FileHandle(dirId, namenodeId);
    handle.serialize(xdr_req);
    xdr_req.writeLongAsHyper(f2Id); // cookie
    xdr_req.writeLongAsHyper(0); // verifier
    xdr_req.writeInt(100); // dirCount
    xdr_req.writeInt(1000); // maxCount

    responsePlus = nfsd.readdirplus(xdr_req.asReadOnlyWrap(), securityHandler,
        new InetSocketAddress(""localhost"", 1234));
    direntPlus = responsePlus.getDirListPlus().getEntries();
    assertTrue(direntPlus.size() == 1);
    EntryPlus3 entryPlus = direntPlus.get(0);
    assertTrue(entryPlus.getName().equals(""f3""));

    // When the cookie is deleted, list starts over no including dot, dotdot
    hdfs.delete(new Path(testdir + ""/f2""), false);

    responsePlus = nfsd.readdirplus(xdr_req.asReadOnlyWrap(), securityHandler,
        new InetSocketAddress(""localhost"", 1234));
    direntPlus = responsePlus.getDirListPlus().getEntries();
    assertTrue(direntPlus.size() == 2); // No dot, dotdot
  }
"
"  @Test
  public void testAlterWriteRequest() throws IOException {
    int len = 20;
    byte[] data = new byte[len];
    ByteBuffer buffer = ByteBuffer.wrap(data);

    for (int i = 0; i < len; i++) {
      buffer.put((byte) i);
    }
    buffer.flip();
    int originalCount = buffer.array().length;
    WRITE3Request request = new WRITE3Request(new FileHandle(), 0, data.length,
        WriteStableHow.UNSTABLE, buffer);

    WriteCtx writeCtx1 = new WriteCtx(request.getHandle(), request.getOffset(),
        request.getCount(), WriteCtx.INVALID_ORIGINAL_COUNT,
        request.getStableHow(), request.getData(), null, 1, false,
        WriteCtx.DataState.NO_DUMP);

    Assert.assertTrue(writeCtx1.getData().array().length == originalCount);

    // Now change the write request
    OpenFileCtx.alterWriteRequest(request, 12);

    WriteCtx writeCtx2 = new WriteCtx(request.getHandle(), request.getOffset(),
        request.getCount(), originalCount, request.getStableHow(),
        request.getData(), null, 2, false, WriteCtx.DataState.NO_DUMP);
    ByteBuffer appendedData = writeCtx2.getData();

    int position = appendedData.position();
    int limit = appendedData.limit();
    Assert.assertTrue(position == 12);
    Assert.assertTrue(limit - position == 8);
    Assert.assertTrue(appendedData.get(position) == (byte) 12);
    Assert.assertTrue(appendedData.get(position + 1) == (byte) 13);
    Assert.assertTrue(appendedData.get(position + 2) == (byte) 14);
    Assert.assertTrue(appendedData.get(position + 7) == (byte) 19);

    // Test current file write offset is at boundaries
    buffer.position(0);
    request = new WRITE3Request(new FileHandle(), 0, data.length,
        WriteStableHow.UNSTABLE, buffer);
    OpenFileCtx.alterWriteRequest(request, 1);
    WriteCtx writeCtx3 = new WriteCtx(request.getHandle(), request.getOffset(),
        request.getCount(), originalCount, request.getStableHow(),
        request.getData(), null, 2, false, WriteCtx.DataState.NO_DUMP);
    appendedData = writeCtx3.getData();
    position = appendedData.position();
    limit = appendedData.limit();
    Assert.assertTrue(position == 1);
    Assert.assertTrue(limit - position == 19);
    Assert.assertTrue(appendedData.get(position) == (byte) 1);
    Assert.assertTrue(appendedData.get(position + 18) == (byte) 19);

    // Reset buffer position before test another boundary
    buffer.position(0);
    request = new WRITE3Request(new FileHandle(), 0, data.length,
        WriteStableHow.UNSTABLE, buffer);
    OpenFileCtx.alterWriteRequest(request, 19);
    WriteCtx writeCtx4 = new WriteCtx(request.getHandle(), request.getOffset(),
        request.getCount(), originalCount, request.getStableHow(),
        request.getData(), null, 2, false, WriteCtx.DataState.NO_DUMP);
    appendedData = writeCtx4.getData();
    position = appendedData.position();
    limit = appendedData.limit();
    Assert.assertTrue(position == 19);
    Assert.assertTrue(limit - position == 1);
    Assert.assertTrue(appendedData.get(position) == (byte) 19);
  }
"
"  @Test
  public void testCheckCommit() throws IOException {
    DFSClient dfsClient = Mockito.mock(DFSClient.class);
    Nfs3FileAttributes attr = new Nfs3FileAttributes();
    HdfsDataOutputStream fos = Mockito.mock(HdfsDataOutputStream.class);
    Mockito.when(fos.getPos()).thenReturn((long) 0);

    NfsConfiguration conf = new NfsConfiguration();
    conf.setBoolean(NfsConfigKeys.LARGE_FILE_UPLOAD, false);
    OpenFileCtx ctx = new OpenFileCtx(fos, attr, ""/dumpFilePath"", dfsClient,
        new ShellBasedIdMapping(conf), false, conf);

    COMMIT_STATUS ret;

    // Test inactive open file context
    ctx.setActiveStatusForTest(false);
    Channel ch = Mockito.mock(Channel.class);
    ret = ctx.checkCommit(dfsClient, 0, ch, 1, attr, false);
    Assert.assertTrue(ret == COMMIT_STATUS.COMMIT_INACTIVE_CTX);

    ctx.getPendingWritesForTest().put(new OffsetRange(5, 10),
        new WriteCtx(null, 0, 0, 0, null, null, null, 0, false, null));
    ret = ctx.checkCommit(dfsClient, 0, ch, 1, attr, false);
    Assert.assertTrue(ret == COMMIT_STATUS.COMMIT_INACTIVE_WITH_PENDING_WRITE);

    // Test request with non zero commit offset
    ctx.setActiveStatusForTest(true);
    Mockito.when(fos.getPos()).thenReturn((long) 10);
    ctx.setNextOffsetForTest(10);
    COMMIT_STATUS status = ctx.checkCommitInternal(5, null, 1, attr, false);
    Assert.assertTrue(status == COMMIT_STATUS.COMMIT_DO_SYNC);
    // Do_SYNC state will be updated to FINISHED after data sync
    ret = ctx.checkCommit(dfsClient, 5, ch, 1, attr, false);
    Assert.assertTrue(ret == COMMIT_STATUS.COMMIT_FINISHED);
    
    status = ctx.checkCommitInternal(10, ch, 1, attr, false);
    Assert.assertTrue(status == COMMIT_STATUS.COMMIT_DO_SYNC);
    ret = ctx.checkCommit(dfsClient, 10, ch, 1, attr, false);
    Assert.assertTrue(ret == COMMIT_STATUS.COMMIT_FINISHED);

    ConcurrentNavigableMap<Long, CommitCtx> commits = ctx
        .getPendingCommitsForTest();
    Assert.assertTrue(commits.size() == 0);
    ret = ctx.checkCommit(dfsClient, 11, ch, 1, attr, false);
    Assert.assertTrue(ret == COMMIT_STATUS.COMMIT_WAIT);
    Assert.assertTrue(commits.size() == 1);
    long key = commits.firstKey();
    Assert.assertTrue(key == 11);

    // Test request with zero commit offset
    commits.remove(new Long(11));
    // There is one pending write [5,10]
    ret = ctx.checkCommit(dfsClient, 0, ch, 1, attr, false);
    Assert.assertTrue(ret == COMMIT_STATUS.COMMIT_WAIT);
    Assert.assertTrue(commits.size() == 1);
    key = commits.firstKey();
    Assert.assertTrue(key == 9);

    // Empty pending writes
    ctx.getPendingWritesForTest().remove(new OffsetRange(5, 10));
    ret = ctx.checkCommit(dfsClient, 0, ch, 1, attr, false);
    Assert.assertTrue(ret == COMMIT_STATUS.COMMIT_FINISHED);
  }
"
"  @Test
  public void testCheckCommitLargeFileUpload() throws IOException {
    DFSClient dfsClient = Mockito.mock(DFSClient.class);
    Nfs3FileAttributes attr = new Nfs3FileAttributes();
    HdfsDataOutputStream fos = Mockito.mock(HdfsDataOutputStream.class);
    Mockito.when(fos.getPos()).thenReturn((long) 0);

    NfsConfiguration conf = new NfsConfiguration();
    conf.setBoolean(NfsConfigKeys.LARGE_FILE_UPLOAD, true);
    OpenFileCtx ctx = new OpenFileCtx(fos, attr, ""/dumpFilePath"", dfsClient,
        new ShellBasedIdMapping(conf), false, conf);

    COMMIT_STATUS ret;

    // Test inactive open file context
    ctx.setActiveStatusForTest(false);
    Channel ch = Mockito.mock(Channel.class);
    ret = ctx.checkCommit(dfsClient, 0, ch, 1, attr, false);
    Assert.assertTrue(ret == COMMIT_STATUS.COMMIT_INACTIVE_CTX);

    ctx.getPendingWritesForTest().put(new OffsetRange(10, 15),
        new WriteCtx(null, 0, 0, 0, null, null, null, 0, false, null));
    ret = ctx.checkCommit(dfsClient, 0, ch, 1, attr, false);
    Assert.assertTrue(ret == COMMIT_STATUS.COMMIT_INACTIVE_WITH_PENDING_WRITE);

    // Test request with non zero commit offset
    ctx.setActiveStatusForTest(true);
    Mockito.when(fos.getPos()).thenReturn((long) 8);
    ctx.setNextOffsetForTest(10);
    COMMIT_STATUS status = ctx.checkCommitInternal(5, null, 1, attr, false);
    Assert.assertTrue(status == COMMIT_STATUS.COMMIT_DO_SYNC);
    // Do_SYNC state will be updated to FINISHED after data sync
    ret = ctx.checkCommit(dfsClient, 5, ch, 1, attr, false);
    Assert.assertTrue(ret == COMMIT_STATUS.COMMIT_FINISHED);
    
    // Test commit sequential writes
    status = ctx.checkCommitInternal(10, ch, 1, attr, false);
    Assert.assertTrue(status == COMMIT_STATUS.COMMIT_SPECIAL_WAIT);
    ret = ctx.checkCommit(dfsClient, 10, ch, 1, attr, false);
    Assert.assertTrue(ret == COMMIT_STATUS.COMMIT_SPECIAL_WAIT);

    // Test commit non-sequential writes
    ConcurrentNavigableMap<Long, CommitCtx> commits = ctx
        .getPendingCommitsForTest();
    Assert.assertTrue(commits.size() == 1);
    ret = ctx.checkCommit(dfsClient, 16, ch, 1, attr, false);
    Assert.assertTrue(ret == COMMIT_STATUS.COMMIT_SPECIAL_SUCCESS);
    Assert.assertTrue(commits.size() == 1);
    
    // Test request with zero commit offset
    commits.remove(new Long(10));
    // There is one pending write [10,15]
    ret = ctx.checkCommitInternal(0, ch, 1, attr, false);
    Assert.assertTrue(ret == COMMIT_STATUS.COMMIT_SPECIAL_WAIT);
    
    ret = ctx.checkCommitInternal(9, ch, 1, attr, false);
    Assert.assertTrue(ret == COMMIT_STATUS.COMMIT_SPECIAL_WAIT);
    Assert.assertTrue(commits.size() == 2);

    // Empty pending writes. nextOffset=10, flushed pos=8
    ctx.getPendingWritesForTest().remove(new OffsetRange(10, 15));
    ret = ctx.checkCommit(dfsClient, 0, ch, 1, attr, false);
    Assert.assertTrue(ret == COMMIT_STATUS.COMMIT_SPECIAL_WAIT);
    
    // Empty pending writes
    ctx.setNextOffsetForTest((long) 8); // flushed pos = 8
    ret = ctx.checkCommit(dfsClient, 0, ch, 1, attr, false);
    Assert.assertTrue(ret == COMMIT_STATUS.COMMIT_FINISHED);
    
  }
"
"  @Test
  public void testCheckCommitAixCompatMode() throws IOException {
    DFSClient dfsClient = Mockito.mock(DFSClient.class);
    Nfs3FileAttributes attr = new Nfs3FileAttributes();
    HdfsDataOutputStream fos = Mockito.mock(HdfsDataOutputStream.class);

    NfsConfiguration conf = new NfsConfiguration();
    conf.setBoolean(NfsConfigKeys.LARGE_FILE_UPLOAD, false);
    // Enable AIX compatibility mode.
    OpenFileCtx ctx = new OpenFileCtx(fos, attr, ""/dumpFilePath"", dfsClient,
        new ShellBasedIdMapping(new NfsConfiguration()), true, conf);
    
    // Test fall-through to pendingWrites check in the event that commitOffset
    // is greater than the number of bytes we've so far flushed.
    Mockito.when(fos.getPos()).thenReturn((long) 2);
    COMMIT_STATUS status = ctx.checkCommitInternal(5, null, 1, attr, false);
    Assert.assertTrue(status == COMMIT_STATUS.COMMIT_FINISHED);
    
    // Test the case when we actually have received more bytes than we're trying
    // to commit.
    ctx.getPendingWritesForTest().put(new OffsetRange(0, 10),
        new WriteCtx(null, 0, 0, 0, null, null, null, 0, false, null));
    Mockito.when(fos.getPos()).thenReturn((long) 10);
    ctx.setNextOffsetForTest((long)10);
    status = ctx.checkCommitInternal(5, null, 1, attr, false);
    Assert.assertTrue(status == COMMIT_STATUS.COMMIT_DO_SYNC);
  }
"
"  @Test
  public void testCheckCommitFromRead() throws IOException {
    DFSClient dfsClient = Mockito.mock(DFSClient.class);
    Nfs3FileAttributes attr = new Nfs3FileAttributes();
    HdfsDataOutputStream fos = Mockito.mock(HdfsDataOutputStream.class);
    Mockito.when(fos.getPos()).thenReturn((long) 0);
    NfsConfiguration config = new NfsConfiguration();

    config.setBoolean(NfsConfigKeys.LARGE_FILE_UPLOAD, false);
    OpenFileCtx ctx = new OpenFileCtx(fos, attr, ""/dumpFilePath"", dfsClient,
        new ShellBasedIdMapping(config), false, config);

    FileHandle h = new FileHandle(1); // fake handle for ""/dumpFilePath""
    COMMIT_STATUS ret;
    WriteManager wm = new WriteManager(new ShellBasedIdMapping(config), config, false);
    assertTrue(wm.addOpenFileStream(h, ctx));
    
    // Test inactive open file context
    ctx.setActiveStatusForTest(false);
    Channel ch = Mockito.mock(Channel.class);
    ret = ctx.checkCommit(dfsClient, 0, ch, 1, attr, true);
    assertEquals( COMMIT_STATUS.COMMIT_INACTIVE_CTX, ret);
    assertEquals(Nfs3Status.NFS3_OK, wm.commitBeforeRead(dfsClient, h, 0));
    
    ctx.getPendingWritesForTest().put(new OffsetRange(10, 15),
        new WriteCtx(null, 0, 0, 0, null, null, null, 0, false, null));
    ret = ctx.checkCommit(dfsClient, 0, ch, 1, attr, true);
    assertEquals(COMMIT_STATUS.COMMIT_INACTIVE_WITH_PENDING_WRITE, ret);
    assertEquals(Nfs3Status.NFS3ERR_IO, wm.commitBeforeRead(dfsClient, h, 0));
    
    // Test request with non zero commit offset
    ctx.setActiveStatusForTest(true);
    Mockito.when(fos.getPos()).thenReturn((long) 10);
    ctx.setNextOffsetForTest((long)10);
    COMMIT_STATUS status = ctx.checkCommitInternal(5, ch, 1, attr, false);
    assertEquals(COMMIT_STATUS.COMMIT_DO_SYNC, status);
    // Do_SYNC state will be updated to FINISHED after data sync
    ret = ctx.checkCommit(dfsClient, 5, ch, 1, attr, true);
    assertEquals(COMMIT_STATUS.COMMIT_FINISHED, ret);
    assertEquals(Nfs3Status.NFS3_OK, wm.commitBeforeRead(dfsClient, h, 5));
 
    status = ctx.checkCommitInternal(10, ch, 1, attr, true);
    assertTrue(status == COMMIT_STATUS.COMMIT_DO_SYNC);
    ret = ctx.checkCommit(dfsClient, 10, ch, 1, attr, true);
    assertEquals(COMMIT_STATUS.COMMIT_FINISHED, ret);
    assertEquals(Nfs3Status.NFS3_OK, wm.commitBeforeRead(dfsClient, h, 10));

    ConcurrentNavigableMap<Long, CommitCtx> commits = ctx
        .getPendingCommitsForTest();
    assertTrue(commits.size() == 0);
    ret = ctx.checkCommit(dfsClient, 11, ch, 1, attr, true);
    assertEquals(COMMIT_STATUS.COMMIT_WAIT, ret);
    assertEquals(0, commits.size()); // commit triggered by read doesn't wait
    assertEquals(Nfs3Status.NFS3ERR_JUKEBOX, wm.commitBeforeRead(dfsClient, h, 11));

    // Test request with zero commit offset
    // There is one pending write [5,10]
    ret = ctx.checkCommit(dfsClient, 0, ch, 1, attr, true);
    assertEquals(COMMIT_STATUS.COMMIT_WAIT, ret);
    assertEquals(0, commits.size());
    assertEquals(Nfs3Status.NFS3ERR_JUKEBOX, wm.commitBeforeRead(dfsClient, h, 0));

    // Empty pending writes
    ctx.getPendingWritesForTest().remove(new OffsetRange(10, 15));
    ret = ctx.checkCommit(dfsClient, 0, ch, 1, attr, true);
    assertEquals(COMMIT_STATUS.COMMIT_FINISHED, ret);
    assertEquals(Nfs3Status.NFS3_OK, wm.commitBeforeRead(dfsClient, h, 0));
  }
"
"  @Test
  public void testCheckCommitFromReadLargeFileUpload() throws IOException {
    DFSClient dfsClient = Mockito.mock(DFSClient.class);
    Nfs3FileAttributes attr = new Nfs3FileAttributes();
    HdfsDataOutputStream fos = Mockito.mock(HdfsDataOutputStream.class);
    Mockito.when(fos.getPos()).thenReturn((long) 0);
    NfsConfiguration config = new NfsConfiguration();

    config.setBoolean(NfsConfigKeys.LARGE_FILE_UPLOAD, true);
    OpenFileCtx ctx = new OpenFileCtx(fos, attr, ""/dumpFilePath"", dfsClient,
        new ShellBasedIdMapping(config), false, config);

    FileHandle h = new FileHandle(1); // fake handle for ""/dumpFilePath""
    COMMIT_STATUS ret;
    WriteManager wm = new WriteManager(new ShellBasedIdMapping(config), config, false);
    assertTrue(wm.addOpenFileStream(h, ctx));
    
    // Test inactive open file context
    ctx.setActiveStatusForTest(false);
    Channel ch = Mockito.mock(Channel.class);
    ret = ctx.checkCommit(dfsClient, 0, ch, 1, attr, true);
    assertEquals( COMMIT_STATUS.COMMIT_INACTIVE_CTX, ret);
    assertEquals(Nfs3Status.NFS3_OK, wm.commitBeforeRead(dfsClient, h, 0));
    
    ctx.getPendingWritesForTest().put(new OffsetRange(10, 15),
        new WriteCtx(null, 0, 0, 0, null, null, null, 0, false, null));
    ret = ctx.checkCommit(dfsClient, 0, ch, 1, attr, true);
    assertEquals(COMMIT_STATUS.COMMIT_INACTIVE_WITH_PENDING_WRITE, ret);
    assertEquals(Nfs3Status.NFS3ERR_IO, wm.commitBeforeRead(dfsClient, h, 0));
    
    // Test request with non zero commit offset
    ctx.setActiveStatusForTest(true);
    Mockito.when(fos.getPos()).thenReturn((long) 6);
    ctx.setNextOffsetForTest((long)10);
    COMMIT_STATUS status = ctx.checkCommitInternal(5, ch, 1, attr, false);
    assertEquals(COMMIT_STATUS.COMMIT_DO_SYNC, status);
    // Do_SYNC state will be updated to FINISHED after data sync
    ret = ctx.checkCommit(dfsClient, 5, ch, 1, attr, true);
    assertEquals(COMMIT_STATUS.COMMIT_FINISHED, ret);
    assertEquals(Nfs3Status.NFS3_OK, wm.commitBeforeRead(dfsClient, h, 5));
 
    // Test request with sequential writes
    status = ctx.checkCommitInternal(9, ch, 1, attr, true);
    assertTrue(status == COMMIT_STATUS.COMMIT_SPECIAL_WAIT);
    ret = ctx.checkCommit(dfsClient, 9, ch, 1, attr, true);
    assertEquals(COMMIT_STATUS.COMMIT_SPECIAL_WAIT, ret);
    assertEquals(Nfs3Status.NFS3ERR_JUKEBOX, wm.commitBeforeRead(dfsClient, h, 9));

    // Test request with non-sequential writes
    ConcurrentNavigableMap<Long, CommitCtx> commits = ctx
        .getPendingCommitsForTest();
    assertTrue(commits.size() == 0);
    ret = ctx.checkCommit(dfsClient, 16, ch, 1, attr, true);
    assertEquals(COMMIT_STATUS.COMMIT_SPECIAL_SUCCESS, ret);
    assertEquals(0, commits.size()); // commit triggered by read doesn't wait
    assertEquals(Nfs3Status.NFS3_OK, wm.commitBeforeRead(dfsClient, h, 16));

    // Test request with zero commit offset
    // There is one pending write [10,15]
    ret = ctx.checkCommit(dfsClient, 0, ch, 1, attr, true);
    assertEquals(COMMIT_STATUS.COMMIT_SPECIAL_WAIT, ret);
    assertEquals(0, commits.size());
    assertEquals(Nfs3Status.NFS3ERR_JUKEBOX, wm.commitBeforeRead(dfsClient, h, 0));

    // Empty pending writes
    ctx.getPendingWritesForTest().remove(new OffsetRange(10, 15));
    ret = ctx.checkCommit(dfsClient, 0, ch, 1, attr, true);
    assertEquals(COMMIT_STATUS.COMMIT_SPECIAL_WAIT, ret);
    assertEquals(Nfs3Status.NFS3ERR_JUKEBOX, wm.commitBeforeRead(dfsClient, h, 0));
  }
"
"  @Test
  public void testWriteStableHow() throws IOException, InterruptedException {
    NfsConfiguration config = new NfsConfiguration();
    DFSClient client = null;
    MiniDFSCluster cluster = null;
    RpcProgramNfs3 nfsd;
    SecurityHandler securityHandler = Mockito.mock(SecurityHandler.class);
    Mockito.when(securityHandler.getUser()).thenReturn(
        System.getProperty(""user.name""));
    String currentUser = System.getProperty(""user.name"");
    config.set(
            DefaultImpersonationProvider.getTestProvider().
                getProxySuperuserGroupConfKey(currentUser),
            ""*"");
    config.set(
            DefaultImpersonationProvider.getTestProvider().
                getProxySuperuserIpConfKey(currentUser),
            ""*"");
    ProxyUsers.refreshSuperUserGroupsConfiguration(config);

    try {
      cluster = new MiniDFSCluster.Builder(config).numDataNodes(1).build();
      cluster.waitActive();
      client = new DFSClient(DFSUtilClient.getNNAddress(config), config);
      int namenodeId = Nfs3Utils.getNamenodeId(config);

      // Use emphral port in case tests are running in parallel
      config.setInt(""nfs3.mountd.port"", 0);
      config.setInt(""nfs3.server.port"", 0);
      
      // Start nfs
      Nfs3 nfs3 = new Nfs3(config);
      nfs3.startServiceInternal(false);
      nfsd = (RpcProgramNfs3) nfs3.getRpcProgram();

      HdfsFileStatus status = client.getFileInfo(""/"");
      FileHandle rootHandle = new FileHandle(status.getFileId(), namenodeId);
      // Create file1
      CREATE3Request createReq = new CREATE3Request(rootHandle, ""file1"",
          Nfs3Constant.CREATE_UNCHECKED, new SetAttr3(), 0);
      XDR createXdr = new XDR();
      createReq.serialize(createXdr);
      CREATE3Response createRsp = nfsd.create(createXdr.asReadOnlyWrap(),
          securityHandler, new InetSocketAddress(""localhost"", 1234));
      FileHandle handle = createRsp.getObjHandle();

      // Test DATA_SYNC
      byte[] buffer = new byte[10];
      for (int i = 0; i < 10; i++) {
        buffer[i] = (byte) i;
      }
      WRITE3Request writeReq = new WRITE3Request(handle, 0, 10,
          WriteStableHow.DATA_SYNC, ByteBuffer.wrap(buffer));
      XDR writeXdr = new XDR();
      writeReq.serialize(writeXdr);
      nfsd.write(writeXdr.asReadOnlyWrap(), null, 1, securityHandler,
          new InetSocketAddress(""localhost"", 1234));

      waitWrite(nfsd, handle, 60000);

      // Readback
      READ3Request readReq = new READ3Request(handle, 0, 10);
      XDR readXdr = new XDR();
      readReq.serialize(readXdr);
      READ3Response readRsp = nfsd.read(readXdr.asReadOnlyWrap(),
          securityHandler, new InetSocketAddress(""localhost"", 1234));

      assertTrue(Arrays.equals(buffer, readRsp.getData().array()));

      // Test FILE_SYNC

      // Create file2
      CREATE3Request createReq2 = new CREATE3Request(rootHandle, ""file2"",
          Nfs3Constant.CREATE_UNCHECKED, new SetAttr3(), 0);
      XDR createXdr2 = new XDR();
      createReq2.serialize(createXdr2);
      CREATE3Response createRsp2 = nfsd.create(createXdr2.asReadOnlyWrap(),
          securityHandler, new InetSocketAddress(""localhost"", 1234));
      FileHandle handle2 = createRsp2.getObjHandle();

      WRITE3Request writeReq2 = new WRITE3Request(handle2, 0, 10,
          WriteStableHow.FILE_SYNC, ByteBuffer.wrap(buffer));
      XDR writeXdr2 = new XDR();
      writeReq2.serialize(writeXdr2);
      nfsd.write(writeXdr2.asReadOnlyWrap(), null, 1, securityHandler,
          new InetSocketAddress(""localhost"", 1234));

      waitWrite(nfsd, handle2, 60000);

      // Readback
      READ3Request readReq2 = new READ3Request(handle2, 0, 10);
      XDR readXdr2 = new XDR();
      readReq2.serialize(readXdr2);
      READ3Response readRsp2 = nfsd.read(readXdr2.asReadOnlyWrap(),
          securityHandler, new InetSocketAddress(""localhost"", 1234));

      assertTrue(Arrays.equals(buffer, readRsp2.getData().array()));
      // FILE_SYNC should sync the file size
      status = client.getFileInfo(""/file2"");
      assertTrue(status.getLen() == 10);

    } finally {
      if (cluster != null) {
        cluster.shutdown();
      }
    }
  }
"
"  @Test
  public void testOOOWrites() throws IOException, InterruptedException {
    NfsConfiguration config = new NfsConfiguration();
    MiniDFSCluster cluster = null;
    RpcProgramNfs3 nfsd;
    final int bufSize = 32;
    final int numOOO = 3;
    SecurityHandler securityHandler = Mockito.mock(SecurityHandler.class);
    Mockito.when(securityHandler.getUser()).thenReturn(
        System.getProperty(""user.name""));
    String currentUser = System.getProperty(""user.name"");
    config.set(
        DefaultImpersonationProvider.getTestProvider().
            getProxySuperuserGroupConfKey(currentUser),
        ""*"");
    config.set(
        DefaultImpersonationProvider.getTestProvider().
            getProxySuperuserIpConfKey(currentUser),
        ""*"");
    ProxyUsers.refreshSuperUserGroupsConfiguration(config);
    // Use emphral port in case tests are running in parallel
    config.setInt(""nfs3.mountd.port"", 0);
    config.setInt(""nfs3.server.port"", 0);

    try {
      cluster = new MiniDFSCluster.Builder(config).numDataNodes(1).build();
      cluster.waitActive();

      Nfs3 nfs3 = new Nfs3(config);
      nfs3.startServiceInternal(false);
      nfsd = (RpcProgramNfs3) nfs3.getRpcProgram();

      DFSClient dfsClient = new DFSClient(DFSUtilClient.getNNAddress(config),
          config);
      int namenodeId = Nfs3Utils.getNamenodeId(config);
      HdfsFileStatus status = dfsClient.getFileInfo(""/"");
      FileHandle rootHandle = new FileHandle(status.getFileId(), namenodeId);

      CREATE3Request createReq = new CREATE3Request(rootHandle,
          ""out-of-order-write"" + System.currentTimeMillis(),
          Nfs3Constant.CREATE_UNCHECKED, new SetAttr3(), 0);
      XDR createXdr = new XDR();
      createReq.serialize(createXdr);
      CREATE3Response createRsp = nfsd.create(createXdr.asReadOnlyWrap(),
          securityHandler, new InetSocketAddress(""localhost"", 1234));
      FileHandle handle = createRsp.getObjHandle();

      byte[][] oooBuf = new byte[numOOO][bufSize];
      for (int i = 0; i < numOOO; i++) {
        Arrays.fill(oooBuf[i], (byte) i);
      }

      for (int i = 0; i < numOOO; i++) {
        final long offset = (numOOO - 1 - i) * bufSize;
        WRITE3Request writeReq = new WRITE3Request(handle, offset, bufSize,
            WriteStableHow.UNSTABLE, ByteBuffer.wrap(oooBuf[i]));
        XDR writeXdr = new XDR();
        writeReq.serialize(writeXdr);
        nfsd.write(writeXdr.asReadOnlyWrap(), null, 1, securityHandler,
            new InetSocketAddress(""localhost"", 1234));
      }

      waitWrite(nfsd, handle, 60000);
      READ3Request readReq = new READ3Request(handle, bufSize, bufSize);
      XDR readXdr = new XDR();
      readReq.serialize(readXdr);
      READ3Response readRsp = nfsd.read(readXdr.asReadOnlyWrap(),
          securityHandler, new InetSocketAddress(""localhost"", config.getInt(
              NfsConfigKeys.DFS_NFS_SERVER_PORT_KEY,
              NfsConfigKeys.DFS_NFS_SERVER_PORT_DEFAULT)));
      assertTrue(Arrays.equals(oooBuf[1], readRsp.getData().array()));
    } finally {
      if (cluster != null) {
        cluster.shutdown();
      }
    }
  }
"
"  @Test
  public void testOverlappingWrites() throws IOException, InterruptedException {
    NfsConfiguration config = new NfsConfiguration();
    MiniDFSCluster cluster = null;
    RpcProgramNfs3 nfsd;
    final int bufSize = 32;
    SecurityHandler securityHandler = Mockito.mock(SecurityHandler.class);
    Mockito.when(securityHandler.getUser()).thenReturn(
        System.getProperty(""user.name""));
    String currentUser = System.getProperty(""user.name"");
    config.set(
        DefaultImpersonationProvider.getTestProvider().
            getProxySuperuserGroupConfKey(currentUser),
        ""*"");
    config.set(
        DefaultImpersonationProvider.getTestProvider().
            getProxySuperuserIpConfKey(currentUser),
        ""*"");
    ProxyUsers.refreshSuperUserGroupsConfiguration(config);
    // Use emphral port in case tests are running in parallel
    config.setInt(""nfs3.mountd.port"", 0);
    config.setInt(""nfs3.server.port"", 0);

    try {
      cluster = new MiniDFSCluster.Builder(config).numDataNodes(1).build();
      cluster.waitActive();

      Nfs3 nfs3 = new Nfs3(config);
      nfs3.startServiceInternal(false);
      nfsd = (RpcProgramNfs3) nfs3.getRpcProgram();

      DFSClient dfsClient = new DFSClient(DFSUtilClient.getNNAddress(config),
          config);
      int namenodeId = Nfs3Utils.getNamenodeId(config);
      HdfsFileStatus status = dfsClient.getFileInfo(""/"");
      FileHandle rootHandle = new FileHandle(status.getFileId(), namenodeId);

      CREATE3Request createReq = new CREATE3Request(rootHandle,
          ""overlapping-writes"" + System.currentTimeMillis(),
          Nfs3Constant.CREATE_UNCHECKED, new SetAttr3(), 0);
      XDR createXdr = new XDR();
      createReq.serialize(createXdr);
      CREATE3Response createRsp = nfsd.create(createXdr.asReadOnlyWrap(),
          securityHandler, new InetSocketAddress(""localhost"", 1234));
      FileHandle handle = createRsp.getObjHandle();
      byte[] buffer = new byte[bufSize];
      for (int i = 0; i < bufSize; i++) {
        buffer[i] = (byte) i;
      }
      int[][] ranges = new int[][] {
          {0, 10},
          {5, 7},
          {5, 5},
          {10, 6},
          {18, 6},
          {20, 6},
          {28, 4},
          {16, 2},
          {25, 4}
      };
      for (int i = 0; i < ranges.length; i++) {
        int x[] = ranges[i];
        byte[] tbuffer = new byte[x[1]];
        for (int j = 0; j < x[1]; j++) {
          tbuffer[j] = buffer[x[0] + j];
        }
        WRITE3Request writeReq = new WRITE3Request(handle, (long)x[0], x[1],
            WriteStableHow.UNSTABLE, ByteBuffer.wrap(tbuffer));
        XDR writeXdr = new XDR();
        writeReq.serialize(writeXdr);
        nfsd.write(writeXdr.asReadOnlyWrap(), null, 1, securityHandler,
            new InetSocketAddress(""localhost"", 1234));
      }

      waitWrite(nfsd, handle, 60000);
      READ3Request readReq = new READ3Request(handle, 0, bufSize);
      XDR readXdr = new XDR();
      readReq.serialize(readXdr);
      READ3Response readRsp = nfsd.read(readXdr.asReadOnlyWrap(),
          securityHandler, new InetSocketAddress(""localhost"", config.getInt(
              NfsConfigKeys.DFS_NFS_SERVER_PORT_KEY,
              NfsConfigKeys.DFS_NFS_SERVER_PORT_DEFAULT)));

      assertTrue(Arrays.equals(buffer, readRsp.getData().array()));
    } finally {
      if (cluster != null) {
        cluster.shutdown();
      }
    }
  }
"
"  @Test
  public void testCheckSequential() throws IOException {
    DFSClient dfsClient = Mockito.mock(DFSClient.class);
    Nfs3FileAttributes attr = new Nfs3FileAttributes();
    HdfsDataOutputStream fos = Mockito.mock(HdfsDataOutputStream.class);
    Mockito.when(fos.getPos()).thenReturn((long) 0);
    NfsConfiguration config = new NfsConfiguration();

    config.setBoolean(NfsConfigKeys.LARGE_FILE_UPLOAD, false);
    OpenFileCtx ctx = new OpenFileCtx(fos, attr, ""/dumpFilePath"", dfsClient,
        new ShellBasedIdMapping(config), false, config);
    
    ctx.getPendingWritesForTest().put(new OffsetRange(5, 10),
        new WriteCtx(null, 0, 0, 0, null, null, null, 0, false, null));
    ctx.getPendingWritesForTest().put(new OffsetRange(10, 15),
        new WriteCtx(null, 0, 0, 0, null, null, null, 0, false, null));
    ctx.getPendingWritesForTest().put(new OffsetRange(20, 25),
        new WriteCtx(null, 0, 0, 0, null, null, null, 0, false, null));

    assertTrue(!ctx.checkSequential(5, 4));
    assertTrue(ctx.checkSequential(9, 5));
    assertTrue(ctx.checkSequential(10, 5));
    assertTrue(ctx.checkSequential(14, 5));
    assertTrue(!ctx.checkSequential(15, 5));
    assertTrue(!ctx.checkSequential(20, 5));
    assertTrue(!ctx.checkSequential(25, 5));
    assertTrue(!ctx.checkSequential(999, 5));
  }
"
"  @Test
  public void testHttpServer() throws Exception {
    Nfs3 nfs = new Nfs3(conf);
    nfs.startServiceInternal(false);
    RpcProgramNfs3 nfsd = (RpcProgramNfs3) nfs.getRpcProgram();
    Nfs3HttpServer infoServer = nfsd.getInfoServer();

    String urlRoot = infoServer.getServerURI().toString();

    // Check default servlets.
    String pageContents = DFSTestUtil.urlGet(new URL(urlRoot + ""/jmx""));
    assertTrue(""Bad contents: "" + pageContents,
        pageContents.contains(""java.lang:type=""));
    System.out.println(""pc:"" + pageContents);

    int port = infoServer.getSecurePort();
    assertTrue(""Can't get https port"", port > 0);
  }
"
"  @Test(timeout = 60000)
  public void testGetattr() throws Exception {
    HdfsFileStatus status = nn.getRpcServer().getFileInfo(""/tmp/bar"");
    long dirId = status.getFileId();
    int namenodeId = Nfs3Utils.getNamenodeId(config);
    FileHandle handle = new FileHandle(dirId, namenodeId);
    XDR xdr_req = new XDR();
    GETATTR3Request req = new GETATTR3Request(handle);
    req.serialize(xdr_req);
    
    // Attempt by an unpriviledged user should fail.
    GETATTR3Response response1 = nfsd.getattr(xdr_req.asReadOnlyWrap(),
        securityHandlerUnpriviledged,
        new InetSocketAddress(""localhost"", 1234));
    assertEquals(""Incorrect return code"", Nfs3Status.NFS3ERR_ACCES,
        response1.getStatus());

    // Attempt by a priviledged user should pass.
    GETATTR3Response response2 = nfsd.getattr(xdr_req.asReadOnlyWrap(),
        securityHandler, new InetSocketAddress(""localhost"", 1234));
    assertEquals(""Incorrect return code"", Nfs3Status.NFS3_OK,
        response2.getStatus());
  }
"
"  @Test(timeout = 60000)
  public void testSetattr() throws Exception {
    HdfsFileStatus status = nn.getRpcServer().getFileInfo(testdir);
    long dirId = status.getFileId();
    int namenodeId = Nfs3Utils.getNamenodeId(config);
    XDR xdr_req = new XDR();
    FileHandle handle = new FileHandle(dirId, namenodeId);
    SetAttr3 symAttr = new SetAttr3(0, 1, 0, 0, null, null,
        EnumSet.of(SetAttrField.UID));
    SETATTR3Request req = new SETATTR3Request(handle, symAttr, false, null);
    req.serialize(xdr_req);

    // Attempt by an unprivileged user should fail.
    SETATTR3Response response1 = nfsd.setattr(xdr_req.asReadOnlyWrap(),
        securityHandlerUnpriviledged,
        new InetSocketAddress(""localhost"", 1234));
    assertEquals(""Incorrect return code"", Nfs3Status.NFS3ERR_ACCES,
        response1.getStatus());

    // Attempt by a priviledged user should pass.
    SETATTR3Response response2 = nfsd.setattr(xdr_req.asReadOnlyWrap(),
        securityHandler, new InetSocketAddress(""localhost"", 1234));
    assertEquals(""Incorrect return code"", Nfs3Status.NFS3_OK,
        response2.getStatus());
  }
"
"  @Test(timeout = 60000)
  public void testLookup() throws Exception {
    HdfsFileStatus status = nn.getRpcServer().getFileInfo(testdir);
    long dirId = status.getFileId();
    int namenodeId = Nfs3Utils.getNamenodeId(config);
    FileHandle handle = new FileHandle(dirId, namenodeId);
    LOOKUP3Request lookupReq = new LOOKUP3Request(handle, ""bar"");
    XDR xdr_req = new XDR();
    lookupReq.serialize(xdr_req);

    // Attempt by an unpriviledged user should fail.
    LOOKUP3Response response1 = nfsd.lookup(xdr_req.asReadOnlyWrap(),
        securityHandlerUnpriviledged,
        new InetSocketAddress(""localhost"", 1234));
    assertEquals(""Incorrect return code"", Nfs3Status.NFS3ERR_ACCES,
        response1.getStatus());

    // Attempt by a priviledged user should pass.
    LOOKUP3Response response2 = nfsd.lookup(xdr_req.asReadOnlyWrap(),
        securityHandler, new InetSocketAddress(""localhost"", 1234));
    assertEquals(""Incorrect return code"", Nfs3Status.NFS3_OK,
        response2.getStatus());
  }
"
"  @Test(timeout = 60000)
  public void testAccess() throws Exception {
    HdfsFileStatus status = nn.getRpcServer().getFileInfo(""/tmp/bar"");
    long dirId = status.getFileId();
    int namenodeId = Nfs3Utils.getNamenodeId(config);
    FileHandle handle = new FileHandle(dirId, namenodeId);
    XDR xdr_req = new XDR();
    ACCESS3Request req = new ACCESS3Request(handle);
    req.serialize(xdr_req);

    // Attempt by an unpriviledged user should fail.
    ACCESS3Response response1 = nfsd.access(xdr_req.asReadOnlyWrap(),
        securityHandlerUnpriviledged,
        new InetSocketAddress(""localhost"", 1234));
    assertEquals(""Incorrect return code"", Nfs3Status.NFS3ERR_ACCES,
        response1.getStatus());

    // Attempt by a priviledged user should pass.
    ACCESS3Response response2 = nfsd.access(xdr_req.asReadOnlyWrap(),
        securityHandler, new InetSocketAddress(""localhost"", 1234));
    assertEquals(""Incorrect return code"", Nfs3Status.NFS3_OK,
        response2.getStatus());
  }
"
"  @Test(timeout = 60000)
  public void testReadlink() throws Exception {
    // Create a symlink first.
    HdfsFileStatus status = nn.getRpcServer().getFileInfo(testdir);
    long dirId = status.getFileId();
    int namenodeId = Nfs3Utils.getNamenodeId(config);
    XDR xdr_req = new XDR();
    FileHandle handle = new FileHandle(dirId, namenodeId);
    SYMLINK3Request req = new SYMLINK3Request(handle, ""fubar"", new SetAttr3(),
        ""bar"");
    req.serialize(xdr_req);
    
    SYMLINK3Response response = nfsd.symlink(xdr_req.asReadOnlyWrap(),
        securityHandler, new InetSocketAddress(""localhost"", 1234));
    assertEquals(""Incorrect return code:"", Nfs3Status.NFS3_OK,
        response.getStatus());

    // Now perform readlink operations.
    FileHandle handle2 = response.getObjFileHandle();
    XDR xdr_req2 = new XDR();
    READLINK3Request req2 = new READLINK3Request(handle2);
    req2.serialize(xdr_req2);

    // Attempt by an unpriviledged user should fail.
    READLINK3Response response1 = nfsd.readlink(xdr_req2.asReadOnlyWrap(),
        securityHandlerUnpriviledged,
        new InetSocketAddress(""localhost"", 1234));
    assertEquals(""Incorrect return code:"", Nfs3Status.NFS3ERR_ACCES,
        response1.getStatus());

    // Attempt by a priviledged user should pass.
    READLINK3Response response2 = nfsd.readlink(xdr_req2.asReadOnlyWrap(),
        securityHandler, new InetSocketAddress(""localhost"", 1234));
    assertEquals(""Incorrect return code:"", Nfs3Status.NFS3_OK,
        response2.getStatus());
  }
"
"  @Test(timeout = 60000)
  public void testRead() throws Exception {
    HdfsFileStatus status = nn.getRpcServer().getFileInfo(""/tmp/bar"");
    long dirId = status.getFileId();
    int namenodeId = Nfs3Utils.getNamenodeId(config);
    FileHandle handle = new FileHandle(dirId, namenodeId);

    READ3Request readReq = new READ3Request(handle, 0, 5);
    XDR xdr_req = new XDR();
    readReq.serialize(xdr_req);

    // Attempt by an unpriviledged user should fail.
    READ3Response response1 = nfsd.read(xdr_req.asReadOnlyWrap(),
        securityHandlerUnpriviledged,
        new InetSocketAddress(""localhost"", 1234));
    assertEquals(""Incorrect return code:"", Nfs3Status.NFS3ERR_ACCES,
        response1.getStatus());

    // Attempt by a priviledged user should pass.
    READ3Response response2 = nfsd.read(xdr_req.asReadOnlyWrap(),
        securityHandler, new InetSocketAddress(""localhost"", 1234));
    assertEquals(""Incorrect return code:"", Nfs3Status.NFS3_OK,
        response2.getStatus());
  }
"
"  @Test(timeout = 120000)
  public void testEncryptedReadWrite() throws Exception {
    final int len = 8192;

    final Path zone = new Path(""/zone"");
    hdfs.mkdirs(zone);
    dfsAdmin.createEncryptionZone(zone, TEST_KEY, NO_TRASH);

    final byte[] buffer = new byte[len];
    for (int i = 0; i < len; i++) {
      buffer[i] = (byte) i;
    }

    final String encFile1 = ""/zone/myfile"";
    createFileUsingNfs(encFile1, buffer);
    commit(encFile1, len);
    assertArrayEquals(""encFile1 not equal"",
        getFileContentsUsingNfs(encFile1, len),
        getFileContentsUsingDfs(encFile1, len));

    /*
     * Same thing except this time create the encrypted file using DFS.
     */
    final String encFile2 = ""/zone/myfile2"";
    final Path encFile2Path = new Path(encFile2);
    DFSTestUtil.createFile(hdfs, encFile2Path, len, (short) 1, 0xFEED);
    assertArrayEquals(""encFile2 not equal"",
        getFileContentsUsingNfs(encFile2, len),
        getFileContentsUsingDfs(encFile2, len));
  }
"
"  @Test(timeout = 60000)
  public void testWrite() throws Exception {
    HdfsFileStatus status = nn.getRpcServer().getFileInfo(""/tmp/bar"");
    long dirId = status.getFileId();
    int namenodeId = Nfs3Utils.getNamenodeId(config);
    FileHandle handle = new FileHandle(dirId, namenodeId);

    byte[] buffer = new byte[10];
    for (int i = 0; i < 10; i++) {
      buffer[i] = (byte) i;
    }

    WRITE3Request writeReq = new WRITE3Request(handle, 0, 10,
        WriteStableHow.DATA_SYNC, ByteBuffer.wrap(buffer));
    XDR xdr_req = new XDR();
    writeReq.serialize(xdr_req);

    // Attempt by an unpriviledged user should fail.
    WRITE3Response response1 = nfsd.write(xdr_req.asReadOnlyWrap(),
        null, 1, securityHandlerUnpriviledged,
        new InetSocketAddress(""localhost"", 1234));
    assertEquals(""Incorrect return code:"", Nfs3Status.NFS3ERR_ACCES,
        response1.getStatus());

    // Attempt by a priviledged user should pass.
    WRITE3Response response2 = nfsd.write(xdr_req.asReadOnlyWrap(),
        null, 1, securityHandler,
        new InetSocketAddress(""localhost"", 1234));
    assertEquals(""Incorrect response:"", null, response2);
  }
"
"  @Test(timeout = 60000)
  public void testCreate() throws Exception {
    HdfsFileStatus status = nn.getRpcServer().getFileInfo(testdir);
    long dirId = status.getFileId();
    int namenodeId = Nfs3Utils.getNamenodeId(config);
    XDR xdr_req = new XDR();
    FileHandle handle = new FileHandle(dirId, namenodeId);
    CREATE3Request req = new CREATE3Request(handle, ""fubar"",
        Nfs3Constant.CREATE_UNCHECKED, new SetAttr3(), 0);
    req.serialize(xdr_req);
    
    // Attempt by an unpriviledged user should fail.
    CREATE3Response response1 = nfsd.create(xdr_req.asReadOnlyWrap(),
        securityHandlerUnpriviledged,
        new InetSocketAddress(""localhost"", 1234));
    assertEquals(""Incorrect return code:"", Nfs3Status.NFS3ERR_ACCES,
        response1.getStatus());

    // Attempt by a priviledged user should pass.
    CREATE3Response response2 = nfsd.create(xdr_req.asReadOnlyWrap(),
        securityHandler, new InetSocketAddress(""localhost"", 1234));
    assertEquals(""Incorrect return code:"", Nfs3Status.NFS3_OK,
        response2.getStatus());
  }
"
"  @Test(timeout = 60000)
  public void testMkdir() throws Exception {//FixME
    HdfsFileStatus status = nn.getRpcServer().getFileInfo(testdir);
    long dirId = status.getFileId();
    int namenodeId = Nfs3Utils.getNamenodeId(config);
    XDR xdr_req = new XDR();
    FileHandle handle = new FileHandle(dirId, namenodeId);
    MKDIR3Request req = new MKDIR3Request(handle, ""fubar1"", new SetAttr3());
    req.serialize(xdr_req);
    
    // Attempt to mkdir by an unprivileged user should fail.
    MKDIR3Response response1 = nfsd.mkdir(xdr_req.asReadOnlyWrap(),
        securityHandlerUnpriviledged,
        new InetSocketAddress(""localhost"", 1234));
    assertEquals(""Incorrect return code:"", Nfs3Status.NFS3ERR_ACCES,
        response1.getStatus());

    XDR xdr_req2 = new XDR();
    MKDIR3Request req2 = new MKDIR3Request(handle, ""fubar2"", new SetAttr3());
    req2.serialize(xdr_req2);
    
    // Attempt to mkdir by a privileged user should pass.
    MKDIR3Response response2 = nfsd.mkdir(xdr_req2.asReadOnlyWrap(),
        securityHandler, new InetSocketAddress(""localhost"", 1234));
    assertEquals(""Incorrect return code:"", Nfs3Status.NFS3_OK,
        response2.getStatus());
  }
"
"  @Test(timeout = 60000)
  public void testSymlink() throws Exception {
    HdfsFileStatus status = nn.getRpcServer().getFileInfo(testdir);
    long dirId = status.getFileId();
    int namenodeId = Nfs3Utils.getNamenodeId(config);
    XDR xdr_req = new XDR();
    FileHandle handle = new FileHandle(dirId, namenodeId);
    SYMLINK3Request req = new SYMLINK3Request(handle, ""fubar"", new SetAttr3(),
        ""bar"");
    req.serialize(xdr_req);

    // Attempt by an unprivileged user should fail.
    SYMLINK3Response response1 = nfsd.symlink(xdr_req.asReadOnlyWrap(),
        securityHandlerUnpriviledged,
        new InetSocketAddress(""localhost"", 1234));
    assertEquals(""Incorrect return code:"", Nfs3Status.NFS3ERR_ACCES,
        response1.getStatus());

    // Attempt by a privileged user should pass.
    SYMLINK3Response response2 = nfsd.symlink(xdr_req.asReadOnlyWrap(),
        securityHandler, new InetSocketAddress(""localhost"", 1234));
    assertEquals(""Incorrect return code:"", Nfs3Status.NFS3_OK,
        response2.getStatus());
  }
"
"  @Test(timeout = 60000)
  public void testRemove() throws Exception {
    HdfsFileStatus status = nn.getRpcServer().getFileInfo(testdir);
    long dirId = status.getFileId();
    int namenodeId = Nfs3Utils.getNamenodeId(config);
    XDR xdr_req = new XDR();
    FileHandle handle = new FileHandle(dirId, namenodeId);
    REMOVE3Request req = new REMOVE3Request(handle, ""bar"");
    req.serialize(xdr_req);

    // Attempt by an unpriviledged user should fail.
    REMOVE3Response response1 = nfsd.remove(xdr_req.asReadOnlyWrap(),
        securityHandlerUnpriviledged,
        new InetSocketAddress(""localhost"", 1234));
    assertEquals(""Incorrect return code:"", Nfs3Status.NFS3ERR_ACCES,
        response1.getStatus());

    // Attempt by a priviledged user should pass.
    REMOVE3Response response2 = nfsd.remove(xdr_req.asReadOnlyWrap(),
        securityHandler, new InetSocketAddress(""localhost"", 1234));
    assertEquals(""Incorrect return code:"", Nfs3Status.NFS3_OK,
        response2.getStatus());
  }
"
"  @Test(timeout = 60000)
  public void testRmdir() throws Exception {
    HdfsFileStatus status = nn.getRpcServer().getFileInfo(testdir);
    long dirId = status.getFileId();
    int namenodeId = Nfs3Utils.getNamenodeId(config);
    XDR xdr_req = new XDR();
    FileHandle handle = new FileHandle(dirId, namenodeId);
    RMDIR3Request req = new RMDIR3Request(handle, ""foo"");
    req.serialize(xdr_req);

    // Attempt by an unprivileged user should fail.
    RMDIR3Response response1 = nfsd.rmdir(xdr_req.asReadOnlyWrap(),
        securityHandlerUnpriviledged,
        new InetSocketAddress(""localhost"", 1234));
    assertEquals(""Incorrect return code:"", Nfs3Status.NFS3ERR_ACCES,
        response1.getStatus());

    // Attempt by a privileged user should pass.
    RMDIR3Response response2 = nfsd.rmdir(xdr_req.asReadOnlyWrap(),
        securityHandler, new InetSocketAddress(""localhost"", 1234));
    assertEquals(""Incorrect return code:"", Nfs3Status.NFS3_OK,
        response2.getStatus());
  }
"
"  @Test(timeout = 60000)
  public void testRename() throws Exception {
    HdfsFileStatus status = nn.getRpcServer().getFileInfo(testdir);
    long dirId = status.getFileId();
    int namenodeId = Nfs3Utils.getNamenodeId(config);
    XDR xdr_req = new XDR();
    FileHandle handle = new FileHandle(dirId, namenodeId);
    RENAME3Request req = new RENAME3Request(handle, ""bar"", handle, ""fubar"");
    req.serialize(xdr_req);
    
    // Attempt by an unprivileged user should fail.
    RENAME3Response response1 = nfsd.rename(xdr_req.asReadOnlyWrap(),
        securityHandlerUnpriviledged,
        new InetSocketAddress(""localhost"", 1234));
    assertEquals(""Incorrect return code:"", Nfs3Status.NFS3ERR_ACCES,
        response1.getStatus());

    // Attempt by a privileged user should pass.
    RENAME3Response response2 = nfsd.rename(xdr_req.asReadOnlyWrap(),
        securityHandler, new InetSocketAddress(""localhost"", 1234));
    assertEquals(""Incorrect return code:"", Nfs3Status.NFS3_OK,
        response2.getStatus());
  }
"
"  @Test(timeout = 60000)
  public void testReaddir() throws Exception {
    HdfsFileStatus status = nn.getRpcServer().getFileInfo(testdir);
    long dirId = status.getFileId();
    int namenodeId = Nfs3Utils.getNamenodeId(config);
    FileHandle handle = new FileHandle(dirId, namenodeId);
    XDR xdr_req = new XDR();
    READDIR3Request req = new READDIR3Request(handle, 0, 0, 100);
    req.serialize(xdr_req);

    // Attempt by an unpriviledged user should fail.
    READDIR3Response response1 = nfsd.readdir(xdr_req.asReadOnlyWrap(),
        securityHandlerUnpriviledged,
        new InetSocketAddress(""localhost"", 1234));
    assertEquals(""Incorrect return code:"", Nfs3Status.NFS3ERR_ACCES,
        response1.getStatus());

    // Attempt by a priviledged user should pass.
    READDIR3Response response2 = nfsd.readdir(xdr_req.asReadOnlyWrap(),
        securityHandler, new InetSocketAddress(""localhost"", 1234));
    assertEquals(""Incorrect return code:"", Nfs3Status.NFS3_OK,
        response2.getStatus());
  }
"
"  @Test(timeout = 60000)
  public void testReaddirplus() throws Exception {
    HdfsFileStatus status = nn.getRpcServer().getFileInfo(testdir);
    long dirId = status.getFileId();
    int namenodeId = Nfs3Utils.getNamenodeId(config);
    FileHandle handle = new FileHandle(dirId, namenodeId);
    XDR xdr_req = new XDR();
    READDIRPLUS3Request req = new READDIRPLUS3Request(handle, 0, 0, 3, 2);
    req.serialize(xdr_req);
    
    // Attempt by an unprivileged user should fail.
    READDIRPLUS3Response response1 = nfsd.readdirplus(xdr_req.asReadOnlyWrap(),
        securityHandlerUnpriviledged,
        new InetSocketAddress(""localhost"", 1234));
    assertEquals(""Incorrect return code:"", Nfs3Status.NFS3ERR_ACCES,
        response1.getStatus());

    // Attempt by a privileged user should pass.
    READDIRPLUS3Response response2 = nfsd.readdirplus(xdr_req.asReadOnlyWrap(),
        securityHandler, new InetSocketAddress(""localhost"", 1234));
    assertEquals(""Incorrect return code:"", Nfs3Status.NFS3_OK,
        response2.getStatus());
  }
"
"  @Test(timeout = 60000)
  public void testFsstat() throws Exception {
    HdfsFileStatus status = nn.getRpcServer().getFileInfo(""/tmp/bar"");
    long dirId = status.getFileId();
    int namenodeId = Nfs3Utils.getNamenodeId(config);
    FileHandle handle = new FileHandle(dirId, namenodeId);
    XDR xdr_req = new XDR();
    FSSTAT3Request req = new FSSTAT3Request(handle);
    req.serialize(xdr_req);
    
    // Attempt by an unpriviledged user should fail.
    FSSTAT3Response response1 = nfsd.fsstat(xdr_req.asReadOnlyWrap(),
        securityHandlerUnpriviledged,
        new InetSocketAddress(""localhost"", 1234));
    assertEquals(""Incorrect return code:"", Nfs3Status.NFS3ERR_ACCES,
        response1.getStatus());

    // Attempt by a priviledged user should pass.
    FSSTAT3Response response2 = nfsd.fsstat(xdr_req.asReadOnlyWrap(),
        securityHandler, new InetSocketAddress(""localhost"", 1234));
    assertEquals(""Incorrect return code:"", Nfs3Status.NFS3_OK,
        response2.getStatus());
  }
"
"  @Test(timeout = 60000)
  public void testFsinfo() throws Exception {
    HdfsFileStatus status = nn.getRpcServer().getFileInfo(""/tmp/bar"");
    long dirId = status.getFileId();
    int namenodeId = Nfs3Utils.getNamenodeId(config);
    FileHandle handle = new FileHandle(dirId, namenodeId);
    XDR xdr_req = new XDR();
    FSINFO3Request req = new FSINFO3Request(handle);
    req.serialize(xdr_req);
    
    // Attempt by an unpriviledged user should fail.
    FSINFO3Response response1 = nfsd.fsinfo(xdr_req.asReadOnlyWrap(),
        securityHandlerUnpriviledged,
        new InetSocketAddress(""localhost"", 1234));
    assertEquals(""Incorrect return code:"", Nfs3Status.NFS3ERR_ACCES,
        response1.getStatus());

    // Attempt by a priviledged user should pass.
    FSINFO3Response response2 = nfsd.fsinfo(xdr_req.asReadOnlyWrap(),
        securityHandler, new InetSocketAddress(""localhost"", 1234));
    assertEquals(""Incorrect return code:"", Nfs3Status.NFS3_OK,
        response2.getStatus());
  }
"
"  @Test(timeout = 60000)
  public void testPathconf() throws Exception {
    HdfsFileStatus status = nn.getRpcServer().getFileInfo(""/tmp/bar"");
    long dirId = status.getFileId();
    int namenodeId = Nfs3Utils.getNamenodeId(config);
    FileHandle handle = new FileHandle(dirId, namenodeId);
    XDR xdr_req = new XDR();
    PATHCONF3Request req = new PATHCONF3Request(handle);
    req.serialize(xdr_req);
    
    // Attempt by an unpriviledged user should fail.
    PATHCONF3Response response1 = nfsd.pathconf(xdr_req.asReadOnlyWrap(),
        securityHandlerUnpriviledged,
        new InetSocketAddress(""localhost"", 1234));
    assertEquals(""Incorrect return code:"", Nfs3Status.NFS3ERR_ACCES,
        response1.getStatus());

    // Attempt by a priviledged user should pass.
    PATHCONF3Response response2 = nfsd.pathconf(xdr_req.asReadOnlyWrap(),
        securityHandler, new InetSocketAddress(""localhost"", 1234));
    assertEquals(""Incorrect return code:"", Nfs3Status.NFS3_OK,
        response2.getStatus());
  }
"
"  @Test(timeout = 60000)
  public void testCommit() throws Exception {
    HdfsFileStatus status = nn.getRpcServer().getFileInfo(""/tmp/bar"");
    long dirId = status.getFileId();
    int namenodeId = Nfs3Utils.getNamenodeId(config);
    FileHandle handle = new FileHandle(dirId, namenodeId);
    XDR xdr_req = new XDR();
    COMMIT3Request req = new COMMIT3Request(handle, 0, 5);
    req.serialize(xdr_req);

    Channel ch = Mockito.mock(Channel.class);

    // Attempt by an unpriviledged user should fail.
    COMMIT3Response response1 = nfsd.commit(xdr_req.asReadOnlyWrap(),
        ch, 1, securityHandlerUnpriviledged,
        new InetSocketAddress(""localhost"", 1234));
    assertEquals(""Incorrect return code:"", Nfs3Status.NFS3ERR_ACCES,
        response1.getStatus());

    // Attempt by a priviledged user should pass.
    COMMIT3Response response2 = nfsd.commit(xdr_req.asReadOnlyWrap(),
        ch, 1, securityHandler,
        new InetSocketAddress(""localhost"", 1234));
    assertEquals(""Incorrect COMMIT3Response:"", null, response2);
  }
"
"  @Test(timeout=10000)
  public void testIdempotent() {
    Object[][] procedures = {
        { Nfs3Constant.NFSPROC3.NULL, 1 },
        { Nfs3Constant.NFSPROC3.GETATTR, 1 },
        { Nfs3Constant.NFSPROC3.SETATTR, 1 },
        { Nfs3Constant.NFSPROC3.LOOKUP, 1 },
        { Nfs3Constant.NFSPROC3.ACCESS, 1 },
        { Nfs3Constant.NFSPROC3.READLINK, 1 },
        { Nfs3Constant.NFSPROC3.READ, 1 },
        { Nfs3Constant.NFSPROC3.WRITE, 1 },
        { Nfs3Constant.NFSPROC3.CREATE, 0 },
        { Nfs3Constant.NFSPROC3.MKDIR, 0 },
        { Nfs3Constant.NFSPROC3.SYMLINK, 0 },
        { Nfs3Constant.NFSPROC3.MKNOD, 0 },
        { Nfs3Constant.NFSPROC3.REMOVE, 0 },
        { Nfs3Constant.NFSPROC3.RMDIR, 0 },
        { Nfs3Constant.NFSPROC3.RENAME, 0 },
        { Nfs3Constant.NFSPROC3.LINK, 0 },
        { Nfs3Constant.NFSPROC3.READDIR, 1 },
        { Nfs3Constant.NFSPROC3.READDIRPLUS, 1 },
        { Nfs3Constant.NFSPROC3.FSSTAT, 1 },
        { Nfs3Constant.NFSPROC3.FSINFO, 1 },
        { Nfs3Constant.NFSPROC3.PATHCONF, 1 },
        { Nfs3Constant.NFSPROC3.COMMIT, 1 } };
    for (Object[] procedure : procedures) {
      boolean idempotent = procedure[1].equals(Integer.valueOf(1));
      Nfs3Constant.NFSPROC3 proc = (Nfs3Constant.NFSPROC3)procedure[0];
      if (idempotent) {
        Assert.assertTrue((""Procedure "" + proc + "" should be idempotent""),
            proc.isIdempotent());
      } else {
        Assert.assertFalse((""Procedure "" + proc + "" should be non-idempotent""),
            proc.isIdempotent());
      }
    }
  }
"
"  @Test
  public void testDeprecatedKeys() {
    NfsConfiguration conf = new NfsConfiguration();
    conf.setInt(""nfs3.server.port"", 998);
    assertTrue(conf.getInt(NfsConfigKeys.DFS_NFS_SERVER_PORT_KEY, 0) == 998);

    conf.setInt(""nfs3.mountd.port"", 999);
    assertTrue(conf.getInt(NfsConfigKeys.DFS_NFS_MOUNTD_PORT_KEY, 0) == 999);

    conf.set(""dfs.nfs.exports.allowed.hosts"", ""host1"");
    assertTrue(conf.get(CommonConfigurationKeys.NFS_EXPORTS_ALLOWED_HOSTS_KEY)
        .equals(""host1""));

    conf.setInt(""dfs.nfs.exports.cache.expirytime.millis"", 1000);
    assertTrue(conf.getInt(
        Nfs3Constant.NFS_EXPORTS_CACHE_EXPIRYTIME_MILLIS_KEY, 0) == 1000);

    conf.setInt(""hadoop.nfs.userupdate.milly"", 10);
    assertTrue(conf.getInt(IdMappingConstant.USERGROUPID_UPDATE_MILLIS_KEY, 0) == 10);

    conf.set(""dfs.nfs3.dump.dir"", ""/nfs/tmp"");
    assertTrue(conf.get(NfsConfigKeys.DFS_NFS_FILE_DUMP_DIR_KEY).equals(
        ""/nfs/tmp""));

    conf.setBoolean(""dfs.nfs3.enableDump"", false);
    assertTrue(conf.getBoolean(NfsConfigKeys.DFS_NFS_FILE_DUMP_KEY, true) == false);

    conf.setInt(""dfs.nfs3.max.open.files"", 500);
    assertTrue(conf.getInt(NfsConfigKeys.DFS_NFS_MAX_OPEN_FILES_KEY, 0) == 500);

    conf.setInt(""dfs.nfs3.stream.timeout"", 6000);
    assertTrue(conf.getInt(NfsConfigKeys.DFS_NFS_STREAM_TIMEOUT_KEY, 0) == 6000);

    conf.set(""dfs.nfs3.export.point"", ""/dir1"");
    assertTrue(conf.get(NfsConfigKeys.DFS_NFS_EXPORT_POINT_KEY).equals(""/dir1""));
  }
"
"  @Test
  public void testGetAccessRightsForUserGroup() throws IOException {
    Nfs3FileAttributes attr = Mockito.mock(Nfs3FileAttributes.class);
    Mockito.when(attr.getUid()).thenReturn(2);
    Mockito.when(attr.getGid()).thenReturn(3);
    Mockito.when(attr.getMode()).thenReturn(448); // 700
    Mockito.when(attr.getType()).thenReturn(NfsFileType.NFSREG.toValue());
    assertEquals(""No access should be allowed as UID does not match attribute over mode 700"",
      0, Nfs3Utils.getAccessRightsForUserGroup(3, 3, null, attr));
    Mockito.when(attr.getUid()).thenReturn(2);
    Mockito.when(attr.getGid()).thenReturn(3);
    Mockito.when(attr.getMode()).thenReturn(56); // 070
    Mockito.when(attr.getType()).thenReturn(NfsFileType.NFSREG.toValue());
    assertEquals(""No access should be allowed as GID does not match attribute over mode 070"",
      0, Nfs3Utils.getAccessRightsForUserGroup(2, 4, null, attr));
    Mockito.when(attr.getUid()).thenReturn(2);
    Mockito.when(attr.getGid()).thenReturn(3);
    Mockito.when(attr.getMode()).thenReturn(7); // 007
    Mockito.when(attr.getType()).thenReturn(NfsFileType.NFSREG.toValue());
    assertEquals(""Access should be allowed as mode is 007 and UID/GID do not match"",
      61 /* RWX */, Nfs3Utils.getAccessRightsForUserGroup(1, 4, new int[] {5, 6}, attr));
    Mockito.when(attr.getUid()).thenReturn(2);
    Mockito.when(attr.getGid()).thenReturn(10);
    Mockito.when(attr.getMode()).thenReturn(288); // 440
    Mockito.when(attr.getType()).thenReturn(NfsFileType.NFSREG.toValue());
    assertEquals(""Access should be allowed as mode is 440 and Aux GID does match"",
      1 /* R */, Nfs3Utils.getAccessRightsForUserGroup(3, 4, new int[] {5, 16, 10}, attr));
    Mockito.when(attr.getUid()).thenReturn(2);
    Mockito.when(attr.getGid()).thenReturn(10);
    Mockito.when(attr.getMode()).thenReturn(448); // 700
    Mockito.when(attr.getType()).thenReturn(NfsFileType.NFSDIR.toValue());
    assertEquals(""Access should be allowed for dir as mode is 700 and UID does match"",
      31 /* Lookup */, Nfs3Utils.getAccessRightsForUserGroup(2, 4, new int[] {5, 16, 10}, attr));
    assertEquals(""No access should be allowed for dir as mode is 700 even though GID does match"",
      0, Nfs3Utils.getAccessRightsForUserGroup(3, 10, new int[] {5, 16, 4}, attr));
    assertEquals(""No access should be allowed for dir as mode is 700 even though AuxGID does match"",
      0, Nfs3Utils.getAccessRightsForUserGroup(3, 20, new int[] {5, 10}, attr));
    
    Mockito.when(attr.getUid()).thenReturn(2);
    Mockito.when(attr.getGid()).thenReturn(10);
    Mockito.when(attr.getMode()).thenReturn(457); // 711
    Mockito.when(attr.getType()).thenReturn(NfsFileType.NFSDIR.toValue());
    assertEquals(""Access should be allowed for dir as mode is 711 and GID matches"",
        2 /* Lookup */, Nfs3Utils.getAccessRightsForUserGroup(3, 10, new int[] {5, 16, 11}, attr));
  }
"
"  @Test
  public void testEviction() throws IOException {
    NfsConfiguration conf = new NfsConfiguration();
    conf.set(FileSystem.FS_DEFAULT_NAME_KEY, ""hdfs://localhost"");

    // Only one entry will be in the cache
    final int MAX_CACHE_SIZE = 1;

    DFSClientCache cache = new DFSClientCache(conf, MAX_CACHE_SIZE);

    int namenodeId = Nfs3Utils.getNamenodeId(conf);
    DFSClient c1 = cache.getDfsClient(""test1"", namenodeId);
    assertTrue(cache.getDfsClient(""test1"", namenodeId)
        .toString().contains(""ugi=test1""));
    assertEquals(c1, cache.getDfsClient(""test1"", namenodeId));
    assertFalse(isDfsClientClose(c1));

    cache.getDfsClient(""test2"", namenodeId);
    assertTrue(isDfsClientClose(c1));
    assertTrue(""cache size should be the max size or less"",
        cache.getClientCache().size() <= MAX_CACHE_SIZE);
  }
"
"  @Test
  public void testGetUserGroupInformationSecure() throws IOException {
    String userName = ""user1"";
    String currentUser = ""test-user"";


    NfsConfiguration conf = new NfsConfiguration();
    conf.set(FileSystem.FS_DEFAULT_NAME_KEY, ""hdfs://localhost"");
    UserGroupInformation currentUserUgi
            = UserGroupInformation.createRemoteUser(currentUser);
    currentUserUgi.setAuthenticationMethod(KERBEROS);
    UserGroupInformation.setLoginUser(currentUserUgi);

    DFSClientCache cache = new DFSClientCache(conf);
    UserGroupInformation ugiResult
            = cache.getUserGroupInformation(userName, currentUserUgi);

    assertThat(ugiResult.getUserName(), is(userName));
    assertThat(ugiResult.getRealUser(), is(currentUserUgi));
    assertThat(
            ugiResult.getAuthenticationMethod(),
            is(UserGroupInformation.AuthenticationMethod.PROXY));
  }
"
"  @Test
  public void testGetUserGroupInformation() throws IOException {
    String userName = ""user1"";
    String currentUser = ""currentUser"";

    UserGroupInformation currentUserUgi = UserGroupInformation
            .createUserForTesting(currentUser, new String[0]);
    NfsConfiguration conf = new NfsConfiguration();
    conf.set(FileSystem.FS_DEFAULT_NAME_KEY, ""hdfs://localhost"");
    DFSClientCache cache = new DFSClientCache(conf);
    UserGroupInformation ugiResult
            = cache.getUserGroupInformation(userName, currentUserUgi);

    assertThat(ugiResult.getUserName(), is(userName));
    assertThat(ugiResult.getRealUser(), is(currentUserUgi));
    assertThat(
            ugiResult.getAuthenticationMethod(),
            is(UserGroupInformation.AuthenticationMethod.PROXY));
  }
"
"  @Test
  public void testNumExports() throws Exception {
    Assert.assertEquals(mountd.getExports().size(),
        viewFs.getChildFileSystems().length);
  }
"
"  @Test
  public void testPaths() throws Exception {
    Assert.assertEquals(hdfs1.resolvePath(new Path(""/user1/file1"")),
        viewFs.resolvePath(new Path(""/hdfs1/file1"")));
    Assert.assertEquals(hdfs1.resolvePath(new Path(""/user1/file2"")),
        viewFs.resolvePath(new Path(""/hdfs1/file2"")));
    Assert.assertEquals(hdfs2.resolvePath(new Path(""/user2/dir2"")),
        viewFs.resolvePath(new Path(""/hdfs2/dir2"")));
  }
"
"  @Test
  public void testFileStatus() throws Exception {
    HdfsFileStatus status = nn1.getRpcServer().getFileInfo(""/user1/file1"");
    FileStatus st = viewFs.getFileStatus(new Path(""/hdfs1/file1""));
    Assert.assertEquals(st.isDirectory(), status.isDirectory());

    HdfsFileStatus status2 = nn2.getRpcServer().getFileInfo(""/user2/dir2"");
    FileStatus st2 = viewFs.getFileStatus(new Path(""/hdfs2/dir2""));
    Assert.assertEquals(st2.isDirectory(), status2.isDirectory());
  }
"
"  @Test (timeout = 60000)
  public void testNfsAccessNN1() throws Exception {
    HdfsFileStatus status = nn1.getRpcServer().getFileInfo(""/user1/file1"");
    int namenodeId = Nfs3Utils.getNamenodeId(config, hdfs1.getUri());
    testNfsGetAttrResponse(status.getFileId(), namenodeId, Nfs3Status.NFS3_OK);
  }
"
"  @Test (timeout = 60000)
  public void testNfsAccessNN2() throws Exception {
    HdfsFileStatus status = nn2.getRpcServer().getFileInfo(""/user2/dir2"");
    int namenodeId = Nfs3Utils.getNamenodeId(config, hdfs2.getUri());
    testNfsGetAttrResponse(status.getFileId(), namenodeId, Nfs3Status.NFS3_OK);
  }
"
"  @Test (timeout = 60000)
  public void testWrongNfsAccess() throws Exception {
    DFSTestUtil.createFile(viewFs, new Path(""/hdfs1/file3""), 0, (short) 1, 0);
    HdfsFileStatus status = nn1.getRpcServer().getFileInfo(""/user1/file3"");
    int namenodeId = Nfs3Utils.getNamenodeId(config, hdfs2.getUri());
    testNfsGetAttrResponse(status.getFileId(), namenodeId,
        Nfs3Status.NFS3ERR_IO);
  }
"
"  @Test (timeout = 60000)
  public void testNfsWriteNN1() throws Exception {
    HdfsFileStatus status = nn1.getRpcServer().getFileInfo(""/user1/write1"");
    int namenodeId = Nfs3Utils.getNamenodeId(config, hdfs1.getUri());
    testNfsWriteResponse(status.getFileId(), namenodeId);
  }
"
"  @Test (timeout = 60000)
  public void testNfsWriteNN2() throws Exception {
    HdfsFileStatus status = nn2.getRpcServer().getFileInfo(""/user2/write2"");
    int namenodeId = Nfs3Utils.getNamenodeId(config, hdfs2.getUri());
    testNfsWriteResponse(status.getFileId(), namenodeId);
  }
"
"  @Test (timeout = 60000)
  public void testNfsRenameMultiNN() throws Exception {
    HdfsFileStatus fromFileStatus = nn1.getRpcServer().getFileInfo(""/user1"");
    int fromNNId = Nfs3Utils.getNamenodeId(config, hdfs1.getUri());
    FileHandle fromHandle =
        new FileHandle(fromFileStatus.getFileId(), fromNNId);

    HdfsFileStatus toFileStatus = nn2.getRpcServer().getFileInfo(""/user2"");
    int toNNId = Nfs3Utils.getNamenodeId(config, hdfs2.getUri());
    FileHandle toHandle = new FileHandle(toFileStatus.getFileId(), toNNId);

    HdfsFileStatus statusBeforeRename =
        nn1.getRpcServer().getFileInfo(""/user1/renameMultiNN"");
    Assert.assertEquals(statusBeforeRename.isDirectory(), false);

    testNfsRename(fromHandle, ""renameMultiNN"",
        toHandle, ""renameMultiNNFail"", Nfs3Status.NFS3ERR_INVAL);

    HdfsFileStatus statusAfterRename =
        nn2.getRpcServer().getFileInfo(""/user2/renameMultiNNFail"");
    Assert.assertEquals(statusAfterRename, null);

    statusAfterRename = nn1.getRpcServer().getFileInfo(""/user1/renameMultiNN"");
    Assert.assertEquals(statusAfterRename.isDirectory(), false);
  }
"
"  @Test (timeout = 60000)
  public void testNfsRenameSingleNN() throws Exception {
    HdfsFileStatus fromFileStatus = nn1.getRpcServer().getFileInfo(""/user1"");
    int fromNNId = Nfs3Utils.getNamenodeId(config, hdfs1.getUri());
    FileHandle fromHandle =
        new FileHandle(fromFileStatus.getFileId(), fromNNId);

    HdfsFileStatus statusBeforeRename =
        nn1.getRpcServer().getFileInfo(""/user1/renameSingleNN"");
    Assert.assertEquals(statusBeforeRename.isDirectory(), false);

    testNfsRename(fromHandle, ""renameSingleNN"",
        fromHandle, ""renameSingleNNSucess"", Nfs3Status.NFS3_OK);

    HdfsFileStatus statusAfterRename =
        nn1.getRpcServer().getFileInfo(""/user1/renameSingleNNSucess"");
    Assert.assertEquals(statusAfterRename.isDirectory(), false);

    statusAfterRename =
        nn1.getRpcServer().getFileInfo(""/user1/renameSingleNN"");
    Assert.assertEquals(statusAfterRename, null);
  }
"
"  @Test
  public void testEviction() throws IOException, InterruptedException {
    NfsConfiguration conf = new NfsConfiguration();

    // Only two entries will be in the cache
    conf.setInt(NfsConfigKeys.DFS_NFS_MAX_OPEN_FILES_KEY, 2);

    DFSClient dfsClient = Mockito.mock(DFSClient.class);
    Nfs3FileAttributes attr = new Nfs3FileAttributes();
    HdfsDataOutputStream fos = Mockito.mock(HdfsDataOutputStream.class);
    Mockito.when(fos.getPos()).thenReturn((long) 0);

    OpenFileCtx context1 = new OpenFileCtx(fos, attr, ""/dumpFilePath"",
        dfsClient, new ShellBasedIdMapping(new NfsConfiguration()));
    OpenFileCtx context2 = new OpenFileCtx(fos, attr, ""/dumpFilePath"",
        dfsClient, new ShellBasedIdMapping(new NfsConfiguration()));
    OpenFileCtx context3 = new OpenFileCtx(fos, attr, ""/dumpFilePath"",
        dfsClient, new ShellBasedIdMapping(new NfsConfiguration()));
    OpenFileCtx context4 = new OpenFileCtx(fos, attr, ""/dumpFilePath"",
        dfsClient, new ShellBasedIdMapping(new NfsConfiguration()));
    OpenFileCtx context5 = new OpenFileCtx(fos, attr, ""/dumpFilePath"",
        dfsClient, new ShellBasedIdMapping(new NfsConfiguration()));

    OpenFileCtxCache cache = new OpenFileCtxCache(conf, 10 * 60 * 100);

    boolean ret = cache.put(new FileHandle(1), context1);
    assertTrue(ret);
    Thread.sleep(1000);
    ret = cache.put(new FileHandle(2), context2);
    assertTrue(ret);
    ret = cache.put(new FileHandle(3), context3);
    assertFalse(ret);
    assertTrue(cache.size() == 2);

    // Wait for the oldest stream to be evict-able, insert again
    Thread.sleep(NfsConfigKeys.DFS_NFS_STREAM_TIMEOUT_MIN_DEFAULT);
    assertTrue(cache.size() == 2);

    ret = cache.put(new FileHandle(3), context3);
    assertTrue(ret);
    assertTrue(cache.size() == 2);
    assertTrue(cache.get(new FileHandle(1)) == null);

    // Test inactive entry is evicted immediately
    context3.setActiveStatusForTest(false);
    ret = cache.put(new FileHandle(4), context4);
    assertTrue(ret);

    // Now the cache has context2 and context4
    // Test eviction failure if all entries have pending work.
    context2.getPendingWritesForTest().put(new OffsetRange(0, 100),
        new WriteCtx(null, 0, 0, 0, null, null, null, 0, false, null));
    context4.getPendingCommitsForTest().put(new Long(100),
        new CommitCtx(0, null, 0, attr));
    Thread.sleep(NfsConfigKeys.DFS_NFS_STREAM_TIMEOUT_MIN_DEFAULT);
    ret = cache.put(new FileHandle(5), context5);
    assertFalse(ret);
  }
"
"  @Test
  public void testScan() throws IOException, InterruptedException {
    NfsConfiguration conf = new NfsConfiguration();

    // Only two entries will be in the cache
    conf.setInt(NfsConfigKeys.DFS_NFS_MAX_OPEN_FILES_KEY, 2);

    DFSClient dfsClient = Mockito.mock(DFSClient.class);
    Nfs3FileAttributes attr = new Nfs3FileAttributes();
    HdfsDataOutputStream fos = Mockito.mock(HdfsDataOutputStream.class);
    Mockito.when(fos.getPos()).thenReturn((long) 0);

    OpenFileCtx context1 = new OpenFileCtx(fos, attr, ""/dumpFilePath"",
        dfsClient, new ShellBasedIdMapping(new NfsConfiguration()));
    OpenFileCtx context2 = new OpenFileCtx(fos, attr, ""/dumpFilePath"",
        dfsClient, new ShellBasedIdMapping(new NfsConfiguration()));
    OpenFileCtx context3 = new OpenFileCtx(fos, attr, ""/dumpFilePath"",
        dfsClient, new ShellBasedIdMapping(new NfsConfiguration()));
    OpenFileCtx context4 = new OpenFileCtx(fos, attr, ""/dumpFilePath"",
        dfsClient, new ShellBasedIdMapping(new NfsConfiguration()));

    OpenFileCtxCache cache = new OpenFileCtxCache(conf, 10 * 60 * 100);

    // Test cleaning expired entry
    boolean ret = cache.put(new FileHandle(1), context1);
    assertTrue(ret);
    ret = cache.put(new FileHandle(2), context2);
    assertTrue(ret);
    Thread.sleep(NfsConfigKeys.DFS_NFS_STREAM_TIMEOUT_MIN_DEFAULT + 1);
    cache.scan(NfsConfigKeys.DFS_NFS_STREAM_TIMEOUT_MIN_DEFAULT);
    assertTrue(cache.size() == 0);

    // Test cleaning inactive entry
    ret = cache.put(new FileHandle(3), context3);
    assertTrue(ret);
    ret = cache.put(new FileHandle(4), context4);
    assertTrue(ret);
    context3.setActiveStatusForTest(false);
    cache.scan(NfsConfigKeys.DFS_NFS_STREAM_TIMEOUT_DEFAULT);
    assertTrue(cache.size() == 1);
    assertTrue(cache.get(new FileHandle(3)) == null);
    assertTrue(cache.get(new FileHandle(4)) != null);
  }
"
"  @Test(timeout = 60000)
  public void testClientAccessPrivilegeForRemove() throws Exception {
    // Configure ro access for nfs1 service
    config.set(""dfs.nfs.exports.allowed.hosts"", ""* ro"");

    // Start nfs
    Nfs3 nfs = new Nfs3(config);
    nfs.startServiceInternal(false);

    RpcProgramNfs3 nfsd = (RpcProgramNfs3) nfs.getRpcProgram();

    // Create a remove request
    HdfsFileStatus status = nn.getRpcServer().getFileInfo(testdir);
    long dirId = status.getFileId();
    int namenodeId = Nfs3Utils.getNamenodeId(config);

    XDR xdr_req = new XDR();
    FileHandle handle = new FileHandle(dirId, namenodeId);
    handle.serialize(xdr_req);
    xdr_req.writeString(""f1"");

    // Remove operation
    REMOVE3Response response = nfsd.remove(xdr_req.asReadOnlyWrap(),
        securityHandler, new InetSocketAddress(""localhost"", 1234));

    // Assert on return code
    assertEquals(""Incorrect return code"", Nfs3Status.NFS3ERR_ACCES,
        response.getStatus());

  }
"
"  @Test
  public void testHdfsExportPoint() throws IOException {
    NfsConfiguration config = new NfsConfiguration();
    MiniDFSCluster cluster = null;

    // Use emphral port in case tests are running in parallel
    config.setInt(""nfs3.mountd.port"", 0);
    config.setInt(""nfs3.server.port"", 0);
    config.set(""nfs.http.address"", ""0.0.0.0:0"");

    try {
      cluster = new MiniDFSCluster.Builder(config).numDataNodes(1).build();
      cluster.waitActive();

      // Start nfs
      final Nfs3 nfsServer = new Nfs3(config);
      nfsServer.startServiceInternal(false);

      Mountd mountd = nfsServer.getMountd();
      RpcProgramMountd rpcMount = (RpcProgramMountd) mountd.getRpcProgram();
      assertTrue(rpcMount.getExports().size() == 1);

      String exportInMountd = rpcMount.getExports().get(0);
      assertTrue(exportInMountd.equals(""/""));

    } finally {
      if (cluster != null) {
        cluster.shutdown();
      }
    }
  }
"
"  @Test
  public void testViewFsMultipleExportPoint() throws IOException {
    NfsConfiguration config = new NfsConfiguration();
    MiniDFSCluster cluster = null;
    String clusterName = RandomStringUtils.randomAlphabetic(10);

    String exportPoint = ""/hdfs1,/hdfs2"";
    config.setStrings(NfsConfigKeys.DFS_NFS_EXPORT_POINT_KEY, exportPoint);
    config.set(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY,
        FsConstants.VIEWFS_SCHEME + ""://"" + clusterName);
    // Use emphral port in case tests are running in parallel
    config.setInt(""nfs3.mountd.port"", 0);
    config.setInt(""nfs3.server.port"", 0);
    config.set(""nfs.http.address"", ""0.0.0.0:0"");

    try {
      cluster =
          new MiniDFSCluster.Builder(config).nnTopology(
              MiniDFSNNTopology.simpleFederatedTopology(2))
              .numDataNodes(2)
              .build();
      cluster.waitActive();
      DistributedFileSystem hdfs1 = cluster.getFileSystem(0);
      DistributedFileSystem hdfs2 = cluster.getFileSystem(1);
      cluster.waitActive();
      Path base1 = new Path(""/user1"");
      Path base2 = new Path(""/user2"");
      hdfs1.delete(base1, true);
      hdfs2.delete(base2, true);
      hdfs1.mkdirs(base1);
      hdfs2.mkdirs(base2);
      ConfigUtil.addLink(config, clusterName, ""/hdfs1"",
          hdfs1.makeQualified(base1).toUri());
      ConfigUtil.addLink(config, clusterName, ""/hdfs2"",
          hdfs2.makeQualified(base2).toUri());

      // Start nfs
      final Nfs3 nfsServer = new Nfs3(config);
      nfsServer.startServiceInternal(false);

      Mountd mountd = nfsServer.getMountd();
      RpcProgramMountd rpcMount = (RpcProgramMountd) mountd.getRpcProgram();
      assertTrue(rpcMount.getExports().size() == 2);

      String exportInMountd1 = rpcMount.getExports().get(0);
      assertTrue(exportInMountd1.equals(""/hdfs1""));

      String exportInMountd2 = rpcMount.getExports().get(1);
      assertTrue(exportInMountd2.equals(""/hdfs2""));

    } finally {
      if (cluster != null) {
        cluster.shutdown();
      }
    }
  }
"
"  @Test
  public void testViewFsInternalExportPoint() throws IOException {
    NfsConfiguration config = new NfsConfiguration();
    MiniDFSCluster cluster = null;
    String clusterName = RandomStringUtils.randomAlphabetic(10);

    String exportPoint = ""/hdfs1/subpath"";
    config.setStrings(NfsConfigKeys.DFS_NFS_EXPORT_POINT_KEY, exportPoint);
    config.set(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY,
        FsConstants.VIEWFS_SCHEME + ""://"" + clusterName);
    // Use emphral port in case tests are running in parallel
    config.setInt(""nfs3.mountd.port"", 0);
    config.setInt(""nfs3.server.port"", 0);
    config.set(""nfs.http.address"", ""0.0.0.0:0"");

    try {
      cluster =
          new MiniDFSCluster.Builder(config).nnTopology(
              MiniDFSNNTopology.simpleFederatedTopology(2))
              .numDataNodes(2)
              .build();
      cluster.waitActive();
      DistributedFileSystem hdfs1 = cluster.getFileSystem(0);
      DistributedFileSystem hdfs2 = cluster.getFileSystem(1);
      cluster.waitActive();
      Path base1 = new Path(""/user1"");
      Path base2 = new Path(""/user2"");
      hdfs1.delete(base1, true);
      hdfs2.delete(base2, true);
      hdfs1.mkdirs(base1);
      hdfs2.mkdirs(base2);
      ConfigUtil.addLink(config, clusterName, ""/hdfs1"",
          hdfs1.makeQualified(base1).toUri());
      ConfigUtil.addLink(config, clusterName, ""/hdfs2"",
          hdfs2.makeQualified(base2).toUri());
      Path subPath = new Path(base1, ""subpath"");
      hdfs1.delete(subPath, true);
      hdfs1.mkdirs(subPath);

      // Start nfs
      final Nfs3 nfsServer = new Nfs3(config);
      nfsServer.startServiceInternal(false);

      Mountd mountd = nfsServer.getMountd();
      RpcProgramMountd rpcMount = (RpcProgramMountd) mountd.getRpcProgram();
      assertTrue(rpcMount.getExports().size() == 1);

      String exportInMountd = rpcMount.getExports().get(0);
      assertTrue(exportInMountd.equals(exportPoint));
    } finally {
      if (cluster != null) {
        cluster.shutdown();
      }
    }
  }
"
"  @Test
  public void testViewFsRootExportPoint() throws IOException {
    NfsConfiguration config = new NfsConfiguration();
    MiniDFSCluster cluster = null;
    String clusterName = RandomStringUtils.randomAlphabetic(10);

    String exportPoint = ""/"";
    config.setStrings(NfsConfigKeys.DFS_NFS_EXPORT_POINT_KEY, exportPoint);
    config.set(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY,
        FsConstants.VIEWFS_SCHEME + ""://"" + clusterName);
    // Use emphral port in case tests are running in parallel
    config.setInt(""nfs3.mountd.port"", 0);
    config.setInt(""nfs3.server.port"", 0);
    config.set(""nfs.http.address"", ""0.0.0.0:0"");

    try {
      cluster =
          new MiniDFSCluster.Builder(config).nnTopology(
              MiniDFSNNTopology.simpleFederatedTopology(2))
              .numDataNodes(2)
              .build();
      cluster.waitActive();
      DistributedFileSystem hdfs1 = cluster.getFileSystem(0);
      DistributedFileSystem hdfs2 = cluster.getFileSystem(1);
      cluster.waitActive();
      Path base1 = new Path(""/user1"");
      Path base2 = new Path(""/user2"");
      hdfs1.delete(base1, true);
      hdfs2.delete(base2, true);
      hdfs1.mkdirs(base1);
      hdfs2.mkdirs(base2);
      ConfigUtil.addLink(config, clusterName, ""/hdfs1"",
          hdfs1.makeQualified(base1).toUri());
      ConfigUtil.addLink(config, clusterName, ""/hdfs2"",
          hdfs2.makeQualified(base2).toUri());

      exception.expect(FileSystemException.class);
      exception.
          expectMessage(""Only HDFS is supported as underlyingFileSystem, ""
              + ""fs scheme:viewfs"");
      // Start nfs
      final Nfs3 nfsServer = new Nfs3(config);
      nfsServer.startServiceInternal(false);
    } finally {
      if (cluster != null) {
        cluster.shutdown();
      }
    }
  }
"
"  @Test
  public void testHdfsInternalExportPoint() throws IOException {
    NfsConfiguration config = new NfsConfiguration();
    MiniDFSCluster cluster = null;

    String exportPoint = ""/myexport1"";
    config.setStrings(NfsConfigKeys.DFS_NFS_EXPORT_POINT_KEY, exportPoint);
    // Use emphral port in case tests are running in parallel
    config.setInt(""nfs3.mountd.port"", 0);
    config.setInt(""nfs3.server.port"", 0);
    config.set(""nfs.http.address"", ""0.0.0.0:0"");
    Path base = new Path(exportPoint);

    try {
      cluster = new MiniDFSCluster.Builder(config).numDataNodes(1).build();
      cluster.waitActive();
      DistributedFileSystem hdfs = cluster.getFileSystem(0);
      hdfs.delete(base, true);
      hdfs.mkdirs(base);

      // Start nfs
      final Nfs3 nfsServer = new Nfs3(config);
      nfsServer.startServiceInternal(false);

      Mountd mountd = nfsServer.getMountd();
      RpcProgramMountd rpcMount = (RpcProgramMountd) mountd.getRpcProgram();
      assertTrue(rpcMount.getExports().size() == 1);

      String exportInMountd = rpcMount.getExports().get(0);
      assertTrue(exportInMountd.equals(exportPoint));

    } finally {
      if (cluster != null) {
        cluster.shutdown();
      }
    }
  }
"
"  @Test
  public void testInvalidFsExport() throws IOException {
    NfsConfiguration config = new NfsConfiguration();
    MiniDFSCluster cluster = null;

    // Use emphral port in case tests are running in parallel
    config.setInt(""nfs3.mountd.port"", 0);
    config.setInt(""nfs3.server.port"", 0);
    config.set(""nfs.http.address"", ""0.0.0.0:0"");

    try {
      cluster = new MiniDFSCluster.Builder(config).numDataNodes(1).build();
      cluster.waitActive();
      config.set(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY,
          FsConstants.LOCAL_FS_URI.toString());

      exception.expect(FileSystemException.class);
      exception.
          expectMessage(""Only HDFS is supported as underlyingFileSystem, ""
              + ""fs scheme:file"");
      // Start nfs
      final Nfs3 nfsServer = new Nfs3(config);
      nfsServer.startServiceInternal(false);
    } finally {
      if (cluster != null) {
        cluster.shutdown();
      }
    }
  }
"
"  @Test
  public void testStart() throws IOException {
    // Start minicluster
    NfsConfiguration config = new NfsConfiguration();
    MiniDFSCluster cluster = new MiniDFSCluster.Builder(config).numDataNodes(1)
        .build();
    cluster.waitActive();
    
    // Use emphral port in case tests are running in parallel
    config.setInt(""nfs3.mountd.port"", 0);
    config.setInt(""nfs3.server.port"", 0);
    
    int newTimeoutMillis = 1000; // 1s
    // Set the new portmap rpc timeout values and check
    config.setInt(NfsConfigKeys.NFS_UDP_CLIENT_PORTMAP_TIMEOUT_MILLIS_KEY,
                  newTimeoutMillis);
    assertTrue(config.getInt(
                      NfsConfigKeys.NFS_UDP_CLIENT_PORTMAP_TIMEOUT_MILLIS_KEY,
          0) == newTimeoutMillis);

    // Start nfs
    Nfs3 nfs3 = new Nfs3(config);
    nfs3.startServiceInternal(false);

    RpcProgramMountd mountd = (RpcProgramMountd) nfs3.getMountd()
        .getRpcProgram();
    mountd.nullOp(new XDR(), 1234, InetAddress.getByName(""localhost""));
    assertTrue(mountd.getPortmapUdpTimeoutMillis() == newTimeoutMillis);
    RpcProgramNfs3 nfsd = (RpcProgramNfs3) nfs3.getRpcProgram();
    nfsd.nullProcedure();
    assertTrue(nfsd.getPortmapUdpTimeoutMillis() == newTimeoutMillis);
    
    cluster.shutdown();
  }
"
"  @Test
  public void testContains() throws Exception {
    DatanodeDescriptor nodeNotInMap = 
      DFSTestUtil.getDatanodeDescriptor(""8.8.8.8"", ""/d2/r4"");
    for (int i=0; i < dataNodes.length; i++) {
      assertTrue(cluster.contains(dataNodes[i]));
    }
    assertFalse(cluster.contains(nodeNotInMap));
  }
"
"  @Test
  public void testNumOfChildren() throws Exception {
    assertEquals(cluster.getNumOfLeaves(), dataNodes.length);
  }
"
"  @Test
  public void testCreateInvalidTopology() throws Exception {
    NetworkTopology invalCluster =
        NetworkTopology.getInstance(new Configuration());
    DatanodeDescriptor invalDataNodes[] = new DatanodeDescriptor[] {
        DFSTestUtil.getDatanodeDescriptor(""1.1.1.1"", ""/d1/r1""),
        DFSTestUtil.getDatanodeDescriptor(""2.2.2.2"", ""/d1/r1""),
        DFSTestUtil.getDatanodeDescriptor(""3.3.3.3"", ""/d1"")
    };
    invalCluster.add(invalDataNodes[0]);
    invalCluster.add(invalDataNodes[1]);
    try {
      invalCluster.add(invalDataNodes[2]);
      fail(""expected InvalidTopologyException"");
    } catch (NetworkTopology.InvalidTopologyException e) {
      assertTrue(e.getMessage().startsWith(""Failed to add ""));
      assertTrue(e.getMessage().contains(
          ""You cannot have a rack and a non-rack node at the same "" +
          ""level of the network topology.""));
    }
  }
"
"  @Test
  public void testRacks() throws Exception {
    assertEquals(cluster.getNumOfRacks(), 6);
    assertTrue(cluster.isOnSameRack(dataNodes[0], dataNodes[1]));
    assertFalse(cluster.isOnSameRack(dataNodes[1], dataNodes[2]));
    assertTrue(cluster.isOnSameRack(dataNodes[2], dataNodes[3]));
    assertTrue(cluster.isOnSameRack(dataNodes[3], dataNodes[4]));
    assertFalse(cluster.isOnSameRack(dataNodes[4], dataNodes[5]));
    assertTrue(cluster.isOnSameRack(dataNodes[5], dataNodes[6]));
  }
"
"  @Test
  public void testGetDistance() throws Exception {
    assertEquals(cluster.getDistance(dataNodes[0], dataNodes[0]), 0);
    assertEquals(cluster.getDistance(dataNodes[0], dataNodes[1]), 2);
    assertEquals(cluster.getDistance(dataNodes[0], dataNodes[3]), 4);
    assertEquals(cluster.getDistance(dataNodes[0], dataNodes[6]), 6);
    // verify the distance is zero as long as two nodes have the same path.
    // They don't need to refer to the same object.
    NodeBase node1 = new NodeBase(dataNodes[0].getHostName(),
        dataNodes[0].getNetworkLocation());
    NodeBase node2 = new NodeBase(dataNodes[0].getHostName(),
        dataNodes[0].getNetworkLocation());
    assertEquals(0, cluster.getDistance(node1, node2));
    // verify the distance can be computed by path.
    // They don't need to refer to the same object or parents.
    NodeBase node3 = new NodeBase(dataNodes[3].getHostName(),
        dataNodes[3].getNetworkLocation());
    NodeBase node4 = new NodeBase(dataNodes[6].getHostName(),
        dataNodes[6].getNetworkLocation());
    assertEquals(0, NetworkTopology.getDistanceByPath(node1, node2));
    assertEquals(4, NetworkTopology.getDistanceByPath(node2, node3));
    assertEquals(6, NetworkTopology.getDistanceByPath(node2, node4));
  }
"
"  @Test
  public void testSortByDistance() throws Exception {
    DatanodeDescriptor[] testNodes = new DatanodeDescriptor[3];
    
    // array contains both local node & local rack node
    testNodes[0] = dataNodes[1];
    testNodes[1] = dataNodes[2];
    testNodes[2] = dataNodes[0];
    cluster.setRandomSeed(0xDEADBEEF);
    cluster.sortByDistance(dataNodes[0], testNodes, testNodes.length);
    assertTrue(testNodes[0] == dataNodes[0]);
    assertTrue(testNodes[1] == dataNodes[1]);
    assertTrue(testNodes[2] == dataNodes[2]);

    // array contains both local node & local rack node & decommissioned node
    DatanodeDescriptor[] dtestNodes = new DatanodeDescriptor[5];
    dtestNodes[0] = dataNodes[8];
    dtestNodes[1] = dataNodes[12];
    dtestNodes[2] = dataNodes[11];
    dtestNodes[3] = dataNodes[9];
    dtestNodes[4] = dataNodes[10];
    cluster.setRandomSeed(0xDEADBEEF);
    cluster.sortByDistance(dataNodes[8], dtestNodes, dtestNodes.length - 2);
    assertTrue(dtestNodes[0] == dataNodes[8]);
    assertTrue(dtestNodes[1] == dataNodes[11]);
    assertTrue(dtestNodes[2] == dataNodes[12]);
    assertTrue(dtestNodes[3] == dataNodes[9]);
    assertTrue(dtestNodes[4] == dataNodes[10]);

    // array contains local node
    testNodes[0] = dataNodes[1];
    testNodes[1] = dataNodes[3];
    testNodes[2] = dataNodes[0];
    cluster.setRandomSeed(0xDEADBEEF);
    cluster.sortByDistance(dataNodes[0], testNodes, testNodes.length);
    assertTrue(testNodes[0] == dataNodes[0]);
    assertTrue(testNodes[1] == dataNodes[1]);
    assertTrue(testNodes[2] == dataNodes[3]);

    // array contains local rack node
    testNodes[0] = dataNodes[5];
    testNodes[1] = dataNodes[3];
    testNodes[2] = dataNodes[1];
    cluster.setRandomSeed(0xDEADBEEF);
    cluster.sortByDistance(dataNodes[0], testNodes, testNodes.length);
    assertTrue(testNodes[0] == dataNodes[1]);
    assertTrue(testNodes[1] == dataNodes[3]);
    assertTrue(testNodes[2] == dataNodes[5]);

    // array contains local rack node which happens to be in position 0
    testNodes[0] = dataNodes[1];
    testNodes[1] = dataNodes[5];
    testNodes[2] = dataNodes[3];
    cluster.setRandomSeed(0xDEADBEEF);
    cluster.sortByDistance(dataNodes[0], testNodes, testNodes.length);
    assertTrue(testNodes[0] == dataNodes[1]);
    assertTrue(testNodes[1] == dataNodes[3]);
    assertTrue(testNodes[2] == dataNodes[5]);

    // Same as previous, but with a different random seed to test randomization
    testNodes[0] = dataNodes[1];
    testNodes[1] = dataNodes[5];
    testNodes[2] = dataNodes[3];
    cluster.setRandomSeed(0xDEAD);
    cluster.sortByDistance(dataNodes[0], testNodes, testNodes.length);
    assertTrue(testNodes[0] == dataNodes[1]);
    assertTrue(testNodes[1] == dataNodes[3]);
    assertTrue(testNodes[2] == dataNodes[5]);

    // Array of just rack-local nodes
    // Expect a random first node
    DatanodeDescriptor first = null;
    boolean foundRandom = false;
    for (int i=5; i<=7; i++) {
      testNodes[0] = dataNodes[5];
      testNodes[1] = dataNodes[6];
      testNodes[2] = dataNodes[7];
      cluster.sortByDistance(dataNodes[i], testNodes, testNodes.length);
      if (first == null) {
        first = testNodes[0];
      } else {
        if (first != testNodes[0]) {
          foundRandom = true;
          break;
        }
      }
    }
    assertTrue(""Expected to find a different first location"", foundRandom);

    // Array of just remote nodes
    // Expect random first node
    first = null;
    for (int i = 1; i <= 4; i++) {
      testNodes[0] = dataNodes[13];
      testNodes[1] = dataNodes[14];
      testNodes[2] = dataNodes[15];
      cluster.sortByDistance(dataNodes[i], testNodes, testNodes.length);
      if (first == null) {
        first = testNodes[0];
      } else {
        if (first != testNodes[0]) {
          foundRandom = true;
          break;
        }
      }
    }
    assertTrue(""Expected to find a different first location"", foundRandom);

    //Reader is not a datanode, but is in one of the datanode's rack.
    testNodes[0] = dataNodes[0];
    testNodes[1] = dataNodes[5];
    testNodes[2] = dataNodes[8];
    Node rackClient = new NodeBase(""/d3/r1/25.25.25"");
    cluster.setRandomSeed(0xDEADBEEF);
    cluster.sortByDistance(rackClient, testNodes, testNodes.length);
    assertTrue(testNodes[0] == dataNodes[8]);
    assertTrue(testNodes[1] == dataNodes[5]);
    assertTrue(testNodes[2] == dataNodes[0]);

    //Reader is not a datanode , but is in one of the datanode's data center.
    testNodes[0] = dataNodes[8];
    testNodes[1] = dataNodes[5];
    testNodes[2] = dataNodes[0];
    Node dcClient = new NodeBase(""/d1/r2/25.25.25"");
    cluster.setRandomSeed(0xDEADBEEF);
    cluster.sortByDistance(dcClient, testNodes, testNodes.length);
    assertTrue(testNodes[0] == dataNodes[0]);
    assertTrue(testNodes[1] == dataNodes[5]);
    assertTrue(testNodes[2] == dataNodes[8]);

  }
"
"  @Test
  public void testRemove() throws Exception {
    for(int i=0; i<dataNodes.length; i++) {
      cluster.remove(dataNodes[i]);
    }
    for(int i=0; i<dataNodes.length; i++) {
      assertFalse(cluster.contains(dataNodes[i]));
    }
    assertEquals(0, cluster.getNumOfLeaves());
    assertEquals(0, cluster.clusterMap.getChildren().size());
    for(int i=0; i<dataNodes.length; i++) {
      cluster.add(dataNodes[i]);
    }
  }
"
"  @Test
  public void testChooseRandomExcludedNode() {
    String scope = ""~"" + NodeBase.getPath(dataNodes[0]);
    Map<Node, Integer> frequency = pickNodesAtRandom(100, scope, null);

    for (Node key : dataNodes) {
      // all nodes except the first should be more than zero
      assertTrue(frequency.get(key) > 0 || key == dataNodes[0]);
    }
  }
"
"  @Test
  public void testChooseRandomExcludedRack() {
    Map<Node, Integer> frequency = pickNodesAtRandom(100, ""~"" + ""/d2"", null);
    // all the nodes on the second rack should be zero
    for (int j = 0; j < dataNodes.length; j++) {
      int freq = frequency.get(dataNodes[j]);
      if (dataNodes[j].getNetworkLocation().startsWith(""/d2"")) {
        assertEquals(0, freq);
      } else {
        assertTrue(freq > 0);
      }
    }
  }
"
"  @Test
  public void testChooseRandomExcludedNodeList() {
    String scope = ""~"" + NodeBase.getPath(dataNodes[0]);
    Set<Node> excludedNodes = new HashSet<>();
    excludedNodes.add(dataNodes[3]);
    excludedNodes.add(dataNodes[5]);
    excludedNodes.add(dataNodes[7]);
    excludedNodes.add(dataNodes[9]);
    excludedNodes.add(dataNodes[13]);
    excludedNodes.add(dataNodes[18]);
    Map<Node, Integer> frequency = pickNodesAtRandom(100, scope, excludedNodes);

    assertEquals(""dn[3] should be excluded"", 0,
        frequency.get(dataNodes[3]).intValue());
    assertEquals(""dn[5] should be exclude18d"", 0,
        frequency.get(dataNodes[5]).intValue());
    assertEquals(""dn[7] should be excluded"", 0,
        frequency.get(dataNodes[7]).intValue());
    assertEquals(""dn[9] should be excluded"", 0,
        frequency.get(dataNodes[9]).intValue());
    assertEquals(""dn[13] should be excluded"", 0,
        frequency.get(dataNodes[13]).intValue());
    assertEquals(""dn[18] should be excluded"", 0,
        frequency.get(dataNodes[18]).intValue());
    for (Node key : dataNodes) {
      if (excludedNodes.contains(key)) {
        continue;
      }
      // all nodes except the first should be more than zero
      assertTrue(frequency.get(key) > 0 || key == dataNodes[0]);
    }
  }
"
"  @Test
  public void testChooseRandomExcludeAllNodes() {
    String scope = ""~"" + NodeBase.getPath(dataNodes[0]);
    Set<Node> excludedNodes = new HashSet<>();
    for (int i = 0; i < dataNodes.length; i++) {
      excludedNodes.add(dataNodes[i]);
    }
    Map<Node, Integer> frequency = pickNodesAtRandom(100, scope, excludedNodes);
    for (Node key : dataNodes) {
      // all nodes except the first should be more than zero
      assertTrue(frequency.get(key) == 0);
    }
  }
"
"  @Test(timeout=180000)
  public void testInvalidNetworkTopologiesNotCachedInHdfs() throws Exception {
    // start a cluster
    Configuration conf = new HdfsConfiguration();
    MiniDFSCluster cluster = null;
    try {
      // bad rack topology
      String racks[] = { ""/a/b"", ""/c"" };
      String hosts[] = { ""foo1.example.com"", ""foo2.example.com"" };
      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(2).
          racks(racks).hosts(hosts).build();
      cluster.waitActive();
      
      NamenodeProtocols nn = cluster.getNameNodeRpc();
      Assert.assertNotNull(nn);
      
      // Wait for one DataNode to register.
      // The other DataNode will not be able to register up because of the rack mismatch.
      DatanodeInfo[] info;
      while (true) {
        info = nn.getDatanodeReport(DatanodeReportType.LIVE);
        Assert.assertFalse(info.length == 2);
        if (info.length == 1) {
          break;
        }
        Thread.sleep(1000);
      }
      // Set the network topology of the other node to the match the network
      // topology of the node that came up.
      int validIdx = info[0].getHostName().equals(hosts[0]) ? 0 : 1;
      int invalidIdx = validIdx == 1 ? 0 : 1;
      StaticMapping.addNodeToRack(hosts[invalidIdx], racks[validIdx]);
      LOG.info(""datanode "" + validIdx + "" came up with network location "" + 
        info[0].getNetworkLocation());

      // Restart the DN with the invalid topology and wait for it to register.
      cluster.restartDataNode(invalidIdx);
      Thread.sleep(5000);
      while (true) {
        info = nn.getDatanodeReport(DatanodeReportType.LIVE);
        if (info.length == 2) {
          break;
        }
        if (info.length == 0) {
          LOG.info(""got no valid DNs"");
        } else if (info.length == 1) {
          LOG.info(""got one valid DN: "" + info[0].getHostName() +
              "" (at "" + info[0].getNetworkLocation() + "")"");
        }
        Thread.sleep(1000);
      }
      Assert.assertEquals(info[0].getNetworkLocation(),
                          info[1].getNetworkLocation());
    } finally {
      if (cluster != null) {
        cluster.shutdown();
      }
    }
  }
"
"  @Test
  public void testBackwardCompatibility() {
    // Test 1 - old configuration key with decimal 
    // umask value should be handled when set using 
    // FSPermission.setUMask() API
    FsPermission perm = new FsPermission((short)18);
    Configuration conf = new Configuration();
    FsPermission.setUMask(conf, perm);
    assertEquals(18, FsPermission.getUMask(conf).toShort());

    // Test 2 - new configuration key is handled
    conf = new Configuration();
    conf.set(FsPermission.UMASK_LABEL, ""022"");
    assertEquals(18, FsPermission.getUMask(conf).toShort());

    // Test 3 - equivalent valid umask
    conf = new Configuration();
    conf.set(FsPermission.UMASK_LABEL, ""0022"");
    assertEquals(18, FsPermission.getUMask(conf).toShort());

    // Test 4 - invalid umask
    conf = new Configuration();
    conf.set(FsPermission.UMASK_LABEL, ""1222"");
    try {
      FsPermission.getUMask(conf);
      fail(""expect IllegalArgumentException happen"");
    } catch (IllegalArgumentException e) {
     //pass, exception successfully trigger
    }

    // Test 5 - invalid umask
    conf = new Configuration();
    conf.set(FsPermission.UMASK_LABEL, ""01222"");
    try {
      FsPermission.getUMask(conf);
      fail(""expect IllegalArgumentException happen"");
    } catch (IllegalArgumentException e) {
     //pass, exception successfully trigger
    }
  }
"
"  @Test
  public void testCreate() throws Exception {
    Configuration conf = new HdfsConfiguration();
    conf.setBoolean(DFSConfigKeys.DFS_PERMISSIONS_ENABLED_KEY, true);
    conf.set(FsPermission.UMASK_LABEL, ""000"");
    MiniDFSCluster cluster = null;
    FileSystem fs = null;

    try {
      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(3).build();
      cluster.waitActive();
      fs = FileSystem.get(conf);
      FsPermission rootPerm = checkPermission(fs, ""/"", null);
      FsPermission inheritPerm = FsPermission.createImmutable(
          (short)(rootPerm.toShort() | 0300));

      FsPermission dirPerm = new FsPermission((short)0777);
      fs.mkdirs(new Path(""/a1/a2/a3""), dirPerm);
      checkPermission(fs, ""/a1"", dirPerm);
      checkPermission(fs, ""/a1/a2"", dirPerm);
      checkPermission(fs, ""/a1/a2/a3"", dirPerm);

      dirPerm = new FsPermission((short)0123);
      FsPermission permission = FsPermission.createImmutable(
        (short)(dirPerm.toShort() | 0300));
      fs.mkdirs(new Path(""/aa/1/aa/2/aa/3""), dirPerm);
      checkPermission(fs, ""/aa/1"", permission);
      checkPermission(fs, ""/aa/1/aa/2"", permission);
      checkPermission(fs, ""/aa/1/aa/2/aa/3"", dirPerm);

      FsPermission filePerm = new FsPermission((short)0444);
      Path p = new Path(""/b1/b2/b3.txt"");
      FSDataOutputStream out = fs.create(p, filePerm,
          true, conf.getInt(CommonConfigurationKeys.IO_FILE_BUFFER_SIZE_KEY, 4096),
          fs.getDefaultReplication(p), fs.getDefaultBlockSize(p), null);
      out.write(123);
      out.close();
      checkPermission(fs, ""/b1"", inheritPerm);
      checkPermission(fs, ""/b1/b2"", inheritPerm);
      checkPermission(fs, ""/b1/b2/b3.txt"", filePerm);
      
      conf.set(FsPermission.UMASK_LABEL, ""022"");
      permission = 
        FsPermission.createImmutable((short)0666);
      FileSystem.mkdirs(fs, new Path(""/c1""), new FsPermission(permission));
      FileSystem.create(fs, new Path(""/c1/c2.txt""),
          new FsPermission(permission));
      checkPermission(fs, ""/c1"", permission);
      checkPermission(fs, ""/c1/c2.txt"", permission);
    } finally {
      try {
        if(fs != null) fs.close();
      } catch(Exception e) {
        LOG.error(StringUtils.stringifyException(e));
      }
      try {
        if(cluster != null) cluster.shutdown();
      } catch(Exception e) {
        LOG.error(StringUtils.stringifyException(e));
      }
    }
  }
"
"  @Test
  public void testFilePermission() throws Exception {
    final Configuration conf = new HdfsConfiguration();
    conf.setBoolean(DFSConfigKeys.DFS_PERMISSIONS_ENABLED_KEY, true);
    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).numDataNodes(3).build();
    cluster.waitActive();

    try {
      nnfs = FileSystem.get(conf);
      // test permissions on files that do not exist
      assertFalse(nnfs.exists(CHILD_FILE1));
      try {
        nnfs.setPermission(CHILD_FILE1, new FsPermission((short)0777));
        assertTrue(false);
      }
      catch(java.io.FileNotFoundException e) {
        LOG.info(""GOOD: got "" + e);
      }
      
      // make sure nn can take user specified permission (with default fs
      // permission umask applied)
      FSDataOutputStream out = nnfs.create(CHILD_FILE1, new FsPermission(
          (short) 0777), true, 1024, (short) 1, 1024, null);
      FileStatus status = nnfs.getFileStatus(CHILD_FILE1);
      // FS_PERMISSIONS_UMASK_DEFAULT is 0022
      assertTrue(status.getPermission().toString().equals(""rwxr-xr-x""));
      nnfs.delete(CHILD_FILE1, false);
      
      // following dir/file creations are legal
      nnfs.mkdirs(CHILD_DIR1);
      status = nnfs.getFileStatus(CHILD_DIR1);
      assertThat(""Expect 755 = 777 (default dir) - 022 (default umask)"",
          status.getPermission().toString(), is(""rwxr-xr-x""));
      out = nnfs.create(CHILD_FILE1);
      status = nnfs.getFileStatus(CHILD_FILE1);
      assertTrue(status.getPermission().toString().equals(""rw-r--r--""));
      byte data[] = new byte[FILE_LEN];
      RAN.nextBytes(data);
      out.write(data);
      out.close();
      nnfs.setPermission(CHILD_FILE1, new FsPermission(""700""));
      status = nnfs.getFileStatus(CHILD_FILE1);
      assertTrue(status.getPermission().toString().equals(""rwx------""));

      // mkdirs with null permission
      nnfs.mkdirs(CHILD_DIR3, null);
      status = nnfs.getFileStatus(CHILD_DIR3);
      assertThat(""Expect 755 = 777 (default dir) - 022 (default umask)"",
          status.getPermission().toString(), is(""rwxr-xr-x""));

      // following read is legal
      byte dataIn[] = new byte[FILE_LEN];
      FSDataInputStream fin = nnfs.open(CHILD_FILE1);
      int bytesRead = fin.read(dataIn);
      assertTrue(bytesRead == FILE_LEN);
      for(int i=0; i<FILE_LEN; i++) {
        assertEquals(data[i], dataIn[i]);
      }

      // test execution bit support for files
      nnfs.setPermission(CHILD_FILE1, new FsPermission(""755""));
      status = nnfs.getFileStatus(CHILD_FILE1);
      assertTrue(status.getPermission().toString().equals(""rwxr-xr-x""));
      nnfs.setPermission(CHILD_FILE1, new FsPermission(""744""));
      status = nnfs.getFileStatus(CHILD_FILE1);
      assertTrue(status.getPermission().toString().equals(""rwxr--r--""));
      nnfs.setPermission(CHILD_FILE1, new FsPermission(""700""));
      
      ////////////////////////////////////////////////////////////////
      // test illegal file/dir creation
      UserGroupInformation userGroupInfo = 
        UserGroupInformation.createUserForTesting(USER_NAME, GROUP_NAMES );
      
      userfs = DFSTestUtil.getFileSystemAs(userGroupInfo, conf);

      // make sure mkdir of a existing directory that is not owned by 
      // this user does not throw an exception.
      userfs.mkdirs(CHILD_DIR1);
      
      // illegal mkdir
      assertTrue(!canMkdirs(userfs, CHILD_DIR2));

      // illegal file creation
      assertTrue(!canCreate(userfs, CHILD_FILE2));

      // illegal file open
      assertTrue(!canOpen(userfs, CHILD_FILE1));

      nnfs.setPermission(ROOT_PATH, new FsPermission((short)0755));
      nnfs.setPermission(CHILD_DIR1, new FsPermission(""777""));
      nnfs.setPermission(new Path(""/""), new FsPermission((short)0777));
      final Path RENAME_PATH = new Path(""/foo/bar"");
      userfs.mkdirs(RENAME_PATH);
      assertTrue(canRename(userfs, RENAME_PATH, CHILD_DIR1));
      // test permissions on files that do not exist
      assertFalse(userfs.exists(CHILD_FILE3));
      try {
        userfs.setPermission(CHILD_FILE3, new FsPermission((short) 0777));
        fail(""setPermission should fail for non-exist file"");
      } catch (java.io.FileNotFoundException ignored) {
      }

      // Make sure any user can create file in root.
      nnfs.setPermission(ROOT_PATH, new FsPermission(""777""));

      testSuperCanChangeOwnerGroup();
      testNonSuperCanChangeToOwnGroup();
      testNonSuperCannotChangeToOtherGroup();
      testNonSuperCannotChangeGroupForOtherFile();
      testNonSuperCannotChangeGroupForNonExistentFile();
      testNonSuperCannotChangeOwner();
      testNonSuperCannotChangeOwnerForOtherFile();
      testNonSuperCannotChangeOwnerForNonExistentFile();
    } finally {
      cluster.shutdown();
    }
  }
"
"  @Test(timeout = 5000)
  public void testDelete() throws Exception {
    fs.setPermission(linkParent, new FsPermission((short) 0555));
    doDeleteLinkParentNotWritable();

    fs.setPermission(linkParent, new FsPermission((short) 0777));
    fs.setPermission(targetParent, new FsPermission((short) 0555));
    fs.setPermission(target, new FsPermission((short) 0555));
    doDeleteTargetParentAndTargetNotWritable();
  }
"
"  @Test
  public void testAclDelete() throws Exception {
    fs.setAcl(linkParent, Arrays.asList(
      aclEntry(ACCESS, USER, ALL),
      aclEntry(ACCESS, USER, user.getUserName(), READ_EXECUTE),
      aclEntry(ACCESS, GROUP, ALL),
      aclEntry(ACCESS, OTHER, ALL)));
    doDeleteLinkParentNotWritable();

    fs.setAcl(linkParent, Arrays.asList(
      aclEntry(ACCESS, USER, ALL),
      aclEntry(ACCESS, GROUP, ALL),
      aclEntry(ACCESS, OTHER, ALL)));
    fs.setAcl(targetParent, Arrays.asList(
      aclEntry(ACCESS, USER, ALL),
      aclEntry(ACCESS, USER, user.getUserName(), READ_EXECUTE),
      aclEntry(ACCESS, GROUP, ALL),
      aclEntry(ACCESS, OTHER, ALL)));
    fs.setAcl(target, Arrays.asList(
      aclEntry(ACCESS, USER, ALL),
      aclEntry(ACCESS, USER, user.getUserName(), READ_EXECUTE),
      aclEntry(ACCESS, GROUP, ALL),
      aclEntry(ACCESS, OTHER, ALL)));
    doDeleteTargetParentAndTargetNotWritable();
  }
"
"  @Test(timeout = 5000)
  public void testReadWhenTargetNotReadable() throws Exception {
    fs.setPermission(target, new FsPermission((short) 0000));
    doReadTargetNotReadable();
  }
"
"  @Test
  public void testAclReadTargetNotReadable() throws Exception {
    fs.setAcl(target, Arrays.asList(
      aclEntry(ACCESS, USER, READ_WRITE),
      aclEntry(ACCESS, USER, user.getUserName(), NONE),
      aclEntry(ACCESS, GROUP, READ),
      aclEntry(ACCESS, OTHER, READ)));
    doReadTargetNotReadable();
  }
"
"  @Test(timeout = 5000)
  public void testFileStatus() throws Exception {
    fs.setPermission(target, new FsPermission((short) 0000));
    doGetFileLinkStatusTargetNotReadable();
  }
"
"  @Test
  public void testAclGetFileLinkStatusTargetNotReadable() throws Exception {
    fs.setAcl(target, Arrays.asList(
      aclEntry(ACCESS, USER, READ_WRITE),
      aclEntry(ACCESS, USER, user.getUserName(), NONE),
      aclEntry(ACCESS, GROUP, READ),
      aclEntry(ACCESS, OTHER, READ)));
    doGetFileLinkStatusTargetNotReadable();
  }
"
"  @Test(timeout = 5000)
  public void testRenameLinkTargetNotWritableFC() throws Exception {
    fs.setPermission(target, new FsPermission((short) 0555));
    fs.setPermission(targetParent, new FsPermission((short) 0555));
    doRenameLinkTargetNotWritableFC();
  }
"
"  @Test
  public void testAclRenameTargetNotWritableFC() throws Exception {
    fs.setAcl(target, Arrays.asList(
      aclEntry(ACCESS, USER, ALL),
      aclEntry(ACCESS, USER, user.getUserName(), READ_EXECUTE),
      aclEntry(ACCESS, GROUP, ALL),
      aclEntry(ACCESS, OTHER, ALL)));
    fs.setAcl(targetParent, Arrays.asList(
      aclEntry(ACCESS, USER, ALL),
      aclEntry(ACCESS, USER, user.getUserName(), READ_EXECUTE),
      aclEntry(ACCESS, GROUP, ALL),
      aclEntry(ACCESS, OTHER, ALL)));
    doRenameLinkTargetNotWritableFC();
  }
"
"  @Test(timeout = 5000)
  public void testRenameSrcNotWritableFC() throws Exception {
    fs.setPermission(linkParent, new FsPermission((short) 0555));
    doRenameSrcNotWritableFC();
  }
"
"  @Test
  public void testAclRenameSrcNotWritableFC() throws Exception {
    fs.setAcl(linkParent, Arrays.asList(
      aclEntry(ACCESS, USER, ALL),
      aclEntry(ACCESS, USER, user.getUserName(), READ_EXECUTE),
      aclEntry(ACCESS, GROUP, ALL),
      aclEntry(ACCESS, OTHER, ALL)));
    doRenameSrcNotWritableFC();
  }
"
"  @Test(timeout = 5000)
  public void testRenameLinkTargetNotWritableFS() throws Exception {
    fs.setPermission(target, new FsPermission((short) 0555));
    fs.setPermission(targetParent, new FsPermission((short) 0555));
    doRenameLinkTargetNotWritableFS();
  }
"
"  @Test
  public void testAclRenameTargetNotWritableFS() throws Exception {
    fs.setAcl(target, Arrays.asList(
      aclEntry(ACCESS, USER, ALL),
      aclEntry(ACCESS, USER, user.getUserName(), READ_EXECUTE),
      aclEntry(ACCESS, GROUP, ALL),
      aclEntry(ACCESS, OTHER, ALL)));
    fs.setAcl(targetParent, Arrays.asList(
      aclEntry(ACCESS, USER, ALL),
      aclEntry(ACCESS, USER, user.getUserName(), READ_EXECUTE),
      aclEntry(ACCESS, GROUP, ALL),
      aclEntry(ACCESS, OTHER, ALL)));
    doRenameLinkTargetNotWritableFS();
  }
"
"  @Test(timeout = 5000)
  public void testRenameSrcNotWritableFS() throws Exception {
    fs.setPermission(linkParent, new FsPermission((short) 0555));
    doRenameSrcNotWritableFS();
  }
"
"  @Test
  public void testAclRenameSrcNotWritableFS() throws Exception {
    fs.setAcl(linkParent, Arrays.asList(
      aclEntry(ACCESS, USER, ALL),
      aclEntry(ACCESS, USER, user.getUserName(), READ_EXECUTE),
      aclEntry(ACCESS, GROUP, ALL),
      aclEntry(ACCESS, OTHER, ALL)));
    doRenameSrcNotWritableFS();
  }
"
"  @Test
  public void testAccess() throws Exception {
    fs.setPermission(target, new FsPermission((short) 0002));
    fs.setAcl(target, Arrays.asList(
        aclEntry(ACCESS, USER, ALL),
        aclEntry(ACCESS, GROUP, NONE),
        aclEntry(ACCESS, USER, user.getShortUserName(), WRITE),
        aclEntry(ACCESS, OTHER, WRITE)));
    FileContext myfc = user.doAs(new PrivilegedExceptionAction<FileContext>() {
      @Override
      public FileContext run() throws IOException {
        return FileContext.getFileContext(conf);
      }
"
"  @Test
  public void testGroupMappingRefresh() throws Exception {
    DFSAdmin admin = new DFSAdmin(config);
    String [] args =  new String[]{""-refreshUserToGroupsMappings""};
    Groups groups = Groups.getUserToGroupsMappingService(config);
    String user = UserGroupInformation.getCurrentUser().getUserName();
    System.out.println(""first attempt:"");
    List<String> g1 = groups.getGroups(user);
    String [] str_groups = new String [g1.size()];
    g1.toArray(str_groups);
    System.out.println(Arrays.toString(str_groups));
    
    System.out.println(""second attempt, should be same:"");
    List<String> g2 = groups.getGroups(user);
    g2.toArray(str_groups);
    System.out.println(Arrays.toString(str_groups));
    for(int i=0; i<g2.size(); i++) {
      assertEquals(""Should be same group "", g1.get(i), g2.get(i));
    }
    admin.run(args);
    System.out.println(""third attempt(after refresh command), should be different:"");
    List<String> g3 = groups.getGroups(user);
    g3.toArray(str_groups);
    System.out.println(Arrays.toString(str_groups));
    for(int i=0; i<g3.size(); i++) {
      assertFalse(""Should be different group: "" + g1.get(i) + "" and "" + g3.get(i), 
          g1.get(i).equals(g3.get(i)));
    }
    
    // test time out
    Thread.sleep(groupRefreshTimeoutSec*1100);
    System.out.println(""fourth attempt(after timeout), should be different:"");
    List<String> g4 = groups.getGroups(user);
    g4.toArray(str_groups);
    System.out.println(Arrays.toString(str_groups));
    for(int i=0; i<g4.size(); i++) {
      assertFalse(""Should be different group "", g3.get(i).equals(g4.get(i)));
    }
  }
"
"  @Test
  public void testRefreshSuperUserGroupsConfiguration() throws Exception {
    final String SUPER_USER = ""super_user"";
    final List<String> groupNames1 = new ArrayList<>();
    groupNames1.add(""gr1"");
    groupNames1.add(""gr2"");
    final List<String> groupNames2 = new ArrayList<>();
    groupNames2.add(""gr3"");
    groupNames2.add(""gr4"");

    //keys in conf
    String userKeyGroups = DefaultImpersonationProvider.getTestProvider().
        getProxySuperuserGroupConfKey(SUPER_USER);
    String userKeyHosts = DefaultImpersonationProvider.getTestProvider().
        getProxySuperuserIpConfKey (SUPER_USER);
    
    config.set(userKeyGroups, ""gr3,gr4,gr5""); // superuser can proxy for this group
    config.set(userKeyHosts,""127.0.0.1"");
    ProxyUsers.refreshSuperUserGroupsConfiguration(config);
    
    UserGroupInformation ugi1 = mock(UserGroupInformation.class);
    UserGroupInformation ugi2 = mock(UserGroupInformation.class);
    UserGroupInformation suUgi = mock(UserGroupInformation.class);
    when(ugi1.getRealUser()).thenReturn(suUgi);
    when(ugi2.getRealUser()).thenReturn(suUgi);

    when(suUgi.getShortUserName()).thenReturn(SUPER_USER); // super user
    when(suUgi.getUserName()).thenReturn(SUPER_USER+""L""); // super user
     
    when(ugi1.getShortUserName()).thenReturn(""user1"");
    when(ugi2.getShortUserName()).thenReturn(""user2"");
    
    when(ugi1.getUserName()).thenReturn(""userL1"");
    when(ugi2.getUserName()).thenReturn(""userL2"");

    // set groups for users
    when(ugi1.getGroups()).thenReturn(groupNames1);
    when(ugi2.getGroups()).thenReturn(groupNames2);


    // check before
    try {
      ProxyUsers.authorize(ugi1, ""127.0.0.1"");
      fail(""first auth for "" + ugi1.getShortUserName() + "" should've failed "");
    } catch (AuthorizationException e) {
      // expected
      System.err.println(""auth for "" + ugi1.getUserName() + "" failed"");
    }
    try {
      ProxyUsers.authorize(ugi2, ""127.0.0.1"");
      System.err.println(""auth for "" + ugi2.getUserName() + "" succeeded"");
      // expected
    } catch (AuthorizationException e) {
      fail(""first auth for "" + ugi2.getShortUserName() + "" should've succeeded: "" + e.getLocalizedMessage());
    }
    
    // refresh will look at configuration on the server side
    // add additional resource with the new value
    // so the server side will pick it up
    String rsrc = ""testGroupMappingRefresh_rsrc.xml"";
    addNewConfigResource(rsrc, userKeyGroups, ""gr2"", userKeyHosts, ""127.0.0.1"");  
    
    DFSAdmin admin = new DFSAdmin(config);
    String [] args = new String[]{""-refreshSuperUserGroupsConfiguration""};
    admin.run(args);
    
    try {
      ProxyUsers.authorize(ugi2, ""127.0.0.1"");
      fail(""second auth for "" + ugi2.getShortUserName() + "" should've failed "");
    } catch (AuthorizationException e) {
      // expected
      System.err.println(""auth for "" + ugi2.getUserName() + "" failed"");
    }
    try {
      ProxyUsers.authorize(ugi1, ""127.0.0.1"");
      System.err.println(""auth for "" + ugi1.getUserName() + "" succeeded"");
      // expected
    } catch (AuthorizationException e) {
      fail(""second auth for "" + ugi1.getShortUserName() + "" should've succeeded: "" + e.getLocalizedMessage());
    }
    
    
  }
"
"  @Test
  public void testMkdirWithExistingDirClear() throws IOException {
    testMkdirWithExistingDir(BLANK_TEST_UMASK, BLANK_PERMISSIONS);
  }
"
"  @Test
  public void testMkdirWithExistingDirOpen() throws IOException {
    testMkdirWithExistingDir(WIDE_OPEN_TEST_UMASK, WIDE_OPEN_PERMISSIONS);
  }
"
"  @Test
  public void testMkdirWithExistingDirMiddle() throws IOException {
    testMkdirWithExistingDir(USER_GROUP_OPEN_TEST_UMASK,
        USER_GROUP_OPEN_PERMISSIONS);
  }
"
"  @Test
  public void testMkdirRecursiveWithNonExistingDirClear() throws IOException {
    // by default parent directories have -wx------ bits set
    testMkdirRecursiveWithNonExistingDir(BLANK_TEST_UMASK, BLANK_PERMISSIONS, 
        PARENT_PERMS_FOR_BLANK_PERMISSIONS);
  }
"
"    @Test
    public void testMin() throws Exception {
        Assert.assertEquals(1.0D, m.min(), 0.0D);
    }
"
"    @Test
    public void testMax() throws Exception {
        Assert.assertEquals(100.0D, m.max(), 0.0D);
    }
"
"    @Test
    public void testAvg() throws Exception {
        int sum = 0;
        for (int i = 1; i <= 100; i++) {
            sum += i;
        }
        Assert.assertEquals((sum / 100.0D), m.avg(), 0.0D);
    }
"
"    @Test
    public void testCount() throws Exception {
        Assert.assertEquals(100.0D, m.count(), 0.0D);
    }
"
"    @Test
    public void test50thPercentile() throws Exception {
        Assert.assertEquals(50.0D, m.getPercentile(50), 0.0D);
    }
"
"    @Test
    public void test75thPercentile() throws Exception {
        Assert.assertEquals(75.0D, m.getPercentile(75), 0.0D);
    }
"
"    @Test
    public void test90thPercentile() throws Exception {
        Assert.assertEquals(90.0D, m.getPercentile(90), 0.0D);
    }
"
"    @Test
    public void test99thPercentile() throws Exception {
        Assert.assertEquals(99.0D, m.getPercentile(99), 0.0D);
    }
"
"    @Test
    public void testSerialization() throws Exception {
        MetricParser metricParser = new MetricParser();
        Tag t1 = new Tag(""tag1=value1"");
        Tag t2 = new Tag(""tag2=value2"");
        Tag avg = new Tag(""sample=avg"");
        Tag min = new Tag(""sample=min"");
        Tag max = new Tag(""sample=max"");
        Tag sum = new Tag(""sample=sum"");
        Tag count = new Tag(""sample=count"");
        Tag p50 = new Tag(""sample=50p"");
        Tag p75 = new Tag(""sample=75p"");
        Tag p90 = new Tag(""sample=90p"");
        Tag p99 = new Tag(""sample=99p"");

        List<Tag> tags = new ArrayList<>();
        tags.add(t1);
        tags.add(t2);
        m.initialize(""sys.cpu.user"", tags);

        byte[] bytes = m.serialize(m);
        String puts = new String(bytes);
        for (String put : puts.split(""\n"")) {
            Metric metric = metricParser.parse(put);
            Assert.assertEquals(""sys.cpu.user_summarized"", metric.getName());
            metric.getTags().forEach(t -> {
                Assert.assertTrue(
                        t.equals(t1) || t.equals(t2) || t.equals(avg) || t.equals(min) || t.equals(max) || t.equals(sum)
                                || t.equals(count) || t.equals(p50) || t.equals(p75) || t.equals(p90) || t.equals(p99));
            });
        }
    }
"
"    @Test
    public void testWrite() throws Exception {
        Thread t = new Thread(server);
        t.start();
        setupPlugin();
        while (!server.ready()) {
            Thread.sleep(1000);
        }
        Assert.assertEquals(0, plugin.write(createMetric()));
        Thread.sleep(100);
        Assert.assertTrue(server.messageReceived());
        plugin.shutdown();
        server.shutdown();
        t.join();
    }
"
"    @Test
    public void testWriteAfterServerRestart() throws Exception {
        Thread t = new Thread(server);
        t.start();
        setupPlugin();
        while (!server.ready()) {
            Thread.sleep(1000);
        }
        Assert.assertEquals(0, plugin.write(createMetric()));
        Thread.sleep(100);
        Assert.assertTrue(server.messageReceived());
        server.shutdown();
        t.join();
        Thread.sleep(2000);

        server.create();
        Thread t2 = new Thread(server);
        t2.start();
        // Need to call this again because the server is not guaranteed to be
        // listening on the same local port as the first time that it was
        // started
        setupPlugin();
        while (!server.ready()) {
            Thread.sleep(1000);
            // Keep sending metrics to plugin to force reconnect
            int result = plugin.write(createMetric());
            System.out.println(""Wrote to client, result: "" + result);
            Assert.assertEquals(0, result);
        }
        Assert.assertEquals(0, plugin.write(createMetric()));
        Thread.sleep(1000);
        Assert.assertTrue(server.messageReceived());
        plugin.shutdown();
        server.shutdown();
        t2.join();
    }
"
"    @Test
    public void testParseWithEscapedCharacters() {

        MetricParser parser = new MetricParser();
        Metric m = parser.parse(""put mymetric 12341234 5.0 tag1=value1,value1 tag2=value2=value2"");

        Assert.assertEquals(""mymetric"", m.getName());
        Assert.assertEquals(12341234, (long) m.getValue().getTimestamp());
        Assert.assertEquals(5.0, (double) m.getValue().getMeasure(), 0);
        List<Tag> expected = new ArrayList<>();
        expected.add(new Tag(""tag1"", ""value1,value1""));
        expected.add(new Tag(""tag2"", ""value2=value2""));
        Assert.assertEquals(expected, m.getTags());
    }
"
"    @Test
    public void testParseMalformatted() {

        MetricParser parser = new MetricParser();
        try {
            // parser should throw an exception
            parser.parse(""put mymetric 12341234 5.0 tag1 tag2=value2"");
            Assert.fail();
        } catch (IllegalArgumentException e) {

        }
    }
"
"    @Test
    public void testListParse() {
        String value = ""tag1=value1,tag2=value2"";
        List<Tag> tags = new TagListParser().parse(value);
        Assert.assertEquals(2, tags.size());
        Assert.assertEquals(new Tag(""tag1"", ""value1""), tags.get(0));
        Assert.assertEquals(new Tag(""tag2"", ""value2""), tags.get(1));
    }
"
"    @Test
    public void testListCombine() {
        List<Tag> tags = new ArrayList<>();
        tags.add(new Tag(""tag1"", ""value1""));
        tags.add(new Tag(""tag2"", ""value2""));
        String combined = new TagListParser().combine(tags);
        Assert.assertEquals(""tag1=value1,tag2=value2"", combined);
    }
"
"    @Test
    public void testListCombineMap() {
        Map<String, String> map = new TreeMap<>();
        map.put(""tag1"", ""value1"");
        map.put(""tag2"", ""value2"");
        String combined = new TagListParser().combine(map);
        Assert.assertEquals(""tag1=value1,tag2=value2"", combined);
    }
"
"    @Test
    public void testParseTagsWithCommas() {

        try {
            String s = ""tag1=value1,tag2=3.4.3_(default\\,_Date\\,_Time)_"";
            new TagListParser().parse(s);
        } catch (Exception e) {
            Assert.fail(e.getMessage());
        }
    }
"
"    @Test
    public void testEquals() {
        Metric m1 = Metric.newBuilder().name(""m1"").tag(""t1"", ""v1"").tag(""t2"", ""v2"").value(1, 0.0).build();
        Metric m2 = Metric.newBuilder().name(""m1"").tag(""t2"", ""v2"").tag(""t1"", ""v1"").value(1, 0.0).build();

        assertTrue(m1.equals(m2));
        assertTrue(m2.equals(m1));

        Metric m3 = Metric.newBuilder().name(""m1"").tag(""t1"", ""v1"").value(1, 0.0).build();
        assertFalse(m1.equals(m3));

        Metric m4 = Metric.newBuilder().name(""m4"").tag(""t2"", ""v2"").tag(""t1"", ""v1"").value(1, 0.0).build();
        assertFalse(m1.equals(m4));

        Metric m5 = Metric.newBuilder().name(""m1"").tag(""t3"", ""v3"").tag(""t2"", ""v2"").tag(""t1"", ""v1"").value(1, 0.0)
                .build();
        assertFalse(m1.equals(m5));

        Metric m6 = Metric.newBuilder().name(""m1"").tag(""t2"", ""v2"").tag(""t1"", ""v1"").value(2, 0.0).build();
        assertFalse(m1.equals(m6));

        Metric m7 = Metric.newBuilder().name(""m1"").tag(""t2"", ""v2"").tag(""t1"", ""v1"").value(1, 1.0).build();
        assertFalse(m1.equals(m7));
    }
"
"    @Test
    public void testJson() throws IOException {
        ObjectMapper mapper = new ObjectMapper();

        String expectedJson = ""{\""name\"":\""m1\"",\""timestamp\"":1,\""measure\"":1.0,\""tags\"":[{\""k1\"":\""v1\""}]}"";

        Metric m1 = Metric.newBuilder().name(""m1"").tag(""k1"", ""v1"").value(1, 1.0).build();

        Metric expectedMetric = mapper.readValue(expectedJson, Metric.class);

        assertTrue(m1.equals(expectedMetric));

        expectedJson = ""{\""name\"":\""m1\"",\""tags\"":[{\""k1\"":\""v1\""}],\""timestamp\"":5,\""measure\"":5.0}"";
        expectedMetric = mapper.readValue(expectedJson, Metric.class);

        assertEquals((long) expectedMetric.getValue().getTimestamp(), 5L);
        assertEquals(expectedMetric.getValue().getMeasure(), 5.0D, 0.0);

    }
"
"    @Test
    public void testBasicAuth() throws Exception {
        BasicAuthLogin login = new BasicAuthLogin();
        login.setUsername(""test"");
        login.setPassword(""pass"");
        testSerialization(login);
    }
"
"    @Test
    public void testCreateSubscription() throws Exception {
        CreateSubscription create = new CreateSubscription();
        create.setSubscriptionId(""1234"");
        testSerialization(create);
    }
"
"    @Test
    public void testCloseSubscription() throws Exception {
        CloseSubscription close = new CloseSubscription();
        close.setSubscriptionId(""1234"");
        testSerialization(close);
    }
"
"    @Test
    public void testAddSubscription() throws Exception {
        AddSubscription add = new AddSubscription();
        add.setSubscriptionId(""1234"");
        add.setMetric(""sys.cpu.user"");
        testSerialization(add);
    }
"
"    @Test
    public void testRemoveSubscription() throws Exception {
        RemoveSubscription remove = new RemoveSubscription();
        remove.setSubscriptionId(""1234"");
        remove.setMetric(""sys.cpu.user"");
        testSerialization(remove);
    }
"
"    @Test(expected = IllegalArgumentException.class)
    public void testSessionIdNull() throws Exception {
        AuthCache.getAuthorizations("""");
    }
"
"    @Test
    public void testGetAuths() throws Exception {
        Authorizations a = AuthCache.getAuthorizations(cookie);
        Assert.assertEquals(""A,B,C"", a.toString());
    }
"
"    @Test(expected = BadCredentialsException.class)
    public void testBasicAuthenticationFailure() {
        UsernamePasswordAuthenticationToken token = new UsernamePasswordAuthenticationToken(""test"", ""test2"");
        AuthenticationService.getAuthenticationManager().authenticate(token);
    }
"
"    @Test
    public void testBasicAuthenticationLogin() {
        UsernamePasswordAuthenticationToken token = new UsernamePasswordAuthenticationToken(""test"", ""test1"");
        Authentication auth = AuthenticationService.getAuthenticationManager().authenticate(token);
        Collection<? extends GrantedAuthority> authorizations = auth.getAuthorities();
        authorizations.forEach(a -> {
            Assert.assertTrue(
                    a.getAuthority().equals(""A"") || a.getAuthority().equals(""B"") || a.getAuthority().equals(""C""));
        });
    }
"
"    @Test
    public void testX509AuthenticationLogin() {
        PreAuthenticatedAuthenticationToken token = new PreAuthenticatedAuthenticationToken(""example.com"",
                ""doesn't matter what I put here"");
        Authentication auth = AuthenticationService.getAuthenticationManager().authenticate(token);
        Collection<? extends GrantedAuthority> authorizations = auth.getAuthorities();
        authorizations.forEach(a -> {
            Assert.assertTrue(
                    a.getAuthority().equals(""D"") || a.getAuthority().equals(""E"") || a.getAuthority().equals(""F""));
        });
    }
"
"    @Test(expected = UsernameNotFoundException.class)
    public void testX509AuthenticationLoginFailed() {
        PreAuthenticatedAuthenticationToken token = new PreAuthenticatedAuthenticationToken(""bad.example.com"",
                ""doesn't matter what I put here"");
        Authentication auth = AuthenticationService.getAuthenticationManager().authenticate(token);
        Collection<? extends GrantedAuthority> authorizations = auth.getAuthorities();
        authorizations.forEach(a -> {
            Assert.assertTrue(
                    a.getAuthority().equals(""D"") || a.getAuthority().equals(""E"") || a.getAuthority().equals(""F""));
        });
    }
"
"    @Test
    public void testMovingAverage() throws Exception {
        SortedMapIterator source = new SortedMapIterator(table);
        TimeSeriesGroupingIterator iter = new TimeSeriesGroupingIterator();
        IteratorSetting settings = new IteratorSetting(100, TimeSeriesGroupingIterator.class);
        settings.addOption(TimeSeriesGroupingIterator.FILTER, ""0.20,0.20,0.20,0.20,0.20"");
        iter.init(source, settings.getOptions(), SCAN_IE);
        iter.seek(new Range(), EMPTY_COL_FAMS, true);

        for (int i = 4; i < 100; i++) {
            checkNextResult(iter, new double[] { i - 4, i - 3, i - 2, i - 1, i });
        }
        assertFalse(iter.hasTop());
    }
"
"    @Test
    public void testMultipleTimeSeriesMovingAverage() throws Exception {
        table.clear();
        long ts = System.currentTimeMillis();
        List<Tag> tags1 = new ArrayList<>();
        tags1.add(new Tag(""host"", ""r01n01""));
        List<Tag> tags2 = new ArrayList<>();
        tags2.add(new Tag(""host"", ""r01n02""));
        for (int i = 0; i < 100; i++) {
            ts += 1000;
            Metric m = new Metric(""sys.cpu.user"", ts, i * 1.0D, tags1);
            byte[] row = MetricAdapter.encodeRowKey(m);
            Key k = new Key(row, tags1.get(0).join().getBytes(StandardCharsets.UTF_8),
                    MetricAdapter.encodeColQual(ts, """"), new byte[0], ts);
            Value v = new Value(MetricAdapter.encodeValue(m.getValue().getMeasure()));
            table.put(k, v);
            Metric m2 = new Metric(""sys.cpu.user"", ts, i * 2.0D, tags2);
            byte[] row2 = MetricAdapter.encodeRowKey(m2);
            Key k2 = new Key(row2, tags2.get(0).join().getBytes(StandardCharsets.UTF_8),
                    MetricAdapter.encodeColQual(ts, """"), new byte[0], ts);
            Value v2 = new Value(MetricAdapter.encodeValue(m2.getValue().getMeasure()));
            table.put(k2, v2);
        }
        SortedMapIterator source = new SortedMapIterator(table);
        TimeSeriesGroupingIterator iter = new TimeSeriesGroupingIterator();
        IteratorSetting settings = new IteratorSetting(100, TimeSeriesGroupingIterator.class);
        settings.addOption(TimeSeriesGroupingIterator.FILTER, ""0.20,0.20,0.20,0.20,0.20"");
        iter.init(source, settings.getOptions(), SCAN_IE);
        iter.seek(new Range(), EMPTY_COL_FAMS, true);

        // this section changed when the key structure changed so that identical
        // colFam values sorted consecutively within an given time period
        for (int i = 4; i < 100; i++) {
            checkNextResult(iter, new double[] { i - 4, i - 3, i - 2, i - 1, i });
        }
        for (int i = 4; i < 100; i++) {
            checkNextResult(iter, new double[] { (i - 4) * 2, (i - 3) * 2, (i - 2) * 2, (i - 1) * 2, i * 2 });
        }
        assertFalse(iter.hasTop());

    }
"
"    @Test
    public void testTimeSeriesDropOff() throws Exception {
        table.clear();
        long ts = System.currentTimeMillis();
        List<Tag> tags1 = new ArrayList<>();
        tags1.add(new Tag(""host"", ""r01n01""));
        List<Tag> tags2 = new ArrayList<>();
        tags2.add(new Tag(""host"", ""r01n02""));
        for (int i = 0; i < 100; i++) {
            ts += 1000;
            Metric m = new Metric(""sys.cpu.user"", ts, i * 1.0D, tags1);
            byte[] row = MetricAdapter.encodeRowKey(m);
            Key k = new Key(row, tags1.get(0).join().getBytes(StandardCharsets.UTF_8),
                    MetricAdapter.encodeColQual(ts, """"), new byte[0], ts);
            Value v = new Value(MetricAdapter.encodeValue(m.getValue().getMeasure()));
            table.put(k, v);
            if (i < 50) {
                // only populate this series 50 times
                Metric m2 = new Metric(""sys.cpu.user"", ts, i * 2.0D, tags2);
                byte[] row2 = MetricAdapter.encodeRowKey(m2);
                Key k2 = new Key(row2, tags2.get(0).join().getBytes(StandardCharsets.UTF_8),
                        MetricAdapter.encodeColQual(ts, """"), new byte[0], ts);
                Value v2 = new Value(MetricAdapter.encodeValue(m2.getValue().getMeasure()));
                table.put(k2, v2);
            }
        }

        SortedMapIterator source = new SortedMapIterator(table);
        TimeSeriesGroupingIterator iter = new TimeSeriesGroupingIterator();
        IteratorSetting settings = new IteratorSetting(100, TimeSeriesGroupingIterator.class);
        settings.addOption(TimeSeriesGroupingIterator.FILTER, ""0.20,0.20,0.20,0.20,0.20"");
        iter.init(source, settings.getOptions(), SCAN_IE);
        iter.seek(new Range(), EMPTY_COL_FAMS, true);

        // this section changed when the key structure changed so that identical
        // colFam values sorted consecutively within an given time period
        for (int i = 4; i < 100; i++) {
            System.out.println(i);
            checkNextResult(iter, new double[] { i - 4, i - 3, i - 2, i - 1, i });
        }
        for (int i = 4; i < 50; i++) {
            System.out.println(i);
            checkNextResult(iter, new double[] { (i - 4) * 2, (i - 3) * 2, (i - 2) * 2, (i - 1) * 2, i * 2 });
        }
        assertFalse(iter.hasTop());
    }
"
"    @Test
    public void testAdditionalTimeSeries() throws Exception {
        table.clear();
        long ts = System.currentTimeMillis();
        List<Tag> tags1 = new ArrayList<>();
        tags1.add(new Tag(""host"", ""r01n01""));
        List<Tag> tags2 = new ArrayList<>();
        tags2.add(new Tag(""host"", ""r01n02""));
        for (int i = 0; i < 100; i++) {
            ts += 1000;
            Metric m = new Metric(""sys.cpu.user"", ts, i * 1.0D, tags1);
            byte[] row = MetricAdapter.encodeRowKey(m);
            Key k = new Key(row, tags1.get(0).join().getBytes(StandardCharsets.UTF_8),
                    MetricAdapter.encodeColQual(ts, """"), new byte[0], ts);
            Value v = new Value(MetricAdapter.encodeValue(m.getValue().getMeasure()));
            table.put(k, v);
            if (i > 50) {
                // only populate this series 50 times
                Metric m2 = new Metric(""sys.cpu.user"", ts, i * 2.0D, tags2);
                byte[] row2 = MetricAdapter.encodeRowKey(m2);
                Key k2 = new Key(row2, tags2.get(0).join().getBytes(StandardCharsets.UTF_8),
                        MetricAdapter.encodeColQual(ts, """"), new byte[0], ts);
                Value v2 = new Value(MetricAdapter.encodeValue(m2.getValue().getMeasure()));
                table.put(k2, v2);
            }
        }
        SortedMapIterator source = new SortedMapIterator(table);
        TimeSeriesGroupingIterator iter = new TimeSeriesGroupingIterator();
        IteratorSetting settings = new IteratorSetting(100, TimeSeriesGroupingIterator.class);
        settings.addOption(TimeSeriesGroupingIterator.FILTER, ""0.20,0.20,0.20,0.20,0.20"");
        iter.init(source, settings.getOptions(), SCAN_IE);
        iter.seek(new Range(), EMPTY_COL_FAMS, true);

        // this section changed when the key structure changed so that identical
        // colFam values sorted consecutively within an given time period
        for (int i = 4; i < 100; i++) {
            checkNextResult(iter, new double[] { i - 4, i - 3, i - 2, i - 1, i });
        }
        for (int i = 55; i < 100; i++) {
            checkNextResult(iter, new double[] { (i - 4) * 2, (i - 3) * 2, (i - 2) * 2, (i - 1) * 2, i * 2 });
        }

        assertFalse(iter.hasTop());

    }
"
"    @Test
    public void testManySparseTimeSeries() throws Exception {
        table.clear();
        long ts = System.currentTimeMillis();
        List<Tag> tags1 = new ArrayList<>();
        tags1.add(new Tag(""host"", ""r01n01""));
        List<Tag> tags2 = new ArrayList<>();
        tags2.add(new Tag(""host"", ""r01n02""));
        List<Tag> tags3 = new ArrayList<>();
        tags3.add(new Tag(""host"", ""r01n03""));
        for (int i = 0; i < 100; i++) {
            ts += 1000;
            Metric m = new Metric(""sys.cpu.user"", ts, i * 1.0D, tags1);
            byte[] row = MetricAdapter.encodeRowKey(m);
            Key k = new Key(row, tags1.get(0).join().getBytes(StandardCharsets.UTF_8),
                    MetricAdapter.encodeColQual(ts, """"), new byte[0], ts);
            Value v = new Value(MetricAdapter.encodeValue(m.getValue().getMeasure()));
            table.put(k, v);
            // jitter the time on the second time series
            Metric m2 = new Metric(""sys.cpu.user"", ts + 50, i * 2.0D, tags2);
            byte[] row2 = MetricAdapter.encodeRowKey(m2);
            Key k2 = new Key(row2, tags2.get(0).join().getBytes(StandardCharsets.UTF_8),
                    MetricAdapter.encodeColQual(ts, """"), new byte[0], ts + 50);
            Value v2 = new Value(MetricAdapter.encodeValue(m2.getValue().getMeasure()));
            table.put(k2, v2);
            Metric m3 = new Metric(""sys.cpu.user"", ts, i * 3.0D, tags3);
            byte[] row3 = MetricAdapter.encodeRowKey(m3);
            Key k3 = new Key(row3, tags3.get(0).join().getBytes(StandardCharsets.UTF_8),
                    MetricAdapter.encodeColQual(ts, """"), new byte[0], ts);
            Value v3 = new Value(MetricAdapter.encodeValue(m3.getValue().getMeasure()));
            table.put(k3, v3);
        }

        SortedMapIterator source = new SortedMapIterator(table);
        TimeSeriesGroupingIterator iter = new TimeSeriesGroupingIterator();
        IteratorSetting settings = new IteratorSetting(100, TimeSeriesGroupingIterator.class);
        settings.addOption(TimeSeriesGroupingIterator.FILTER, ""0.20,0.20,0.20,0.20,0.20"");
        iter.init(source, settings.getOptions(), SCAN_IE);
        iter.seek(new Range(), EMPTY_COL_FAMS, true);

        LinkedList<Double> first = new LinkedList<>();
        first.add(0D);
        first.add(1D);
        first.add(2D);
        first.add(3D);
        first.add(4D);
        LinkedList<Double> second = new LinkedList<>();
        second.add(0D);
        second.add(2D);
        second.add(4D);
        second.add(6D);
        second.add(8D);
        LinkedList<Double> third = new LinkedList<>();
        third.add(0D);
        third.add(3D);
        third.add(6D);
        third.add(9D);
        third.add(12D);

        // this section changed when the key structure changed so that identical
        // colFam values sorted consecutively within an given time period
        for (int i = 4; i < 100; i++) {
            checkNextResult(iter, first);
            shiftAndAdd(first, 1);
        }
        for (int i = 4; i < 100; i++) {
            System.out.println(i);
            checkNextResult(iter, second);
            shiftAndAdd(second, 2);
        }
        for (int i = 4; i < 100; i++) {
            checkNextResult(iter, third);
            shiftAndAdd(third, 3);
        }
        assertFalse(iter.hasTop());
    }
"
"    @Test
    public void testConstantTimeRate() throws Exception {
        SortedMapIterator source = new SortedMapIterator(table);
        RateIterator iter = new RateIterator();
        IteratorSetting settings = new IteratorSetting(100, RateIterator.class);
        iter.init(source, settings.getOptions(), SCAN_IE);
        iter.seek(new Range(), EMPTY_COL_FAMS, true);
        for (int i = 0; i < 99; i++) {
            assertTrue(iter.hasTop());
            assertEquals(0.001D, MetricAdapter.decodeValue(iter.getTopValue().get()), 0.0D);
            iter.next();
        }
        assertFalse(iter.hasTop());
    }
"
"    @Test
    public void testRateWithTimeJitter() throws Exception {
        table.clear();
        Random r = new Random(111131131L);
        long ts = System.currentTimeMillis();
        for (int i = 1; i <= 100; i++) {
            ts += 1000 + r.nextInt(100);
            Metric m = new Metric(""sys.cpu.user"", ts, i * 1.0D, tags);
            byte[] row = MetricAdapter.encodeRowKey(m);
            Key k = new Key(row, tags.get(0).join().getBytes(StandardCharsets.UTF_8),
                    MetricAdapter.encodeColQual(ts, """"), new byte[0], ts);
            Value v = new Value(MetricAdapter.encodeValue(m.getValue().getMeasure()));
            table.put(k, v);
        }

        SortedMapIterator source = new SortedMapIterator(table);
        source.seek(new Range(), EMPTY_COL_FAMS, true);
        long prevTs = -1L;
        Double prevValue = null;
        List<Double> expected = new ArrayList<>();
        while (source.hasTop()) {
            Key k = source.getTopKey();
            Value v = source.getTopValue();
            if (prevTs != -1L) {
                Double thisValue = MetricAdapter.decodeValue(v.get());
                expected.add((thisValue + (prevValue * -1)) / (k.getTimestamp() - prevTs));
            }
            prevTs = k.getTimestamp();
            prevValue = MetricAdapter.decodeValue(v.get());
            source.next();
        }

        assertEquals(99, expected.size());
        source = new SortedMapIterator(table);
        RateIterator iter = new RateIterator();
        IteratorSetting settings = new IteratorSetting(100, RateIterator.class);
        iter.init(source, settings.getOptions(), SCAN_IE);
        iter.seek(new Range(), EMPTY_COL_FAMS, true);
        for (int i = 0; i < 99; i++) {
            assertTrue(iter.hasTop());
            assertEquals(expected.get(i), MetricAdapter.decodeValue(iter.getTopValue().get()), 0.0D);
            iter.next();
        }
        assertFalse(iter.hasTop());
    }
"
"    @Test
    public void testCounterRate() throws Exception {
        table.clear();
        long ts = System.currentTimeMillis();
        for (int j = 0; j < 10; j++) {
            for (int i = 1; i <= 10; i++) {
                ts += 1000;
                Metric m = new Metric(""sys.cpu.user"", ts, i * 1.0D, tags);
                byte[] row = MetricAdapter.encodeRowKey(m);
                Key k = new Key(row, tags.get(0).join().getBytes(StandardCharsets.UTF_8),
                        MetricAdapter.encodeColQual(ts, """"), new byte[0], ts);
                Value v = new Value(MetricAdapter.encodeValue(m.getValue().getMeasure()));
                table.put(k, v);
            }
        }

        SortedMapIterator source = new SortedMapIterator(table);
        RateIterator iter = new RateIterator();
        IteratorSetting settings = new IteratorSetting(100, RateIterator.class);

        QueryRequest.RateOption option = new QueryRequest.RateOption();
        option.setCounter(true);
        option.setCounterMax(0);
        RateIterator.setRateOptions(settings, option);

        iter.init(source, settings.getOptions(), SCAN_IE);
        iter.seek(new Range(), EMPTY_COL_FAMS, true);
        for (int i = 0; i < 99; i++) {
            assertTrue(iter.hasTop());
            assertEquals(0.001D, MetricAdapter.decodeValue(iter.getTopValue().get()), 0.0D);
            iter.next();
        }
        assertFalse(iter.hasTop());
    }
"
"    @Test
    public void testCounterRateWithMax() throws Exception {
        table.clear();
        long ts = System.currentTimeMillis();
        for (int j = 0; j < 10; j++) {
            for (int i = 0; i < 10; i++) {
                ts += 1000;
                Metric m = new Metric(""sys.cpu.user"", ts, i * 1.0D, tags);
                byte[] row = MetricAdapter.encodeRowKey(m);
                Key k = new Key(row, tags.get(0).join().getBytes(StandardCharsets.UTF_8),
                        MetricAdapter.encodeColQual(ts, """"), new byte[0], ts);
                Value v = new Value(MetricAdapter.encodeValue(m.getValue().getMeasure()));
                table.put(k, v);
            }
        }

        SortedMapIterator source = new SortedMapIterator(table);
        RateIterator iter = new RateIterator();
        IteratorSetting settings = new IteratorSetting(100, RateIterator.class);

        QueryRequest.RateOption option = new QueryRequest.RateOption();
        option.setCounter(true);
        option.setCounterMax(10);
        RateIterator.setRateOptions(settings, option);

        iter.init(source, settings.getOptions(), SCAN_IE);
        iter.seek(new Range(), EMPTY_COL_FAMS, true);
        for (int i = 0; i < 99; i++) {
            assertTrue(iter.hasTop());
            assertEquals(0.001D, MetricAdapter.decodeValue(iter.getTopValue().get()), 0.0D);
            iter.next();
        }
        assertFalse(iter.hasTop());
    }
"
"    @Test
    public void testCounterRateWithReset() throws Exception {
        table.clear();
        long ts = System.currentTimeMillis();
        for (int j = 0; j < 10; j++) {
            for (int i = 0; i < 10; i++) {
                ts += 1000;
                Metric m = new Metric(""sys.cpu.user"", ts, i * 1.0D, tags);
                byte[] row = MetricAdapter.encodeRowKey(m);
                Key k = new Key(row, tags.get(0).join().getBytes(StandardCharsets.UTF_8),
                        MetricAdapter.encodeColQual(ts, """"), new byte[0], ts);
                Value v = new Value(MetricAdapter.encodeValue(m.getValue().getMeasure()));
                table.put(k, v);
            }
        }

        SortedMapIterator source = new SortedMapIterator(table);
        RateIterator iter = new RateIterator();
        IteratorSetting settings = new IteratorSetting(100, RateIterator.class);

        QueryRequest.RateOption option = new QueryRequest.RateOption();
        option.setCounter(true);
        option.setCounterMax(Long.MAX_VALUE);
        option.setResetValue(1);
        RateIterator.setRateOptions(settings, option);

        iter.init(source, settings.getOptions(), SCAN_IE);
        iter.seek(new Range(), EMPTY_COL_FAMS, true);
        for (int i = 0; i < 99; i++) {
            assertTrue(iter.hasTop());
            assertEquals(((i + 1) % 10 == 0 ? 0.0D : 0.001D), MetricAdapter.decodeValue(iter.getTopValue().get()),
                    0.0D);
            iter.next();
        }
        assertFalse(iter.hasTop());
    }
"
"    @Test(expected = IllegalArgumentException.class)
    public void testDefaultMissing() throws Exception {
        SortedMap<Key, Value> table = new TreeMap<>();
        SortedKeyValueIterator<Key, Value> source = new SortedMapIterator(table);
        MetricAgeOffIterator iter = new MetricAgeOffIterator();
        HashMap<String, String> options = new HashMap<>();
        iter.init(source, options, null);
    }
"
"    @Test
    public void testDefault() throws Exception {
        SortedMap<Key, Value> table = new TreeMap<>();
        table.put(new Key(MetricAdapter.encodeRowKey(""sys.cpu.user"", TEST_TIME), new byte[0], new byte[0], new byte[0],
                TEST_TIME), EMPTY_VALUE);
        table.put(new Key(MetricAdapter.encodeRowKey(""sys.cpu.user"", TEST_TIME + 1), new byte[0], new byte[0],
                new byte[0], TEST_TIME + 1), EMPTY_VALUE);
        table.put(new Key(MetricAdapter.encodeRowKey(""sys.cpu.user"", TEST_TIME + 2), new byte[0], new byte[0],
                new byte[0], TEST_TIME + 2), EMPTY_VALUE);
        table.put(new Key(MetricAdapter.encodeRowKey(""sys.cpu.user"", TEST_TIME + 3), new byte[0], new byte[0],
                new byte[0], TEST_TIME + 3), EMPTY_VALUE);
        table.put(new Key(MetricAdapter.encodeRowKey(""sys.cpu.user"", TEST_TIME + 4), new byte[0], new byte[0],
                new byte[0], TEST_TIME + 4), EMPTY_VALUE);
        table.put(new Key(MetricAdapter.encodeRowKey(""sys.cpu.user"", TEST_TIME + 5), new byte[0], new byte[0],
                new byte[0], TEST_TIME + 5), EMPTY_VALUE);

        SortedKeyValueIterator<Key, Value> source = new SortedMapIterator(table);
        MetricAgeOffIterator iter = new MetricAgeOffIterator();
        HashMap<String, String> options = new HashMap<>();
        options.put(MetricAgeOffIterator.AGE_OFF_PREFIX + ""default"", Integer.toString(1 * ONE_DAY));
        iter.init(source, options, null);
        iter.seek(new Range(), columnFamilies, true);
        int seen = 0;
        while (iter.hasTop()) {
            Key k = iter.getTopKey();
            Assert.assertTrue(k.getTimestamp() >= TEST_TIME && k.getTimestamp() <= TEST_TIME + 5);
            seen++;
            iter.next();
        }
        Assert.assertEquals(6, seen);
    }
"
"    @Test
    public void testMixed() throws Exception {
        SortedMap<Key, Value> table = new TreeMap<>();
        table.put(new Key(MetricAdapter.encodeRowKey(""sys.cpu.idle"", TEST_TIME), new byte[0], new byte[0], new byte[0],
                TEST_TIME), EMPTY_VALUE);
        table.put(new Key(MetricAdapter.encodeRowKey(""sys.cpu.idle"", TEST_TIME + 1), new byte[0], new byte[0],
                new byte[0], TEST_TIME + 1), EMPTY_VALUE);
        table.put(new Key(MetricAdapter.encodeRowKey(""sys.cpu.idle"", TEST_TIME + 2), new byte[0], new byte[0],
                new byte[0], TEST_TIME + 2), EMPTY_VALUE);
        table.put(new Key(MetricAdapter.encodeRowKey(""sys.cpu.idle"", TEST_TIME + 3), new byte[0], new byte[0],
                new byte[0], TEST_TIME + 3), EMPTY_VALUE);
        table.put(new Key(MetricAdapter.encodeRowKey(""sys.cpu.idle"", TEST_TIME + 4), new byte[0], new byte[0],
                new byte[0], TEST_TIME + 4), EMPTY_VALUE);
        table.put(new Key(MetricAdapter.encodeRowKey(""sys.cpu.idle"", TEST_TIME + 5), new byte[0], new byte[0],
                new byte[0], TEST_TIME + 5), EMPTY_VALUE);
        table.put(new Key(MetricAdapter.encodeRowKey(""sys.cpu.user"", TEST_TIME), new byte[0], new byte[0], new byte[0],
                TEST_TIME), EMPTY_VALUE);
        table.put(new Key(MetricAdapter.encodeRowKey(""sys.cpu.user"", TEST_TIME + 1), new byte[0], new byte[0],
                new byte[0], TEST_TIME + 1), EMPTY_VALUE);
        table.put(new Key(MetricAdapter.encodeRowKey(""sys.cpu.user"", TEST_TIME + 2), new byte[0], new byte[0],
                new byte[0], TEST_TIME + 2), EMPTY_VALUE);
        table.put(new Key(MetricAdapter.encodeRowKey(""sys.cpu.user"", TEST_TIME + 3), new byte[0], new byte[0],
                new byte[0], TEST_TIME + 3), EMPTY_VALUE);
        table.put(new Key(MetricAdapter.encodeRowKey(""sys.cpu.user"", TEST_TIME + 4), new byte[0], new byte[0],
                new byte[0], TEST_TIME + 4), EMPTY_VALUE);
        table.put(new Key(MetricAdapter.encodeRowKey(""sys.cpu.user"", TEST_TIME + 5), new byte[0], new byte[0],
                new byte[0], TEST_TIME + 5), EMPTY_VALUE);

        SortedKeyValueIterator<Key, Value> source = new SortedMapIterator(table);
        MetricAgeOffIterator iter = new MetricAgeOffIterator();
        HashMap<String, String> options = new HashMap<>();
        options.put(MetricAgeOffIterator.AGE_OFF_PREFIX + ""default"", Integer.toString(1 * ONE_DAY));
        iter.init(source, options, null);
        iter.seek(new Range(), columnFamilies, true);
        int seen = 0;
        while (iter.hasTop()) {
            Key k = iter.getTopKey();
            Assert.assertTrue(k.getTimestamp() >= TEST_TIME && k.getTimestamp() <= TEST_TIME + 5);
            seen++;
            iter.next();
        }
        Assert.assertEquals(12, seen);
    }
"
"    @Test
    public void testAgeoffMixed() throws Exception {
        SortedMap<Key, Value> table = new TreeMap<>();
        table.put(new Key(MetricAdapter.encodeRowKey(""sys.cpu.idle"", TEST_TIME - (3 * ONE_DAY)), new byte[0],
                new byte[0], new byte[0], TEST_TIME - (3 * ONE_DAY)), EMPTY_VALUE);
        table.put(new Key(MetricAdapter.encodeRowKey(""sys.cpu.idle"", TEST_TIME - (2 * ONE_DAY)), new byte[0],
                new byte[0], new byte[0], TEST_TIME - (2 * ONE_DAY)), EMPTY_VALUE);
        table.put(new Key(MetricAdapter.encodeRowKey(""sys.cpu.idle"", TEST_TIME - (1 * ONE_DAY)), new byte[0],
                new byte[0], new byte[0], TEST_TIME - (1 * ONE_DAY)), EMPTY_VALUE);
        table.put(new Key(MetricAdapter.encodeRowKey(""sys.cpu.idle"", TEST_TIME), new byte[0], new byte[0], new byte[0],
                TEST_TIME), EMPTY_VALUE);
        table.put(new Key(MetricAdapter.encodeRowKey(""sys.cpu.idle"", TEST_TIME + ONE_DAY), new byte[0], new byte[0],
                new byte[0], TEST_TIME + ONE_DAY), EMPTY_VALUE);
        table.put(new Key(MetricAdapter.encodeRowKey(""sys.cpu.idle"", TEST_TIME + (2 * ONE_DAY)), new byte[0],
                new byte[0], new byte[0], TEST_TIME + (2 * ONE_DAY)), EMPTY_VALUE);
        table.put(new Key(MetricAdapter.encodeRowKey(""sys.cpu.user"", TEST_TIME - (3 * ONE_DAY)), new byte[0],
                new byte[0], new byte[0], TEST_TIME - (3 * ONE_DAY)), EMPTY_VALUE);
        table.put(new Key(MetricAdapter.encodeRowKey(""sys.cpu.user"", TEST_TIME - (2 * ONE_DAY)), new byte[0],
                new byte[0], new byte[0], TEST_TIME - (2 * ONE_DAY)), EMPTY_VALUE);
        table.put(new Key(MetricAdapter.encodeRowKey(""sys.cpu.user"", TEST_TIME - (1 * ONE_DAY)), new byte[0],
                new byte[0], new byte[0], TEST_TIME - (1 * ONE_DAY)), EMPTY_VALUE);
        table.put(new Key(MetricAdapter.encodeRowKey(""sys.cpu.user"", TEST_TIME), new byte[0], new byte[0], new byte[0],
                TEST_TIME), EMPTY_VALUE);
        table.put(new Key(MetricAdapter.encodeRowKey(""sys.cpu.user"", TEST_TIME + ONE_DAY), new byte[0], new byte[0],
                new byte[0], TEST_TIME + ONE_DAY), EMPTY_VALUE);
        table.put(new Key(MetricAdapter.encodeRowKey(""sys.cpu.user"", TEST_TIME + (2 * ONE_DAY)), new byte[0],
                new byte[0], new byte[0], TEST_TIME + (2 * ONE_DAY)), EMPTY_VALUE);

        SortedKeyValueIterator<Key, Value> source = new SortedMapIterator(table);
        MetricAgeOffIterator iter = new MetricAgeOffIterator();
        HashMap<String, String> options = new HashMap<>();
        options.put(MetricAgeOffIterator.AGE_OFF_PREFIX + ""default"", Integer.toString(1 * ONE_DAY));
        options.put(MetricAgeOffIterator.AGE_OFF_PREFIX + ""sys.cpu.user"", Integer.toString(2 * ONE_DAY));
        iter.init(source, options, null);
        iter.seek(new Range(), columnFamilies, true);
        int seen = 0;
        while (iter.hasTop()) {
            Key k = iter.getTopKey();
            Assert.assertTrue(
                    k.getTimestamp() >= (TEST_TIME - (2 * ONE_DAY)) && k.getTimestamp() <= TEST_TIME + (2 * ONE_DAY));
            seen++;
            iter.next();
        }
        Assert.assertEquals(7, seen);

    }
"
"    @Test
    public void testSeekPastEndKey() throws Exception {
        SortedMap<Key, Value> table = new TreeMap<>();
        table.put(new Key(MetricAdapter.encodeRowKey(""sys.cpu.user"", TEST_TIME), new byte[0], new byte[0], new byte[0],
                TEST_TIME), EMPTY_VALUE);
        table.put(new Key(MetricAdapter.encodeRowKey(""sys.cpu.user"", TEST_TIME + 1), new byte[0], new byte[0],
                new byte[0], TEST_TIME + 1), EMPTY_VALUE);
        table.put(new Key(MetricAdapter.encodeRowKey(""sys.cpu.user"", TEST_TIME + 2), new byte[0], new byte[0],
                new byte[0], TEST_TIME + 2), EMPTY_VALUE);
        table.put(new Key(MetricAdapter.encodeRowKey(""sys.cpu.user"", TEST_TIME + 3), new byte[0], new byte[0],
                new byte[0], TEST_TIME + 3), EMPTY_VALUE);
        table.put(new Key(MetricAdapter.encodeRowKey(""sys.cpu.user"", TEST_TIME + 4), new byte[0], new byte[0],
                new byte[0], TEST_TIME + 4), EMPTY_VALUE);
        table.put(new Key(MetricAdapter.encodeRowKey(""sys.cpu.user"", TEST_TIME + 5), new byte[0], new byte[0],
                new byte[0], TEST_TIME + 5), EMPTY_VALUE);

        SortedKeyValueIterator<Key, Value> source = new SortedMapIterator(table);
        MetricAgeOffIterator iter = new MetricAgeOffIterator();
        HashMap<String, String> options = new HashMap<>();
        options.put(MetricAgeOffIterator.AGE_OFF_PREFIX + ""default"", Integer.toString(1));
        iter.init(source, options, null);
        iter.seek(new Range(new Key(""sys.cpu.user""), true,
                new Key(MetricAdapter.encodeRowKey(""sys.cpu.user"", TEST_TIME + 3), new byte[0], new byte[0],
                        new byte[0], TEST_TIME + 3),
                true), columnFamilies, true);
        int seen = 0;
        while (iter.hasTop()) {
            Key k = iter.getTopKey();
            Assert.assertTrue(k.getTimestamp() >= TEST_TIME && k.getTimestamp() <= TEST_TIME + 5);
            seen++;
            iter.next();
        }
        Assert.assertEquals(0, seen);
    }
"
"    @Test(expected = IllegalArgumentException.class)
    public void testDefaultMissing() throws Exception {
        MetricAgeOffFilter filter = new MetricAgeOffFilter();
        HashMap<String, String> options = new HashMap<>();
        filter.init(null, options, null);
    }
"
"    @Test
    public void testDefault() throws Exception {
        MetricAgeOffFilter filter = new MetricAgeOffFilter();
        HashMap<String, String> options = new HashMap<>();
        options.put(MetricAgeOffFilter.AGE_OFF_PREFIX + ""default"", Integer.toString(1 * ONE_DAY));
        filter.init(null, options, null);
        assertTrue(filter.accept(new Key(MetricAdapter.encodeRowKey(""sys.cpu.user"", TEST_TIME), new byte[0],
                new byte[0], new byte[0], TEST_TIME), null));
        assertTrue(filter.accept(new Key(MetricAdapter.encodeRowKey(""sys.cpu.user"", TEST_TIME + 1), new byte[0],
                new byte[0], new byte[0], TEST_TIME + 1), null));
        assertTrue(filter.accept(new Key(MetricAdapter.encodeRowKey(""sys.cpu.user"", TEST_TIME + 2), new byte[0],
                new byte[0], new byte[0], TEST_TIME + 2), null));
        assertTrue(filter.accept(new Key(MetricAdapter.encodeRowKey(""sys.cpu.user"", TEST_TIME + 3), new byte[0],
                new byte[0], new byte[0], TEST_TIME + 3), null));
        assertTrue(filter.accept(new Key(MetricAdapter.encodeRowKey(""sys.cpu.user"", TEST_TIME + 4), new byte[0],
                new byte[0], new byte[0], TEST_TIME + 4), null));
        assertTrue(filter.accept(new Key(MetricAdapter.encodeRowKey(""sys.cpu.user"", TEST_TIME + 5), new byte[0],
                new byte[0], new byte[0], TEST_TIME + 5), null));
    }
"
"    @Test
    public void testMixed() throws Exception {
        MetricAgeOffFilter filter = new MetricAgeOffFilter();
        HashMap<String, String> options = new HashMap<>();
        options.put(MetricAgeOffFilter.AGE_OFF_PREFIX + ""default"", Integer.toString(1 * ONE_DAY));
        filter.init(null, options, null);
        assertTrue(filter.accept(new Key(MetricAdapter.encodeRowKey(""sys.cpu.idle"", TEST_TIME), new byte[0],
                new byte[0], new byte[0], TEST_TIME), null));
        assertTrue(filter.accept(new Key(MetricAdapter.encodeRowKey(""sys.cpu.idle"", TEST_TIME + 1), new byte[0],
                new byte[0], new byte[0], TEST_TIME + 1), null));
        assertTrue(filter.accept(new Key(MetricAdapter.encodeRowKey(""sys.cpu.idle"", TEST_TIME + 2), new byte[0],
                new byte[0], new byte[0], TEST_TIME + 2), null));
        assertTrue(filter.accept(new Key(MetricAdapter.encodeRowKey(""sys.cpu.idle"", TEST_TIME + 3), new byte[0],
                new byte[0], new byte[0], TEST_TIME + 3), null));
        assertTrue(filter.accept(new Key(MetricAdapter.encodeRowKey(""sys.cpu.idle"", TEST_TIME + 4), new byte[0],
                new byte[0], new byte[0], TEST_TIME + 4), null));
        assertTrue(filter.accept(new Key(MetricAdapter.encodeRowKey(""sys.cpu.idle"", TEST_TIME + 5), new byte[0],
                new byte[0], new byte[0], TEST_TIME + 5), null));
        assertTrue(filter.accept(new Key(MetricAdapter.encodeRowKey(""sys.cpu.user"", TEST_TIME), new byte[0],
                new byte[0], new byte[0], TEST_TIME), null));
        assertTrue(filter.accept(new Key(MetricAdapter.encodeRowKey(""sys.cpu.user"", TEST_TIME + 1), new byte[0],
                new byte[0], new byte[0], TEST_TIME + 1), null));
        assertTrue(filter.accept(new Key(MetricAdapter.encodeRowKey(""sys.cpu.user"", TEST_TIME + 2), new byte[0],
                new byte[0], new byte[0], TEST_TIME + 2), null));
        assertTrue(filter.accept(new Key(MetricAdapter.encodeRowKey(""sys.cpu.user"", TEST_TIME + 3), new byte[0],
                new byte[0], new byte[0], TEST_TIME + 3), null));
        assertTrue(filter.accept(new Key(MetricAdapter.encodeRowKey(""sys.cpu.user"", TEST_TIME + 4), new byte[0],
                new byte[0], new byte[0], TEST_TIME + 4), null));
        assertTrue(filter.accept(new Key(MetricAdapter.encodeRowKey(""sys.cpu.user"", TEST_TIME + 5), new byte[0],
                new byte[0], new byte[0], TEST_TIME + 5), null));
    }
"
"    @Test
    public void testAgeoffMixed() throws Exception {
        MetricAgeOffFilter filter = new MetricAgeOffFilter();
        HashMap<String, String> options = new HashMap<>();
        options.put(MetricAgeOffFilter.AGE_OFF_PREFIX + ""default"", Integer.toString(1 * ONE_DAY));
        options.put(MetricAgeOffFilter.AGE_OFF_PREFIX + ""sys.cpu.user"", Integer.toString(2 * ONE_DAY));
        filter.init(null, options, null);
        assertFalse(filter.accept(new Key(MetricAdapter.encodeRowKey(""sys.cpu.idle"", TEST_TIME - (3 * ONE_DAY)),
                new byte[0], new byte[0], new byte[0], TEST_TIME - (3 * ONE_DAY)), null));
        assertFalse(filter.accept(new Key(MetricAdapter.encodeRowKey(""sys.cpu.idle"", TEST_TIME - (2 * ONE_DAY)),
                new byte[0], new byte[0], new byte[0], TEST_TIME - (2 * ONE_DAY)), null));
        assertFalse(filter.accept(new Key(MetricAdapter.encodeRowKey(""sys.cpu.idle"", TEST_TIME - (1 * ONE_DAY)),
                new byte[0], new byte[0], new byte[0], TEST_TIME - (1 * ONE_DAY)), null));
        assertTrue(filter.accept(new Key(MetricAdapter.encodeRowKey(""sys.cpu.idle"", TEST_TIME), new byte[0],
                new byte[0], new byte[0], TEST_TIME), null));
        assertTrue(filter.accept(new Key(MetricAdapter.encodeRowKey(""sys.cpu.idle"", TEST_TIME + ONE_DAY), new byte[0],
                new byte[0], new byte[0], TEST_TIME + ONE_DAY), null));
        assertTrue(filter.accept(new Key(MetricAdapter.encodeRowKey(""sys.cpu.idle"", TEST_TIME + (2 * ONE_DAY)),
                new byte[0], new byte[0], new byte[0], TEST_TIME + (2 * ONE_DAY)), null));
        assertFalse(filter.accept(new Key(MetricAdapter.encodeRowKey(""sys.cpu.user"", TEST_TIME - (3 * ONE_DAY)),
                new byte[0], new byte[0], new byte[0], TEST_TIME - (3 * ONE_DAY)), null));
        assertFalse(filter.accept(new Key(MetricAdapter.encodeRowKey(""sys.cpu.user"", TEST_TIME - (2 * ONE_DAY)),
                new byte[0], new byte[0], new byte[0], TEST_TIME - (2 * ONE_DAY)), null));
        assertTrue(filter.accept(new Key(MetricAdapter.encodeRowKey(""sys.cpu.user"", TEST_TIME - (1 * ONE_DAY)),
                new byte[0], new byte[0], new byte[0], TEST_TIME - (1 * ONE_DAY)), null));
        assertTrue(filter.accept(new Key(MetricAdapter.encodeRowKey(""sys.cpu.user"", TEST_TIME), new byte[0],
                new byte[0], new byte[0], TEST_TIME), null));
        assertTrue(filter.accept(new Key(MetricAdapter.encodeRowKey(""sys.cpu.user"", TEST_TIME + ONE_DAY), new byte[0],
                new byte[0], new byte[0], TEST_TIME + ONE_DAY), null));
        assertTrue(filter.accept(new Key(MetricAdapter.encodeRowKey(""sys.cpu.user"", TEST_TIME + (2 * ONE_DAY)),
                new byte[0], new byte[0], new byte[0], TEST_TIME + (2 * ONE_DAY)), null));
    }
"
"    @Test
    public void testRegex1() throws Exception {
        String tags = ""tag1=value1,tag2=value2,tag3=value3"";
        StringBuffer pattern = new StringBuffer();
        pattern.append(""(^|.*,)"");
        pattern.append(""tag2"");
        pattern.append(""="");
        pattern.append(""value2"");
        pattern.append(""(,.*|$)"");
        Pattern p = Pattern.compile(pattern.toString());
        assertTrue(p.matcher(tags).matches());
    }
"
"    @Test
    public void testRegex2() throws Exception {
        String tags = ""tag1=value1,tag2=value2,tag3=value3"";
        StringBuffer pattern = new StringBuffer();
        pattern.append(""(^|.*,)"");
        pattern.append(""tag2"");
        pattern.append(""="");
        pattern.append(""value\\d"");
        pattern.append(""(,.*|$)"");
        Pattern p = Pattern.compile(pattern.toString());
        assertTrue(p.matcher(tags).matches());
    }
"
"    @Test
    public void testRegex3() throws Exception {
        String tags = ""tag1=value1,tag2=value2,tag3=value3"";
        StringBuffer pattern = new StringBuffer();
        pattern.append(""(^|.*,)"");
        pattern.append(""tag2"");
        pattern.append(""="");
        pattern.append(""(value2|value3)"");
        pattern.append(""(,.*|$)"");
        Pattern p = Pattern.compile(pattern.toString());
        assertTrue(p.matcher(tags).matches());
    }
"
"    @Test
    public void testSerialization() throws IOException, ClassNotFoundException {

        long start = System.currentTimeMillis();
        WrappedGorillaCompressor originalCompressor = new WrappedGorillaCompressor(start);
        long t = start;

        for (int x = 1; x <= 10; x++) {
            originalCompressor.addValue(t, 10);
            t = t + 1000;
        }
        originalCompressor.close();
        ByteArrayOutputStream outputStream = new ByteArrayOutputStream();
        ObjectOutputStream oos = new ObjectOutputStream(outputStream);
        oos.writeObject(originalCompressor);
        oos.close();

        ByteArrayInputStream inputStream = new ByteArrayInputStream(outputStream.toByteArray());
        ObjectInputStream ois = new ObjectInputStream(inputStream);
        WrappedGorillaCompressor copyCompressor = (WrappedGorillaCompressor) ois.readObject();

        GorillaDecompressor d = new GorillaDecompressor(new LongArrayInput(copyCompressor.getCompressorOutput()));

        LinkedList<Pair> q = new LinkedList<>();
        Pair p = null;
        while ((p = d.readPair()) != null) {
            q.add(p);
        }

        Assert.assertEquals(10, q.size());
        Assert.assertEquals(start, q.peekFirst().getTimestamp());
        Assert.assertEquals(start + 9000, q.peekLast().getTimestamp());
    }
"
"    @Test
    public void testHDFSWrite() throws Exception {

        try {
            Configuration configuration = new Configuration();
            FileSystem fs = FileSystem.get(new URI(""hdfs://localhost:8020""), configuration);
            GorillaStore store = new GorillaStore(fs, ""mymetric"", new timely.Configuration());

            long start = System.currentTimeMillis();
            WrappedGorillaCompressor originalCompressor = new WrappedGorillaCompressor(start);
            long t = start;

            for (int x = 1; x <= 10; x++) {
                originalCompressor.addValue(t, 10);
                t = t + 1000;
            }
            originalCompressor.close();

            store.writeCompressor(""mymetric"", originalCompressor);

            List<WrappedGorillaCompressor> archived = store.readCompressors(fs, new Path(""/timely/cache/mymetric""));

            for (WrappedGorillaCompressor c : archived) {

                GorillaDecompressor d = new GorillaDecompressor(new LongArrayInput(c.getCompressorOutput()));
                LinkedList<Pair> q = new LinkedList<>();
                Pair p = null;
                while ((p = d.readPair()) != null) {
                    q.add(p);
                }
            }
        } catch (Exception e) {
            System.out.println(e.getMessage());
        }
    }
"
"    @Test
    public void testDownsampleIterator() throws TimelyException {

        long BASETIME = System.currentTimeMillis();
        // align basetime to a downsample period
        BASETIME = BASETIME - (BASETIME % (1000 * 60));
        DataStoreCache mmStore = getMetricMemoryStore1(BASETIME);

        QueryRequest query = new QueryRequest();
        query.setStart(BASETIME);
        query.setEnd(BASETIME + 1440000);
        query.setMsResolution(true);
        QueryRequest.SubQuery subQuery = new QueryRequest.SubQuery();
        subQuery.setDownsample(Optional.of(""1m-avg""));
        subQuery.setMetric(""metric.number.1"");
        subQuery.addTag(""host"", "".*"");
        query.setQueries(Collections.singleton(subQuery));

        SortedKeyValueIterator<org.apache.accumulo.core.data.Key, org.apache.accumulo.core.data.Value> itr = null;
        try {
            long firstTimestamp = -1;
            long lastTimestamp = -1;
            int numSamples = 0;
            itr = mmStore.setupIterator(query, subQuery, new Authorizations(), Long.MAX_VALUE);
            while (itr.hasTop()) {
                itr.next();
                Map<Set<Tag>, Aggregation> aggregations = AggregationIterator.decodeValue(itr.getTopValue());
                for (Map.Entry<Set<Tag>, Aggregation> entry : aggregations.entrySet()) {
                    for (Sample s : entry.getValue()) {
                        numSamples++;
                        if (firstTimestamp == -1) {
                            firstTimestamp = s.timestamp;
                        }
                        lastTimestamp = s.timestamp;
                    }
                }
            }
            Assert.assertEquals(""First timestamp incorrect"", BASETIME, firstTimestamp);
            Assert.assertEquals(""Last timestamp incorrect"", BASETIME + 1440000, lastTimestamp);
            Assert.assertEquals(""Number of samples incorrect"", 50, numSamples);
        } catch (IOException | ClassNotFoundException e) {
            LOG.error(""exception in test"", e);
        }
    }
"
"    @Test
    public void testRateIterator() throws TimelyException {

        long BASETIME = System.currentTimeMillis();
        // align basetime to a downsample period
        BASETIME = BASETIME - (BASETIME % 1000);
        DataStoreCache mmStore = getMetricMemoryStore2(BASETIME);

        QueryRequest query = new QueryRequest();
        query.setStart(BASETIME);
        query.setEnd(BASETIME + 1440000);
        query.setMsResolution(true);
        QueryRequest.SubQuery subQuery = new QueryRequest.SubQuery();
        subQuery.setDownsample(Optional.of(""1ms-avg""));
        subQuery.setMetric(""metric.number.1"");
        subQuery.addTag(""host"", "".*"");
        QueryRequest.RateOption rateOption = new QueryRequest.RateOption();
        rateOption.setCounter(false);
        subQuery.setRate(true);
        subQuery.setRateOptions(rateOption);
        query.setQueries(Collections.singleton(subQuery));

        int x = 0;
        SortedKeyValueIterator<org.apache.accumulo.core.data.Key, org.apache.accumulo.core.data.Value> itr = null;
        try {
            // long firstTimestamp = Long.MAX_VALUE;
            long firstTimestamp = -1;
            long lastTimestamp = -1;
            int numSamples = 0;
            itr = mmStore.setupIterator(query, subQuery, new Authorizations(), Long.MAX_VALUE);
            while (itr.hasTop()) {
                itr.next();
                Map<Set<Tag>, Aggregation> aggregations = AggregationIterator.decodeValue(itr.getTopValue());
                for (Map.Entry<Set<Tag>, Aggregation> entry : aggregations.entrySet()) {
                    for (Sample s : entry.getValue()) {
                        numSamples++;
                        if (firstTimestamp == -1) {
                            firstTimestamp = s.timestamp;
                        }
                        lastTimestamp = s.timestamp;
                        // if (s.timestamp < firstTimestamp) {
                        // firstTimestamp = s.timestamp;
                        // }
                        // if (s.timestamp > lastTimestamp) {
                        // lastTimestamp = s.timestamp;
                        // }
                    }
                }
            }
            Assert.assertEquals(""First timestamp incorrect"", BASETIME + 1000, firstTimestamp);
            Assert.assertEquals(""Last timestamp incorrect"", BASETIME + 1440000, lastTimestamp);
            Assert.assertEquals(""Number of samples incorrect"", 2880, numSamples);
        } catch (IOException | ClassNotFoundException e) {
            LOG.error(""exception in test"", e);
        }
    }
"
"    @Test
    public void testOne() {

        GorillaStore gStore = new GorillaStore();

        long now = System.currentTimeMillis();
        gStore.addValue(now += 100, 1.123);
        gStore.addValue(now += 100, 2.314);
        gStore.addValue(now += 100, 3.856);
        gStore.addValue(now += 100, 4.7678);
        gStore.addValue(now += 100, 5.8966);
        gStore.addValue(now += 100, 6.0976);
        gStore.addValue(now += 100, 1.2345);

        List<WrappedGorillaDecompressor> decompressorList = gStore.getDecompressors(0, Long.MAX_VALUE);
        Pair pair = null;
        for (WrappedGorillaDecompressor w : decompressorList) {
            while ((pair = w.readPair()) != null) {
                System.out.println(pair.getTimestamp() + "" --> "" + pair.getDoubleValue());
            }
        }

        System.out.println(""---------------"");

        gStore.addValue(now += 100, 2.3456);
        gStore.addValue(now += 100, 3.4567);

        decompressorList = gStore.getDecompressors(0, Long.MAX_VALUE);
        pair = null;
        for (WrappedGorillaDecompressor w : decompressorList) {
            while ((pair = w.readPair()) != null) {
                System.out.println(pair.getTimestamp() + "" --> "" + pair.getDoubleValue());
            }
        }
    }
"
"    @Test
    public void testExtentOfStorage() {

        GorillaStore gStore = new GorillaStore();

        HashMap<String, String> tags = new HashMap<>();
        tags.put(""host"", ""localhost"");

        long start = System.currentTimeMillis();
        long timestamp = start;

        for (int x = 1; x <= 100; x++) {

            System.out.println(""adding value x:"" + x);
            gStore.addValue(timestamp, 2.0);
            timestamp = timestamp + 1000;

            if (x % 10 == 0) {
                gStore.archiveCurrentCompressor();
            }
            if (x < 50) {
                continue;
            }

            System.out.println(""fetching values x:"" + x);
            long totalObservations = 0;

            List<WrappedGorillaDecompressor> decompressorList = gStore.getDecompressors(start, timestamp);
            Pair pair = null;
            for (WrappedGorillaDecompressor w : decompressorList) {
                while ((pair = w.readPair()) != null) {
                    totalObservations++;
                    // System.out.println(pair.getTimestamp() + "" --> "" +
                    // pair.getDoubleValue());
                }
            }

            Assert.assertEquals(""Unexpected number of total observations"", x, totalObservations);

        }

    }
"
"    @Test
    public void testOne() throws TimelyException {

        long now = System.currentTimeMillis();
        DataStoreCache mmStore = getMetricMemoryStore1(now);

        QueryRequest query = new QueryRequest();
        query.setStart(now);
        query.setEnd(now + 100000);
        query.setMsResolution(true);
        QueryRequest.SubQuery subQuery = new QueryRequest.SubQuery();
        subQuery.setDownsample(Optional.of(""5s-avg""));
        subQuery.setMetric(""mymetric"");
        subQuery.addTag(""host"", "".*"");
        query.setQueries(Collections.singleton(subQuery));

        try {
            List<QueryResponse> responseList = mmStore.query(query);
            for (QueryResponse response : responseList) {
                System.out.println(response.toString());
            }
        } catch (TimelyException e) {
            e.printStackTrace();
        }
    }
"
"    @Test
    public void testStorage() throws TimelyException {

        long now = System.currentTimeMillis();
        DataStoreCache mmStore = getMetricMemoryStore2(now);

        QueryRequest query = new QueryRequest();
        query.setStart(now);
        query.setEnd(now + 86400000);
        query.setMsResolution(true);
        QueryRequest.SubQuery subQuery = new QueryRequest.SubQuery();
        subQuery.setDownsample(Optional.of(""5m-avg""));
        subQuery.setMetric(""metric.number.1"");
        subQuery.addTag(""host"", "".*"");
        query.setQueries(Collections.singleton(subQuery));

        try {
            List<QueryResponse> responseList = mmStore.query(query);
            for (QueryResponse response : responseList) {
                System.out.println(response.toString());
            }
        } catch (TimelyException e) {
            e.printStackTrace();
        }
    }
"
"    @Test
    public void TestExtentOfStorage() {
        DataStoreCache mmStore = new DataStoreCache(configuration);

        HashMap<String, String> tags = new HashMap<>();
        tags.put(""host"", ""localhost"");

        long start = System.currentTimeMillis();
        long timestamp = start;

        for (int x = 1; x <= 100; x++) {

            Metric m = createMetric(""test.metric"", tags, 2.0, timestamp);
            mmStore.store(m);
            mmStore.flushCaches(-1);
            timestamp = timestamp + 60000;

            QueryRequest query = new QueryRequest();
            query.setStart(start);
            query.setEnd(start + 86400000);
            query.setMsResolution(true);
            QueryRequest.SubQuery subQuery = new QueryRequest.SubQuery();
            // subQuery.setDownsample(Optional.of(""5m-avg""));
            subQuery.setMetric(""test.metric"");
            query.setQueries(Collections.singleton(subQuery));

            try {
                List<QueryResponse> responseList = mmStore.query(query);
                long totalObservations = 0;
                for (QueryResponse r : responseList) {
                    totalObservations += r.getDps().size();
                }
                Assert.assertEquals(""Unexpected number of total observations"", x, totalObservations);

            } catch (TimelyException e) {
                e.printStackTrace();
            }

        }

    }
"
"    @Test
    public void testCreateDeserialization() throws Exception {
        // @formatter:off
		String json = ""{ ""
				       + ""\""operation\"" : \""create\"",""
				       + "" \""sessionId\"": \""1234\""""
				    + ""}"";
		// @formatter:on
        WebSocketRequest request = JsonUtil.getObjectMapper().readValue(json.getBytes(), WebSocketRequest.class);
        Assert.assertNotNull(request);
        Assert.assertEquals(CreateSubscription.class, request.getClass());
        Assert.assertEquals(""1234"", ((CreateSubscription) request).getSessionId());
    }
"
"    @Test
    public void testRemoveDeserialization() throws Exception {
        // @formatter:off
		String json = ""{ ""
				       + ""\""operation\"" : \""remove\"",""
				       + "" \""sessionId\"": \""1234\"",""
				       + "" \""metric\"" : \""sys.cpu.user\""""
				    + ""}"";
		// @formatter:on
        WebSocketRequest request = JsonUtil.getObjectMapper().readValue(json.getBytes(), WebSocketRequest.class);
        Assert.assertNotNull(request);
        Assert.assertEquals(RemoveSubscription.class, request.getClass());
        Assert.assertEquals(""1234"", ((RemoveSubscription) request).getSessionId());
        Assert.assertEquals(""sys.cpu.user"", ((RemoveSubscription) request).getMetric());
    }
"
"    @Test
    public void testCloseDeserialization() throws Exception {
        // @formatter:off
		String json = ""{ ""
				       + ""\""operation\"" : \""close\"",""
				       + "" \""sessionId\"": \""1234\""""
				    + ""}"";
		// @formatter:on
        WebSocketRequest request = JsonUtil.getObjectMapper().readValue(json.getBytes(), WebSocketRequest.class);
        Assert.assertNotNull(request);
        Assert.assertEquals(CloseSubscription.class, request.getClass());
        Assert.assertEquals(""1234"", ((CloseSubscription) request).getSessionId());
    }
"
"    @Test
    public void testAddDeserialization() throws Exception {
        // @formatter:off
		String json = ""{"" +
						""\""operation\"" : \""add\"","" +
						""\""sessionId\"" : \""1234\"","" +
					    "" \""metric\"" : \""sys.cpu.user\"""" +
					  ""}"";
		// @formatter:on
        WebSocketRequest request = JsonUtil.getObjectMapper().readValue(json.getBytes(), WebSocketRequest.class);
        Assert.assertNotNull(request);
        Assert.assertEquals(AddSubscription.class, request.getClass());
        Assert.assertEquals(""1234"", ((AddSubscription) request).getSessionId());
        Assert.assertEquals(""sys.cpu.user"", ((AddSubscription) request).getMetric());
        Assert.assertEquals(false, ((AddSubscription) request).getTags().isPresent());
        Assert.assertEquals(false, ((AddSubscription) request).getStartTime().isPresent());
    }
"
"    @Test
    public void testAddDeserializationWithTime() throws Exception {
        // @formatter:off
		String json = ""{"" +
						""\""operation\"" : \""add\"","" +
						""\""sessionId\"" : \""1234\"","" +
					    ""\""metric\"" : \""sys.cpu.user\"","" +
						""\""startTime\"" : \""1000\"""" +
					  ""}"";
		// @formatter:on
        WebSocketRequest request = JsonUtil.getObjectMapper().readValue(json.getBytes(), WebSocketRequest.class);
        Assert.assertNotNull(request);
        Assert.assertEquals(AddSubscription.class, request.getClass());
        Assert.assertEquals(""1234"", ((AddSubscription) request).getSessionId());
        Assert.assertEquals(""sys.cpu.user"", ((AddSubscription) request).getMetric());
        Assert.assertEquals(false, ((AddSubscription) request).getTags().isPresent());
        Assert.assertEquals(true, ((AddSubscription) request).getStartTime().isPresent());
        long time = ((AddSubscription) request).getStartTime().get();
        Assert.assertEquals(1000L, time);
    }
"
"    @Test
    public void testAddDeserializationWithTimeAndTags() throws Exception {
        // @formatter:off
		String json = ""{"" +
						""\""operation\"" : \""add\"","" +
						""\""sessionId\"" : \""1234\"","" +
					    ""\""metric\"" : \""sys.cpu.user\"","" +
						""\""tags\"" : {"" +
					       ""\""tag2\"" : \""value2\"","" +
					       ""\""tag1\"" : \""value1\"""" +
					    ""},"" +
						""\""startTime\"" : \""1000\"","" +
					    ""\""endTime\"" : \""2000\""""+
					  ""}"";
		// @formatter:on
        WebSocketRequest request = JsonUtil.getObjectMapper().readValue(json.getBytes(), WebSocketRequest.class);
        Assert.assertNotNull(request);
        Assert.assertEquals(AddSubscription.class, request.getClass());
        Assert.assertEquals(""1234"", ((AddSubscription) request).getSessionId());
        Assert.assertEquals(""sys.cpu.user"", ((AddSubscription) request).getMetric());
        Assert.assertEquals(true, ((AddSubscription) request).getTags().isPresent());
        Map<String, String> tags = ((AddSubscription) request).getTags().get();
        Assert.assertTrue(tags.containsKey(""tag1""));
        Assert.assertEquals(""value1"", tags.get(""tag1""));
        Assert.assertTrue(tags.containsKey(""tag2""));
        Assert.assertEquals(""value2"", tags.get(""tag2""));
        Assert.assertEquals(true, ((AddSubscription) request).getStartTime().isPresent());
        long start = ((AddSubscription) request).getStartTime().get();
        Assert.assertEquals(1000L, start);
        Assert.assertEquals(true, ((AddSubscription) request).getEndTime().isPresent());
        long end = ((AddSubscription) request).getEndTime().get();
        Assert.assertEquals(2000L, end);
    }
"
"    @Test
    public void testAddDeserializationWithStartAndDelayTimeAndTags() throws Exception {
        // @formatter:off
		String json = ""{"" +
						""\""operation\"" : \""add\"","" +
						""\""sessionId\"" : \""1234\"","" +
					    ""\""metric\"" : \""sys.cpu.user\"","" +
						""\""tags\"" : {"" +
					       ""\""tag2\"" : \""value2\"","" +
					       ""\""tag1\"" : \""value1\"""" +
					    ""},"" +
						""\""startTime\"" : \""1000\"","" +
					    ""\""delayTime\"" : \""500\"""" +
					  ""}"";
		// @formatter:on
        WebSocketRequest request = JsonUtil.getObjectMapper().readValue(json.getBytes(), WebSocketRequest.class);
        Assert.assertNotNull(request);
        Assert.assertEquals(AddSubscription.class, request.getClass());
        Assert.assertEquals(""1234"", ((AddSubscription) request).getSessionId());
        Assert.assertEquals(""sys.cpu.user"", ((AddSubscription) request).getMetric());
        Assert.assertEquals(true, ((AddSubscription) request).getTags().isPresent());
        Map<String, String> tags = ((AddSubscription) request).getTags().get();
        Assert.assertTrue(tags.containsKey(""tag1""));
        Assert.assertEquals(""value1"", tags.get(""tag1""));
        Assert.assertTrue(tags.containsKey(""tag2""));
        Assert.assertEquals(""value2"", tags.get(""tag2""));
        Assert.assertEquals(true, ((AddSubscription) request).getStartTime().isPresent());
        long time = ((AddSubscription) request).getStartTime().get();
        Assert.assertEquals(1000L, time);
        long delay = ((AddSubscription) request).getDelayTime().get();
        Assert.assertEquals(500L, delay);
    }
"
"    @Test
    public void testToKeys() {
        Meta one = new Meta(""sys.cpu.user"", ""tag1"", ""value1"");
        List<Key> keys = one.toKeys();
        Assert.assertTrue(keys.contains(new Key(""m:sys.cpu.user"")));
        Assert.assertTrue(keys.contains(new Key(""t:sys.cpu.user"", ""tag1"")));
        Assert.assertTrue(keys.contains(new Key(""v:sys.cpu.user"", ""tag1"", ""value1"")));
    }
"
"    @Test
    public void testResponse1() throws Exception {
        SearchLookupResponse response = new SearchLookupResponse();
        response.setType(""LOOKUP"");
        response.setMetric(""sys.cpu.user"");
        response.putTag(""host"", ""localhost"");
        response.putTag(""rack"", ""r1"");
        response.setTime(1500);
        List<Result> results = new ArrayList<>();
        Result r1 = new Result();
        r1.setMetric(""sys.cpu.idle"");
        r1.setTsuid(""000011000008203D00"");
        r1.putTag(""host"", ""localhost"");
        r1.putTag(""rack"", ""r1"");
        Result r2 = new Result();
        r2.setMetric(""sys.cpu.user"");
        r2.setTsuid(""000011000008203D01"");
        r2.putTag(""host"", ""localhost"");
        r2.putTag(""rack"", ""r1"");
        results.add(r1);
        results.add(r2);
        response.setResults(results);
        response.setTotalResults(results.size());
        String r = JsonUtil.getObjectMapper().writeValueAsString(response);
        String expected = ""{\""type\"":\""LOOKUP\"",\""metric\"":\""sys.cpu.user\"",\""tags\"":{\""rack\"":\""r1\"",\""host\"":\""localhost\""},\""limit\"":0,\""time\"":1500,\""totalResults\"":2,\""results\"":[{\""tags\"":{\""rack\"":\""r1\"",\""host\"":\""localhost\""},\""metric\"":\""sys.cpu.idle\"",\""tsuid\"":\""000011000008203D00\""},{\""tags\"":{\""rack\"":\""r1\"",\""host\"":\""localhost\""},\""metric\"":\""sys.cpu.user\"",\""tsuid\"":\""000011000008203D01\""}]}"";
        Assert.assertEquals(expected, r);
        SearchLookupResponse slr = JsonUtil.getObjectMapper().readValue(r, SearchLookupResponse.class);
        Assert.assertEquals(response, slr);
    }
"
"    @Test
    public void testGenerateHtml() throws Exception {
        Configuration cfg = TestConfiguration.createMinimalConfigurationForTest();
        MetaCache cache = MetaCacheFactory.getCache(cfg);
        cache.add(new Meta(""sys.cpu.user"", ""host"", ""localhost""));
        cache.add(new Meta(""sys.cpu.user"", ""instance"", ""0""));
        cache.add(new Meta(""sys.cpu.idle"", ""host"", ""localhost""));
        cache.add(new Meta(""sys.cpu.idle"", ""instance"", ""0""));
        TestMetricsResponse r = new TestMetricsResponse(cfg);
        String html = r.generateHtml().toString();
        Assert.assertTrue(html.contains(""<td>sys.cpu.idle</td>""));
        Assert.assertTrue(html.contains(""<td>host=localhost instance=0 </td>""));
        Assert.assertTrue(html.contains(""<td>sys.cpu.user</td>""));
        Assert.assertTrue(html.contains(""<td>host=localhost instance=0 </td>""));
    }
"
"    @Test
    public void testGenerateHtmlWithIgnoredTags() throws Exception {
        Configuration cfg = TestConfiguration.createMinimalConfigurationForTest();
        cfg.getMetricsReportIgnoredTags().add(""instance"");
        MetaCache cache = MetaCacheFactory.getCache(cfg);
        cache.add(new Meta(""sys.cpu.user"", ""host"", ""localhost""));
        cache.add(new Meta(""sys.cpu.user"", ""instance"", ""0""));
        cache.add(new Meta(""sys.cpu.idle"", ""host"", ""localhost""));
        cache.add(new Meta(""sys.cpu.idle"", ""instance"", ""0""));
        TestMetricsResponse r = new TestMetricsResponse(cfg);
        String html = r.generateHtml().toString();
        Assert.assertTrue(html.contains(""<td>sys.cpu.idle</td>""));
        Assert.assertTrue(html.contains(""<td>host=localhost </td>""));
        Assert.assertTrue(html.contains(""<td>sys.cpu.user</td>""));
        Assert.assertTrue(html.contains(""<td>host=localhost </td>""));
    }
"
"    @Test
    public void testSuggestResponseEmpty() throws Exception {
        SuggestResponse response = new SuggestResponse();
        String r = JsonUtil.getObjectMapper().writeValueAsString(response);
        Assert.assertEquals(""[]"", r);
    }
"
"    @Test
    public void testSuggestResponse() throws Exception {
        SuggestResponse response = new SuggestResponse();
        response.addSuggestion(""sys.cpu.idle"");
        response.addSuggestion(""sys.cpu.user"");
        String r = JsonUtil.getObjectMapper().writeValueAsString(response);
        Assert.assertEquals(""[\""sys.cpu.idle\"",\""sys.cpu.user\""]"", r);
    }
"
"    @Test
    public void testAggregatorsResponseEmpty() throws Exception {
        AggregatorsResponse response = new AggregatorsResponse();
        String r = JsonUtil.getObjectMapper().writeValueAsString(response);
        Assert.assertEquals(""[]"", r);
    }
"
"    @Test
    public void testAggregatorsResponse() throws Exception {
        AggregatorsResponse response = new AggregatorsResponse();
        response.addAggregator(""min"");
        response.addAggregator(""max"");
        String r = JsonUtil.getObjectMapper().writeValueAsString(response);
        Assert.assertEquals(""[\""min\"",\""max\""]"", r);
    }
"
"    @Test
    public void testEmptyResponse() throws Exception {
        String r = JsonUtil.getObjectMapper().writeValueAsString(Collections.emptyList());
        Assert.assertEquals(""[]"", r);
    }
"
"    @Test
    public void testOneResponse() throws Exception {
        QueryResponse r = new QueryResponse();
        r.setMetric(""sys.cpu.user"");
        r.putTag(""host"", ""localhost"");
        r.putTag(""rack"", ""r1"");
        r.putDps(""1234567890"", 4.5);
        r.putDps(""1234567900"", 3.5);
        r.putDps(""1234567910"", 2.5);
        String result = JsonUtil.getObjectMapper().writeValueAsString(Collections.singletonList(r));
        String expected = ""[{\""metric\"":\""sys.cpu.user\"",\""tags\"":{\""rack\"":\""r1\"",\""host\"":\""localhost\""},\""aggregatedTags\"":[],\""dps\"":{\""1234567890\"":4.5,\""1234567900\"":3.5,\""1234567910\"":2.5}}]"";
        Assert.assertEquals(expected, result);
    }
"
"    @Test
    public void testNumberFormat() {
        String m = ""sys.cpu.user"";
        long time = System.currentTimeMillis();
        double value = ThreadLocalRandom.current().nextDouble(0.0D, 100.0D);
        String put = MessageFormat.format(FMT, m, time, value, ""host=localhost"", ""rack=r1"");
        NumberFormat formattedDouble = DecimalFormat.getInstance();
        formattedDouble.setMaximumFractionDigits(3);
        String newValue = formattedDouble.format(value);
        Assert.assertEquals(""put sys.cpu.user "" + time + "" "" + newValue + "" host=localhost rack=r1"", put);
    }
"
"    @Test
    public void testContents() {
        Meta one = new Meta(""sys.cpu.user"", ""tag1"", ""value1"");
        Meta two = new Meta(""sys.cpu.user"", ""tag2"", ""value2"");
        Meta three = new Meta(""sys.cpu.user"", ""tag3"", ""value3"");
        MetaKeySet mks = new MetaKeySet();
        mks.addAll(one.toKeys());
        mks.addAll(two.toKeys());
        mks.addAll(three.toKeys());
        Assert.assertEquals(7, mks.size());
        Assert.assertTrue(mks.contains(new Key(""m:sys.cpu.user"")));
        Assert.assertTrue(mks.contains(new Key(""t:sys.cpu.user"", ""tag1"")));
        Assert.assertTrue(mks.contains(new Key(""t:sys.cpu.user"", ""tag2"")));
        Assert.assertTrue(mks.contains(new Key(""t:sys.cpu.user"", ""tag3"")));
        Assert.assertTrue(mks.contains(new Key(""v:sys.cpu.user"", ""tag1"", ""value1"")));
        Assert.assertTrue(mks.contains(new Key(""v:sys.cpu.user"", ""tag2"", ""value2"")));
        Assert.assertTrue(mks.contains(new Key(""v:sys.cpu.user"", ""tag3"", ""value3"")));
    }
"
"    @Test
    public void testToMutations() {
        Meta one = new Meta(""sys.cpu.user"", ""tag1"", ""value1"");
        Meta two = new Meta(""sys.cpu.user"", ""tag2"", ""value2"");
        Meta three = new Meta(""sys.cpu.user"", ""tag3"", ""value3"");
        MetaKeySet mks = new MetaKeySet();
        mks.addAll(one.toKeys());
        mks.addAll(two.toKeys());
        mks.addAll(three.toKeys());
        List<Mutation> muts = mks.toMutations();
        Mutation e1 = new Mutation(""m:sys.cpu.user"");
        e1.put("""", """", MetaKeySet.NULL_VALUE);
        Mutation e2 = new Mutation(""t:sys.cpu.user"");
        e2.put(""tag1"", """", MetaKeySet.NULL_VALUE);
        e2.put(""tag2"", """", MetaKeySet.NULL_VALUE);
        e2.put(""tag3"", """", MetaKeySet.NULL_VALUE);
        Mutation e3 = new Mutation(""v:sys.cpu.user"");
        e3.put(""tag1"", ""value1"", MetaKeySet.NULL_VALUE);
        e3.put(""tag2"", ""value2"", MetaKeySet.NULL_VALUE);
        e3.put(""tag3"", ""value3"", MetaKeySet.NULL_VALUE);
        Assert.assertEquals(3, muts.size());
        Assert.assertTrue(muts.contains(e1));
        Assert.assertTrue(muts.contains(e2));
        Assert.assertTrue(muts.contains(e3));
    }
"
"    @Test
    public void simple() {
        Aggregation asample = new Aggregation(new Avg());
        for (int i = 10; i < 30; i++) {
            asample.add(i, i - 10);
        }
        for (int i = 10; i < 30; i++) {
            asample.add(i, i);
        }
        int i = 0;
        for (Sample sample : asample) {
            assertEquals(10 + i, sample.timestamp);
            assertTrue(sample.timestamp < 30);
            assertEquals(i + 5, (int) sample.value);
            i++;
        }
        assertEquals(20, i);
        asample = new Aggregation(new Sum());
        for (int j = 0; j < 5; j++) {
            for (int k = 10; k < 100; k++) {
                asample.add(k, j + 0.);
            }
        }
        i = 0;
        for (Sample sample : asample) {
            assertEquals(10 + i, sample.timestamp);
            assertEquals((1 + 2 + 3 + 4), sample.value, 0.0D);
            i++;
        }
        assertEquals(100 - 10, i);
    }
"
"    @Test
    public void simple() {
        Downsample dsample = new Downsample(10, 30, 1, new Avg());
        for (int i = 10; i < 30; i++) {
            dsample.add(i, i - 10);
        }
        int i = 0;
        for (Sample sample : dsample) {
            assertEquals(10 + i, sample.timestamp);
            assertTrue(sample.timestamp < 30);
            assertEquals(i, (int) sample.value);
            i++;
        }
        assertEquals(20, i);
        dsample = new Downsample(10, 100, 7, new Sum());
        for (int j = 0; j < 5; j++) {
            for (int k = 10; k < 100; k++) {
                dsample.add(k, j + 0.);
            }
        }
        i = 0;
        for (Sample sample : dsample) {
            assertEquals((1 + 2 + 3 + 4) * Math.min(7, (100 - (10 + i * 7))), sample.value, 0.0D);
            assertEquals(10 + i * 7, sample.timestamp);
            i++;
        }
        assertEquals((100 - 10) / 7 + 1, i);
        dsample = new Downsample(10, 30, 10, new Avg());
        for (int j = 10; j < 30; j++) {
            for (int k = 0; k < 10; k++) {
                dsample.add(j, k + 0.);
            }
        }
        for (int j = 0; j < 100; j++) {
            dsample.add(15, 0);
        }
        i = 0;
        for (Sample sample : dsample) {
            if (i == 0) {
                assertEquals(2.25, sample.value, 0.0D);
            } else {
                assertEquals(4.5, sample.value, 0.0D);
            }
            assertEquals(10 * i + 10, sample.timestamp);
            i++;
        }
        assertEquals(2, i);
    }
"
"    @Test
    public void testCombineTrivial() throws Exception {
        Downsample ds = new Downsample(0, 1000, 100, new Avg());
        for (int i = 0; i < 1000; i += 100) {
            ds.add(i, .2);
        }
        Downsample result = Downsample.combineDownsample(Collections.singleton(ds), null);
        int count = 0;
        for (Sample s : result) {
            assertEquals(.2, s.value, 0.0D);
            count++;
        }
        assertEquals(10, count);
    }
"
"    @Test
    public void testCombineMissingReport() throws Exception {
        Downsample ds = new Downsample(0, 1000, 100, new Avg());
        for (int i = 0; i < 1000; i += 100) {
            if (i != 700) {
                ds.add(i, .2);
            }
        }
        Downsample result = Downsample.combineDownsample(Collections.singleton(ds), null);
        int count = 0;
        for (Sample s : result) {
            assertEquals(.2, s.value, 0.0D);
            count++;
        }
        assertEquals(9, count);
    }
"
"    @Test
    public void testDownsampleStartCalculation() throws Exception {
        long queryStart = System.currentTimeMillis() - 86400000;
        long period = 60000;
        long keyTimestamp = queryStart + (86400000 / 2 + 3256);

        Set<Long> expectedStartTimes = new HashSet<>();
        for (long i = queryStart; i < queryStart + 86400000; i += period) {
            expectedStartTimes.add(i);
        }
        assertEquals(1440, expectedStartTimes.size());

        long sampleStart = keyTimestamp - ((keyTimestamp - queryStart) % period);
        assertTrue(expectedStartTimes.contains(sampleStart));

    }
"
"    @Test
    public void simpleGetOneSample() throws Exception {
        // check that data gets pulled out
        AggregationIterator iter = new AggregationIterator();
        Map<Set<Tag>, Aggregation> samples = runQuery(iter, testData1, 100);
        assertEquals(1, samples.size());
        for (Entry<Set<Tag>, Aggregation> entry : samples.entrySet()) {
            Set<Tag> tags = entry.getKey();
            assertEquals(1, tags.size());
            assertEquals(Collections.singleton(new Tag(""host"", "".*"")), tags);
            long ts = 0;
            int count = 0;
            for (Sample sample : entry.getValue()) {
                assertEquals(ts, sample.timestamp);
                ts += 100;
                assertEquals(0.2, sample.value, 0.0001);
                count++;
            }
            assertEquals(1000, ts);
            assertEquals(10, count);
        }
    }
"
"    @Test
    public void simpleAggregatedSample() throws Exception {
        AggregationIterator iter = new AggregationIterator();
        Map<Set<Tag>, Aggregation> samples = runQuery(iter, testData2, 100);
        assertEquals(1, samples.size());
        for (Entry<Set<Tag>, Aggregation> entry : samples.entrySet()) {
            Set<Tag> tags = entry.getKey();
            assertEquals(1, tags.size());
            assertEquals(Collections.singleton(new Tag(""host"", "".*"")), tags);
            long ts = 0;
            int count = 0;
            for (Sample sample : entry.getValue()) {
                assertEquals(ts, sample.timestamp);
                ts += 100;
                assertEquals(count == 0 ? 0.2 : (count == 10 ? 0.5 : 0.35), sample.value, 0.0001);
                count++;
            }
            assertEquals(11, count);
        }
    }
"
"    @Test
    public void simpleGetOneSample() throws Exception {
        // check that data gets pulled out
        DownsampleIterator iter = new DownsampleIterator();
        Map<Set<Tag>, Downsample> samples = runQuery(iter, testData1, 100, -1);
        assertEquals(1, samples.size());
        for (Entry<Set<Tag>, Downsample> entry : samples.entrySet()) {
            Set<Tag> tags = entry.getKey();
            assertEquals(1, tags.size());
            assertEquals(Collections.singleton(new Tag(""host"", ""host1"")), tags);
            long ts = 0;
            for (Sample sample : entry.getValue()) {
                assertEquals(ts, sample.timestamp);
                ts += 100;
                assertEquals(0.2, sample.value, 0.0001);
            }
            assertEquals(1000, ts);
        }
    }
"
"    @Test
    public void simpleGetTwoSamples() throws Exception {
        DownsampleIterator iter = new DownsampleIterator();
        Map<Set<Tag>, Downsample> samples = runQuery(iter, testData2, 100, -1);
        assertEquals(2, samples.size());
        for (Tag tag : new Tag[] { new Tag(""host"", ""host1""), new Tag(""host"", ""host2"") }) {
            Downsample dsample = samples.get(Collections.singleton(tag));
            assertNotNull(dsample);
            long ts = 0;
            double value = .2;
            if (tag.getValue().equals(""host2"")) {
                value = .5;
            }
            int count = 0;
            for (Sample sample : dsample) {
                assertEquals(ts, sample.timestamp);
                ts += 100;
                assertEquals(value, sample.value, 0.0001);
                count++;
            }
            assertEquals(10, count);
        }
    }
"
"    @Test
    public void simpleTestDownsampling() throws Exception {
        DownsampleIterator iter = new DownsampleIterator();
        Map<Set<Tag>, Downsample> samples = runQuery(iter, testData2, 200, -1);
        assertEquals(2, samples.size());
        for (Tag tag : new Tag[] { new Tag(""host"", ""host1""), new Tag(""host"", ""host2"") }) {
            Downsample dsample = samples.get(Collections.singleton(tag));
            assertNotNull(dsample);
            long ts = 0;
            double value = .2;
            if (tag.getValue().equals(""host2"")) {
                value = .5;
            }
            int count = 0;
            for (Sample sample : dsample) {
                assertEquals(ts, sample.timestamp);
                ts += 200;
                assertEquals(value, sample.value, 0.0001);
                count++;
            }
            assertEquals(5, count);
        }
    }
"
"    @Test
    public void memoryEstimatorTestSmallObjects() {
        long maxMemory = 1000;
        long start = System.currentTimeMillis();
        long period = 500l;
        long sizeOfObjects = 20;
        SampleObject o = new SampleObject();
        DownsampleMemoryEstimator memoryEstimator = new DownsampleMemoryEstimator(maxMemory, start, period);
        boolean shouldReturn = false;
        for (long x = 100; x <= 5000; x += 100) {
            long timestamp = start + x;
            o.setSizeInBytes(o.sizeInBytes() + sizeOfObjects);
            shouldReturn = memoryEstimator.shouldReturnBasedOnMemoryUsage(timestamp, o);
            if (memoryEstimator.isNewBucket()) {
                long memoryPercentageUsedCalculated = Math.round((double) o.sizeInBytes() / maxMemory * 100);
                long memoryPercentageUsedEstimate = Math.round(memoryEstimator.getMemoryUsedPercentage());
                long percentError = Math.round(Math.abs(memoryPercentageUsedCalculated - memoryPercentageUsedEstimate)
                        / memoryPercentageUsedCalculated * 100);
                assertTrue(percentError == 0);
            }

            if (shouldReturn) {
                o.setSizeInBytes(0);
                memoryEstimator.reset();
            }
        }
        assertTrue(shouldReturn);
    }
"
"    @Test
    public void memoryEstimatorTestLargeObjects() {
        long maxMemory = 10000;
        long start = System.currentTimeMillis();
        long period = 500l;
        long sizeOfObjects = 200;
        SampleObject o = new SampleObject();
        DownsampleMemoryEstimator memoryEstimator = new DownsampleMemoryEstimator(maxMemory, start, period);
        boolean shouldReturn = false;
        for (long x = 100; x <= 5000; x += 100) {
            long timestamp = start + x;
            o.setSizeInBytes(o.sizeInBytes() + sizeOfObjects);
            shouldReturn = memoryEstimator.shouldReturnBasedOnMemoryUsage(timestamp, o);
            if (memoryEstimator.isNewBucket()) {
                long memoryPercentageUsedCalculated = Math.round((double) o.sizeInBytes() / maxMemory * 100);
                long memoryPercentageUsedEstimate = Math.round(memoryEstimator.getMemoryUsedPercentage());
                long percentError = Math.round(Math.abs(memoryPercentageUsedCalculated - memoryPercentageUsedEstimate)
                        / memoryPercentageUsedCalculated * 100);
                assertTrue(percentError == 0);
                assertTrue(memoryEstimator.isHighVolumeBuckets());
            }

            if (shouldReturn) {
                o.setSizeInBytes(0);
                memoryEstimator.reset();
            }
        }
        assertTrue(shouldReturn);
    }
"
"    @Test
    public void testDownsampleCombining() throws Exception {

        int numTagVariations = 2;
        int sampleInterval = 50;
        int elapsedTime = 100;
        int skipInterval = 10;
        SortedMap<Key, Value> testData3 = createTestData3(elapsedTime, skipInterval, numTagVariations);
        DownsampleIterator iter = new DownsampleIterator();
        Map<Set<Tag>, Downsample> samples = runQuery(iter, testData3, sampleInterval, 1000);
        assertEquals(numTagVariations, samples.size());
        long totalBuckets = 0;
        for (Entry<Set<Tag>, Downsample> entry : samples.entrySet()) {
            totalBuckets = totalBuckets + entry.getValue().getNumBuckets();
        }
        assertEquals((elapsedTime / sampleInterval) * numTagVariations, totalBuckets);
    }
"
"    @Test
    public void testVersion() throws Exception {
        final TestServer m = new TestServer(conf);
        m.run();
        try (Socket sock = new Socket(""127.0.0.1"", 54321);
                PrintWriter writer = new PrintWriter(sock.getOutputStream(), true);) {
            writer.write(""version\n"");
            writer.flush();
            while (1 != m.getTcpRequests().getCount()) {
                Thread.sleep(5);
            }
            Assert.assertEquals(1, m.getTcpRequests().getResponses().size());
            Assert.assertEquals(VersionRequest.class, m.getTcpRequests().getResponses().get(0).getClass());
            VersionRequest v = (VersionRequest) m.getTcpRequests().getResponses().get(0);
            Assert.assertEquals(VersionRequest.VERSION, v.getVersion());
        } finally {
            m.shutdown();
        }
    }
"
"    @Test
    public void testPut() throws Exception {
        final TestServer m = new TestServer(conf);
        m.run();
        try (Socket sock = new Socket(""127.0.0.1"", 54321);
                PrintWriter writer = new PrintWriter(sock.getOutputStream(), true);) {
            writer.write(""put sys.cpu.user "" + TEST_TIME + "" 1.0 tag1=value1 tag2=value2\n"");
            writer.flush();
            while (1 != m.getTcpRequests().getCount()) {
                Thread.sleep(5);
            }
            Assert.assertEquals(1, m.getTcpRequests().getResponses().size());
            Assert.assertEquals(MetricRequest.class, m.getTcpRequests().getResponses().get(0).getClass());
            final MetricRequest actual = (MetricRequest) m.getTcpRequests().getResponses().get(0);
            // @formatter:off
            final MetricRequest expected = new MetricRequest(
                    Metric.newBuilder()
                            .name(""sys.cpu.user"")
                            .value(TEST_TIME, 1.0D)
                            .tag(new Tag(""tag1"", ""value1""))
                            .tag(new Tag(""tag2"", ""value2""))
                            .build()
            );
            // @formatter on
            Assert.assertEquals(expected, actual);
        } finally {
            m.shutdown();
        }
    }
"
"    @Test
    public void testPutMultiple() throws Exception {

        final TestServer m = new TestServer(conf);
        m.run();
        try (Socket sock = new Socket(""127.0.0.1"", 54321);
                PrintWriter writer = new PrintWriter(sock.getOutputStream(), true)) {
            // @formatter:off
            writer.write(""put sys.cpu.user "" + TEST_TIME + "" 1.0 tag1=value1 tag2=value2\n""
                       + ""put sys.cpu.idle "" + (TEST_TIME + 1) + "" 1.0 tag3=value3 tag4=value4\n"");
            writer.flush();
            while (2 != m.getTcpRequests().getCount()) {
                Thread.sleep(5);
            }
            Assert.assertEquals(2, m.getTcpRequests().getResponses().size());
            Assert.assertEquals(MetricRequest.class, m.getTcpRequests().getResponses().get(0).getClass());
            MetricRequest actual = (MetricRequest) m.getTcpRequests().getResponses().get(0);
            MetricRequest expected = new MetricRequest(
                    Metric.newBuilder()
                            .name(""sys.cpu.user"")
                            .value(TEST_TIME, 1.0D)
                            .tag(new Tag(""tag1"", ""value1""))
                            .tag(new Tag(""tag2"", ""value2""))
                            .build()
            );
            Assert.assertEquals(expected, actual);

            Assert.assertEquals(MetricRequest.class, m.getTcpRequests().getResponses().get(1).getClass());
            actual = (MetricRequest) m.getTcpRequests().getResponses().get(1);
            expected = new MetricRequest(
                    Metric.newBuilder()
                        .name(""sys.cpu.idle"")
                        .value(TEST_TIME + 1, 1.0D)
                        .tag(new Tag(""tag3"", ""value3""))
                        .tag(new Tag(""tag4"", ""value4""))
                        .build()
            );
            // @formatter:on
            Assert.assertEquals(expected, actual);

        } finally {
            m.shutdown();
        }
    }
"
"    @Test
    public void testPutMultipleBinary() throws Exception {

        FlatBufferBuilder builder = new FlatBufferBuilder(1);

        int[] metric = new int[2];
        Map<String, String> t = new HashMap<>();
        t.put(""tag1"", ""value1"");
        t.put(""tag2"", ""value2"");
        metric[0] = createMetric(builder, ""sys.cpu.user"", TEST_TIME, 1.0D, t);
        t = new HashMap<>();
        t.put(""tag3"", ""value3"");
        t.put(""tag4"", ""value4"");
        metric[1] = createMetric(builder, ""sys.cpu.idle"", TEST_TIME + 1, 1.0D, t);

        int metricVector = timely.api.flatbuffer.Metrics.createMetricsVector(builder, metric);

        timely.api.flatbuffer.Metrics.startMetrics(builder);
        timely.api.flatbuffer.Metrics.addMetrics(builder, metricVector);
        int metrics = timely.api.flatbuffer.Metrics.endMetrics(builder);
        timely.api.flatbuffer.Metrics.finishMetricsBuffer(builder, metrics);

        ByteBuffer binary = builder.dataBuffer();
        byte[] data = new byte[binary.remaining()];
        binary.get(data, 0, binary.remaining());
        LOG.debug(""Sending {} bytes"", data.length);

        final TestServer m = new TestServer(conf);
        m.run();
        try (Socket sock = new Socket(""127.0.0.1"", 54321);) {
            sock.getOutputStream().write(data);
            sock.getOutputStream().flush();
            while (2 != m.getTcpRequests().getCount()) {
                LOG.debug(""Thread sleeping"");
                Thread.sleep(5);
            }
            Assert.assertEquals(2, m.getTcpRequests().getResponses().size());
            Assert.assertEquals(MetricRequest.class, m.getTcpRequests().getResponses().get(0).getClass());
            // @formatter:off
            MetricRequest actual = (MetricRequest) m.getTcpRequests().getResponses().get(0);
            MetricRequest expected = new MetricRequest(
                    Metric.newBuilder()
                            .name(""sys.cpu.user"")
                            .value(TEST_TIME, 1.0D)
                            .tag(new Tag(""tag1"", ""value1""))
                            .tag(new Tag(""tag2"", ""value2""))
                            .build()
            );
            Assert.assertEquals(expected, actual);

            Assert.assertEquals(MetricRequest.class, m.getTcpRequests().getResponses().get(1).getClass());
            actual = (MetricRequest) m.getTcpRequests().getResponses().get(1);
            expected = new MetricRequest(
                    Metric.newBuilder()
                            .name(""sys.cpu.idle"")
                            .value(TEST_TIME + 1, 1.0D)
                            .tag(new Tag(""tag3"", ""value3""))
                            .tag(new Tag(""tag4"", ""value4""))
                            .build()
            );
            // @formatter:on
            Assert.assertEquals(expected, actual);

        } finally {
            m.shutdown();
        }
    }
"
"    @Test
    public void testPutInvalidTimestamp() throws Exception {
        final TestServer m = new TestServer(conf);
        m.run();
        try (Socket sock = new Socket(""127.0.0.1"", 54321);
                PrintWriter writer = new PrintWriter(sock.getOutputStream(), true);
                BufferedReader reader = new BufferedReader(new InputStreamReader(sock.getInputStream()));) {
            writer.write(""put sys.cpu.user "" + TEST_TIME + ""Z"" + "" 1.0 tag1=value1 tag2=value2\n"");
            writer.flush();
            sleepUninterruptibly(WAIT_SECONDS, TimeUnit.SECONDS);
            Assert.assertEquals(0, m.getTcpRequests().getCount());
        } finally {
            m.shutdown();
        }
    }
"
"    @Test
    public void testPersistence() throws Exception {
        final Server s = new Server(conf);
        s.run();
        try {
            put(""sys.cpu.user "" + TEST_TIME + "" 1.0 tag1=value1 tag2=value2"",
                    ""sys.cpu.idle "" + (TEST_TIME + 1) + "" 1.0 tag3=value3 tag4=value4"",
                    ""sys.cpu.idle "" + (TEST_TIME + 2) + "" 1.0 tag3=value3 tag4=value4"");
            sleepUninterruptibly(WAIT_SECONDS, TimeUnit.SECONDS);
        } finally {
            s.shutdown();
        }
        final ZooKeeperInstance inst = new ZooKeeperInstance(mac.getClientConfig());
        final Connector connector = inst.getConnector(""root"", new PasswordToken(""secret"".getBytes(UTF_8)));
        assertTrue(connector.namespaceOperations().exists(""timely""));
        assertTrue(connector.tableOperations().exists(""timely.metrics""));
        assertTrue(connector.tableOperations().exists(""timely.meta""));
        int count = 0;
        for (final Entry<Key, Value> entry : connector.createScanner(""timely.metrics"", Authorizations.EMPTY)) {
            LOG.info(""Entry: "" + entry);
            final double value = ByteBuffer.wrap(entry.getValue().get()).getDouble();
            assertEquals(1.0, value, 1e-9);
            count++;
        }
        assertEquals(6, count);
        count = 0;
        for (final Entry<Key, Value> entry : connector.createScanner(""timely.meta"", Authorizations.EMPTY)) {
            LOG.info(""Meta entry: "" + entry);
            count++;
        }
        assertEquals(10, count);
        // count w/out versioning iterator to make sure that the optimization
        // for writing is working
        connector.tableOperations().removeIterator(""timely.meta"", ""vers"", EnumSet.of(IteratorScope.scan));
        // wait for zookeeper propagation
        sleepUninterruptibly(WAIT_SECONDS, TimeUnit.SECONDS);
        count = 0;
        for (final Entry<Key, Value> entry : connector.createScanner(""timely.meta"", Authorizations.EMPTY)) {
            LOG.info(""Meta no vers iter: "" + entry);
            count++;
        }
        assertEquals(10, count);
    }
"
"    @Test
    public void testWriteTo() {
        final String user = target(""user"").request().accept(""application/json"").get(String.class);
        // {""createdOn"":1412036891919,""id"":12345,""name"":""smallnest""}]
        assertTrue(user.indexOf(""createdOn"") > 0);
        assertTrue(user.indexOf(""\""id\"":12345"") > 0);
        assertTrue(user.indexOf(""\""name\"":\""smallnest\"""") > 0);
    }
"
"        //System.out.println(""@@@@@Test Pretty"");
        final String user = target(""user"").queryParam(""pretty"", ""true"").request().accept(""application/json"").get(String.class);
        // {""createdOn"":1412036891919,""id"":12345,""name"":""smallnest""}]
        assertTrue(user.indexOf(""createdOn"") > 0);
        assertTrue(user.indexOf(""\""id\"":12345"") > 0);
        assertTrue(user.indexOf(""\""name\"":\""smallnest\"""") > 0);
        //response does not contain a return character
        //assertTrue(user.indexOf(""\n\t"") > 0);

    }
"
"    @Test
    public void testReadFrom() {
        final User user = target(""user"").request().accept(""application/json"").get(User.class);
        assertNotNull(user);
        assertNotNull(user.getCreatedOn());
        assertEquals(user.getId().longValue(), 12345L);
        assertEquals(user.getName(), ""smallnest"");
    }
"
"    @Test
    public void testWriteLiteBasicStr() throws UnsupportedEncodingException {
        String targetStr = new String(IOUtils.DIGITS);
        this.doTestWrite(targetStr);
    }
"
"    @Test
    public void testWriteLiteSpecilaStr() throws UnsupportedEncodingException {
        this.doTestWrite(this.makeSpecialChars());
    }
"
"    @Test
    public void testWriteLargeBasicStr() throws UnsupportedEncodingException {
        String tmp = new String(IOUtils.DIGITS);
        StringBuilder builder = new StringBuilder();
        for (int i = 0; i < 200; i++) {
            builder.append(tmp);
        }
        this.doTestWrite(builder.toString());
    }
"
"    @Test
    public void testWriteLargeSpecialStr() throws UnsupportedEncodingException {

        String tmp = this.makeSpecialChars();
        StringBuilder builder = new StringBuilder();
        for (int i = 0; i < 200; i++) {
            builder.append(tmp);
        }
        this.doTestWrite(builder.toString());
    }
"
"    @Test
    public void test_large() throws Exception {
        SerializeWriter writer = new SerializeWriter();

        for (int i = 0; i < 1024 * 1024; ++i) {
            writer.write(i);
        }

        writer.close();
    }
"
"    @Test
    public void testParse() {
        logger.info(""parsing json string:"" + jsonString);
        TestBean testBean = (TestBean) JSON.parse(jsonString);
        assert testBean.getData() != null;
        assert ""tester"".equals(testBean.getName());
        assert ""value"".equals(testBean.getData().getString(""key""));
    }
"
"    @Test
    public void testBug569() {
        //ç¬¬ä¸æ¬¡ååºååæ¯ä½¿ç¨ç MyResponseï¼ æ²¡ææå®æ³åç±»åï¼è²ä¼¼ä¼ç¼å­ MyResponseï¼ åé¢å¨è°ç¨çMyResponse<?>ååºååå°±åå½±åäº
        MyResponse resp1 = JSON.parseObject(jsonData, mType1, configBug569, featureValues,
                features != null ? features : EMPTY_SERIALIZER_FEATURES);

        //expect MyResponse<JSONArray<JSONObject>>
        MyResponse resp = JSON.parseObject(jsonData, mType, configBug569, featureValues,
                features != null ? features : EMPTY_SERIALIZER_FEATURES);
        Assert.assertNotNull(resp);
        Assert.assertNotNull(resp.getResult());
        Assert.assertEquals(JSONArray.class, resp.getResult().getClass());//è¿éä¼åå° resp1 çå½±å
    }
"
"    @Test
    public void testFixBug569() {
        MyResponse resp1 = JSON.parseObject(jsonData, mType1, config, featureValues,
                features != null ? features : EMPTY_SERIALIZER_FEATURES);

        //expect MyResponse<List<Dept>>
        MyResponse resp = JSON.parseObject(jsonData, mType, config, featureValues,
                features != null ? features : EMPTY_SERIALIZER_FEATURES);
        Assert.assertNotNull(resp);
        Assert.assertNotNull(resp.getResult());
        Assert.assertEquals(ArrayList.class, resp.getResult().getClass());
    }
"
"    @Test
    public void testBug1884() {
        Calendar cale = Calendar.getInstance();
        cale.clear();
        cale.setTimeZone( TimeZone.getTimeZone( ""GMT+7"" ) );
        cale.set( 2018, Calendar.MAY, 31, 19, 13, 42 );
        Date date = cale.getTime();

        String s1 = ""{date: \""2018-05-31T19:13:42+07:00\""}""; // éè¯¯ç
        String s2 = ""{date: \""2018-05-31T19:13:42.000+07:00\""}""; // æ­£ç¡®ç
        Date date1 = JSON.parseObject( s1, JSONObject.class ).getDate( ""date"" );
        Date date2 = JSON.parseObject( s2, JSONObject.class ).getDate( ""date"" );
        assertEquals(date1, date2);
        assertEquals(date, date1);
        assertEquals(date, date2);
    }
"
"    @Test
    public void testBug376() {
        Calendar cale = Calendar.getInstance();
        cale.clear();
        cale.setTimeZone( TimeZone.getTimeZone( ""GMT"" ) );
        cale.set( 2018, Calendar.MAY, 31, 19, 13, 42 );
        Date date = cale.getTime();

        String s1 = ""{date: \""2018-05-31T19:13:42Z\""}"";
        String s2 = ""{date: \""2018-05-31T19:13:42.000Z\""}"";

        Date date1 = JSON.parseObject( s1, JSONObject.class ).getDate( ""date"" );
        Date date2 = JSON.parseObject( s2, JSONObject.class ).getDate( ""date"" );

        assertEquals( date1, date2 );
        assertEquals( date, date1 );
        assertEquals( date, date2 );
    }
"
"    @Test(expected = JSONException.class)
    public void parseObjectWithNotExistTypeThrowException() {
        String s = ""{\""@type\"":\""com.alibaba.fastjson.ValueBean\"",\""val\"":1}"";
        JSONObject.parseObject(s, ValueBean.class);
    }
"
"    @Test
    public void parseObjectWithNotExistType() {
        String s = ""{\""@type\"":\""com.alibaba.fastjson.ValueBean\"",\""val\"":1}"";
        ValueBean v = JSONObject.parseObject(s, ValueBean.class, Feature.IgnoreAutoType);
        Assert.assertNotNull(v);
        Assert.assertEquals(new Integer(1), v.getVal());
    }
"
"    @Test
    public void parseWithNotExistType() {
        String s = ""{\""@type\"":\""com.alibaba.fastjson.ValueBean\"",\""val\"":1}"";
        Object object = JSONObject.parse(s);
        Assert.assertNotNull(object);
        Assert.assertTrue(object instanceof JSONObject);
        Assert.assertEquals(new Integer(1), JSONObject.class.cast(object).getInteger(""val""));
    }
"
"    @Test
    public void parseWithExistType() {
        String s = ""{\""@type\"":\""com.alibaba.fastjson.deserializer.ValueBean\"",\""val\"":1}"";
        Object object = JSONObject.parse(s);
        Assert.assertNotNull(object);
        Assert.assertTrue(object instanceof ValueBean);
        Assert.assertEquals(new Integer(1), ValueBean.class.cast(object).getVal());
    }
"
"    @Test
    public void parseObjectWithExistType() {
        String s = ""{\""@type\"":\""com.alibaba.fastjson.deserializer.ValueBean\"",\""val\"":1}"";
        ValueBean object = JSONObject.parseObject(s, ValueBean.class);
        Assert.assertNotNull(object);
        Assert.assertEquals(new Integer(1), object.getVal());
    }
"
"    @Test
    public void testIssue1463() {
        String str = doubleDeserialization(wenshao);
        try {
            wenshao = JSON.parseObject(str, Person.class);
        } catch (Throwable ex) {
            Assert.assertEquals(ex.getCause() instanceof NullPointerException, false);
        }
    }
"
"  @Test
  public void charArrayCompare1() throws Throwable {

    // Arrange
    String src = """";
    int offset = 7;
    char[] dest = { '\u0000' };

    // Act
    Class<?> c = Reflector.forName(""com.alibaba.fastjson.parser.JSONScanner"");
    Method m = c.getDeclaredMethod(""charArrayCompare"", Reflector.forName(""java.lang.String""), Reflector.forName(""int""), Reflector.forName(""char []""));
    m.setAccessible(true);
    boolean retval = (Boolean)m.invoke(null, src, offset, dest);

    // Assert result
    Assert.assertEquals(false, retval);

  }
"
"  @Test
  public void charArrayCompare3() throws Throwable {

    // Arrange
    String src = ""!!!!!!!\""&&"";
    int offset = 6;
    char[] dest = { '\u0000' };

    // Act
    Class<?> c = Reflector.forName(""com.alibaba.fastjson.parser.JSONScanner"");
    Method m = c.getDeclaredMethod(""charArrayCompare"", Reflector.forName(""java.lang.String""), Reflector.forName(""int""), Reflector.forName(""char []""));
    m.setAccessible(true);
    boolean retval = (Boolean)m.invoke(null, src, offset, dest);

    // Assert result
    Assert.assertEquals(false, retval);

  }
"
"  @Test
  public void charArrayCompare4() throws Throwable {

    // Arrange
    String src = ""!\""&&&&&"";
    int offset = 0;
    char[] dest = { };

    // Act
    Class<?> c = Reflector.forName(""com.alibaba.fastjson.parser.JSONScanner"");
    Method m = c.getDeclaredMethod(""charArrayCompare"", Reflector.forName(""java.lang.String""), Reflector.forName(""int""), Reflector.forName(""char []""));
    m.setAccessible(true);
    boolean retval = (Boolean)m.invoke(null, src, offset, dest);

    // Assert result
    Assert.assertEquals(true, retval);

  }
"
"  @Test
  public void checkDate1() throws Throwable {

    // Arrange
    char y0 = '2';
    char y1 = '1';
    char y2 = '1';
    char y3 = '1';
    char M0 = '1';
    char M1 = '0';
    int d0 = 51;
    int d1 = 48;

    // Act
    Class<?> c = Reflector.forName(""com.alibaba.fastjson.parser.JSONScanner"");
    Method m = c.getDeclaredMethod(""checkDate"", Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""int""), Reflector.forName(""int""));
    m.setAccessible(true);
    boolean retval = (Boolean)m.invoke(null, y0, y1, y2, y3, M0, M1, d0, d1);

    // Assert result
    Assert.assertEquals(true, retval);

  }
"
"  @Test
  public void checkDate2() throws Throwable {

    // Arrange
    char y0 = '2';
    char y1 = '1';
    char y2 = '1';
    char y3 = '1';
    char M0 = '0';
    char M1 = '\u8031';
    int d0 = 0;
    int d1 = 0;

    // Act
    Class<?> c = Reflector.forName(""com.alibaba.fastjson.parser.JSONScanner"");
    Method m = c.getDeclaredMethod(""checkDate"", Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""int""), Reflector.forName(""int""));
    m.setAccessible(true);
    boolean retval = (Boolean)m.invoke(null, y0, y1, y2, y3, M0, M1, d0, d1);

    // Assert result
    Assert.assertEquals(false, retval);

  }
"
"  @Test
  public void checkDate3() throws Throwable {

    // Arrange
    char y0 = '2';
    char y1 = '1';
    char y2 = '1';
    char y3 = '1';
    char M0 = '1';
    char M1 = '0';
    int d0 = 49;
    int d1 = 32810;

    // Act
    Class<?> c = Reflector.forName(""com.alibaba.fastjson.parser.JSONScanner"");
    Method m = c.getDeclaredMethod(""checkDate"", Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""int""), Reflector.forName(""int""));
    m.setAccessible(true);
    boolean retval = (Boolean)m.invoke(null, y0, y1, y2, y3, M0, M1, d0, d1);

    // Assert result
    Assert.assertEquals(false, retval);

  }
"
"  @Test
  public void checkDate4() throws Throwable {

    // Arrange
    char y0 = '2';
    char y1 = '1';
    char y2 = '1';
    char y3 = '1';
    char M0 = '1';
    char M1 = '0';
    int d0 = 50;
    int d1 = 32810;

    // Act
    Class<?> c = Reflector.forName(""com.alibaba.fastjson.parser.JSONScanner"");
    Method m = c.getDeclaredMethod(""checkDate"", Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""int""), Reflector.forName(""int""));
    m.setAccessible(true);
    boolean retval = (Boolean)m.invoke(null, y0, y1, y2, y3, M0, M1, d0, d1);

    // Assert result
    Assert.assertEquals(false, retval);

  }
"
"  @Test
  public void checkDate5() throws Throwable {

    // Arrange
    char y0 = '4';
    char y1 = '\u0000';
    char y2 = '\u0000';
    char y3 = '\u0000';
    char M0 = '\u0000';
    char M1 = '\u0000';
    int d0 = 0;
    int d1 = 0;

    // Act
    Class<?> c = Reflector.forName(""com.alibaba.fastjson.parser.JSONScanner"");
    Method m = c.getDeclaredMethod(""checkDate"", Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""int""), Reflector.forName(""int""));
    m.setAccessible(true);
    boolean retval = (Boolean)m.invoke(null, y0, y1, y2, y3, M0, M1, d0, d1);

    // Assert result
    Assert.assertEquals(false, retval);

  }
"
"  @Test
  public void checkDate6() throws Throwable {

    // Arrange
    char y0 = '\u0000';
    char y1 = '\u0000';
    char y2 = '\u0000';
    char y3 = '\u0000';
    char M0 = '\u0000';
    char M1 = '\u0000';
    int d0 = 0;
    int d1 = 0;

    // Act
    Class<?> c = Reflector.forName(""com.alibaba.fastjson.parser.JSONScanner"");
    Method m = c.getDeclaredMethod(""checkDate"", Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""int""), Reflector.forName(""int""));
    m.setAccessible(true);
    boolean retval = (Boolean)m.invoke(null, y0, y1, y2, y3, M0, M1, d0, d1);

    // Assert result
    Assert.assertEquals(false, retval);

  }
"
"  @Test
  public void checkDate7() throws Throwable {

    // Arrange
    char y0 = '2';
    char y1 = '1';
    char y2 = '0';
    char y3 = '0';
    char M0 = '0';
    char M1 = '\u0000';
    int d0 = 0;
    int d1 = 0;

    // Act
    Class<?> c = Reflector.forName(""com.alibaba.fastjson.parser.JSONScanner"");
    Method m = c.getDeclaredMethod(""checkDate"", Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""int""), Reflector.forName(""int""));
    m.setAccessible(true);
    boolean retval = (Boolean)m.invoke(null, y0, y1, y2, y3, M0, M1, d0, d1);

    // Assert result
    Assert.assertEquals(false, retval);

  }
"
"  @Test
  public void checkDate8() throws Throwable {

    // Arrange
    char y0 = '2';
    char y1 = '\u0011';
    char y2 = '0';
    char y3 = '\u0830';
    char M0 = '1';
    char M1 = '\u0000';
    int d0 = 0;
    int d1 = 0;

    // Act
    Class<?> c = Reflector.forName(""com.alibaba.fastjson.parser.JSONScanner"");
    Method m = c.getDeclaredMethod(""checkDate"", Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""int""), Reflector.forName(""int""));
    m.setAccessible(true);
    boolean retval = (Boolean)m.invoke(null, y0, y1, y2, y3, M0, M1, d0, d1);

    // Assert result
    Assert.assertEquals(false, retval);

  }
"
"  @Test
  public void checkDate9() throws Throwable {

    // Arrange
    char y0 = '2';
    char y1 = '1';
    char y2 = '1';
    char y3 = '1';
    char M0 = '1';
    char M1 = '0';
    int d0 = 49;
    int d1 = 49;

    // Act
    Class<?> c = Reflector.forName(""com.alibaba.fastjson.parser.JSONScanner"");
    Method m = c.getDeclaredMethod(""checkDate"", Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""int""), Reflector.forName(""int""));
    m.setAccessible(true);
    boolean retval = (Boolean)m.invoke(null, y0, y1, y2, y3, M0, M1, d0, d1);

    // Assert result
    Assert.assertEquals(true, retval);

  }
"
"  @Test
  public void checkDate10() throws Throwable {

    // Arrange
    char y0 = '2';
    char y1 = '1';
    char y2 = '1';
    char y3 = '1';
    char M0 = '1';
    char M1 = '0';
    int d0 = 8388658;
    int d1 = 32810;

    // Act
    Class<?> c = Reflector.forName(""com.alibaba.fastjson.parser.JSONScanner"");
    Method m = c.getDeclaredMethod(""checkDate"", Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""int""), Reflector.forName(""int""));
    m.setAccessible(true);
    boolean retval = (Boolean)m.invoke(null, y0, y1, y2, y3, M0, M1, d0, d1);

    // Assert result
    Assert.assertEquals(false, retval);

  }
"
"  @Test
  public void checkDate11() throws Throwable {

    // Arrange
    char y0 = '2';
    char y1 = '1';
    char y2 = '1';
    char y3 = '1';
    char M0 = '1';
    char M1 = '0';
    int d0 = 48;
    int d1 = 49;

    // Act
    Class<?> c = Reflector.forName(""com.alibaba.fastjson.parser.JSONScanner"");
    Method m = c.getDeclaredMethod(""checkDate"", Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""int""), Reflector.forName(""int""));
    m.setAccessible(true);
    boolean retval = (Boolean)m.invoke(null, y0, y1, y2, y3, M0, M1, d0, d1);

    // Assert result
    Assert.assertEquals(true, retval);

  }
"
"  @Test
  public void checkDate12() throws Throwable {

    // Arrange
    char y0 = '2';
    char y1 = '1';
    char y2 = '\u8030';
    char y3 = '\u0830';
    char M0 = '1';
    char M1 = '\u0000';
    int d0 = 0;
    int d1 = 0;

    // Act
    Class<?> c = Reflector.forName(""com.alibaba.fastjson.parser.JSONScanner"");
    Method m = c.getDeclaredMethod(""checkDate"", Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""int""), Reflector.forName(""int""));
    m.setAccessible(true);
    boolean retval = (Boolean)m.invoke(null, y0, y1, y2, y3, M0, M1, d0, d1);

    // Assert result
    Assert.assertEquals(false, retval);

  }
"
"  @Test
  public void checkDate13() throws Throwable {

    // Arrange
    char y0 = '2';
    char y1 = '1';
    char y2 = '1';
    char y3 = '1';
    char M0 = '1';
    char M1 = '0';
    int d0 = 48;
    int d1 = 0;

    // Act
    Class<?> c = Reflector.forName(""com.alibaba.fastjson.parser.JSONScanner"");
    Method m = c.getDeclaredMethod(""checkDate"", Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""int""), Reflector.forName(""int""));
    m.setAccessible(true);
    boolean retval = (Boolean)m.invoke(null, y0, y1, y2, y3, M0, M1, d0, d1);

    // Assert result
    Assert.assertEquals(false, retval);

  }
"
"  @Test
  public void checkDate14() throws Throwable {

    // Arrange
    char y0 = '2';
    char y1 = '1';
    char y2 = '1';
    char y3 = '1';
    char M0 = '1';
    char M1 = '0';
    int d0 = 51;
    int d1 = -2147483600;

    // Act
    Class<?> c = Reflector.forName(""com.alibaba.fastjson.parser.JSONScanner"");
    Method m = c.getDeclaredMethod(""checkDate"", Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""int""), Reflector.forName(""int""));
    m.setAccessible(true);
    boolean retval = (Boolean)m.invoke(null, y0, y1, y2, y3, M0, M1, d0, d1);

    // Assert result
    Assert.assertEquals(false, retval);

  }
"
"  @Test
  public void checkDate15() throws Throwable {

    // Arrange
    char y0 = '2';
    char y1 = '1';
    char y2 = '1';
    char y3 = '1';
    char M0 = '1';
    char M1 = '\u8031';
    int d0 = 0;
    int d1 = 0;

    // Act
    Class<?> c = Reflector.forName(""com.alibaba.fastjson.parser.JSONScanner"");
    Method m = c.getDeclaredMethod(""checkDate"", Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""int""), Reflector.forName(""int""));
    m.setAccessible(true);
    boolean retval = (Boolean)m.invoke(null, y0, y1, y2, y3, M0, M1, d0, d1);

    // Assert result
    Assert.assertEquals(false, retval);

  }
"
"  @Test
  public void checkDate16() throws Throwable {

    // Arrange
    char y0 = '2';
    char y1 = '1';
    char y2 = '0';
    char y3 = '\u0830';
    char M0 = '1';
    char M1 = '\u0000';
    int d0 = 0;
    int d1 = 0;

    // Act
    Class<?> c = Reflector.forName(""com.alibaba.fastjson.parser.JSONScanner"");
    Method m = c.getDeclaredMethod(""checkDate"", Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""int""), Reflector.forName(""int""));
    m.setAccessible(true);
    boolean retval = (Boolean)m.invoke(null, y0, y1, y2, y3, M0, M1, d0, d1);

    // Assert result
    Assert.assertEquals(false, retval);

  }
"
"  @Test
  public void checkDate17() throws Throwable {

    // Arrange
    char y0 = '2';
    char y1 = '1';
    char y2 = '0';
    char y3 = '0';
    char M0 = '\u0000';
    char M1 = '\u0000';
    int d0 = 0;
    int d1 = 0;

    // Act
    Class<?> c = Reflector.forName(""com.alibaba.fastjson.parser.JSONScanner"");
    Method m = c.getDeclaredMethod(""checkDate"", Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""int""), Reflector.forName(""int""));
    m.setAccessible(true);
    boolean retval = (Boolean)m.invoke(null, y0, y1, y2, y3, M0, M1, d0, d1);

    // Assert result
    Assert.assertEquals(false, retval);

  }
"
"  @Test
  public void checkTime1() throws Throwable {

    // Arrange
    JSONScanner objectUnderTest = ((JSONScanner)Reflector.getInstance(""com.alibaba.fastjson.parser.JSONScanner""));
    objectUnderTest.hasSpecial = false;
    objectUnderTest.token = 0;
    objectUnderTest.locale = null;
    objectUnderTest.np = 0;
    objectUnderTest.features = 0;
    Reflector.setField(objectUnderTest, ""text"", """");
    objectUnderTest.calendar = null;
    objectUnderTest.matchStat = 0;
    objectUnderTest.bp = 0;
    Reflector.setField(objectUnderTest, ""len"", 0);
    objectUnderTest.stringDefaultValue = """";
    objectUnderTest.pos = 0;
    objectUnderTest.sp = 0;
    objectUnderTest.sbuf = null;
    objectUnderTest.ch = '\u0000';
    objectUnderTest.timeZone = null;
    objectUnderTest.eofPos = 0;
    char h0 = '\u0000';
    char h1 = '\u0000';
    char m0 = '\u0000';
    char m1 = '\u0000';
    char s0 = '\u0000';
    char s1 = '\u0000';

    // Act
    Class<?> c = Reflector.forName(""com.alibaba.fastjson.parser.JSONScanner"");
    Method m = c.getDeclaredMethod(""checkTime"", Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""));
    m.setAccessible(true);
    boolean retval = (Boolean)m.invoke(objectUnderTest, h0, h1, m0, m1, s0, s1);

    // Assert result
    Assert.assertEquals(false, retval);

  }
"
"  @Test
  public void checkTime2() throws Throwable {

    // Arrange
    JSONScanner objectUnderTest = ((JSONScanner)Reflector.getInstance(""com.alibaba.fastjson.parser.JSONScanner""));
    objectUnderTest.hasSpecial = false;
    objectUnderTest.token = 0;
    objectUnderTest.locale = null;
    objectUnderTest.np = 0;
    objectUnderTest.features = 0;
    Reflector.setField(objectUnderTest, ""text"", """");
    objectUnderTest.calendar = null;
    objectUnderTest.matchStat = 0;
    objectUnderTest.bp = 0;
    Reflector.setField(objectUnderTest, ""len"", 0);
    objectUnderTest.stringDefaultValue = """";
    objectUnderTest.pos = 0;
    objectUnderTest.sp = 0;
    objectUnderTest.sbuf = null;
    objectUnderTest.ch = '\u0000';
    objectUnderTest.timeZone = null;
    objectUnderTest.eofPos = 0;
    char h0 = '2';
    char h1 = '\u0000';
    char m0 = '\u0000';
    char m1 = '\u0000';
    char s0 = '\u0000';
    char s1 = '\u0000';

    // Act
    Class<?> c = Reflector.forName(""com.alibaba.fastjson.parser.JSONScanner"");
    Method m = c.getDeclaredMethod(""checkTime"", Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""));
    m.setAccessible(true);
    boolean retval = (Boolean)m.invoke(objectUnderTest, h0, h1, m0, m1, s0, s1);

    // Assert result
    Assert.assertEquals(false, retval);

  }
"
"  @Test
  public void checkTime3() throws Throwable {

    // Arrange
    JSONScanner objectUnderTest = ((JSONScanner)Reflector.getInstance(""com.alibaba.fastjson.parser.JSONScanner""));
    objectUnderTest.hasSpecial = false;
    objectUnderTest.token = 0;
    objectUnderTest.locale = null;
    objectUnderTest.np = 0;
    objectUnderTest.features = 0;
    Reflector.setField(objectUnderTest, ""text"", """");
    objectUnderTest.calendar = null;
    objectUnderTest.matchStat = 0;
    objectUnderTest.bp = 0;
    Reflector.setField(objectUnderTest, ""len"", 0);
    objectUnderTest.stringDefaultValue = """";
    objectUnderTest.pos = 0;
    objectUnderTest.sp = 0;
    objectUnderTest.sbuf = null;
    objectUnderTest.ch = '\u0000';
    objectUnderTest.timeZone = null;
    objectUnderTest.eofPos = 0;
    char h0 = '0';
    char h1 = '<';
    char m0 = '\u0000';
    char m1 = '\u0000';
    char s0 = '\u0000';
    char s1 = '\u0000';

    // Act
    Class<?> c = Reflector.forName(""com.alibaba.fastjson.parser.JSONScanner"");
    Method m = c.getDeclaredMethod(""checkTime"", Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""));
    m.setAccessible(true);
    boolean retval = (Boolean)m.invoke(objectUnderTest, h0, h1, m0, m1, s0, s1);

    // Assert result
    Assert.assertEquals(false, retval);

  }
"
"  @Test
  public void checkTime4() throws Throwable {

    // Arrange
    JSONScanner objectUnderTest = ((JSONScanner)Reflector.getInstance(""com.alibaba.fastjson.parser.JSONScanner""));
    objectUnderTest.hasSpecial = false;
    objectUnderTest.token = 0;
    objectUnderTest.locale = null;
    objectUnderTest.np = 0;
    objectUnderTest.features = 0;
    Reflector.setField(objectUnderTest, ""text"", """");
    objectUnderTest.calendar = null;
    objectUnderTest.matchStat = 0;
    objectUnderTest.bp = 0;
    Reflector.setField(objectUnderTest, ""len"", 0);
    objectUnderTest.stringDefaultValue = """";
    objectUnderTest.pos = 0;
    objectUnderTest.sp = 0;
    objectUnderTest.sbuf = null;
    objectUnderTest.ch = '\u0000';
    objectUnderTest.timeZone = null;
    objectUnderTest.eofPos = 0;
    char h0 = '0';
    char h1 = ' ';
    char m0 = '\u0000';
    char m1 = '\u0000';
    char s0 = '\u0000';
    char s1 = '\u0000';

    // Act
    Class<?> c = Reflector.forName(""com.alibaba.fastjson.parser.JSONScanner"");
    Method m = c.getDeclaredMethod(""checkTime"", Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""));
    m.setAccessible(true);
    boolean retval = (Boolean)m.invoke(objectUnderTest, h0, h1, m0, m1, s0, s1);

    // Assert result
    Assert.assertEquals(false, retval);

  }
"
"  @Test
  public void checkTime5() throws Throwable {

    // Arrange
    JSONScanner objectUnderTest = ((JSONScanner)Reflector.getInstance(""com.alibaba.fastjson.parser.JSONScanner""));
    objectUnderTest.hasSpecial = false;
    objectUnderTest.token = 0;
    objectUnderTest.locale = null;
    objectUnderTest.np = 0;
    objectUnderTest.features = 0;
    Reflector.setField(objectUnderTest, ""text"", """");
    objectUnderTest.calendar = null;
    objectUnderTest.matchStat = 0;
    objectUnderTest.bp = 0;
    Reflector.setField(objectUnderTest, ""len"", 0);
    objectUnderTest.stringDefaultValue = """";
    objectUnderTest.pos = 0;
    objectUnderTest.sp = 0;
    objectUnderTest.sbuf = null;
    objectUnderTest.ch = '\u0000';
    objectUnderTest.timeZone = null;
    objectUnderTest.eofPos = 0;
    char h0 = '2';
    char h1 = '5';
    char m0 = '\u0000';
    char m1 = '\u0000';
    char s0 = '\u0000';
    char s1 = '\u0000';

    // Act
    Class<?> c = Reflector.forName(""com.alibaba.fastjson.parser.JSONScanner"");
    Method m = c.getDeclaredMethod(""checkTime"", Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""));
    m.setAccessible(true);
    boolean retval = (Boolean)m.invoke(objectUnderTest, h0, h1, m0, m1, s0, s1);

    // Assert result
    Assert.assertEquals(false, retval);

  }
"
"  @Test
  public void checkTime6() throws Throwable {

    // Arrange
    JSONScanner objectUnderTest = ((JSONScanner)Reflector.getInstance(""com.alibaba.fastjson.parser.JSONScanner""));
    objectUnderTest.hasSpecial = false;
    objectUnderTest.token = 0;
    objectUnderTest.locale = null;
    objectUnderTest.np = 0;
    objectUnderTest.features = 0;
    Reflector.setField(objectUnderTest, ""text"", """");
    objectUnderTest.calendar = null;
    objectUnderTest.matchStat = 0;
    objectUnderTest.bp = 0;
    Reflector.setField(objectUnderTest, ""len"", 0);
    objectUnderTest.stringDefaultValue = """";
    objectUnderTest.pos = 0;
    objectUnderTest.sp = 0;
    objectUnderTest.sbuf = null;
    objectUnderTest.ch = '\u0000';
    objectUnderTest.timeZone = null;
    objectUnderTest.eofPos = 0;
    char h0 = '0';
    char h1 = '9';
    char m0 = '1';
    char m1 = '\u0000';
    char s0 = '\u0000';
    char s1 = '\u0000';

    // Act
    Class<?> c = Reflector.forName(""com.alibaba.fastjson.parser.JSONScanner"");
    Method m = c.getDeclaredMethod(""checkTime"", Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""));
    m.setAccessible(true);
    boolean retval = (Boolean)m.invoke(objectUnderTest, h0, h1, m0, m1, s0, s1);

    // Assert result
    Assert.assertEquals(false, retval);

  }
"
"  @Test
  public void checkTime7() throws Throwable {

    // Arrange
    JSONScanner objectUnderTest = ((JSONScanner)Reflector.getInstance(""com.alibaba.fastjson.parser.JSONScanner""));
    objectUnderTest.hasSpecial = false;
    objectUnderTest.token = 0;
    objectUnderTest.locale = null;
    objectUnderTest.np = 0;
    objectUnderTest.features = 0;
    Reflector.setField(objectUnderTest, ""text"", """");
    objectUnderTest.calendar = null;
    objectUnderTest.matchStat = 0;
    objectUnderTest.bp = 0;
    Reflector.setField(objectUnderTest, ""len"", 0);
    objectUnderTest.stringDefaultValue = """";
    objectUnderTest.pos = 0;
    objectUnderTest.sp = 0;
    objectUnderTest.sbuf = null;
    objectUnderTest.ch = '\u0000';
    objectUnderTest.timeZone = null;
    objectUnderTest.eofPos = 0;
    char h0 = '1';
    char h1 = '=';
    char m0 = '1';
    char m1 = '\u0000';
    char s0 = '\u0000';
    char s1 = '\u0000';

    // Act
    Class<?> c = Reflector.forName(""com.alibaba.fastjson.parser.JSONScanner"");
    Method m = c.getDeclaredMethod(""checkTime"", Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""));
    m.setAccessible(true);
    boolean retval = (Boolean)m.invoke(objectUnderTest, h0, h1, m0, m1, s0, s1);

    // Assert result
    Assert.assertEquals(false, retval);

  }
"
"  @Test
  public void checkTime8() throws Throwable {

    // Arrange
    JSONScanner objectUnderTest = ((JSONScanner)Reflector.getInstance(""com.alibaba.fastjson.parser.JSONScanner""));
    objectUnderTest.hasSpecial = false;
    objectUnderTest.token = 0;
    objectUnderTest.locale = null;
    objectUnderTest.np = 0;
    objectUnderTest.features = 0;
    Reflector.setField(objectUnderTest, ""text"", """");
    objectUnderTest.calendar = null;
    objectUnderTest.matchStat = 0;
    objectUnderTest.bp = 0;
    Reflector.setField(objectUnderTest, ""len"", 0);
    objectUnderTest.stringDefaultValue = """";
    objectUnderTest.pos = 0;
    objectUnderTest.sp = 0;
    objectUnderTest.sbuf = null;
    objectUnderTest.ch = '\u0000';
    objectUnderTest.timeZone = null;
    objectUnderTest.eofPos = 0;
    char h0 = '1';
    char h1 = '9';
    char m0 = '1';
    char m1 = '\u0000';
    char s0 = '\u0000';
    char s1 = '\u0000';

    // Act
    Class<?> c = Reflector.forName(""com.alibaba.fastjson.parser.JSONScanner"");
    Method m = c.getDeclaredMethod(""checkTime"", Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""));
    m.setAccessible(true);
    boolean retval = (Boolean)m.invoke(objectUnderTest, h0, h1, m0, m1, s0, s1);

    // Assert result
    Assert.assertEquals(false, retval);

  }
"
"  @Test
  public void checkTime9() throws Throwable {

    // Arrange
    JSONScanner objectUnderTest = ((JSONScanner)Reflector.getInstance(""com.alibaba.fastjson.parser.JSONScanner""));
    objectUnderTest.hasSpecial = false;
    objectUnderTest.token = 0;
    objectUnderTest.locale = null;
    objectUnderTest.np = 0;
    objectUnderTest.features = 0;
    Reflector.setField(objectUnderTest, ""text"", """");
    objectUnderTest.calendar = null;
    objectUnderTest.matchStat = 0;
    objectUnderTest.bp = 0;
    Reflector.setField(objectUnderTest, ""len"", 0);
    objectUnderTest.stringDefaultValue = """";
    objectUnderTest.pos = 0;
    objectUnderTest.sp = 0;
    objectUnderTest.sbuf = null;
    objectUnderTest.ch = '\u0000';
    objectUnderTest.timeZone = null;
    objectUnderTest.eofPos = 0;
    char h0 = '1';
    char h1 = '9';
    char m0 = ' ';
    char m1 = '\u0000';
    char s0 = '\u0000';
    char s1 = '\u0000';

    // Act
    Class<?> c = Reflector.forName(""com.alibaba.fastjson.parser.JSONScanner"");
    Method m = c.getDeclaredMethod(""checkTime"", Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""));
    m.setAccessible(true);
    boolean retval = (Boolean)m.invoke(objectUnderTest, h0, h1, m0, m1, s0, s1);

    // Assert result
    Assert.assertEquals(false, retval);

  }
"
"  @Test
  public void checkTime10() throws Throwable {

    // Arrange
    JSONScanner objectUnderTest = ((JSONScanner)Reflector.getInstance(""com.alibaba.fastjson.parser.JSONScanner""));
    objectUnderTest.hasSpecial = false;
    objectUnderTest.token = 0;
    objectUnderTest.locale = null;
    objectUnderTest.np = 0;
    objectUnderTest.features = 0;
    Reflector.setField(objectUnderTest, ""text"", """");
    objectUnderTest.calendar = null;
    objectUnderTest.matchStat = 0;
    objectUnderTest.bp = 0;
    Reflector.setField(objectUnderTest, ""len"", 0);
    objectUnderTest.stringDefaultValue = """";
    objectUnderTest.pos = 0;
    objectUnderTest.sp = 0;
    objectUnderTest.sbuf = null;
    objectUnderTest.ch = '\u0000';
    objectUnderTest.timeZone = null;
    objectUnderTest.eofPos = 0;
    char h0 = '1';
    char h1 = '9';
    char m0 = '4';
    char m1 = '3';
    char s0 = '1';
    char s1 = '\u0000';

    // Act
    Class<?> c = Reflector.forName(""com.alibaba.fastjson.parser.JSONScanner"");
    Method m = c.getDeclaredMethod(""checkTime"", Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""));
    m.setAccessible(true);
    boolean retval = (Boolean)m.invoke(objectUnderTest, h0, h1, m0, m1, s0, s1);

    // Assert result
    Assert.assertEquals(false, retval);

  }
"
"  @Test
  public void checkTime11() throws Throwable {

    // Arrange
    JSONScanner objectUnderTest = ((JSONScanner)Reflector.getInstance(""com.alibaba.fastjson.parser.JSONScanner""));
    objectUnderTest.hasSpecial = false;
    objectUnderTest.token = 0;
    objectUnderTest.locale = null;
    objectUnderTest.np = 0;
    objectUnderTest.features = 0;
    Reflector.setField(objectUnderTest, ""text"", """");
    objectUnderTest.calendar = null;
    objectUnderTest.matchStat = 0;
    objectUnderTest.bp = 0;
    Reflector.setField(objectUnderTest, ""len"", 0);
    objectUnderTest.stringDefaultValue = """";
    objectUnderTest.pos = 0;
    objectUnderTest.sp = 0;
    objectUnderTest.sbuf = null;
    objectUnderTest.ch = '\u0000';
    objectUnderTest.timeZone = null;
    objectUnderTest.eofPos = 0;
    char h0 = '1';
    char h1 = '9';
    char m0 = '6';
    char m1 = '0';
    char s0 = '1';
    char s1 = '\u0430';

    // Act
    Class<?> c = Reflector.forName(""com.alibaba.fastjson.parser.JSONScanner"");
    Method m = c.getDeclaredMethod(""checkTime"", Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""));
    m.setAccessible(true);
    boolean retval = (Boolean)m.invoke(objectUnderTest, h0, h1, m0, m1, s0, s1);

    // Assert result
    Assert.assertEquals(false, retval);

  }
"
"  @Test
  public void checkTime12() throws Throwable {

    // Arrange
    JSONScanner objectUnderTest = ((JSONScanner)Reflector.getInstance(""com.alibaba.fastjson.parser.JSONScanner""));
    objectUnderTest.hasSpecial = false;
    objectUnderTest.token = 0;
    objectUnderTest.locale = null;
    objectUnderTest.np = 0;
    objectUnderTest.features = 0;
    Reflector.setField(objectUnderTest, ""text"", """");
    objectUnderTest.calendar = null;
    objectUnderTest.matchStat = 0;
    objectUnderTest.bp = 0;
    Reflector.setField(objectUnderTest, ""len"", 0);
    objectUnderTest.stringDefaultValue = """";
    objectUnderTest.pos = 0;
    objectUnderTest.sp = 0;
    objectUnderTest.sbuf = null;
    objectUnderTest.ch = '\u0000';
    objectUnderTest.timeZone = null;
    objectUnderTest.eofPos = 0;
    char h0 = '1';
    char h1 = '9';
    char m0 = '6';
    char m1 = '0';
    char s0 = '6';
    char s1 = '\u0430';

    // Act
    Class<?> c = Reflector.forName(""com.alibaba.fastjson.parser.JSONScanner"");
    Method m = c.getDeclaredMethod(""checkTime"", Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""));
    m.setAccessible(true);
    boolean retval = (Boolean)m.invoke(objectUnderTest, h0, h1, m0, m1, s0, s1);

    // Assert result
    Assert.assertEquals(false, retval);

  }
"
"  @Test
  public void checkTime13() throws Throwable {

    // Arrange
    JSONScanner objectUnderTest = ((JSONScanner)Reflector.getInstance(""com.alibaba.fastjson.parser.JSONScanner""));
    objectUnderTest.hasSpecial = false;
    objectUnderTest.token = 0;
    objectUnderTest.locale = null;
    objectUnderTest.np = 0;
    objectUnderTest.features = 0;
    Reflector.setField(objectUnderTest, ""text"", """");
    objectUnderTest.calendar = null;
    objectUnderTest.matchStat = 0;
    objectUnderTest.bp = 0;
    Reflector.setField(objectUnderTest, ""len"", 0);
    objectUnderTest.stringDefaultValue = """";
    objectUnderTest.pos = 0;
    objectUnderTest.sp = 0;
    objectUnderTest.sbuf = null;
    objectUnderTest.ch = '\u0000';
    objectUnderTest.timeZone = null;
    objectUnderTest.eofPos = 0;
    char h0 = '1';
    char h1 = '9';
    char m0 = '6';
    char m1 = '0';
    char s0 = '>';
    char s1 = '\u0430';

    // Act
    Class<?> c = Reflector.forName(""com.alibaba.fastjson.parser.JSONScanner"");
    Method m = c.getDeclaredMethod(""checkTime"", Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""));
    m.setAccessible(true);
    boolean retval = (Boolean)m.invoke(objectUnderTest, h0, h1, m0, m1, s0, s1);

    // Assert result
    Assert.assertEquals(false, retval);

  }
"
"  @Test
  public void checkTime14() throws Throwable {

    // Arrange
    JSONScanner objectUnderTest = ((JSONScanner)Reflector.getInstance(""com.alibaba.fastjson.parser.JSONScanner""));
    objectUnderTest.hasSpecial = false;
    objectUnderTest.token = 0;
    objectUnderTest.locale = null;
    objectUnderTest.np = 0;
    objectUnderTest.features = 0;
    Reflector.setField(objectUnderTest, ""text"", """");
    objectUnderTest.calendar = null;
    objectUnderTest.matchStat = 0;
    objectUnderTest.bp = 0;
    Reflector.setField(objectUnderTest, ""len"", 0);
    objectUnderTest.stringDefaultValue = """";
    objectUnderTest.pos = 0;
    objectUnderTest.sp = 0;
    objectUnderTest.sbuf = null;
    objectUnderTest.ch = '\u0000';
    objectUnderTest.timeZone = null;
    objectUnderTest.eofPos = 0;
    char h0 = '1';
    char h1 = '9';
    char m0 = '6';
    char m1 = '0';
    char s0 = '6';
    char s1 = '0';

    // Act
    Class<?> c = Reflector.forName(""com.alibaba.fastjson.parser.JSONScanner"");
    Method m = c.getDeclaredMethod(""checkTime"", Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""));
    m.setAccessible(true);
    boolean retval = (Boolean)m.invoke(objectUnderTest, h0, h1, m0, m1, s0, s1);

    // Assert result
    Assert.assertEquals(true, retval);

  }
"
"  @Test
  public void checkTime15() throws Throwable {

    // Arrange
    JSONScanner objectUnderTest = ((JSONScanner)Reflector.getInstance(""com.alibaba.fastjson.parser.JSONScanner""));
    objectUnderTest.hasSpecial = false;
    objectUnderTest.token = 0;
    objectUnderTest.locale = null;
    objectUnderTest.np = 0;
    objectUnderTest.features = 0;
    Reflector.setField(objectUnderTest, ""text"", """");
    objectUnderTest.calendar = null;
    objectUnderTest.matchStat = 0;
    objectUnderTest.bp = 0;
    Reflector.setField(objectUnderTest, ""len"", 0);
    objectUnderTest.stringDefaultValue = """";
    objectUnderTest.pos = 0;
    objectUnderTest.sp = 0;
    objectUnderTest.sbuf = null;
    objectUnderTest.ch = '\u0000';
    objectUnderTest.timeZone = null;
    objectUnderTest.eofPos = 0;
    char h0 = '1';
    char h1 = '9';
    char m0 = '4';
    char m1 = '3';
    char s0 = '1';
    char s1 = '\u0430';

    // Act
    Class<?> c = Reflector.forName(""com.alibaba.fastjson.parser.JSONScanner"");
    Method m = c.getDeclaredMethod(""checkTime"", Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""));
    m.setAccessible(true);
    boolean retval = (Boolean)m.invoke(objectUnderTest, h0, h1, m0, m1, s0, s1);

    // Assert result
    Assert.assertEquals(false, retval);

  }
"
"  @Test
  public void checkTime16() throws Throwable {

    // Arrange
    JSONScanner objectUnderTest = ((JSONScanner)Reflector.getInstance(""com.alibaba.fastjson.parser.JSONScanner""));
    objectUnderTest.hasSpecial = false;
    objectUnderTest.token = 0;
    objectUnderTest.locale = null;
    objectUnderTest.np = 0;
    objectUnderTest.features = 0;
    Reflector.setField(objectUnderTest, ""text"", """");
    objectUnderTest.calendar = null;
    objectUnderTest.matchStat = 0;
    objectUnderTest.bp = 0;
    Reflector.setField(objectUnderTest, ""len"", 0);
    objectUnderTest.stringDefaultValue = """";
    objectUnderTest.pos = 0;
    objectUnderTest.sp = 0;
    objectUnderTest.sbuf = null;
    objectUnderTest.ch = '\u0000';
    objectUnderTest.timeZone = null;
    objectUnderTest.eofPos = 0;
    char h0 = '1';
    char h1 = '9';
    char m0 = '4';
    char m1 = ':';
    char s0 = '\u0000';
    char s1 = '\u0000';

    // Act
    Class<?> c = Reflector.forName(""com.alibaba.fastjson.parser.JSONScanner"");
    Method m = c.getDeclaredMethod(""checkTime"", Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""));
    m.setAccessible(true);
    boolean retval = (Boolean)m.invoke(objectUnderTest, h0, h1, m0, m1, s0, s1);

    // Assert result
    Assert.assertEquals(false, retval);

  }
"
"  @Test
  public void checkTime17() throws Throwable {

    // Arrange
    JSONScanner objectUnderTest = ((JSONScanner)Reflector.getInstance(""com.alibaba.fastjson.parser.JSONScanner""));
    objectUnderTest.hasSpecial = false;
    objectUnderTest.token = 0;
    objectUnderTest.locale = null;
    objectUnderTest.np = 0;
    objectUnderTest.features = 0;
    Reflector.setField(objectUnderTest, ""text"", """");
    objectUnderTest.calendar = null;
    objectUnderTest.matchStat = 0;
    objectUnderTest.bp = 0;
    Reflector.setField(objectUnderTest, ""len"", 0);
    objectUnderTest.stringDefaultValue = """";
    objectUnderTest.pos = 0;
    objectUnderTest.sp = 0;
    objectUnderTest.sbuf = null;
    objectUnderTest.ch = '\u0000';
    objectUnderTest.timeZone = null;
    objectUnderTest.eofPos = 0;
    char h0 = '1';
    char h1 = '9';
    char m0 = '6';
    char m1 = '1';
    char s0 = '1';
    char s1 = '\u0430';

    // Act
    Class<?> c = Reflector.forName(""com.alibaba.fastjson.parser.JSONScanner"");
    Method m = c.getDeclaredMethod(""checkTime"", Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""), Reflector.forName(""char""));
    m.setAccessible(true);
    boolean retval = (Boolean)m.invoke(objectUnderTest, h0, h1, m0, m1, s0, s1);

    // Assert result
    Assert.assertEquals(false, retval);

  }
"
"  @Test
  public void info1() throws Throwable {

    // Arrange
    JSONScanner objectUnderTest = ((JSONScanner)Reflector.getInstance(""com.alibaba.fastjson.parser.JSONScanner""));
    objectUnderTest.hasSpecial = false;
    objectUnderTest.token = 0;
    Locale locale = ((Locale)Reflector.getInstance(""java.util.Locale""));
    objectUnderTest.locale = locale;
    objectUnderTest.np = 0;
    objectUnderTest.features = 0;
    Reflector.setField(objectUnderTest, ""text"", ""(((("");
    objectUnderTest.calendar = null;
    objectUnderTest.matchStat = 0;
    objectUnderTest.bp = 7;
    Reflector.setField(objectUnderTest, ""len"", 0);
    objectUnderTest.stringDefaultValue = ""!!!!"";
    objectUnderTest.pos = 0;
    objectUnderTest.sp = 0;
    char[] charArray = { '\u0000' };
    objectUnderTest.sbuf = charArray;
    objectUnderTest.ch = '\u0000';
    objectUnderTest.timeZone = null;
    objectUnderTest.eofPos = 0;

    // Act
    String retval = objectUnderTest.info();

    // Assert result
    Assert.assertEquals(""pos 7, json : (((("", retval);

  }
"
"  @Test
  public void isEOF1() throws Throwable {

    // Arrange
    JSONScanner objectUnderTest = ((JSONScanner)Reflector.getInstance(""com.alibaba.fastjson.parser.JSONScanner""));
    objectUnderTest.hasSpecial = false;
    objectUnderTest.token = 0;
    objectUnderTest.locale = null;
    objectUnderTest.np = 0;
    objectUnderTest.features = 0;
    Reflector.setField(objectUnderTest, ""text"", null);
    objectUnderTest.calendar = null;
    objectUnderTest.matchStat = 0;
    objectUnderTest.bp = 0;
    Reflector.setField(objectUnderTest, ""len"", 0);
    objectUnderTest.stringDefaultValue = null;
    objectUnderTest.pos = 0;
    objectUnderTest.sp = 0;
    objectUnderTest.sbuf = null;
    objectUnderTest.ch = '\u001a';
    objectUnderTest.timeZone = null;
    objectUnderTest.eofPos = 0;

    // Act
    boolean retval = objectUnderTest.isEOF();

    // Assert result
    Assert.assertEquals(true, retval);

  }
"
"  @Test
  public void isEOF2() throws Throwable {

    // Arrange
    JSONScanner objectUnderTest = ((JSONScanner)Reflector.getInstance(""com.alibaba.fastjson.parser.JSONScanner""));
    objectUnderTest.hasSpecial = false;
    objectUnderTest.token = 0;
    objectUnderTest.locale = null;
    objectUnderTest.np = 0;
    objectUnderTest.features = 0;
    Reflector.setField(objectUnderTest, ""text"", null);
    objectUnderTest.calendar = null;
    objectUnderTest.matchStat = 0;
    objectUnderTest.bp = 1;
    Reflector.setField(objectUnderTest, ""len"", 0);
    objectUnderTest.stringDefaultValue = null;
    objectUnderTest.pos = 0;
    objectUnderTest.sp = 0;
    objectUnderTest.sbuf = null;
    objectUnderTest.ch = '\u0000';
    objectUnderTest.timeZone = null;
    objectUnderTest.eofPos = 0;

    // Act
    boolean retval = objectUnderTest.isEOF();

    // Assert result
    Assert.assertEquals(false, retval);

  }
"
"  @Test
  public void isEOF3() throws Throwable {

    // Arrange
    JSONScanner objectUnderTest = ((JSONScanner)Reflector.getInstance(""com.alibaba.fastjson.parser.JSONScanner""));
    objectUnderTest.hasSpecial = false;
    objectUnderTest.token = 0;
    objectUnderTest.locale = null;
    objectUnderTest.np = 0;
    objectUnderTest.features = 0;
    Reflector.setField(objectUnderTest, ""text"", null);
    objectUnderTest.calendar = null;
    objectUnderTest.matchStat = 0;
    objectUnderTest.bp = 1;
    Reflector.setField(objectUnderTest, ""len"", 0);
    objectUnderTest.stringDefaultValue = null;
    objectUnderTest.pos = 0;
    objectUnderTest.sp = 0;
    objectUnderTest.sbuf = null;
    objectUnderTest.ch = '\u001a';
    objectUnderTest.timeZone = null;
    objectUnderTest.eofPos = 0;

    // Act
    boolean retval = objectUnderTest.isEOF();

    // Assert result
    Assert.assertEquals(false, retval);

  }
"
"  @Test
  public void eq1() throws Throwable {
    // Arrange
    Object a = -1;
    Object b = null;
    // Act
    Class<?> c = Reflector.forName(""com.alibaba.fastjson.JSONPath"");
    Method m = c.getDeclaredMethod(""eq"", Reflector.forName(""java.lang.Object""), Reflector.forName(""java.lang.Object""));
    m.setAccessible(true);
    boolean retval = (Boolean)m.invoke(null, a, b);
    // Assert result
    Assert.assertEquals(false, retval);
  }
"
"  @Test
  public void eq2() throws Throwable {
    // Arrange
    Object a = null;
    Object b = null;
    // Act
    Class<?> c = Reflector.forName(""com.alibaba.fastjson.JSONPath"");
    Method m = c.getDeclaredMethod(""eq"", Reflector.forName(""java.lang.Object""), Reflector.forName(""java.lang.Object""));
    m.setAccessible(true);
    boolean retval = (Boolean)m.invoke(null, a, b);
    // Assert result
    Assert.assertEquals(true, retval);
  }
"
"  @Test
  public void isDigitFirst1() throws Throwable {
    // Arrange
    char ch = '2';
    // Act
    Class<?> c = Reflector.forName(""com.alibaba.fastjson.JSONPath$JSONPathParser"");
    Method m = c.getDeclaredMethod(""isDigitFirst"", Reflector.forName(""char""));
    m.setAccessible(true);
    boolean retval = (Boolean)m.invoke(null, ch);
    // Assert result
    Assert.assertEquals(true, retval);
  }
"
"  @Test
  public void isDigitFirst2() throws Throwable {
    // Arrange
    char ch = ':';
    // Act
    Class<?> c = Reflector.forName(""com.alibaba.fastjson.JSONPath$JSONPathParser"");
    Method m = c.getDeclaredMethod(""isDigitFirst"", Reflector.forName(""char""));
    m.setAccessible(true);
    boolean retval = (Boolean)m.invoke(null, ch);
    // Assert result
    Assert.assertEquals(false, retval);
  }
"
"  @Test
  public void isDigitFirst3() throws Throwable {
    // Arrange
    char ch = '\u0000';
    // Act
    Class<?> c = Reflector.forName(""com.alibaba.fastjson.JSONPath$JSONPathParser"");
    Method m = c.getDeclaredMethod(""isDigitFirst"", Reflector.forName(""char""));
    m.setAccessible(true);
    boolean retval = (Boolean)m.invoke(null, ch);
    // Assert result
    Assert.assertEquals(false, retval);
  }
"
"  @Test
  public void isEOF1() throws Throwable {
    // Arrange
    Object objectUnderTest = Reflector.getInstance(""com.alibaba.fastjson.JSONPath$JSONPathParser"");
    Reflector.setField(objectUnderTest, ""path"", """");
    Reflector.setField(objectUnderTest, ""pos"", -2147483647);
    Reflector.setField(objectUnderTest, ""level"", 0);
    Reflector.setField(objectUnderTest, ""ch"", '\u0000');
    // Act
    Class<?> c = Reflector.forName(""com.alibaba.fastjson.JSONPath$JSONPathParser"");
    Method m = c.getDeclaredMethod(""isEOF"");
    m.setAccessible(true);
    boolean retval = (Boolean)m.invoke(objectUnderTest);
    // Assert result
    Assert.assertEquals(false, retval);
  }
"
"  @Test
  public void isEOF2() throws Throwable {
    // Arrange
    Object objectUnderTest = Reflector.getInstance(""com.alibaba.fastjson.JSONPath$JSONPathParser"");
    Reflector.setField(objectUnderTest, ""path"", ""!"");
    Reflector.setField(objectUnderTest, ""pos"", 1);
    Reflector.setField(objectUnderTest, ""level"", 0);
    Reflector.setField(objectUnderTest, ""ch"", '\u0000');
    // Act
    Class<?> c = Reflector.forName(""com.alibaba.fastjson.JSONPath$JSONPathParser"");
    Method m = c.getDeclaredMethod(""isEOF"");
    m.setAccessible(true);
    boolean retval = (Boolean)m.invoke(objectUnderTest);
    // Assert result
    Assert.assertEquals(true, retval);
  }
"
"    @Test
    public void testFixBug1763_2() {
        BaseResult<PageResult<CouponResult>> data = JSON.parseObject(jsonStr, new TypeReference<BaseResult<PageResult<T>>>(clazz){}.getType());

        Assert.assertTrue(data.isSuccess());
        Assert.assertTrue(data.getContent().getList().size() == 2);
        Assert.assertTrue(data.getContent().getList().get(0).getId().equals(10000001L));
        Assert.assertEquals(CouponResult.class, data.getContent().getList().get(0).getClass());
    }
"
"    @Test
    public void testBug1763_2() {
        BaseResult<PageResult<CouponResult>> data = JSON.parseObject(jsonStr, new TypeReferenceBug1763_2<BaseResult<PageResult<T>>>(clazz){}.getType());

        Assert.assertTrue(data.isSuccess());
        Assert.assertTrue(data.getContent().getList().size() == 2);
        try {
            data.getContent().getList().get(0).getId();
        } catch (Throwable ex) {
            Assert.assertEquals(ex.getCause() instanceof ClassCastException, false);
        }
    }
"
"    @Test
    public void testBug89() {
        try {
            String s = ""{\""a\"":Ð·ãâ )_,\""}"";
            JSON.parseObject(s);
            fail(""Expect JSONException"");
        } catch (JSONException e) {
            // good
        }
    }
"
"    @Test
    public  void test_jsonp() throws Exception {
        FastJsonJsonView view = new FastJsonJsonView();

        Assert.assertNotNull(view.getFastJsonConfig());
        view.setFastJsonConfig(new FastJsonConfig());
        view.setExtractValueFromSingleKeyModel(true);
        view.setDisableCaching(true);

        MockHttpServletRequest request = new MockHttpServletRequest();
        request.addParameter(""callback"", ""queryName"");
        MockHttpServletResponse response = new MockHttpServletResponse();


        Assert.assertEquals(true, view.isExtractValueFromSingleKeyModel());


        view.render(Collections.singletonMap(""abc"", ""cdeä¸­æ""), request, response);
        String contentAsString = response.getContentAsString();
        int contentLength = response.getContentLength();

        Assert.assertEquals(contentLength, contentAsString.getBytes(view.getFastJsonConfig().getCharset().name()).length);
    }
"
"    @Test
    public  void test_jsonp_invalidParam() throws Exception {
        FastJsonJsonView view = new FastJsonJsonView();

        Assert.assertNotNull(view.getFastJsonConfig());
        view.setFastJsonConfig(new FastJsonConfig());
        view.setExtractValueFromSingleKeyModel(true);
        view.setDisableCaching(true);

        MockHttpServletRequest request = new MockHttpServletRequest();
        request.addParameter(""callback"", ""-methodName"");
        MockHttpServletResponse response = new MockHttpServletResponse();


        Assert.assertEquals(true, view.isExtractValueFromSingleKeyModel());


        view.render(Collections.singletonMap(""doesn't matter"", Collections.singletonMap(""abc"", ""cdeä¸­æ"")), request, response);
        String contentAsString = response.getContentAsString();
        Assert.assertTrue(contentAsString.startsWith(""{\""abc\"":\""cdeä¸­æ\""}""));

    }
"
"	@Test
	public void test1() throws Exception {
		
		JSONObject json = new JSONObject();
		
		json.put(""id"", 123);
		
		json.put(""name"", ""ååå"");
		
		mockMvc.perform(
				(post(""/fastjson/test1"").characterEncoding(""UTF-8"").content(json.toJSONString()).contentType(MediaType.APPLICATION_JSON)
						))
//		.andExpect(status().isOk())
				.andDo(print());
	}
"
"	@Test
	public void test2() throws Exception {
		
		String jsonStr = ""[{\""name\"":\""p1\"",\""sonList\"":[{\""name\"":\""s1\""}]},{\""name\"":\""p2\"",\""sonList\"":[{\""name\"":\""s2\""},{\""name\"":\""s3\""}]}]"";
		
		mockMvc.perform(
				(post(""/fastjson/test2"").characterEncoding(""UTF-8"").content(jsonStr).contentType(MediaType.APPLICATION_JSON)
						))
//		.andExpect(status().isOk())
				.andDo(print());
	}
"
"    @Test
    public void isInjectComponent() {
        wac.getBean(JSONPResponseBodyAdvice.class);
        wac.getBean(FastJsonViewResponseBodyAdvice.class);
    }
"
"    @Test
    public void test8() throws Exception {
        mockMvc.perform(
                (post(""/jsonp-fastjsonview/test8"").characterEncoding(""UTF-8"")
                        .contentType(FastJsonHttpMessageConverter.APPLICATION_JAVASCRIPT))).andExpect(status().isOk()).andDo(print());
    }
"
"    @Test
    public void test8_2() throws Exception {
//        ResultActions actions = mockMvc.perform((post(""/jsonp-fastjsonview/test8?callback=fnUpdateSome"").characterEncoding(
//                ""UTF-8"")));
//        actions.andDo(print());
//        actions.andExpect(status().isOk()).andExpect(content().contentType(APPLICATION_JAVASCRIPT))
//                .andExpect(content().string(""fnUpdateSome({\""id\"":100,\""name\"":\""æµè¯\""})""));

        MvcResult mvcResult = mockMvc.perform(post(""/jsonp-fastjsonview/test8?callback=fnUpdateSome"").characterEncoding(""UTF-8""))
                .andExpect(request().asyncStarted())
                .andReturn();


        mockMvc.perform(asyncDispatch(mvcResult))
                .andExpect(status().isOk())
                .andExpect(content().contentType(FastJsonHttpMessageConverter.APPLICATION_JAVASCRIPT))
                .andExpect(content().string(""/**/fnUpdateSome({})""));
    }
"
"    @Test
    public void isInjectComponent() {
        wac.getBean(FastJsonViewResponseBodyAdvice.class);
    }
"
"    @Test
    public void test1() throws Exception {
        mockMvc.perform(
                (post(""/fastjsonview/test1"").characterEncoding(""UTF-8"")
                        .contentType(MediaType.APPLICATION_JSON))).andExpect(status
                ().isOk()).andDo(print()
        ).andExpect(content().string(""{\""id\"":100,\""name\"":\""æµè¯\""}""));
    }
"
"    @Test
    public void test2() throws Exception {
        mockMvc.perform(
                (post(""/fastjsonview/test2"").characterEncoding(""UTF-8"")
                        .contentType(MediaType.APPLICATION_JSON))).andExpect(status
                ().isOk()).andDo(print()
        ).andExpect(content().string(""{\""description\"":\""fastjsonviewæ³¨è§£æµè¯\"",\""stock\"":\""haha\""}""));
    }
"
"    @Test
    public void test3() throws Exception {
        mockMvc.perform(
                (post(""/fastjsonview/test3"").characterEncoding(""UTF-8"")
                        .contentType(MediaType.APPLICATION_JSON))).andExpect(status
                ().isOk()).andDo(print()).andExpect(content().string(""{\""id\"":100,\""name\"":\""æµè¯\"",\""rootDepartment\"":{\""description\"":\""é¨é¨1æè¿°\""}}""));
    }
"
"    @Test
    public void test4() throws Exception {
        mockMvc.perform(
                (post(""/fastjsonview/test4"").characterEncoding(""UTF-8"")
                        .contentType(MediaType.APPLICATION_JSON))).andExpect(status
                ().isOk()).andDo(print()).andExpect(content().string(""{\""id\"":100,\""name\"":\""æµè¯\"",\""rootDepartment\"":{\""children\"":[],\""id\"":1,\""members\"":[],\""name\"":\""é¨é¨1\""}}""));
    }
"
"    @Test
    public void test5() throws Exception {
        mockMvc.perform(
                (post(""/fastjsonview/test5"").characterEncoding(""UTF-8"")
                        .contentType(MediaType.APPLICATION_JSON))).andExpect(status
                ().isOk()).andDo(print()).andExpect(content().string(""{\""description\"":\""fastjsonviewæ³¨è§£æµè¯\"",\""id\"":100,\""name\"":\""æµè¯\"",\""rootDepartment\"":{\""children\"":[],\""id\"":1,\""members\"":[],\""name\"":\""é¨é¨1\""},\""stock\"":\""haha\""}""));
    }
"
"    @Test
    public void test6() throws Exception {
        mockMvc.perform(
                (post(""/fastjsonview/test6"").characterEncoding(""UTF-8"")
                        .contentType(MediaType.APPLICATION_JSON))).andExpect(status
                ().isOk()).andDo(print()).andExpect(content().string(""{\""id\"":100}""));
    }
"
"    @Test
    public void test1() throws Exception {

        JSONObject json = new JSONObject();

        json.put(""id"", 123);

        json.put(""name"", ""ååå"");

        mockMvc.perform(
                (post(""/fastjson/test1"").characterEncoding(""UTF-8"").content(
                        json.toJSONString())
                        .contentType(MediaType.APPLICATION_JSON)))
                // .andExpect(status().isOk())
                .andDo(print());
    }
"
"    @Test
    public void test2() throws Exception {

        String jsonStr = ""[{\""name\"":\""p1\"",\""sonList\"":[{\""name\"":\""s1\""}]},{\""name\"":\""p2\"",\""sonList\"":[{\""name\"":\""s2\""},{\""name\"":\""s3\""}]}]"";

        mockMvc.perform(
                (post(""/fastjson/test2"").characterEncoding(""UTF-8"").content(
                        jsonStr).contentType(MediaType.APPLICATION_JSON)))
                // .andExpect(status().isOk())
                .andDo(print());
    }
"
"    @Test
    public void test3() throws Exception {
        List<Object> list = this.mockMvc.perform(post(""/fastjson/test3""))
                .andReturn().getResponse().getHeaderValues(""Content-Length"");
        Assert.assertNotEquals(list.size(), 0);
    }
"
"    @Test
    public void test4() throws Exception {

        String jsonStr = ""{\""t\"":{\""id\"":123,\""name\"":\""ååå\""}}"";

        mockMvc.perform(
                (post(""/fastjson/test4"").characterEncoding(""UTF-8"").content(
                        jsonStr).contentType(MediaType.APPLICATION_JSON)))
                .andDo(print());
    }
"
"    @Test
    public void test5() throws Exception {

        String jsonStr = ""{\""packet\"":{\""smsType\"":\""USER_LOGIN\""}}"";

        mockMvc.perform(
                (post(""/fastjson/test5"").characterEncoding(""UTF-8"").content(
                        jsonStr).contentType(MediaType.APPLICATION_JSON)))
                .andDo(print());
    }
"
"    @Test
    public void test6() throws Exception {

        mockMvc.perform(
                (post(""/fastjson/test6"").characterEncoding(""UTF-8"")
                        .param(""userId"", ""1234"")
                        .param(""flag"", ""0"")
                        .contentType(MediaType.APPLICATION_FORM_URLENCODED)))
                .andDo(print());
    }
"
"    @Test
    public void isInjectComponent() {
        wac.getBean(FastJsonpResponseBodyAdvice.class);
    }
"
"    @Test
    public void test1() throws Exception {

        JSONObject json = new JSONObject();

        json.put(""id"", 123);

        json.put(""name"", ""ååå"");

        mockMvc.perform(
                (post(""/fastjson/test1"").characterEncoding(""UTF-8"").content(json.toJSONString())
                        .contentType(MediaType.APPLICATION_JSON))).andExpect(status().isOk()).andDo(print());
    }
"
"    @Test
    public void test1_2() throws Exception {

        JSONObject json = new JSONObject();

        json.put(""id"", 123);

        json.put(""name"", ""ååå"");

        ResultActions actions = mockMvc.perform((post(""/fastjson/test1?callback=fnUpdateSome"").characterEncoding(
                ""UTF-8"").content(json.toJSONString()).contentType(MediaType.APPLICATION_JSON)));
        actions.andDo(print());
        actions.andExpect(status().isOk()).andExpect(content().contentType(APPLICATION_JAVASCRIPT))
                .andExpect(content().string(""/**/fnUpdateSome({\""name\"":\""ååå\"",\""id\"":123})""));
    }
"
"    @Test
    public void test2() throws Exception {

        String jsonStr = ""[{\""name\"":\""p1\"",\""sonList\"":[{\""name\"":\""s1\""}]},{\""name\"":\""p2\"",\""sonList\"":[{\""name\"":\""s2\""},{\""name\"":\""s3\""}]}]"";

        mockMvc.perform(
                (post(""/fastjson/test2"").characterEncoding(""UTF-8"").content(jsonStr)
                        .contentType(MediaType.APPLICATION_JSON))).andExpect(status().isOk()).andDo(print());
    }
"
"    @Test
    public void test2_2() throws Exception {

        String jsonStr = ""[{\""name\"":\""p1\"",\""sonList\"":[{\""name\"":\""s1\""}]},{\""name\"":\""p2\"",\""sonList\"":[{\""name\"":\""s2\""},{\""name\"":\""s3\""}]}]"";

        ResultActions actions = mockMvc.perform((post(""/fastjson/test2?jsonp=fnUpdateSome"").characterEncoding(""UTF-8"")
                .content(jsonStr).contentType(MediaType.APPLICATION_JSON)));
        actions.andDo(print());
        actions.andExpect(status().isOk()).andExpect(content().contentType(APPLICATION_JAVASCRIPT))
                .andExpect(content().string(""/**/fnUpdateSome({\""p1\"":1,\""p2\"":2})""));
    }
"
"    @Test
    public void test3() throws Exception {
        List<Object> list = this.mockMvc.perform(post(""/fastjson/test3"")).andReturn().getResponse()
                .getHeaderValues(""Content-Length"");
        Assert.assertNotEquals(list.size(), 0);
    }
"
"    @Test
    public void test3_2() throws Exception {
        ResultActions actions = this.mockMvc.perform(post(""/fastjson/test3?jsonp=fnUpdateSome""));
        actions.andDo(print());
        actions.andExpect(status().isOk()).andExpect(content().contentType(APPLICATION_JAVASCRIPT))
                .andExpect(content().string(""/**/fnUpdateSome({})""));
    }
"
"    @Test
    public void test4() throws Exception {

        String jsonStr = ""{\""t\"":{\""id\"":123,\""name\"":\""ååå\""}}"";

        mockMvc.perform(
                (post(""/fastjson/test4"").characterEncoding(""UTF-8"").content(jsonStr)
                        .contentType(MediaType.APPLICATION_JSON))).andDo(print());
    }
"
"    @Test
    public void test4_2() throws Exception {

        String jsonStr = ""{\""t\"":{\""id\"":123,\""name\"":\""ååå\""}}"";

        ResultActions actions = mockMvc.perform((post(""/fastjson/test4?callback=myUpdate"").characterEncoding(""UTF-8"")
                .content(jsonStr).contentType(MediaType.APPLICATION_JSON)));
        actions.andDo(print());
        actions.andExpect(status().isOk())
                .andExpect(content().contentType(APPLICATION_JAVASCRIPT))
                .andExpect(content().string(""/**/myUpdate(\""{\\\""t\\\"":{\\\""id\\\"":123,\\\""name\\\"":\\\""ååå\\\""}}\"")""));
    }
"
"    @Test
    public void test5() throws Exception {

        String jsonStr = ""{\""packet\"":{\""smsType\"":\""USER_LOGIN\""}}"";

        mockMvc.perform(
                (post(""/fastjson/test5"").characterEncoding(""UTF-8"").content(jsonStr)
                        .contentType(MediaType.APPLICATION_JSON))).andDo(print());
    }
"
"  @Test
  public void testSpark2Service() throws Exception {
    ApplicationManager applicationManager = deploy(NamespaceId.DEFAULT, Spark2TestApp.class);
    SparkManager manager = applicationManager.getSparkManager(ScalaSparkServiceProgram.class.getSimpleName()).start();

    URL url = manager.getServiceURL(5, TimeUnit.MINUTES);
    Assert.assertNotNull(url);

    // GET request to sum n numbers.
    URL sumURL = url.toURI().resolve(""sum?n="" + Joiner.on(""&n="").join(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)).toURL();
    HttpURLConnection urlConn = (HttpURLConnection) sumURL.openConnection();
    Assert.assertEquals(HttpURLConnection.HTTP_OK, urlConn.getResponseCode());
    try (InputStream is = urlConn.getInputStream()) {
      Assert.assertEquals(55, Integer.parseInt(new String(ByteStreams.toByteArray(is), StandardCharsets.UTF_8)));
    }
  }
"
"  @Test
  public void testSparkWithObjectStore() throws Exception {
    ApplicationManager applicationManager = deploy(NamespaceId.DEFAULT, SparkAppUsingObjectStore.class);

    DataSetManager<ObjectStore<String>> keysManager = getDataset(""keys"");
    prepareInputData(keysManager);

    SparkManager sparkManager = applicationManager.getSparkManager(CharCountProgram.class.getSimpleName()).start();
    sparkManager.waitForRun(ProgramRunStatus.RUNNING, 10, TimeUnit.SECONDS);
    sparkManager.waitForStopped(60, TimeUnit.SECONDS);

    DataSetManager<KeyValueTable> countManager = getDataset(""count"");
    checkOutputData(countManager);

    // validate that the table emitted metrics
    // one read + one write in beforeSubmit(), increment (= read + write) in main -> 4
    Tasks.waitFor(4L, new Callable<Long>() {
      @Override
      public Long call() throws Exception {
        Collection<MetricTimeSeries> metrics =
          getMetricsManager().query(new MetricDataQuery(
            0,
            System.currentTimeMillis() / 1000L,
            Integer.MAX_VALUE,
            ""system."" + Constants.Metrics.Name.Dataset.OP_COUNT,
            AggregationFunction.SUM,
            ImmutableMap.of(Constants.Metrics.Tag.NAMESPACE, DefaultId.NAMESPACE.getNamespace(),
                            Constants.Metrics.Tag.APP, SparkAppUsingObjectStore.class.getSimpleName(),
                            Constants.Metrics.Tag.SPARK, CharCountProgram.class.getSimpleName(),
                            Constants.Metrics.Tag.DATASET, ""totals""),
            Collections.<String>emptyList()));
        if (metrics.isEmpty()) {
          return 0L;
        }
        Assert.assertEquals(1, metrics.size());
        MetricTimeSeries ts = metrics.iterator().next();
        Assert.assertEquals(1, ts.getTimeValues().size());
        return ts.getTimeValues().get(0).getValue();
      }
"
"  @Test
  public void testScalaSparkWithObjectStore() throws Exception {
    ApplicationManager applicationManager = deploy(NamespaceId.DEFAULT, SparkAppUsingObjectStore.class);

    DataSetManager<ObjectStore<String>> keysManager = getDataset(""keys"");
    prepareInputData(keysManager);

    SparkManager sparkManager = applicationManager.getSparkManager(ScalaCharCountProgram.class.getSimpleName()).start();
    sparkManager.waitForRun(ProgramRunStatus.RUNNING, 10, TimeUnit.SECONDS);
    sparkManager.waitForStopped(60, TimeUnit.SECONDS);

    DataSetManager<KeyValueTable> countManager = getDataset(""count"");
    checkOutputData(countManager);
  }
"
"  @Test
  public void testScalaSparkCrossNSStream() throws Exception {
    // create a namespace for input and create a file set instance
    NamespaceMeta inputNSMeta = new NamespaceMeta.Builder().setName(""inputSpaceForSpark"").build();
    getNamespaceAdmin().create(inputNSMeta);
    DatasetId inputDatasetId = inputNSMeta.getNamespaceId().dataset(""input"");
    addDatasetInstance(FileSet.class.getName(), inputDatasetId,
                       FileSetProperties.builder().setInputFormat(TextInputFormat.class).build());

    // create a namespace for dataset and add the dataset instance in it
    NamespaceMeta outputNSMeta = new NamespaceMeta.Builder().setName(""crossNSDataset"").build();
    getNamespaceAdmin().create(outputNSMeta);
    addDatasetInstance(outputNSMeta.getNamespaceId().dataset(""count""), ""keyValueTable"");

    // write something to the input dataset
    Location inputFile = this.<FileSet>getDataset(inputDatasetId).get().getLocation(""inputFile"");
    try (PrintStream printer = new PrintStream(inputFile.getOutputStream(), true, ""UTF-8"")) {
      for (int i = 0; i < 50; i++) {
        printer.println(String.valueOf(i));
      }
    }

    // deploy the spark app in another namespace (default)
    ApplicationManager applicationManager = deploy(NamespaceId.DEFAULT, SparkAppUsingObjectStore.class);

    Map<String, String> args = new HashMap<>();
    args.put(ScalaCrossNSProgram.INPUT_NAMESPACE(), inputNSMeta.getNamespaceId().getNamespace());
    args.put(ScalaCrossNSProgram.OUTPUT_NAMESPACE(), outputNSMeta.getNamespaceId().getNamespace());
    args.put(ScalaCrossNSProgram.OUTPUT_NAME(), ""count"");

    FileSetArguments.setInputPath(args, ""inputFile"");

    SparkManager sparkManager =
      applicationManager.getSparkManager(ScalaCrossNSProgram.class.getSimpleName()).start(args);
    sparkManager.waitForRun(ProgramRunStatus.RUNNING, 10, TimeUnit.SECONDS);
    sparkManager.waitForStopped(60, TimeUnit.SECONDS);

    // get the dataset from the other namespace where we expect it to exist and compare the data
    DataSetManager<KeyValueTable> countManager = getDataset(outputNSMeta.getNamespaceId().dataset(""count""));
    KeyValueTable results = countManager.get();
    for (int i = 0; i < 50; i++) {
      byte[] key = String.valueOf(i).getBytes(Charsets.UTF_8);
      Assert.assertArrayEquals(key, results.read(key));
    }
  }
"
"  @Test
  public void testScalaSparkCrossNSDataset() throws Exception {
    // Deploy and create a dataset in namespace datasetSpaceForSpark
    NamespaceMeta inputDSNSMeta = new NamespaceMeta.Builder().setName(""datasetSpaceForSpark"").build();
    getNamespaceAdmin().create(inputDSNSMeta);
    deploy(inputDSNSMeta.getNamespaceId(), SparkAppUsingObjectStore.class);
    DataSetManager<ObjectStore<String>> keysManager = getDataset(inputDSNSMeta.getNamespaceId().dataset(""keys""));
    prepareInputData(keysManager);

    Map<String, String> args = ImmutableMap.of(ScalaCharCountProgram.INPUT_DATASET_NAMESPACE(),
                                               inputDSNSMeta.getNamespaceId().getNamespace(),
                                               ScalaCharCountProgram.INPUT_DATASET_NAME(), ""keys"");

    ApplicationManager applicationManager = deploy(NamespaceId.DEFAULT, SparkAppUsingObjectStore.class);
    SparkManager sparkManager =
      applicationManager.getSparkManager(ScalaCharCountProgram.class.getSimpleName()).start(args);
    sparkManager.waitForRun(ProgramRunStatus.RUNNING, 10, TimeUnit.SECONDS);
    sparkManager.waitForStopped(60, TimeUnit.SECONDS);

    DataSetManager<KeyValueTable> countManager = getDataset(""count"");
    checkOutputData(countManager);
  }
"
"  @Test
  public void testSparkWithLocalFiles() throws Exception {
    testSparkWithLocalFiles(SparkAppUsingLocalFiles.class,
                            SparkAppUsingLocalFiles.JavaSparkUsingLocalFiles.class.getSimpleName(), ""java"");
    testSparkWithLocalFiles(SparkAppUsingLocalFiles.class,
                            SparkAppUsingLocalFiles.ScalaSparkUsingLocalFiles.class.getSimpleName(), ""scala"");
  }
"
"  @Test
  public void testPySpark() throws Exception {
    if (TestBase.getCurrentSparkCompat() == SparkCompat.SPARK3_2_12) {
      //For spark 3 we need python 3, otherwise skip test
      try {
        Process python = new ProcessBuilder(""python3"", ""-V"").start();
        int resultCode = python.waitFor();
        assumeTrue(""Python3 returned error, result code: "" + resultCode,
                   resultCode == 0);
      } catch (IOException e) {
        assumeNoException(""Python3 can't be started"", e);
      }
    }
    ApplicationManager appManager = deploy(NamespaceId.DEFAULT, Spark2TestApp.class);

    // Write some data to a local file
    File inputFile = TEMP_FOLDER.newFile();
    try (BufferedWriter writer = Files.newBufferedWriter(inputFile.toPath(), StandardCharsets.UTF_8)) {
      for (int i = 0; i < 100; i++) {
        writer.write(""Event "" + i);
        writer.newLine();
      }
    }

    File outputDir = new File(TMP_FOLDER.newFolder(), ""output"");
    appManager.getSparkManager(PythonSpark2.class.getSimpleName()).startAndWaitForGoodRun(
      ImmutableMap.of(""input.file"", inputFile.getAbsolutePath(),
                      ""output.path"", outputDir.getAbsolutePath()),
      ProgramRunStatus.COMPLETED, 2, TimeUnit.MINUTES);

    // Verify the result
    File resultFile = DirUtils.listFiles(outputDir).stream()
      .filter(f -> !f.getName().endsWith("".crc""))
      .filter(f -> !f.getName().startsWith(""_SUCCESS""))
      .findFirst()
      .orElse(null);
    Assert.assertNotNull(resultFile);

    List<String> lines = Files.readAllLines(resultFile.toPath(), StandardCharsets.UTF_8);
    Assert.assertFalse(lines.isEmpty());

    // Expected only even number
    int count = 0;
    for (String line : lines) {
      line = line.trim();
      if (!line.isEmpty()) {
        Assert.assertEquals(""Event "" + count, line);
        count += 2;
      }
    }

    Assert.assertEquals(100, count);

    final Map<String, String> tags = ImmutableMap.of(
      Constants.Metrics.Tag.NAMESPACE, NamespaceId.DEFAULT.getNamespace(),
      Constants.Metrics.Tag.APP, Spark2TestApp.class.getSimpleName(),
      Constants.Metrics.Tag.SPARK, PythonSpark2.class.getSimpleName());

    Tasks.waitFor(100L, () ->  getMetricsManager().getTotalMetric(tags, ""user.body""),
                  5, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);
  }
"
"  @Test
  public void testDeployApplicationInNamespace() throws Exception {
    NamespaceId namespace = new NamespaceId(""Test1"");
    NamespaceMeta namespaceMeta = new NamespaceMeta.Builder().setName(namespace).build();
    getNamespaceClient().create(namespaceMeta);
    ClientConfig clientConfig = new ClientConfig.Builder(getClientConfig()).build();
    deployApplication(namespace, AllProgramsApp.class);

    // Check the default namespaces applications to see whether the application wasn't made in the default namespace
    ClientConfig defaultClientConfig = new ClientConfig.Builder(getClientConfig()).build();
    Assert.assertTrue(new ApplicationClient(defaultClientConfig).list(NamespaceId.DEFAULT).isEmpty());

    ApplicationClient applicationClient = new ApplicationClient(clientConfig);
    Assert.assertEquals(AllProgramsApp.NAME, applicationClient.list(namespace).get(0).getName());
    applicationClient.delete(namespace.app(AllProgramsApp.NAME));
    Assert.assertTrue(new ApplicationClient(clientConfig).list(namespace).isEmpty());

  }
"
"  @Test
  public void testSQLQuery() throws Exception {
    getTestManager().deployDatasetModule(NamespaceId.DEFAULT.datasetModule(""my-kv""), AppUsingCustomModule.Module.class);

    DatasetAdmin dsAdmin = getTestManager().addDatasetInstance(""myKeyValueTable"",
                                                               NamespaceId.DEFAULT.dataset(""myTable""));
    Assert.assertTrue(dsAdmin.exists());

    ApplicationManager appManager = deployApplication(NamespaceId.DEFAULT, AppUsingCustomModule.class);
    ServiceManager serviceManager = appManager.getServiceManager(""MyService"").start();
    serviceManager.waitForRun(ProgramRunStatus.RUNNING, 10, TimeUnit.SECONDS);

    put(serviceManager, ""a"", ""1"");
    put(serviceManager, ""b"", ""2"");
    put(serviceManager, ""c"", ""1"");

    try (
      Connection connection = getTestManager().getQueryClient(NamespaceId.DEFAULT);
      // the value (character) ""1"" corresponds to the decimal 49. In hex, that is 31.
      ResultSet results = connection.prepareStatement(""select key from dataset_mytable where hex(value) = '31'"")
        .executeQuery()
    ) {
      // run a query over the dataset
      Assert.assertTrue(results.next());
      Assert.assertEquals(""a"", results.getString(1));
      Assert.assertTrue(results.next());
      Assert.assertEquals(""c"", results.getString(1));
      Assert.assertFalse(results.next());
    }

    dsAdmin.drop();
    Assert.assertFalse(dsAdmin.exists());
  }
"
"  @Test
  public void testLogBufferWriter() throws Exception {
    String absolutePath = TMP_FOLDER.newFolder().getAbsolutePath();

    LogBufferWriter writer = new LogBufferWriter(absolutePath, 100000, () -> { });
    ImmutableList<byte[]> events = getLoggingEvents();
    Iterator<LogBufferEvent> writtenEvents = writer.write(events.iterator()).iterator();
    writer.close();

    int i = 0;
    int startPos = 0;
    // verify if correct offsets were set without file rotation
    while (writtenEvents.hasNext()) {
      LogBufferEvent bufferEvent = writtenEvents.next();
      Assert.assertEquals(String.valueOf(i++), bufferEvent.getLogEvent().getMessage());
      // There will not be any rotation.
      Assert.assertEquals(bufferEvent.getOffset().getFilePos(), startPos);
      startPos = startPos + Bytes.SIZEOF_INT + serializer.toBytes(bufferEvent.getLogEvent()).length;
    }

    // verify if the events were serialized and written correctly
    try (DataInputStream dis = new DataInputStream(new FileInputStream(absolutePath + ""/0.buf""))) {
      for (byte[] eventBytes : events) {
        ILoggingEvent event = serializer.fromBytes(ByteBuffer.wrap(eventBytes));
        Assert.assertEquals(event.getMessage(), getEvent(dis, serializer.toBytes(event).length).getMessage());
      }
    }
  }
"
"  @Test
  public void testFileRotation() throws Exception {
    // Make sure rotation happens after every event is written
    LogBufferWriter writer = new LogBufferWriter(TMP_FOLDER.newFolder().getAbsolutePath(), 10, () -> { });
    ImmutableList<byte[]> events = getLoggingEvents();
    Iterator<LogBufferEvent> writtenEvents = writer.write(events.iterator()).iterator();
    writer.close();

    int i = 0;
    // verify if correct offsets and file id were set with file rotation
    while (writtenEvents.hasNext()) {
      LogBufferEvent bufferEvent = writtenEvents.next();
      Assert.assertEquals(bufferEvent.getLogEvent().getMessage(), String.valueOf(i));
      // There will be 2 events in one file.
      Assert.assertEquals(i++ + "".buf"", bufferEvent.getOffset().getFileId() + "".buf"");
      Assert.assertEquals(0, bufferEvent.getOffset().getFilePos());
    }
  }
"
"  @Test (expected = IOException.class)
  public void testWritesOnClosedWriter() throws IOException {
    LogBufferWriter writer = new LogBufferWriter(TMP_FOLDER.newFolder().getAbsolutePath(), 100000, () -> { });
    writer.close();
    // should throw IOException
    writer.write(ImmutableList.of(
      serializer.toBytes(createLoggingEvent(""test.logger"", Level.INFO, ""0"", 1,
                                            new WorkerLoggingContext(""default"", ""app1"", ""worker1"", ""run1"",
                                                                     ""instance1"")))).iterator());
  }
"
"  @Test
  public void testLogBufferRecoveryService() throws Exception {
    String absolutePath = TMP_FOLDER.newFolder().getAbsolutePath();

    // create and start pipeline
    LoggerContext loggerContext = LogPipelineTestUtil.createLoggerContext(""WARN"",
                                                                          ImmutableMap.of(""test.logger"", ""INFO""),
                                                                          MockAppender.class.getName());
    final MockAppender appender = LogPipelineTestUtil.getAppender(loggerContext.getLogger(Logger.ROOT_LOGGER_NAME),
                                                                  ""Test"", MockAppender.class);
    MockCheckpointManager checkpointManager = new MockCheckpointManager();
    LogBufferPipelineConfig config = new LogBufferPipelineConfig(1024L, 300L, 500L, 4);
    loggerContext.start();
    LogBufferProcessorPipeline pipeline = new LogBufferProcessorPipeline(
      new LogProcessorPipelineContext(CConfiguration.create(), ""test"", loggerContext, NO_OP_METRICS_CONTEXT, 0),
      config, checkpointManager, 0);

    // start the pipeline
    pipeline.startAndWait();

    // write directly to log buffer
    LogBufferWriter writer = new LogBufferWriter(absolutePath, 250, () -> { });
    ImmutableList<byte[]> events = getLoggingEvents();
    writer.write(events.iterator()).iterator();
    writer.close();

    // start log buffer reader to read log events from files. keep the batch size as 2 so that there are more than 1
    // iterations
    LogBufferRecoveryService service = new LogBufferRecoveryService(ImmutableList.of(pipeline),
                                                                    ImmutableList.of(checkpointManager),
                                                                    absolutePath, 2, new AtomicBoolean(true));
    service.startAndWait();

    Tasks.waitFor(5, () -> appender.getEvents().size(), 120, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);

    service.stopAndWait();
    pipeline.stopAndWait();
    loggerContext.stop();
  }
"
"  @Test
  public void testWrites() throws Exception {
    CConfiguration cConf = CConfiguration.create();
    String absolutePath = TMP_FOLDER.newFolder().getAbsolutePath();
    cConf.set(Constants.LogBuffer.LOG_BUFFER_BASE_DIR, absolutePath);
    cConf.setLong(Constants.LogBuffer.LOG_BUFFER_MAX_FILE_SIZE_BYTES, 100000);

    LoggerContext loggerContext = LogPipelineTestUtil
      .createLoggerContext(""WARN"", ImmutableMap.of(""test.logger"", ""INFO""), MockAppender.class.getName());
    final MockAppender appender =
      LogPipelineTestUtil.getAppender(loggerContext.getLogger(ch.qos.logback.classic.Logger.ROOT_LOGGER_NAME),
                                      ""Test"", MockAppender.class);
    MockCheckpointManager checkpointManager = new MockCheckpointManager();
    LogBufferPipelineConfig config = new LogBufferPipelineConfig(1024L, 300L, 500L, 4);
    loggerContext.start();
    LogBufferProcessorPipeline pipeline = new LogBufferProcessorPipeline(
      new LogProcessorPipelineContext(CConfiguration.create(), ""test"", loggerContext, NO_OP_METRICS_CONTEXT, 0),
      config, checkpointManager, 0);
    // start the pipeline
    pipeline.startAndWait();

    ConcurrentLogBufferWriter writer = new ConcurrentLogBufferWriter(cConf, ImmutableList.of(pipeline), () -> { });
    ImmutableList<byte[]> events = getLoggingEvents();
    writer.process(new LogBufferRequest(0, events));

    // verify if the events were written to log buffer
    try (DataInputStream dis = new DataInputStream(new FileInputStream(absolutePath + ""/0.buf""))) {
      for (byte[] eventBytes : events) {
        ILoggingEvent event = serializer.fromBytes(ByteBuffer.wrap(eventBytes));
        Assert.assertEquals(event.getMessage(), getEvent(dis, serializer.toBytes(event).length).getMessage());
      }
    }

    // verify if the pipeline has processed the messages.
    Tasks.waitFor(5, () -> appender.getEvents().size(), 60, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);
    pipeline.stopAndWait();
    loggerContext.stop();
  }
"
"  @Test
  public void testConcurrentWrites() throws Exception {
    int threadCount = 20;

    CConfiguration cConf = CConfiguration.create();
    String absolutePath = TMP_FOLDER.newFolder().getAbsolutePath();
    cConf.set(Constants.LogBuffer.LOG_BUFFER_BASE_DIR, absolutePath);
    cConf.setLong(Constants.LogBuffer.LOG_BUFFER_MAX_FILE_SIZE_BYTES, 100000);

    LoggerContext loggerContext = LogPipelineTestUtil
      .createLoggerContext(""WARN"", ImmutableMap.of(""test.logger"", ""INFO""), MockAppender.class.getName());
    final MockAppender appender =
      LogPipelineTestUtil.getAppender(loggerContext.getLogger(ch.qos.logback.classic.Logger.ROOT_LOGGER_NAME),
                                      ""Test"", MockAppender.class);
    MockCheckpointManager checkpointManager = new MockCheckpointManager();
    LogBufferPipelineConfig config = new LogBufferPipelineConfig(1024L, 300L, 500L, 4);
    loggerContext.start();
    LogBufferProcessorPipeline pipeline = new LogBufferProcessorPipeline(
      new LogProcessorPipelineContext(CConfiguration.create(), ""test"", loggerContext, NO_OP_METRICS_CONTEXT, 0),
      config, checkpointManager, 0);
    // start the pipeline
    pipeline.startAndWait();

    ConcurrentLogBufferWriter writer = new ConcurrentLogBufferWriter(cConf, ImmutableList.of(pipeline), () -> { });
    ImmutableList<byte[]> events = getLoggingEvents();

    ExecutorService executor = Executors.newFixedThreadPool(threadCount);
    final CyclicBarrier barrier = new CyclicBarrier(threadCount + 1);
    for (int i = 0; i < threadCount; i++) {
      executor.submit(() -> {
        try {
          barrier.await();
          writer.process(new LogBufferRequest(0, events));
        } catch (Exception e) {
          LOG.error(""Exception raised when processing log events."", e);
        }
      });
    }

    barrier.await();
    executor.shutdown();
    Assert.assertTrue(executor.awaitTermination(1, TimeUnit.MINUTES));

    // verify if the events were written to log buffer
    try (DataInputStream dis = new DataInputStream(new FileInputStream(absolutePath + ""/0.buf""))) {
      for (int i = 0; i < threadCount; i++) {
        for (byte[] eventBytes : events) {
          ILoggingEvent event = serializer.fromBytes(ByteBuffer.wrap(eventBytes));
          Assert.assertEquals(event.getMessage(), getEvent(dis, serializer.toBytes(event).length).getMessage());
        }
      }
    }

    // verify if the pipeline has processed the messages.
    Tasks.waitFor(100, () -> appender.getEvents().size(), 60, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);
    pipeline.stopAndWait();
    loggerContext.stop();
  }
"
"  @Test
  public void testHandler() throws Exception {
    CConfiguration cConf = CConfiguration.create();
    String absolutePath = TMP_FOLDER.newFolder().getAbsolutePath();
    cConf.set(Constants.LogBuffer.LOG_BUFFER_BASE_DIR, absolutePath);
    cConf.setLong(Constants.LogBuffer.LOG_BUFFER_MAX_FILE_SIZE_BYTES, 100000);

    LoggerContext loggerContext = LogPipelineTestUtil
      .createLoggerContext(""WARN"", ImmutableMap.of(""test.logger"", ""INFO""), MockAppender.class.getName());
    final MockAppender appender =
      LogPipelineTestUtil.getAppender(loggerContext.getLogger(ch.qos.logback.classic.Logger.ROOT_LOGGER_NAME),
                                      ""Test"", MockAppender.class);

    LogBufferProcessorPipeline pipeline = getLogPipeline(loggerContext);
    pipeline.startAndWait();

    ConcurrentLogBufferWriter writer = new ConcurrentLogBufferWriter(cConf, ImmutableList.of(pipeline), () -> { });

    NettyHttpService httpService = NettyHttpService.builder(""RemoteAppenderTest"")
      .setHttpHandlers(new LogBufferHandler(writer))
      .setExceptionHandler(new HttpExceptionHandler())
      .build();

    httpService.start();

    RemoteLogAppender remoteLogAppender = getRemoteAppender(cConf, httpService);
    remoteLogAppender.start();

    List<ILoggingEvent> events = getLoggingEvents();
    WorkerLoggingContext loggingContext =
      new WorkerLoggingContext(""default"", ""app1"", ""worker1"", ""run1"", ""instance1"");
    for (int i = 0; i < 1000; i++) {
      remoteLogAppender.append(new LogMessage(events.get(i % events.size()), loggingContext));
    }

    Tasks.waitFor(1000, () -> appender.getEvents().size(), 120, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);

    remoteLogAppender.stop();
    httpService.stop();
    pipeline.stopAndWait();
    loggerContext.stop();
  }
"
"  @Test
  public void testLogBufferCleanerService() throws Exception {
    String absolutePath = TMP_FOLDER.newFolder().getAbsolutePath();

    MockCheckpointManager checkpointManager = new MockCheckpointManager();
    // update checkpoints
    checkpointManager.saveCheckpoints(ImmutableMap.of(0, new TestCheckpoint(2L, 0L, 1L)));

    // write directly to log buffer, keep file size 10 bytes so that more files are created
    LogBufferWriter writer = new LogBufferWriter(absolutePath, 10,
                                                 new LogBufferCleaner(ImmutableList.of(checkpointManager),
                                                                      absolutePath, new AtomicBoolean(true)));
    ImmutableList<byte[]> events = getLoggingEvents();
    List<byte[]> subset = new ArrayList<>();
    subset.add(events.get(0));
    subset.add(events.get(1));
    subset.add(events.get(2));
    writer.write(subset.iterator()).iterator();

    // should delete file 0 an1
    File file0 = new File(absolutePath, ""0.buff"");
    File file1 = new File(absolutePath, ""1.buff"");
    Tasks.waitFor(true, () -> !file0.exists() && !file1.exists(), 120, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);

    // update checkpoints
    checkpointManager.saveCheckpoints(ImmutableMap.of(0, new TestCheckpoint(5L, 0L, 1L)));

    subset.add(events.get(3));
    subset.add(events.get(4));
    subset.add(events.get(5));
    writer.write(subset.iterator()).iterator();
    writer.close();

    // should delete file 2, 3 and 4
    File file2 = new File(absolutePath, ""2.buff"");
    File file3 = new File(absolutePath, ""3.buff"");
    File file4 = new File(absolutePath, ""4.buff"");
    Tasks.waitFor(true, () -> !file2.exists() && !file3.exists() && !file4.exists(), 120, TimeUnit.SECONDS,
                  100, TimeUnit.MILLISECONDS);

  }
"
"  @Test
  public void testLogReader() throws Exception {
    String absolutePath = TMP_FOLDER.newFolder().getAbsolutePath();

    LogBufferWriter writer = new LogBufferWriter(absolutePath, 250, () -> { });
    ImmutableList<byte[]> events = getLoggingEvents();
    Iterable<LogBufferEvent> writtenEvents = writer.write(events.iterator());
    writer.close();

    List<LogBufferEvent> logBufferEvents = new LinkedList<>();
    // read from start positions, tests case where no checkpoints are persisted
    LogBufferReader reader = new LogBufferReader(absolutePath, 2, 3, -1, -1);
    Iterator<LogBufferEvent> iterator = writtenEvents.iterator();
    verifyEvents(logBufferEvents, reader, iterator);
    reader.close();

    // this should skip first and second event, this is because log buffer offsets are offset for event that is
    // already stored. so in this case, skip first event and skip second event as second event is the last stored event
    reader = new LogBufferReader(absolutePath, 2, 3, 0, 145);
    iterator = writtenEvents.iterator();
    iterator.next();
    iterator.next();
    verifyEvents(logBufferEvents, reader, iterator);
    reader.close();
  }
"
"  @Test
  public void testSingleAppender() throws Exception {
    LoggerContext loggerContext = LogPipelineTestUtil.createLoggerContext(""WARN"",
                                                                          ImmutableMap.of(""test.logger"", ""INFO""),
                                                                          MockAppender.class.getName());
    final MockAppender appender = LogPipelineTestUtil.getAppender(loggerContext.getLogger(Logger.ROOT_LOGGER_NAME),
                                                                  ""Test"", MockAppender.class);
    MockCheckpointManager checkpointManager = new MockCheckpointManager();
    LogBufferPipelineConfig config = new LogBufferPipelineConfig(1024L, 300L, 500L, 4);
    loggerContext.start();
    LogBufferProcessorPipeline pipeline = new LogBufferProcessorPipeline(
      new LogProcessorPipelineContext(CConfiguration.create(), ""test"", loggerContext, NO_OP_METRICS_CONTEXT, 0),
      config, checkpointManager, 0);
    // start the pipeline
    pipeline.startAndWait();

    // start thread to write to incomingEventQueue
    List<ILoggingEvent> events = getLoggingEvents();
    AtomicInteger i = new AtomicInteger(0);
    List<LogBufferEvent> bufferEvents = events.stream().map(event -> {
      LogBufferEvent lbe = new LogBufferEvent(event, serializer.toBytes(event).length,
                                              new LogBufferFileOffset(0, i.get()));
      i.incrementAndGet();
      return lbe;
    }).collect(Collectors.toList());

    // start a thread to send log buffer events to pipeline
    ExecutorService executorService = Executors.newSingleThreadExecutor();
    executorService.execute(() -> {
      for (int count = 0; count < 40; count++) {
        pipeline.processLogEvents(bufferEvents.iterator());
        try {
          Thread.sleep(100);
        } catch (InterruptedException e) {
          // should not happen
        }
      }
    });

    // wait for pipeline to append all the logs to appender. The DEBUG message should get filtered out.
    Tasks.waitFor(200, () -> appender.getEvents().size(), 60, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);
    executorService.shutdown();
    pipeline.stopAndWait();
    loggerContext.stop();
  }
"
"  @Test
  public void testBasicSort() throws Exception {
    String topic = ""testPipeline"";
    LoggerContext loggerContext = LogPipelineTestUtil.createLoggerContext(""WARN"",
                                                                          ImmutableMap.of(""test.logger"", ""INFO""),
                                                                          MockAppender.class.getName());
    final MockAppender appender = LogPipelineTestUtil.getAppender(loggerContext.getLogger(Logger.ROOT_LOGGER_NAME),
                                                                  ""Test"", MockAppender.class);
    TestCheckpointManager checkpointManager = new TestCheckpointManager();
    KafkaPipelineConfig config = new KafkaPipelineConfig(topic, Collections.singleton(0), 1024L, 300L, 1048576, 500L);
    KAFKA_TESTER.createTopic(topic, 1);

    loggerContext.start();
    KafkaLogProcessorPipeline pipeline = new KafkaLogProcessorPipeline(
      new LogProcessorPipelineContext(CConfiguration.create(), ""test"", loggerContext, NO_OP_METRICS_CONTEXT, 0),
      checkpointManager,
      KAFKA_TESTER.getBrokerService(), config);

    pipeline.startAndWait();

    // Publish some log messages to Kafka
    long now = System.currentTimeMillis();
    publishLog(topic, ImmutableList.of(
      LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""0"", now - 1000),
      LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""2"", now - 700),
      LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""3"", now - 500),
      LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""1"", now - 900),
      LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.DEBUG, ""hidden"", now - 600),
      LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""4"", now - 100))
    );

    // Since the messages are published in one batch, the processor should be able to fetch all of them,
    // hence the sorting order should be deterministic.
    // The DEBUG message should get filtered out
    Tasks.waitFor(5, () -> appender.getEvents().size(), 5, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);

    for (int i = 0; i < 5; i++) {
      Assert.assertEquals(Integer.toString(i), appender.getEvents().poll().getMessage());
    }

    // Now publish large messages that exceed the maximum queue size (1024). It should trigger writing regardless of
    // the event timestamp
    List<ILoggingEvent> events = new ArrayList<>(500);
    now = System.currentTimeMillis();
    for (int i = 0; i < 500; i++) {
      // The event timestamp is 10 seconds in future.
      events.add(LogPipelineTestUtil.createLoggingEvent(""test.large.logger"",
                                                        Level.WARN, ""Large logger "" + i, now + 10000));
    }
    publishLog(topic, events);

    Tasks.waitFor(true, () -> !appender.getEvents().isEmpty(), 5, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);

    events.clear();
    events.addAll(appender.getEvents());

    for (int i = 0; i < events.size(); i++) {
      Assert.assertEquals(""Large logger "" + i, events.get(i).getMessage());
    }

    pipeline.stopAndWait();
    loggerContext.stop();

    Assert.assertNull(appender.getEvents());
  }
"
"  @Test
  public void testRegularFlush() throws Exception {
    String topic = ""testFlush"";
    LoggerContext loggerContext = LogPipelineTestUtil.createLoggerContext(""WARN"",
                                                                          ImmutableMap.of(""test.logger"", ""INFO""),
                                                                          MockAppender.class.getName());
    final MockAppender appender = LogPipelineTestUtil.getAppender(loggerContext.getLogger(Logger.ROOT_LOGGER_NAME),
                                                                  ""Test"", MockAppender.class);
    TestCheckpointManager checkpointManager = new TestCheckpointManager();

    // Use a longer checkpoint time and a short event delay. Should expect flush called at least once
    // per event delay.
    KafkaPipelineConfig config = new KafkaPipelineConfig(topic, Collections.singleton(0), 1024, 100, 1048576, 2000);
    KAFKA_TESTER.createTopic(topic, 1);

    loggerContext.start();
    KafkaLogProcessorPipeline pipeline = new KafkaLogProcessorPipeline(
      new LogProcessorPipelineContext(CConfiguration.create(), ""test"", loggerContext, NO_OP_METRICS_CONTEXT, 0),
      checkpointManager,
      KAFKA_TESTER.getBrokerService(), config);

    pipeline.startAndWait();

    // Even when there is no event, the flush should still get called.
    Tasks.waitFor(5, appender::getFlushCount, 3, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);

    // Publish some logs
    long now = System.currentTimeMillis();
    publishLog(topic, ImmutableList.of(
      LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""0"", now - 500),
      LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""1"", now - 300),
      LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""2"", now + 100)
    ));

    // Wait until getting all logs.
    Tasks.waitFor(3, () -> appender.getEvents().size(), 3, TimeUnit.SECONDS, 200, TimeUnit.MILLISECONDS);

    pipeline.stopAndWait();

    // Should get at least 20 flush calls, since the checkpoint is every 2 seconds
    Assert.assertTrue(appender.getFlushCount() >= 20);
  }
"
"  @Test
  public void testMetricsAppender() throws Exception {
    Injector injector = KAFKA_TESTER.getInjector();
    MetricsCollectionService collectionService = injector.getInstance(MetricsCollectionService.class);
    collectionService.startAndWait();
    LoggerContext loggerContext = new LocalAppenderContext(injector.getInstance(TransactionRunner.class),
                                                           injector.getInstance(LocationFactory.class),
                                                           injector.getInstance(MetricsCollectionService.class));
    final File logDir = TEMP_FOLDER.newFolder();
    loggerContext.putProperty(""logDirectory"", logDir.getAbsolutePath());

    LogPipelineConfigurator configurator = new LogPipelineConfigurator(CConfiguration.create());
    configurator.setContext(loggerContext);
    URL configURL = getClass().getClassLoader().getResource(""pipeline-metric-appender.xml"");
    Assert.assertNotNull(configURL);
    configurator.doConfigure(configURL);

    String topic = ""metricsPipeline"";
    TestCheckpointManager checkpointManager = new TestCheckpointManager();
    KafkaPipelineConfig config = new KafkaPipelineConfig(topic, Collections.singleton(0), 1024L, 100L, 1048576, 200L);
    KAFKA_TESTER.createTopic(topic, 1);

    loggerContext.start();
    KafkaLogProcessorPipeline pipeline = new KafkaLogProcessorPipeline(
      new LogProcessorPipelineContext(CConfiguration.create(), ""testMetricAppender"",
                                      loggerContext, NO_OP_METRICS_CONTEXT, 0),
      checkpointManager,
      KAFKA_TESTER.getBrokerService(), config);

    pipeline.startAndWait();

    // Publish some log messages to Kafka
    long now = System.currentTimeMillis();
    WorkerLoggingContext loggingContext =
      new WorkerLoggingContext(""default"", ""app1"", ""worker1"", ""run1"", ""instance1"");
    publishLog(topic,
               ImmutableList.of(
                 LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""0"", now - 1000),
                 LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""2"", now - 700),
                 LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""3"", now - 500),
                 LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""1"", now - 900),
                 LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.DEBUG, ""hidden"", now - 600),
                 LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""4"", now - 100)),
               loggingContext
    );

    WorkflowProgramLoggingContext workflowProgramLoggingContext =
      new WorkflowProgramLoggingContext(""default"", ""app1"", ""wflow1"", ""run1"", ProgramType.MAPREDUCE, ""mr1"", ""mrun1"");

    publishLog(topic,
               ImmutableList.of(
                 LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.WARN, ""0"", now - 1000),
                 LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.WARN, ""2"", now - 700),
                 LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.TRACE, ""3"", now - 500)),
               workflowProgramLoggingContext
    );

    ServiceLoggingContext serviceLoggingContext =
      new ServiceLoggingContext(NamespaceId.SYSTEM.getNamespace(), Constants.Logging.COMPONENT_NAME,
                                Constants.Service.TRANSACTION);
    publishLog(topic,
               ImmutableList.of(
                 LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.ERROR, ""0"", now - 1000),
                 LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.ERROR, ""2"", now - 700),
                 LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.ERROR, ""3"", now - 500),
                 LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""1"", now - 900)),
               serviceLoggingContext
    );

    final MetricStore metricStore = injector.getInstance(MetricStore.class);
    try {
      verifyMetricsWithRetry(metricStore,
                             new MetricDataQuery(0, Integer.MAX_VALUE, Integer.MAX_VALUE,
                                                 ""system.app.log.info"",
                                                 AggregationFunction.SUM,
                                                 LoggingContextHelper.getMetricsTags(loggingContext),
                                                 new ArrayList<>()), 5L);

      verifyMetricsWithRetry(metricStore,
                             new MetricDataQuery(0, Integer.MAX_VALUE, Integer.MAX_VALUE,
                                                 ""system.app.log.debug"",
                                                 AggregationFunction.SUM,
                                                 LoggingContextHelper.getMetricsTags(loggingContext),
                                                 new ArrayList<>()), 1L);

      verifyMetricsWithRetry(metricStore,
                             new MetricDataQuery(0, Integer.MAX_VALUE, Integer.MAX_VALUE,
                                                 ""system.app.log.warn"",
                                                 AggregationFunction.SUM,
                                                 // mapreduce metrics context
                                                 ImmutableMap.of(Constants.Metrics.Tag.NAMESPACE, ""default"",
                                                                 Constants.Metrics.Tag.APP, ""app1"",
                                                                 Constants.Metrics.Tag.MAPREDUCE, ""mr1"",
                                                                 Constants.Metrics.Tag.RUN_ID, ""mrun1""),
                                                 new ArrayList<>()), 2L);
      verifyMetricsWithRetry(metricStore,
                             new MetricDataQuery(0, Integer.MAX_VALUE, Integer.MAX_VALUE,
                                                 ""system.app.log.trace"",
                                                 AggregationFunction.SUM,
                                                 // workflow metrics context
                                                 ImmutableMap.of(Constants.Metrics.Tag.NAMESPACE, ""default"",
                                                                 Constants.Metrics.Tag.APP, ""app1"",
                                                                 Constants.Metrics.Tag.WORKFLOW, ""wflow1"",
                                                                 Constants.Metrics.Tag.RUN_ID, ""run1""),
                                                 new ArrayList<>()), 1L);
      verifyMetricsWithRetry(metricStore,
                             new MetricDataQuery(0, Integer.MAX_VALUE, Integer.MAX_VALUE,
                                                 ""system.services.log.error"",
                                                 AggregationFunction.SUM,
                                                 LoggingContextHelper.getMetricsTags(serviceLoggingContext),
                                                 new ArrayList<>()), 3L);
    } finally {
      pipeline.stopAndWait();
      loggerContext.stop();
      collectionService.stopAndWait();
    }
  }
"
"  @Test
  public void testMultiAppenders() throws Exception {
    final File logDir = TEMP_FOLDER.newFolder();
    LoggerContext loggerContext = new LoggerContext();
    loggerContext.putProperty(""logDirectory"", logDir.getAbsolutePath());

    LogPipelineConfigurator configurator = new LogPipelineConfigurator(CConfiguration.create());
    configurator.setContext(loggerContext);
    URL configURL = getClass().getClassLoader().getResource(""pipeline-multi-appenders.xml"");
    Assert.assertNotNull(configURL);
    configurator.doConfigure(configURL);

    String topic = ""testMultiAppenders"";
    TestCheckpointManager checkpointManager = new TestCheckpointManager();
    KafkaPipelineConfig config = new KafkaPipelineConfig(topic, Collections.singleton(0), 1024L, 100L, 1048576, 200L);
    KAFKA_TESTER.createTopic(topic, 1);

    loggerContext.start();
    KafkaLogProcessorPipeline pipeline = new KafkaLogProcessorPipeline(
      new LogProcessorPipelineContext(CConfiguration.create(), ""testMultiAppenders"",
                                      loggerContext, NO_OP_METRICS_CONTEXT, 0),
      checkpointManager,
      KAFKA_TESTER.getBrokerService(), config);

    pipeline.startAndWait();

    // Publish some log messages to Kafka using a non-specific logger
    long now = System.currentTimeMillis();
    publishLog(topic, ImmutableList.of(
      LogPipelineTestUtil.createLoggingEvent(""logger.trace"", Level.TRACE, ""TRACE"", now - 1000),
      LogPipelineTestUtil.createLoggingEvent(""logger.debug"", Level.DEBUG, ""DEBUG"", now - 900),
      LogPipelineTestUtil.createLoggingEvent(""logger.info"", Level.INFO, ""INFO"", now - 800),
      LogPipelineTestUtil.createLoggingEvent(""logger.warn"", Level.WARN, ""WARN"", now - 700),
      LogPipelineTestUtil.createLoggingEvent(""logger.error"", Level.ERROR, ""ERROR"", now - 600)
    ));

    // All logs should get logged to the default.log file
    Tasks.waitFor(true, () -> {
      File logFile = new File(logDir, ""default.log"");
      List<String> lines = Files.readAllLines(logFile.toPath(), StandardCharsets.UTF_8);
      return Arrays.asList(""TRACE"", ""DEBUG"", ""INFO"", ""WARN"", ""ERROR"").equals(lines);
    }, 5, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);

    // Publish some more log messages via the non-additive ""test.info"" logger.
    now = System.currentTimeMillis();
    publishLog(topic, ImmutableList.of(
      LogPipelineTestUtil.createLoggingEvent(""test.info.trace"", Level.TRACE, ""TRACE"", now - 1000),
      LogPipelineTestUtil.createLoggingEvent(""test.info.debug"", Level.DEBUG, ""DEBUG"", now - 900),
      LogPipelineTestUtil.createLoggingEvent(""test.info"", Level.INFO, ""INFO"", now - 800),
      LogPipelineTestUtil.createLoggingEvent(""test.info.warn"", Level.WARN, ""WARN"", now - 700),
      LogPipelineTestUtil.createLoggingEvent(""test.info.error"", Level.ERROR, ""ERROR"", now - 600)
    ));

    // Only logs with INFO or above level should get written to the info.log file
    Tasks.waitFor(true, () -> {
      File logFile = new File(logDir, ""info.log"");
      List<String> lines = Files.readAllLines(logFile.toPath(), StandardCharsets.UTF_8);
      return Arrays.asList(""INFO"", ""WARN"", ""ERROR"").equals(lines);
    }, 5, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);

    // The default.log file shouldn't be changed, because the test.info logger is non additive
    File defaultLogFile = new File(logDir, ""default.log"");
    List<String> lines = Files.readAllLines(defaultLogFile.toPath(), StandardCharsets.UTF_8);
    Assert.assertEquals(Arrays.asList(""TRACE"", ""DEBUG"", ""INFO"", ""WARN"", ""ERROR""), lines);

    // Publish a log messages via the additive ""test.error"" logger.
    now = System.currentTimeMillis();
    publishLog(topic, ImmutableList.of(
      LogPipelineTestUtil.createLoggingEvent(""test.error.1.2"", Level.ERROR, ""ERROR"", now - 1000)
    ));

    // Expect the log get appended to both the error.log file as well as the default.log file
    Tasks.waitFor(true, () -> {
      File logFile = new File(logDir, ""error.log"");
      List<String> lines1 = Files.readAllLines(logFile.toPath(), StandardCharsets.UTF_8);
      if (!Collections.singletonList(""ERROR"").equals(lines1)) {
        return false;
      }

      logFile = new File(logDir, ""default.log"");
      lines1 = Files.readAllLines(logFile.toPath(), StandardCharsets.UTF_8);
      return Arrays.asList(""TRACE"", ""DEBUG"", ""INFO"", ""WARN"", ""ERROR"", ""ERROR"").equals(lines1);
    }, 5, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);

    pipeline.stopAndWait();
    loggerContext.stop();
  }
"
"  @Test
  public void testOutOfOrderEvents() throws Exception {
    String topic = ""testOutOfOrderEvents"";
    KafkaPipelineConfig config = new KafkaPipelineConfig(topic, Collections.singleton(0), 1024L, EVENT_DELAY_MILLIS,
                                                         1048576, 200L);
    KAFKA_TESTER.createTopic(topic, 1);

    // Publish log messages to Kafka and wait for all messages to be published
    long baseTime = System.currentTimeMillis() - 2 * EVENT_DELAY_MILLIS;
    List<ILoggingEvent> outOfOrderEvents = ImmutableList.of(
      createLoggingEvent(""test.logger"", Level.INFO, ""0"", baseTime - 20 * 1000 - EVENT_DELAY_MILLIS),
      createLoggingEvent(""test.logger"", Level.INFO, ""0"", baseTime - 20 * 1000 - EVENT_DELAY_MILLIS),
      createLoggingEvent(""test.logger"", Level.INFO, ""1"", baseTime - 7 * 1000 - EVENT_DELAY_MILLIS),
      createLoggingEvent(""test.logger"", Level.INFO, ""2"", baseTime - 9 * 100),
      createLoggingEvent(""test.logger"", Level.INFO, ""3"", baseTime - 500),
      createLoggingEvent(""test.logger"", Level.INFO, ""1"", baseTime - 9 * 1000),
      createLoggingEvent(""test.logger"", Level.INFO, ""1"", baseTime - 9 * 1000 + EVENT_DELAY_MILLIS / 2),
      createLoggingEvent(""test.logger"", Level.INFO, ""1"", baseTime - 9 * 1000),
      createLoggingEvent(""test.logger"", Level.INFO, ""1"", baseTime - 9 * 1000 - EVENT_DELAY_MILLIS / 2),
      createLoggingEvent(""test.logger"", Level.INFO, ""1"", baseTime - 10 * 1000),
      createLoggingEvent(""test.logger"", Level.INFO, ""1"", baseTime - 600),
      createLoggingEvent(""test.logger"", Level.INFO, ""5"", baseTime - 20 * 1000),
      createLoggingEvent(""test.logger"", Level.INFO, ""5"", baseTime - 20 * 1000 + EVENT_DELAY_MILLIS / 2),
      createLoggingEvent(""test.logger"", Level.INFO, ""6"", baseTime - 600),
      createLoggingEvent(""test.logger"", Level.INFO, ""6"", baseTime - 10 * 1000),
      createLoggingEvent(""test.logger"", Level.INFO, ""7"", baseTime - 2 * 1000 + EVENT_DELAY_MILLIS),
      createLoggingEvent(""test.logger"", Level.INFO, ""8"", baseTime - 7 * 1000 + EVENT_DELAY_MILLIS),
      createLoggingEvent(""test.logger"", Level.INFO, ""4"", baseTime - 100 + EVENT_DELAY_MILLIS));
    publishLog(topic, outOfOrderEvents);
    waitForAllLogsPublished(topic, outOfOrderEvents.size());

    KafkaOffsetResolver offsetResolver = new KafkaOffsetResolver(KAFKA_TESTER.getBrokerService(), config);
    // Use every event's timestamp as target time and assert that found offset with target timestamp
    // matches the expected offset
    for (ILoggingEvent event : outOfOrderEvents) {
      assertOffsetResolverResult(offsetResolver, outOfOrderEvents, event.getTimeStamp(), baseTime);
    }

    // Try to find the offset with an event time that has timestamp earlier than all event timestamps in Kafka
    assertOffsetResolverResult(offsetResolver, outOfOrderEvents,
                               baseTime - 10 * EVENT_DELAY_MILLIS, baseTime);

    // Try to find the offset with an event time that has timestamp larger than all event timestamps in Kafka
    assertOffsetResolverResult(offsetResolver, outOfOrderEvents,
                               baseTime + 10 * EVENT_DELAY_MILLIS, baseTime);

    // Use a random number between (timestamp - EVENT_DELAY_MILLIS, timestamp + EVENT_DELAY_MILLIS) as target time
    // and assert that found offset with target timestamp matches the expected offset
    for (int i = 0; i < 10; i++) {
      for (ILoggingEvent event : outOfOrderEvents) {
        assertOffsetResolverResult(offsetResolver, outOfOrderEvents,
                                   event.getTimeStamp() + RANDOM.nextInt() % EVENT_DELAY_MILLIS, baseTime);
      }
    }
  }
"
"  @Test
  public void testInOrderEvents() throws InterruptedException, IOException {
    String topic = ""testInOrderEvents"";
    KafkaPipelineConfig config = new KafkaPipelineConfig(topic, Collections.singleton(0), 1024L, EVENT_DELAY_MILLIS,
                                                         1048576, 200L);
    KAFKA_TESTER.createTopic(topic, 1);

    // Publish log messages to Kafka and wait for all messages to be published
    long baseTime = System.currentTimeMillis() - EVENT_DELAY_MILLIS;
    List<ILoggingEvent> inOrderEvents = new ArrayList<>();
    for (int i = 0; i < 20; i++) {
      inOrderEvents.add(createLoggingEvent(""test.logger"", Level.INFO, Integer.toString(i), baseTime + i));
    }
    publishLog(topic, inOrderEvents);
    waitForAllLogsPublished(topic, inOrderEvents.size());

    KafkaOffsetResolver offsetResolver = new KafkaOffsetResolver(KAFKA_TESTER.getBrokerService(), config);
    // Use every event's timestamp as target time and assert that found offset is the next offset of the current offset
    for (int i = 0; i < inOrderEvents.size(); i++) {
      long targetTime = inOrderEvents.get(i).getTimeStamp();
      long offset = offsetResolver.getStartOffset(Long.MAX_VALUE, targetTime, 0);
      Assert.assertEquals(""Failed to find the expected event with the target time: "" + targetTime,
                          i + 1, offset);
    }
  }
"
"  @Test
  public void testOrdering() {
    TimeEventQueue<TimestampedEvent, Integer> eventQueue = new TimeEventQueue<>(ImmutableSet.of(1, 3));
    List<TimestampedEvent> expected = new ArrayList<>();

    // Put 10 events to partition 1, with both increasing timestamps and offsets
    for (int i = 0; i < 10; i++) {
      TimestampedEvent event = new TimestampedEvent(i, ""m1"" + i);
      eventQueue.add(event, i, 10, 1, i);
      expected.add(event);
    }

    // Put 10 events to partition 3, with increasing offsets, but non-sorted, some duplicated timestamps
    for (int i = 0; i < 10; i++) {
      long timestamp = i % 3;
      TimestampedEvent event = new TimestampedEvent(timestamp, ""m3"" + i);
      eventQueue.add(event, timestamp, 10, 3, i);
      expected.add(event);
    }

    // Sort the expected result based on timestamp. Since Collections.sort is stable sort,
    // partition 1 events will always be before partition 3 if the timestamps are the same
    Collections.sort(expected, new Comparator<TimestampedEvent>() {
      @Override
      public int compare(TimestampedEvent o1, TimestampedEvent o2) {
        return Long.compare(o1.getTimestamp(), o2.getTimestamp());
      }
"
"  @Test
  public void testKafkaOffset() {
    TimeEventQueue<String, Integer> eventQueue = new TimeEventQueue<>(ImmutableSet.of(1, 2));

    // Insert 6 events, with timestamps going back and forth
    eventQueue.add(""m7"", 7L, 10, 1, 0);
    eventQueue.add(""m5"", 5L, 10, 2, 1);
    eventQueue.add(""m10"", 10L, 10, 1, 2);
    eventQueue.add(""m11"", 11L, 10, 2, 3);
    eventQueue.add(""m2"", 2L, 10, 1, 4);
    eventQueue.add(""m8"", 8L, 10, 2, 5);

    // Have 6 events of size 10, so total size should be 60
    Assert.assertEquals(60, eventQueue.getEventSize());

    // Events should be time ordered when getting from iterator
    TimeEventQueue.EventIterator<String, Integer> iterator = eventQueue.iterator();

    Assert.assertEquals(""m2"", iterator.next());
    Assert.assertEquals(1, iterator.getPartition());
    Assert.assertEquals(4, iterator.getOffset().intValue());
    iterator.remove();
    Assert.assertEquals(50, eventQueue.getEventSize());
    Assert.assertEquals(0, eventQueue.getSmallestOffset(1).intValue());
    Assert.assertEquals(1, eventQueue.getSmallestOffset(2).intValue());

    Assert.assertEquals(""m5"", iterator.next());
    Assert.assertEquals(2, iterator.getPartition());
    Assert.assertEquals(1, iterator.getOffset().intValue());
    iterator.remove();
    Assert.assertEquals(40, eventQueue.getEventSize());
    Assert.assertEquals(0, eventQueue.getSmallestOffset(1).intValue());
    Assert.assertEquals(3, eventQueue.getSmallestOffset(2).intValue());

    Assert.assertEquals(""m7"", iterator.next());
    Assert.assertEquals(1, iterator.getPartition());
    Assert.assertEquals(0, iterator.getOffset().intValue());
    iterator.remove();
    Assert.assertEquals(30, eventQueue.getEventSize());
    Assert.assertEquals(2, eventQueue.getSmallestOffset(1).intValue());
    Assert.assertEquals(3, eventQueue.getSmallestOffset(2).intValue());

    Assert.assertEquals(""m8"", iterator.next());
    Assert.assertEquals(2, iterator.getPartition());
    Assert.assertEquals(5, iterator.getOffset().intValue());
    iterator.remove();
    Assert.assertEquals(20, eventQueue.getEventSize());
    Assert.assertEquals(2, eventQueue.getSmallestOffset(1).intValue());
    Assert.assertEquals(3, eventQueue.getSmallestOffset(2).intValue());

    Assert.assertEquals(""m10"", iterator.next());
    Assert.assertEquals(1, iterator.getPartition());
    Assert.assertEquals(2, iterator.getOffset().intValue());
    iterator.remove();
    Assert.assertEquals(10, eventQueue.getEventSize());
    Assert.assertTrue(eventQueue.isEmpty(1));

    Assert.assertEquals(""m11"", iterator.next());
    Assert.assertEquals(2, iterator.getPartition());
    Assert.assertEquals(3, iterator.getOffset().intValue());
    iterator.remove();
    Assert.assertEquals(0, eventQueue.getEventSize());
    Assert.assertTrue(eventQueue.isEmpty(2));

    Assert.assertTrue(eventQueue.isEmpty());
  }
"
"  @Test (expected = IllegalArgumentException.class)
  public void testInvalidPartition() {
    TimeEventQueue<String, Integer> eventQueue = new TimeEventQueue<>(Collections.singleton(1));
    eventQueue.add(""test"", 1L, 10, 2, 0);
  }
"
"  @Test (expected = IllegalStateException.class)
  public void testIllegalRemove() {
    TimeEventQueue<String, Integer> eventQueue = new TimeEventQueue<>(Collections.singleton(1));
    eventQueue.add(""test"", 1L, 10, 1, 0);
    Iterator<String> iterator = eventQueue.iterator();
    iterator.next();
    iterator.remove();
    iterator.remove();
  }
"
"  @Test
  public void test() throws Exception {
    LoggerContext loggerContext = LogPipelineTestUtil.createLoggerContext(""WARN"",
                                                                          ImmutableMap.of(""test.logger"", ""INFO""),
                                                                          MockAppender.class.getName());
    LogProcessorPipelineContext context = new LogProcessorPipelineContext(CConfiguration.create(),
                                                                          ""test"", loggerContext, NO_OP_METRICS_CONTEXT,
                                                                          0);
    context.start();
    TimeEventQueueProcessor<TestOffset> processor = new TimeEventQueueProcessor<>(context, 50, 1,
                                                                                  ImmutableList.of(0));
    long now = System.currentTimeMillis();
    List<ILoggingEvent> events = ImmutableList.of(
      LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""1"", now - 1000),
      LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""3"", now - 700),
      LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""5"", now - 500),
      LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""2"", now - 900),
      LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.ERROR, ""4"", now - 600),
      LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""6"", now - 100));

    ProcessedEventMetadata<TestOffset> metadata = processor.process(0, new TransformingIterator(events.iterator()));
    // all 6 events should be processed. This is because when the buffer is full after 5 events, time event queue
    // processor should append existing buffered events and enqueue 6th event
    Assert.assertEquals(6, metadata.getTotalEventsProcessed());
    for (Map.Entry<Integer, Checkpoint<TestOffset>> entry : metadata.getCheckpoints().entrySet()) {
      Checkpoint<TestOffset> value = entry.getValue();
      // offset should be max offset processed so far
      Assert.assertEquals(6, value.getOffset().getOffset());
    }
  }
"
"  @Test
  public void testEffectiveLevel() throws Exception {
    LoggerContext context = new LoggerContext();
    JoranConfigurator configurator = new JoranConfigurator();
    configurator.setContext(context);
    configurator.doConfigure(new InputSource(new StringReader(generateLogback(""WARN"", ImmutableMap.of(
      ""test"", ""INFO"",
      ""test.a"", ""ERROR"",
      ""test.a.X"", ""DEBUG"",
      ""test.a.X$1"", ""OFF""
    )))));

    Assert.assertSame(context.getLogger(""test""), Loggers.getEffectiveLogger(context, ""test""));
    Assert.assertSame(context.getLogger(""test.a""), Loggers.getEffectiveLogger(context, ""test.a""));
    Assert.assertSame(context.getLogger(""test.a.X""), Loggers.getEffectiveLogger(context, ""test.a.X""));
    Assert.assertSame(context.getLogger(""test.a.X$1""), Loggers.getEffectiveLogger(context, ""test.a.X$1""));

    Assert.assertSame(context.getLogger(Logger.ROOT_LOGGER_NAME),
                      Loggers.getEffectiveLogger(context, ""defaultToRoot""));
    Assert.assertSame(context.getLogger(""test""),
                      Loggers.getEffectiveLogger(context, ""test.defaultToTest""));
    Assert.assertSame(context.getLogger(""test.a""),
                      Loggers.getEffectiveLogger(context, ""test.a.defaultToTestDotA""));
    Assert.assertSame(context.getLogger(""test.a.X""),
                      Loggers.getEffectiveLogger(context, ""test.a.X.defaultToTestDotADotX""));
  }
"
"  @Test
  public void testFileMetadataReadWrite() throws Exception {
    TransactionRunner transactionRunner = injector.getInstance(TransactionRunner.class);
    FileMetaDataWriter fileMetaDataWriter = new FileMetaDataWriter(transactionRunner);
    LogPathIdentifier logPathIdentifier =
      new LogPathIdentifier(NamespaceId.DEFAULT.getNamespace(), ""testApp"", ""testFlow"");
    LocationFactory locationFactory = injector.getInstance(LocationFactory.class);
    Location location = locationFactory.create(TMP_FOLDER.newFolder().getPath()).append(""/logs"");
    long currentTime = System.currentTimeMillis();
    for (int i = 10; i <= 100; i += 10) {
      // i is the event time
      fileMetaDataWriter.writeMetaData(logPathIdentifier, i, currentTime,
                                       location.append(Integer.toString(i)));
    }

    // for the timestamp 80, add new new log path id with different current time.

    fileMetaDataWriter.writeMetaData(logPathIdentifier, 80, currentTime + 1,
                                     location.append(""81""));

    fileMetaDataWriter.writeMetaData(logPathIdentifier, 80, currentTime + 2,
                                     location.append(""82""));

    // reader test
    FileMetaDataReader fileMetadataReader = injector.getInstance(FileMetaDataReader.class);

    Assert.assertEquals(12, fileMetadataReader.listFiles(logPathIdentifier, 0, 100).size());
    Assert.assertEquals(5, fileMetadataReader.listFiles(logPathIdentifier, 20, 50).size());
    Assert.assertEquals(2, fileMetadataReader.listFiles(logPathIdentifier, 100, 150).size());

    // should include the latest file with event start time 80.
    List<LogLocation> locationList = fileMetadataReader.listFiles(logPathIdentifier, 81, 85);
    Assert.assertEquals(1, locationList.size());
    Assert.assertEquals(80, locationList.get(0).getEventTimeMs());
    Assert.assertEquals(location.append(""82""), locationList.get(0).getLocation());

    Assert.assertEquals(1, fileMetadataReader.listFiles(logPathIdentifier, 150, 1000).size());
  }
"
"  @Test
  public void testFileMetadataReadWriteAcrossFormats() throws Exception {
    TransactionRunner transactionRunner = injector.getInstance(TransactionRunner.class);
    FileMetaDataWriter fileMetaDataWriter = new FileMetaDataWriter(transactionRunner);
    LogPathIdentifier logPathIdentifier =
      new LogPathIdentifier(NamespaceId.DEFAULT.getNamespace(), ""testApp"", ""testFlow"");
    LocationFactory locationFactory = injector.getInstance(LocationFactory.class);
    Location location = locationFactory.create(TMP_FOLDER.newFolder().getPath()).append(""/logs"");
    long currentTime = System.currentTimeMillis();

    long eventTime = currentTime + 20;
    long newCurrentTime = currentTime + 100;
    // 10 files in new format
    for (int i = 1; i <= 10; i++) {
      fileMetaDataWriter.writeMetaData(logPathIdentifier, eventTime + i, newCurrentTime + i,
                                       location.append(""testFileNew"" + Integer.toString(i)));
    }
    // reader test
    FileMetaDataReader fileMetadataReader = injector.getInstance(FileMetaDataReader.class);

    // scan only in new files time range
    List<LogLocation> locations = fileMetadataReader.listFiles(logPathIdentifier, eventTime + 2, eventTime + 6);
    // should include files from currentTime (1..6)
    Assert.assertEquals(6, locations.size());
    for (LogLocation logLocation : locations) {
      Assert.assertEquals(LogLocation.VERSION_1, logLocation.getFrameworkVersion());
    }

    // scan time range across formats
    locations = fileMetadataReader.listFiles(logPathIdentifier, currentTime + 2, eventTime + 6);
    // should include files from new range (1..6)
    Assert.assertEquals(6, locations.size());

    for (int i = 0; i < locations.size(); i++) {
      Assert.assertEquals(LogLocation.VERSION_1, locations.get(i).getFrameworkVersion());
      Assert.assertEquals(location.append(""testFileNew"" + Integer.toString(i + 1)), locations.get(i).getLocation());
    }
  }
"
"  @Test
  public void testFramework() throws Exception {
    DistributedLogFramework framework = injector.getInstance(DistributedLogFramework.class);
    CConfiguration cConf = injector.getInstance(CConfiguration.class);

    framework.startAndWait();

    // Send some logs to Kafka.
    LoggingContext context = new ServiceLoggingContext(NamespaceId.SYSTEM.getNamespace(),
                                                       Constants.Logging.COMPONENT_NAME,
                                                       ""test"");

    // Make sure all events get flushed in the same batch
    long eventTimeBase = System.currentTimeMillis() + cConf.getInt(Constants.Logging.PIPELINE_EVENT_DELAY_MS);
    final int msgCount = 50;
    for (int i = 0; i < msgCount; i++) {
      // Publish logs in descending timestamp order
      publishLog(
        cConf.get(Constants.Logging.KAFKA_TOPIC), context,
        ImmutableList.of(
          createLoggingEvent(""io.cdap.test."" + i, Level.INFO, ""Testing "" + i, eventTimeBase - i)
        )
      );
    }

    // Read the logs back. They should be sorted by timestamp order.
    final FileMetaDataReader metaDataReader = injector.getInstance(FileMetaDataReader.class);
    Tasks.waitFor(true, () -> {
      List<LogLocation> locations = metaDataReader.listFiles(new LogPathIdentifier(NamespaceId.SYSTEM.getNamespace(),
                                                                                    Constants.Logging.COMPONENT_NAME,
                                                                                   ""test""), 0, Long.MAX_VALUE);
      if (locations.size() != 1) {
        return false;
      }
      LogLocation location = locations.get(0);
      int i = 0;
      try {
        try (CloseableIterator<LogEvent> iter = location.readLog(Filter.EMPTY_FILTER, 0, Long.MAX_VALUE, msgCount)) {
          while (iter.hasNext()) {
            String expectedMsg = ""Testing "" + (msgCount - i - 1);
            LogEvent event = iter.next();
            if (!expectedMsg.equals(event.getLoggingEvent().getMessage())) {
              return false;
            }
            i++;
          }
          return i == msgCount;
        }
      } catch (Exception e) {
        // It's possible the file is an invalid Avro file due to a race between creation of the meta data
        // and the time when actual content are flushed to the file
        return false;
      }
    }, 10, TimeUnit.SECONDS, msgCount, TimeUnit.MILLISECONDS);

    framework.stopAndWait();

    String kafkaTopic = cConf.get(Constants.Logging.KAFKA_TOPIC);
    // Check the checkpoint is persisted correctly. Since all messages are processed,
    // the checkpoint should be the same as the message count.
    CheckpointManager<KafkaOffset> checkpointManager = getCheckpointManager(kafkaTopic);
    Checkpoint<KafkaOffset> checkpoint = checkpointManager.getCheckpoint(0);
    Assert.assertEquals(msgCount, checkpoint.getOffset().getNextOffset());
  }
"
"  @Test
  public void testEmptySerialization() throws Exception {
    Logger logger = LoggerFactory.getLogger(LoggingEventSerializerTest.class);
    LoggingEventSerializer serializer = new LoggingEventSerializer();
    ch.qos.logback.classic.spi.LoggingEvent iLoggingEvent = new ch.qos.logback.classic.spi.LoggingEvent(
      getClass().getName(), (ch.qos.logback.classic.Logger) logger, Level.ERROR, ""message"", null, null);
    iLoggingEvent.setThreadName(""thread-1"");
    iLoggingEvent.setTimeStamp(10000000L);

    // Serialize
    ILoggingEvent event = new LogMessage(iLoggingEvent, LoggingContextAccessor.getLoggingContext());
    byte [] serializedBytes = serializer.toBytes(event);

    // De-serialize
    ILoggingEvent actualEvent = serializer.fromBytes(ByteBuffer.wrap(serializedBytes));
    assertLoggingEventEquals(iLoggingEvent, actualEvent);
  }
"
"  @Test
  public void testSerialization() throws Exception {
    Map<String, String> mdcMap = Maps.newHashMap();
    mdcMap.put(""mdc1"", ""mdc-val1"");
    mdcMap.put(""mdc2"", null);
    mdcMap.put(null, null);

    Map<String, String> contextMap = Maps.newHashMap();
    contextMap.put(""p1"", ""ctx-val1"");
    contextMap.put(""p2"", null);
    contextMap.put(null, null);

    LoggingEventSerializer serializer = new LoggingEventSerializer();
    ch.qos.logback.classic.spi.LoggingEvent iLoggingEvent = new ch.qos.logback.classic.spi.LoggingEvent();
    iLoggingEvent.setThreadName(""threadName1"");
    iLoggingEvent.setLevel(Level.INFO);
    iLoggingEvent.setMessage(""Log message1"");
    iLoggingEvent.setArgumentArray(new Object[]{null, ""arg2"", ""100"", null});
    iLoggingEvent.setLoggerName(""loggerName1"");

    iLoggingEvent.setLoggerContextRemoteView(new LoggerContextVO(""logger_context1"", contextMap, 12345634234L));

    Exception e1 = new Exception(null, null);
    Exception e2 = new Exception(""Test Exception2"", e1);
    iLoggingEvent.setThrowableProxy(new ThrowableProxy(e2));
    iLoggingEvent.prepareForDeferredProcessing();
    ((ThrowableProxy) iLoggingEvent.getThrowableProxy()).calculatePackagingData();

    iLoggingEvent.setCallerData(new StackTraceElement[]{
      new StackTraceElement(""com.Class1"", ""methodName1"", ""fileName1"", 10),
      null,
      new StackTraceElement(""com.Class2"", ""methodName2"", ""fileName2"", 20),
      new StackTraceElement(""com.Class3"",  ""methodName3"", null, 30),
      null
    });

    iLoggingEvent.setMarker(null);
    iLoggingEvent.getMDCPropertyMap().putAll(mdcMap);
    iLoggingEvent.setTimeStamp(1234567890L);

    // Serialize
    ILoggingEvent event = new LogMessage(iLoggingEvent, LoggingContextAccessor.getLoggingContext());
    byte [] serializedBytes = serializer.toBytes(event);

    // De-serialize
    ILoggingEvent actualEvent = serializer.fromBytes(ByteBuffer.wrap(serializedBytes));
    System.out.println(actualEvent);
    assertLoggingEventEquals(iLoggingEvent, actualEvent);
  }
"
"  @Test
  public void testOldSystemLoggingContext() throws Exception {
    // see: CDAP-7482
    Map<String, String> mdcMap = new HashMap<>();
    mdcMap.put(ServiceLoggingContext.TAG_SYSTEM_ID, ""ns1"");
    mdcMap.put(ComponentLoggingContext.TAG_COMPONENT_ID, ""comp1"");
    mdcMap.put(ServiceLoggingContext.TAG_SERVICE_ID, ""ser1"");

    ch.qos.logback.classic.spi.LoggingEvent iLoggingEvent = new ch.qos.logback.classic.spi.LoggingEvent();
    iLoggingEvent.setCallerData(new StackTraceElement[] { null });
    iLoggingEvent.setMDCPropertyMap(mdcMap);

    Assert.assertTrue(LoggingContextHelper.getLoggingContext(iLoggingEvent.getMDCPropertyMap())
                        instanceof ServiceLoggingContext);
  }
"
"  @Test
  public void testNullSerialization() throws Exception {
    Logger logger = LoggerFactory.getLogger(LoggingEventSerializerTest.class);
    LoggingEventSerializer serializer = new LoggingEventSerializer();
    ch.qos.logback.classic.spi.LoggingEvent iLoggingEvent = new ch.qos.logback.classic.spi.LoggingEvent(
      null, (ch.qos.logback.classic.Logger) logger, null, null, null, null);
    iLoggingEvent.setThreadName(null);
    iLoggingEvent.setLevel(null);
    iLoggingEvent.setMessage(null);
    iLoggingEvent.setArgumentArray(null);
    iLoggingEvent.setLoggerName(null);
    iLoggingEvent.setLoggerContextRemoteView(null);
    iLoggingEvent.setThrowableProxy(null);
    iLoggingEvent.setCallerData(null);
    iLoggingEvent.setMarker(null);
    iLoggingEvent.setMDCPropertyMap(null);
    iLoggingEvent.setTimeStamp(10000000L);

    // Serialize
    ILoggingEvent event = new LogMessage(iLoggingEvent, LoggingContextAccessor.getLoggingContext());
    byte [] serializedBytes = serializer.toBytes(event);

    // De-serialize
    ILoggingEvent actualEvent = serializer.fromBytes(ByteBuffer.wrap(serializedBytes));

    iLoggingEvent.setLevel(Level.ERROR);
    assertLoggingEventEquals(iLoggingEvent, actualEvent);
  }
"
"  @Test
  public void testDecodeTimestamp() throws IOException {
    long timestamp = System.currentTimeMillis();

    ch.qos.logback.classic.spi.LoggingEvent event = new ch.qos.logback.classic.spi.LoggingEvent();
    event.setLevel(Level.INFO);
    event.setLoggerName(""test.logger"");
    event.setMessage(""Some test"");
    event.setTimeStamp(timestamp);

    // Serialize it
    LoggingEventSerializer serializer = new LoggingEventSerializer();
    byte[] bytes = serializer.toBytes(event);

    // Decode timestamp
    Assert.assertEquals(timestamp, serializer.decodeEventTimestamp(ByteBuffer.wrap(bytes)));
  }
"
"  @Test
  public void testSerialize() throws Exception {
    Logger logger = LoggerFactory.getLogger(LoggingEventTest.class);
    ch.qos.logback.classic.spi.LoggingEvent iLoggingEvent = new ch.qos.logback.classic.spi.LoggingEvent(
      getClass().getName(), (ch.qos.logback.classic.Logger) logger, Level.ERROR, ""Log message1"", null,
      new Object[] {""arg1"", ""arg2"", ""100""});
    iLoggingEvent.setLoggerName(""loggerName1"");
    iLoggingEvent.setThreadName(""threadName1"");
    iLoggingEvent.setTimeStamp(1234567890L);
    iLoggingEvent.setLoggerContextRemoteView(new LoggerContextVO(""loggerContextRemoteView"",
                                                                 ImmutableMap.of(""key1"", ""value1"", ""key2"", ""value2""),
                                                                 100000L));
    Map<String, String> mdcMap = Maps.newHashMap(ImmutableMap.of(""mdck1"", ""mdcv1"", ""mdck2"", ""mdck2""));
    iLoggingEvent.setMDCPropertyMap(mdcMap);

    LoggingEventSerializer serializer = new LoggingEventSerializer();
    byte[] encoded = serializer.toBytes(new LogMessage(iLoggingEvent, LoggingContextAccessor.getLoggingContext()));

    ILoggingEvent decodedEvent = serializer.fromBytes(ByteBuffer.wrap(encoded));
    LoggingEventSerializerTest.assertLoggingEventEquals(iLoggingEvent, decodedEvent);
  }
"
"  @Test
  public void testEmptySerialize() throws Exception {
    Logger logger = LoggerFactory.getLogger(LoggingEventTest.class);
    ch.qos.logback.classic.spi.LoggingEvent iLoggingEvent = new ch.qos.logback.classic.spi.LoggingEvent(
      getClass().getName(), (ch.qos.logback.classic.Logger) logger, Level.ERROR, null, null, null);

    LoggingEventSerializer serializer = new LoggingEventSerializer();
    byte[] encoded = serializer.toBytes(new LogMessage(iLoggingEvent, LoggingContextAccessor.getLoggingContext()));

    ILoggingEvent decodedEvent = serializer.fromBytes(ByteBuffer.wrap(encoded));
    LoggingEventSerializerTest.assertLoggingEventEquals(iLoggingEvent, decodedEvent);
  }
"
"  @Test
  public void testScanAndDeleteNewMetadata() throws Exception {
    TransactionRunner transactionRunner = injector.getInstance(TransactionRunner.class);

    FileMetaDataWriter fileMetaDataWriter = new FileMetaDataWriter(transactionRunner);
    FileMetadataCleaner fileMetadataCleaner = new FileMetadataCleaner(transactionRunner);
    try {
      long currentTime = System.currentTimeMillis();
      long eventTimestamp = currentTime - 100;
      LogPathIdentifier logPathIdentifier = new LogPathIdentifier(""testNs2"", ""testApp"", ""testFlow"");
      LocationFactory locationFactory = injector.getInstance(LocationFactory.class);
      List<String> expected = new ArrayList<>();
      for (int i = 0; i < 100; i++) {
        Location location = locationFactory.create(""testFlowFile"" + i);
        // values : event time is 100ms behind current timestamp
        fileMetaDataWriter.writeMetaData(logPathIdentifier, eventTimestamp + i, currentTime + i, location);
        expected.add(location.toURI().getPath());
      }

      long tillTime = currentTime + 50;
      List<FileMetadataCleaner.DeletedEntry> deletedEntries =
        fileMetadataCleaner.scanAndGetFilesToDelete(tillTime, 100);
      // we should have deleted 51 rows, till time is inclusive
      Assert.assertEquals(51, deletedEntries.size());
      int count = 0;
      for (FileMetadataCleaner.DeletedEntry deletedEntry : deletedEntries) {
        Assert.assertEquals(expected.get(count), deletedEntry.getPath());
        count += 1;
      }
      // now add 10 entries for spark
      logPathIdentifier = new LogPathIdentifier(""testNs2"", ""testApp"", ""testSpark"");
      expected = new ArrayList<>();

      for (int i = 0; i < 10; i++) {
        Location location = locationFactory.create(""testSparkFile"" + i);
        // values : event time is 100ms behind current timestamp
        fileMetaDataWriter.writeMetaData(logPathIdentifier, eventTimestamp + i, currentTime + i, location);
        expected.add(location.toURI().getPath());
      }

      // lets keep the same till time - this should only delete the spark entries now
      deletedEntries = fileMetadataCleaner.scanAndGetFilesToDelete(tillTime, 100);
      // we should have deleted 51 rows, till time is inclusive
      Assert.assertEquals(10, deletedEntries.size());
      count = 0;
      for (FileMetadataCleaner.DeletedEntry deletedEntry : deletedEntries) {
        Assert.assertEquals(expected.get(count), deletedEntry.getPath());
        count += 1;
      }

      // now add 10 entries in mr context in time range 60-70
      logPathIdentifier = new LogPathIdentifier(""testNs2"", ""testApp"", ""testMr"");
      expected = new ArrayList<>();

      // flow should come up at the beginning in the expected list
      for (int i = 51; i <= 70; i++) {
        expected.add(locationFactory.create(""testFlowFile"" + i).toURI().getPath());
      }

      for (int i = 0; i < 10; i++) {
        Location location = locationFactory.create(""testMrFile"" + i);
        // values : event time is 100ms behind current timestamp
        fileMetaDataWriter.writeMetaData(logPathIdentifier, eventTimestamp + i, currentTime + i, location);
        expected.add(location.toURI().getPath());
      }

      List<String> nextExpected = new ArrayList<>();
      logPathIdentifier = new LogPathIdentifier(""testNs2"", ""testApp"", ""testCustomAction"");
      for (int i = 90; i < 100; i++) {
        Location location = locationFactory.create(""testActionFile"" + i);
        // values : event time is 100ms behind current timestamp
        fileMetaDataWriter.writeMetaData(logPathIdentifier, eventTimestamp + i, currentTime + i, location);
        nextExpected.add(location.toURI().getPath());
      }

      tillTime = currentTime + 70;
      // lets delete till 70.
      deletedEntries = fileMetadataCleaner.scanAndGetFilesToDelete(tillTime, 100);
      // we should have deleted 51-70 files of flow and 0-9 files of spark files in that order and 0 files of action.
      Assert.assertEquals(30, deletedEntries.size());
      count = 0;
      for (FileMetadataCleaner.DeletedEntry deletedEntry : deletedEntries) {
        Assert.assertEquals(expected.get(count), deletedEntry.getPath());
        count += 1;
      }

      // now delete till currentTime + 100, this should delete all remaining entries.
      // custom action should come first and then flow entries

      tillTime = currentTime + 100;
      // lets delete till 100.
      deletedEntries = fileMetadataCleaner.scanAndGetFilesToDelete(tillTime, 100);
      // we should have deleted 90-99 of custom action(10) 71-99 (29) files of flow.
      for (int i = 71; i < 100; i++) {
        nextExpected.add(locationFactory.create(""testFlowFile"" + i).toURI().getPath());
      }
      Assert.assertEquals(39, deletedEntries.size());
      count = 0;
      for (FileMetadataCleaner.DeletedEntry deletedEntry : deletedEntries) {
        Assert.assertEquals(nextExpected.get(count), deletedEntry.getPath());
        count += 1;
      }

      // now lets do a delete with till time  = currentTime + 1000, this should return empty result
      tillTime = currentTime + 1000;
      deletedEntries = fileMetadataCleaner.scanAndGetFilesToDelete(tillTime, 100);
      Assert.assertEquals(0, deletedEntries.size());
    } finally {
      // cleanup meta
      deleteAllMetaEntries(transactionRunner);
    }
  }
"
"  @Test
  public void testFileMetadataWithCommonContextPrefix() throws Exception {
    TransactionRunner transactionRunner = injector.getInstance(TransactionRunner.class);

    FileMetaDataWriter fileMetaDataWriter = new FileMetaDataWriter(transactionRunner);
    FileMetaDataReader fileMetadataReader = injector.getInstance(FileMetaDataReader.class);
    FileMetadataCleaner fileMetadataCleaner = new FileMetadataCleaner(transactionRunner);
    try {
      List<LogPathIdentifier> logPathIdentifiers = new ArrayList<>();
      // we write entries where program id is of format testFlow{1..20},
      // this should be able to scan and delete common prefix programs like testFlow1, testFlow10 during clenaup.
      for (int i = 1; i <= 20; i++) {
        logPathIdentifiers.add(new LogPathIdentifier(NamespaceId.DEFAULT.getNamespace(),
                                                     ""testApp"", String.format(""testFlow%s"", i)));
      }

      LocationFactory locationFactory = injector.getInstance(LocationFactory.class);
      Location location = locationFactory.create(TMP_FOLDER.newFolder().getPath()).append(""/logs"");
      long currentTime = System.currentTimeMillis();
      long newCurrentTime = currentTime + 100;

      for (int i = 1; i <= 20; i++) {
        LogPathIdentifier identifier = logPathIdentifiers.get(i - 1);
        for (int j = 0; j < 10; j++) {
          fileMetaDataWriter.writeMetaData(identifier, newCurrentTime + j, newCurrentTime + j,
                                           location.append(""testFileNew"" + Integer.toString(j)));
        }
      }

      List<LogLocation> locations;
      for (int i = 1; i <= 20; i++) {
        locations = fileMetadataReader.listFiles(logPathIdentifiers.get(i - 1),
                                                 newCurrentTime, newCurrentTime + 10);
        // should include files from currentTime (0..9)
        Assert.assertEquals(10, locations.size());
      }

      long tillTime = newCurrentTime + 4;
      List<FileMetadataCleaner.DeletedEntry> deleteEntries =
        fileMetadataCleaner.scanAndGetFilesToDelete(tillTime, 100);
      // 20 context, 5 entries each
      Assert.assertEquals(100, deleteEntries.size());
      for (int i = 1; i <= 20; i++) {
        locations = fileMetadataReader.listFiles(logPathIdentifiers.get(i - 1),
                                                 newCurrentTime, newCurrentTime + 10);
        // should include files from time (5..9)
        Assert.assertEquals(5, locations.size());
        int startIndex = 5;
        for (LogLocation logLocation : locations) {
          Assert.assertEquals(String.format(""testFileNew%s"", startIndex), logLocation.getLocation().getName());
          startIndex++;
        }
      }
    } finally {
      // cleanup meta
      deleteAllMetaEntries(transactionRunner);
    }
  }
"
"  @Test
  public void testWithBatchSizeLargerThanNumOfFiles() throws Exception {
    TransactionRunner transactionRunner = injector.getInstance(TransactionRunner.class);

    FileMetaDataWriter fileMetaDataWriter = new FileMetaDataWriter(transactionRunner);
    FileMetaDataReader fileMetadataReader = injector.getInstance(FileMetaDataReader.class);
    FileMetadataCleaner fileMetadataCleaner = new FileMetadataCleaner(transactionRunner);
    try {
      LogPathIdentifier identifier = new LogPathIdentifier(NamespaceId.DEFAULT.getNamespace(),
                                                           ""testApp"", String.format(""testFlow%s"", 0));

      LocationFactory locationFactory = injector.getInstance(LocationFactory.class);
      Location location = locationFactory.create(TMP_FOLDER.newFolder().getPath()).append(""/logs"");
      long currentTime = System.currentTimeMillis();
      long newCurrentTime = currentTime + 100;

      for (int j = 0; j < 10; j++) {
        fileMetaDataWriter.writeMetaData(identifier, newCurrentTime + j, newCurrentTime + j,
                                         location.append(""testFileNew"" + Integer.toString(j)));
      }

      List<LogLocation> locations;
      locations = fileMetadataReader.listFiles(identifier, newCurrentTime, newCurrentTime + 10);
      // should include files from currentTime (0..9)
      Assert.assertEquals(10, locations.size());

      long tillTime = newCurrentTime + 4;
      List<FileMetadataCleaner.DeletedEntry> deleteEntries =
        fileMetadataCleaner.scanAndGetFilesToDelete(tillTime, 1000);
      Assert.assertEquals(5, deleteEntries.size());
    } finally {
      // cleanup meta
      deleteAllMetaEntries(transactionRunner);
    }
  }
"
"  @Test
  public void testLogCleanup() throws Exception {
    TransactionRunner transactionRunner = injector.getInstance(TransactionRunner.class);
    FileMetadataCleaner fileMetadataCleaner = new FileMetadataCleaner(transactionRunner);
    LocationFactory locationFactory = injector.getInstance(LocationFactory.class);
    long currentTime = System.currentTimeMillis();
    LogPathIdentifier logPathIdentifier = new LogPathIdentifier(""testNs"", ""testApp"", ""testEntity"");
    FileMetaDataWriter fileMetaDataWriter = new FileMetaDataWriter(transactionRunner);
    long startTime = currentTime - 5000;
    Location dirLocation = locationFactory.create(""logs"");
    dirLocation.mkdirs();
    // create 20 files, add them in past time range
    for (int i = 0; i < 20; i++) {
      Location location = dirLocation.append(""test"" + i);
      location.createNew();
      fileMetaDataWriter.writeMetaData(logPathIdentifier, startTime + i, startTime + i, location);
    }

    Assert.assertEquals(20, dirLocation.list().size());
    LogCleaner logCleaner = new LogCleaner(fileMetadataCleaner, locationFactory, 100, 60);
    logCleaner.run();
    FileMetaDataReader fileMetaDataReader = injector.getInstance(FileMetaDataReader.class);
    // all meta data should be deleted
    Assert.assertEquals(0, fileMetaDataReader.listFiles(logPathIdentifier, 0, System.currentTimeMillis()).size());
    // we are not asserting file existence as the delete could fail and we don't guarantee file deletion.
  }
"
"  @Test
  public void testGetLogNext() throws Exception {
    LoggingContext loggingContext = new WorkerLoggingContext(""TFL_NS_1"", ""APP_1"", ""WORKER_1"", ""RUN1"", ""INSTANCE1"");
    FileLogReader logReader = injector.getInstance(FileLogReader.class);
    LoggingTester tester = new LoggingTester();
    tester.testGetNext(logReader, loggingContext);
  }
"
"  @Test
  public void testGetLogPrev() throws Exception {
    LoggingContext loggingContext = new WorkerLoggingContext(""TFL_NS_1"", ""APP_1"", ""WORKER_1"", ""RUN1"", ""INSTANCE1"");
    FileLogReader logReader = injector.getInstance(FileLogReader.class);
    LoggingTester tester = new LoggingTester();
    tester.testGetPrev(logReader, loggingContext);
  }
"
"  @Test
  public void testGetLog() throws Exception {
    // LogReader.getLog is tested in LogSaverTest for distributed mode
    LoggingContext loggingContext = new WorkerLoggingContext(""TFL_NS_1"", ""APP_1"", ""WORKER_1"", ""RUN1"", ""INSTANCE1"");
    FileLogReader logTail = injector.getInstance(FileLogReader.class);
    LoggingTester.LogCallback logCallback1 = new LoggingTester.LogCallback();
    logTail.getLogPrev(loggingContext, ReadRange.LATEST, 60, Filter.EMPTY_FILTER,
                       logCallback1);
    List<LogEvent> allEvents = logCallback1.getEvents();
    Assert.assertEquals(60, allEvents.size());

    List<LogEvent> events =
      Lists.newArrayList(logTail.getLog(loggingContext, allEvents.get(10).getLoggingEvent().getTimeStamp(),
                                        allEvents.get(15).getLoggingEvent().getTimeStamp(), Filter.EMPTY_FILTER));

    Assert.assertEquals(5, events.size());
    Assert.assertEquals(allEvents.get(10).getLoggingEvent().getFormattedMessage(),
                        events.get(0).getLoggingEvent().getFormattedMessage());
    Assert.assertEquals(allEvents.get(14).getLoggingEvent().getFormattedMessage(),
                        events.get(4).getLoggingEvent().getFormattedMessage());


    events =
      Lists.newArrayList(logTail.getLog(loggingContext, allEvents.get(0).getLoggingEvent().getTimeStamp(),
                                        allEvents.get(59).getLoggingEvent().getTimeStamp(), Filter.EMPTY_FILTER));
    Assert.assertEquals(59, events.size());
    Assert.assertEquals(allEvents.get(0).getLoggingEvent().getFormattedMessage(),
                        events.get(0).getLoggingEvent().getFormattedMessage());
    Assert.assertEquals(allEvents.get(58).getLoggingEvent().getFormattedMessage(),
                        events.get(58).getLoggingEvent().getFormattedMessage());

    events =
      Lists.newArrayList(logTail.getLog(loggingContext, allEvents.get(12).getLoggingEvent().getTimeStamp(),
                                        allEvents.get(41).getLoggingEvent().getTimeStamp(), Filter.EMPTY_FILTER));
    Assert.assertEquals(29, events.size());
    Assert.assertEquals(allEvents.get(12).getLoggingEvent().getFormattedMessage(),
                        events.get(0).getLoggingEvent().getFormattedMessage());
    Assert.assertEquals(allEvents.get(40).getLoggingEvent().getFormattedMessage(),
                        events.get(28).getLoggingEvent().getFormattedMessage());

    events =
      Lists.newArrayList(logTail.getLog(loggingContext, allEvents.get(22).getLoggingEvent().getTimeStamp(),
                                        allEvents.get(38).getLoggingEvent().getTimeStamp(), Filter.EMPTY_FILTER));
    Assert.assertEquals(16, events.size());
    Assert.assertEquals(allEvents.get(22).getLoggingEvent().getFormattedMessage(),
                        events.get(0).getLoggingEvent().getFormattedMessage());
    Assert.assertEquals(allEvents.get(37).getLoggingEvent().getFormattedMessage(),
                        events.get(15).getLoggingEvent().getFormattedMessage());

    events =
      Lists.newArrayList(logTail.getLog(loggingContext, allEvents.get(41).getLoggingEvent().getTimeStamp(),
                                        allEvents.get(59).getLoggingEvent().getTimeStamp(), Filter.EMPTY_FILTER));
    Assert.assertEquals(18, events.size());
    Assert.assertEquals(allEvents.get(41).getLoggingEvent().getFormattedMessage(),
                        events.get(0).getLoggingEvent().getFormattedMessage());
    Assert.assertEquals(allEvents.get(58).getLoggingEvent().getFormattedMessage(),
                        events.get(17).getLoggingEvent().getFormattedMessage());

    // Try with null run id, should get all logs for WORKER_1
    LoggingContext loggingContext1 = new WorkerLoggingContext(""TFL_NS_1"", ""APP_1"", ""WORKER_1"", null, ""INSTANCE1"");
    events =
      Lists.newArrayList(logTail.getLog(loggingContext1, 0, Long.MAX_VALUE, Filter.EMPTY_FILTER));
    Assert.assertEquals(120, events.size());
  }
"
"  @Test
  public void testCDAPLogAppender() {
    int syncInterval = 1024 * 1024;
    CDAPLogAppender cdapLogAppender = new CDAPLogAppender();

    cdapLogAppender.setSyncIntervalBytes(syncInterval);
    cdapLogAppender.setMaxFileLifetimeMs(TimeUnit.DAYS.toMillis(1));
    cdapLogAppender.setMaxFileSizeInBytes(104857600);
    cdapLogAppender.setDirPermissions(""700"");
    cdapLogAppender.setFilePermissions(""600"");
    cdapLogAppender.setFileRetentionDurationDays(1);
    cdapLogAppender.setLogCleanupIntervalMins(10);
    cdapLogAppender.setFileCleanupBatchSize(100);
    AppenderContext context = new LocalAppenderContext(injector.getInstance(TransactionRunner.class),
                                                       injector.getInstance(LocationFactory.class),
                                                       new NoOpMetricsCollectionService());
    context.start();
    cdapLogAppender.setContext(context);
    cdapLogAppender.start();

    FileMetaDataReader fileMetaDataReader = injector.getInstance(FileMetaDataReader.class);
    LoggingEvent event =
      new LoggingEvent(""io.cdap.Test"",
                       (ch.qos.logback.classic.Logger) LoggerFactory.getLogger(Logger.ROOT_LOGGER_NAME),
                       Level.ERROR , ""test message"", null, null);
    Map<String, String> properties = new HashMap<>();
    properties.put(NamespaceLoggingContext.TAG_NAMESPACE_ID, ""default"");
    properties.put(ApplicationLoggingContext.TAG_APPLICATION_ID, ""testApp"");
    properties.put(UserServiceLoggingContext.TAG_USER_SERVICE_ID, ""testService"");

    event.setMDCPropertyMap(properties);

    cdapLogAppender.doAppend(event);
    cdapLogAppender.stop();
    context.stop();

    try {
      List<LogLocation> files = fileMetaDataReader.listFiles(cdapLogAppender.getLoggingPath(properties),
                                                             0, Long.MAX_VALUE);
      Assert.assertEquals(1, files.size());
      LogLocation logLocation = files.get(0);
      Assert.assertEquals(LogLocation.VERSION_1, logLocation.getFrameworkVersion());
      Assert.assertTrue(logLocation.getLocation().exists());
      CloseableIterator<LogEvent> logEventCloseableIterator =
        logLocation.readLog(Filter.EMPTY_FILTER, 0, Long.MAX_VALUE, Integer.MAX_VALUE);
      int logCount = 0;
      while (logEventCloseableIterator.hasNext()) {
        logCount++;
        LogEvent logEvent = logEventCloseableIterator.next();
        Assert.assertEquals(event.getMessage(), logEvent.getLoggingEvent().getMessage());
      }
      logEventCloseableIterator.close();
      Assert.assertEquals(1, logCount);
      // checking permission
      String expectedPermissions = ""rw-------"";
      for (LogLocation file : files) {
        Location location = file.getLocation();
        Assert.assertEquals(expectedPermissions, location.getPermissions());
      }
    } catch (Exception e) {
      Assert.fail();
    }
  }
"
"  @Test
  public void testCDAPLogAppenderRotation() throws Exception {
    int syncInterval = 1024 * 1024;
    FileMetaDataReader fileMetaDataReader = injector.getInstance(FileMetaDataReader.class);
    CDAPLogAppender cdapLogAppender = new CDAPLogAppender();
    AppenderContext context = new LocalAppenderContext(injector.getInstance(TransactionRunner.class),
                                                       injector.getInstance(LocationFactory.class),
                                                       new NoOpMetricsCollectionService());
    context.start();

    cdapLogAppender.setSyncIntervalBytes(syncInterval);
    cdapLogAppender.setMaxFileLifetimeMs(500);
    cdapLogAppender.setMaxFileSizeInBytes(104857600);
    cdapLogAppender.setDirPermissions(""750"");
    cdapLogAppender.setFilePermissions(""640"");
    cdapLogAppender.setFileRetentionDurationDays(1);
    cdapLogAppender.setLogCleanupIntervalMins(10);
    cdapLogAppender.setFileCleanupBatchSize(100);
    cdapLogAppender.setContext(context);
    cdapLogAppender.start();

    Map<String, String> properties = new HashMap<>();
    properties.put(NamespaceLoggingContext.TAG_NAMESPACE_ID, ""testTimeRotation"");
    properties.put(ApplicationLoggingContext.TAG_APPLICATION_ID, ""testApp"");
    properties.put(UserServiceLoggingContext.TAG_USER_SERVICE_ID, ""testService"");

    long currentTimeMillisEvent1 = System.currentTimeMillis();

    LoggingEvent event1 =
      getLoggingEvent(""io.cdap.Test1"",
                      (ch.qos.logback.classic.Logger) LoggerFactory.getLogger(Logger.ROOT_LOGGER_NAME),
                      Level.ERROR , ""test message 1"", properties);

    event1.setTimeStamp(currentTimeMillisEvent1);
    cdapLogAppender.doAppend(event1);

    // Pause pass the max file lifetime ms
    TimeUnit.MILLISECONDS.sleep(500);

    long currentTimeMillisEvent2 = System.currentTimeMillis();

    LoggingEvent event2 = getLoggingEvent(""io.cdap.Test2"",
                                          (ch.qos.logback.classic.Logger) LoggerFactory.getLogger(
                                            Logger.ROOT_LOGGER_NAME), Level.ERROR , ""test message 2"", properties);
    event2.setTimeStamp(currentTimeMillisEvent1 + 1000);
    cdapLogAppender.doAppend(event2);
    cdapLogAppender.stop();
    context.stop();

    try {
      List<LogLocation> files = fileMetaDataReader.listFiles(cdapLogAppender.getLoggingPath(properties),
                                                             0, Long.MAX_VALUE);
      Assert.assertEquals(2, files.size());
      assertLogEventDetails(event1, files.get(0));
      assertLogEventDetails(event2, files.get(1));
      Assert.assertEquals(currentTimeMillisEvent1, files.get(0).getEventTimeMs());
      Assert.assertEquals(currentTimeMillisEvent1 + 1000, files.get(1).getEventTimeMs());
      Assert.assertTrue(files.get(0).getFileCreationTimeMs() >= currentTimeMillisEvent1);
      Assert.assertTrue(files.get(1).getFileCreationTimeMs() >= currentTimeMillisEvent2);

      // checking permission
      String expectedPermissions = ""rw-r-----"";
      for (LogLocation file : files) {
        Location location = file.getLocation();
        Assert.assertEquals(expectedPermissions, location.getPermissions());
      }
    } catch (Exception e) {
      Assert.fail();
    }
  }
"
"  @Test
  public void testCDAPLogAppenderSizeBasedRotation() throws Exception {
    int syncInterval = 1024 * 1024;
    FileMetaDataReader fileMetaDataReader = injector.getInstance(FileMetaDataReader.class);
    CDAPLogAppender cdapLogAppender = new CDAPLogAppender();
    AppenderContext context = new LocalAppenderContext(injector.getInstance(TransactionRunner.class),
                                                       injector.getInstance(LocationFactory.class),
                                                       new NoOpMetricsCollectionService());
    context.start();

    cdapLogAppender.setSyncIntervalBytes(syncInterval);
    cdapLogAppender.setMaxFileLifetimeMs(TimeUnit.DAYS.toMillis(1));
    cdapLogAppender.setMaxFileSizeInBytes(500);
    cdapLogAppender.setDirPermissions(""750"");
    cdapLogAppender.setFilePermissions(""640"");
    cdapLogAppender.setFileRetentionDurationDays(1);
    cdapLogAppender.setLogCleanupIntervalMins(10);
    cdapLogAppender.setFileCleanupBatchSize(100);
    cdapLogAppender.setContext(context);
    cdapLogAppender.start();

    Map<String, String> properties = new HashMap<>();
    properties.put(NamespaceLoggingContext.TAG_NAMESPACE_ID, ""testSizeRotation"");
    properties.put(ApplicationLoggingContext.TAG_APPLICATION_ID, ""testApp"");
    properties.put(UserServiceLoggingContext.TAG_USER_SERVICE_ID, ""testService"");

    long currentTimeMillisEvent1 = System.currentTimeMillis();

    LoggingEvent event1 =
      getLoggingEvent(""io.cdap.Test1"",
                      (ch.qos.logback.classic.Logger) LoggerFactory.getLogger(Logger.ROOT_LOGGER_NAME),
                      Level.ERROR , ""test message 1"", properties);

    event1.setTimeStamp(currentTimeMillisEvent1);
    cdapLogAppender.doAppend(event1);
    // sync updates the file size
    cdapLogAppender.sync();

    long currentTimeMillisEvent2 = System.currentTimeMillis();
    LoggingEvent event2 = getLoggingEvent(""io.cdap.Test2"",
                                          (ch.qos.logback.classic.Logger) LoggerFactory.getLogger(
                                            Logger.ROOT_LOGGER_NAME), Level.ERROR , ""test message 2"", properties);
    event2.setTimeStamp(currentTimeMillisEvent2);
    // one new append, we will rotate to new file as the file size limit is very low and last append exceeded that.
    cdapLogAppender.doAppend(event2);
    cdapLogAppender.stop();
    context.stop();

    try {
      List<LogLocation> files = fileMetaDataReader.listFiles(cdapLogAppender.getLoggingPath(properties),
                                                             0, Long.MAX_VALUE);
      Assert.assertEquals(2, files.size());
      assertLogEventDetails(event1, files.get(0));
      assertLogEventDetails(event2, files.get(1));
      Assert.assertEquals(currentTimeMillisEvent1, files.get(0).getEventTimeMs());
      Assert.assertEquals(currentTimeMillisEvent2, files.get(1).getEventTimeMs());
      Assert.assertTrue(files.get(0).getFileCreationTimeMs() >= currentTimeMillisEvent1);
      Assert.assertTrue(files.get(1).getFileCreationTimeMs() >= currentTimeMillisEvent2);
    } catch (Exception e) {
      Assert.fail();
    }
  }
"
"  @Test
  public void testLogFileManager() throws Exception {
    int syncInterval = 1024 * 1024;
    long maxLifeTimeMs = 50;
    long maxFileSizeInBytes = 104857600;
    FileMetaDataWriter fileMetaDataWriter = new FileMetaDataWriter(injector.getInstance(TransactionRunner.class));
    LogFileManager logFileManager = new LogFileManager(""700"", ""600"", maxLifeTimeMs, maxFileSizeInBytes, syncInterval,
                                                       fileMetaDataWriter,
                                                       injector.getInstance(LocationFactory.class));
    LogPathIdentifier logPathIdentifier = new LogPathIdentifier(""test"", ""testApp"", ""testFlow"");
    long timestamp = System.currentTimeMillis();
    LogFileOutputStream outputStream = logFileManager.getLogFileOutputStream(logPathIdentifier, timestamp);
    LoggingEvent event1 =
      getLoggingEvent(""io.cdap.Test1"",
                      (ch.qos.logback.classic.Logger) LoggerFactory.getLogger(Logger.ROOT_LOGGER_NAME),
                      Level.ERROR , ""test message 1"");
    outputStream.append(event1);
    // we are doing this, instead of calling getLogFileOutputStream to avoid race, if test machine can be slow.
    Assert.assertNotNull((logFileManager.getActiveOutputStream(logPathIdentifier)));
    TimeUnit.MILLISECONDS.sleep(60);
    logFileManager.flush();
    // should be closed on flush, should return null
    Assert.assertNull((logFileManager.getActiveOutputStream(logPathIdentifier)));
    LogFileOutputStream newLogOutStream = logFileManager.getLogFileOutputStream(logPathIdentifier, timestamp);
    // make sure the new location we got is different
    Assert.assertNotEquals(outputStream.getLocation(), newLogOutStream.getLocation());
  }
"
"  @Test
  public void testDistributedLogPrevBoth() throws Exception {
    ReadRange readRange = new ReadRange(0, Long.MAX_VALUE, LogOffset.INVALID_KAFKA_OFFSET);
    testDistributedLogPrev(readRange, LOGGING_CONTEXT_BOTH, 16, 4, ""TestDistributedLogReader Log message1 "", 60);

    readRange = new ReadRange(System.currentTimeMillis() - TimeUnit.DAYS.toMillis(1),
                                        System.currentTimeMillis(), LogOffset.INVALID_KAFKA_OFFSET);
    testDistributedLogPrev(readRange, LOGGING_CONTEXT_BOTH, 16, 4, ""TestDistributedLogReader Log message1 "", 60);

    testDistributedLogPrev(ReadRange.LATEST, LOGGING_CONTEXT_BOTH, 9, 8, ""TestDistributedLogReader Log message1 "", 60);
  }
"
"  @Test
  public void testDistributedLogNextBoth() throws Exception {
    ReadRange readRange = new ReadRange(0, Long.MAX_VALUE, LogOffset.INVALID_KAFKA_OFFSET);
    testDistributedLogNext(readRange, LOGGING_CONTEXT_BOTH, 20, 3, ""TestDistributedLogReader Log message1 "", 60, 0);

    readRange = new ReadRange(System.currentTimeMillis() - TimeUnit.DAYS.toMillis(1),
                              System.currentTimeMillis(), LogOffset.INVALID_KAFKA_OFFSET);
    testDistributedLogNext(readRange, LOGGING_CONTEXT_BOTH, 20, 3, ""TestDistributedLogReader Log message1 "", 60, 0);

    testDistributedLogNext(ReadRange.LATEST, LOGGING_CONTEXT_BOTH, 1, 3,
                           ""TestDistributedLogReader Log message1 "", 3, 57);
  }
"
"  @Test
  public void testDistributedLogPrevFile() throws Exception {
    ReadRange readRange = new ReadRange(0, Long.MAX_VALUE, LogOffset.INVALID_KAFKA_OFFSET);
    testDistributedLogPrev(readRange, LOGGING_CONTEXT_FILE, 7, 6, ""TestDistributedLogReader Log message2 "", 40);

    readRange = new ReadRange(System.currentTimeMillis() - TimeUnit.DAYS.toMillis(1),
                                        System.currentTimeMillis(), LogOffset.INVALID_KAFKA_OFFSET);

    testDistributedLogPrev(readRange, LOGGING_CONTEXT_FILE, 7, 6, ""TestDistributedLogReader Log message2 "", 40);

    testDistributedLogPrev(ReadRange.LATEST, LOGGING_CONTEXT_FILE, 7, 6, ""TestDistributedLogReader Log message2 "", 40);
  }
"
"  @Test
  public void testDistributedLogNextFile() throws Exception {
    ReadRange readRange = new ReadRange(0, Long.MAX_VALUE, LogOffset.INVALID_KAFKA_OFFSET);

    testDistributedLogNext(readRange, LOGGING_CONTEXT_FILE, 14, 3, ""TestDistributedLogReader Log message2 "", 40, 0);

    readRange = new ReadRange(System.currentTimeMillis() - TimeUnit.DAYS.toMillis(1),
                              System.currentTimeMillis(), LogOffset.INVALID_KAFKA_OFFSET);
    testDistributedLogNext(readRange, LOGGING_CONTEXT_FILE, 14, 3, ""TestDistributedLogReader Log message2 "", 40, 0);

    testDistributedLogNext(ReadRange.LATEST, LOGGING_CONTEXT_FILE, 1, 5,
                           ""TestDistributedLogReader Log message2 "", 5, 35);
  }
"
"  @Test
  public void testDistributedLogPrevKafka() throws Exception {
    ReadRange readRange = new ReadRange(0, Long.MAX_VALUE, LogOffset.INVALID_KAFKA_OFFSET);
    testDistributedLogPrev(readRange, LOGGING_CONTEXT_KAFKA, 5, 6, ""TestDistributedLogReader Log message3 "", 30);

    readRange = new ReadRange(System.currentTimeMillis() - TimeUnit.DAYS.toMillis(1),
                                        System.currentTimeMillis(), LogOffset.INVALID_KAFKA_OFFSET);

    testDistributedLogPrev(readRange, LOGGING_CONTEXT_KAFKA, 5, 6, ""TestDistributedLogReader Log message3 "", 30);

    testDistributedLogPrev(ReadRange.LATEST, LOGGING_CONTEXT_KAFKA, 5, 6, ""TestDistributedLogReader Log message3 "", 30);
  }
"
"  @Test
  public void testDistributedLogNextKafka() throws Exception {
    ReadRange readRange = new ReadRange(0, Long.MAX_VALUE, LogOffset.INVALID_KAFKA_OFFSET);
    testDistributedLogNext(readRange, LOGGING_CONTEXT_KAFKA, 10, 3, ""TestDistributedLogReader Log message3 "", 30, 0);

    readRange = new ReadRange(System.currentTimeMillis() - TimeUnit.DAYS.toMillis(1),
                              System.currentTimeMillis(), LogOffset.INVALID_KAFKA_OFFSET);
    testDistributedLogNext(readRange, LOGGING_CONTEXT_KAFKA, 10, 3, ""TestDistributedLogReader Log message3 "", 30, 0);

    testDistributedLogNext(ReadRange.LATEST, LOGGING_CONTEXT_KAFKA, 1, 8,
                           ""TestDistributedLogReader Log message3 "", 8, 22);
  }
"
"  @Test
  public void testGetNext() throws Exception {
    // Check with null runId and null instanceId
    LoggingContext loggingContext = new WorkerLoggingContext(""TKL_NS_1"", ""APP_1"", ""FLOW_1"", ""RUN1"", ""INSTANCE1"");
    KafkaLogReader logReader = KAFKA_TESTER.getInjector().getInstance(KafkaLogReader.class);
    LoggingTester tester = new LoggingTester();
    tester.testGetNext(logReader, loggingContext);
  }
"
"  @Test
  public void testGetPrev() throws Exception {
    LoggingContext loggingContext = new WorkerLoggingContext(""TKL_NS_1"", ""APP_1"", ""FLOW_1"", ""RUN1"", ""INSTANCE1"");
    KafkaLogReader logReader = KAFKA_TESTER.getInjector().getInstance(KafkaLogReader.class);
    LoggingTester tester = new LoggingTester();
    tester.testGetPrev(logReader, loggingContext);
  }
"
"  @Test
  public void testPartitionKey() throws Exception {
    CConfiguration cConf = KAFKA_TESTER.getCConf();
    // set kafka partition key to application
    cConf.set(Constants.Logging.LOG_PUBLISH_PARTITION_KEY, ""application"");

    Logger logger = LoggerFactory.getLogger(""TestKafkaLogging"");
    LoggingContext loggingContext = new WorkerLoggingContext(""TKL_NS_2"", ""APP_2"", ""FLOW_2"", ""RUN2"", ""INSTANCE2"");
    LoggingContextAccessor.setLoggingContext(loggingContext);
    for (int i = 0; i < 40; ++i) {
      logger.warn(""TKL_NS_2 Test log message {} {} {}"", i, ""arg1"", ""arg2"", new Exception(""test exception""));
    }

    loggingContext = new WorkerLoggingContext(""TKL_NS_2"", ""APP_2"", ""FLOW_3"", ""RUN3"", ""INSTANCE3"");
    LoggingContextAccessor.setLoggingContext(loggingContext);
    for (int i = 0; i < 40; ++i) {
      logger.warn(""TKL_NS_2 Test log message {} {} {}"", i, ""arg1"", ""arg2"", new Exception(""test exception""));
    }

    final Multimap<Integer, String> actual = ArrayListMultimap.create();

    KAFKA_TESTER.getPublishedMessages(KAFKA_TESTER.getCConf().get(Constants.Logging.KAFKA_TOPIC),
                                      ImmutableSet.of(0, 1), 40, new Function<FetchedMessage, String>() {
        @Override
        public String apply(final FetchedMessage input) {
          try {
            Map.Entry<Integer, String> entry = convertFetchedMessage(input);
            actual.put(entry.getKey(), entry.getValue());
          } catch (IOException e) {
            // should never happen
          }
          return """";
        }
"
"  @Test
  public void testResilientLogging() throws Exception {
    Configuration hConf = new Configuration();
    CConfiguration cConf = CConfiguration.create();

    File datasetDir = new File(tmpFolder.newFolder(), ""datasetUser"");
    //noinspection ResultOfMethodCallIgnored
    datasetDir.mkdirs();

    cConf.set(Constants.Dataset.Manager.OUTPUT_DIR, datasetDir.getAbsolutePath());
    cConf.set(Constants.Service.MASTER_SERVICES_BIND_ADDRESS, ""localhost"");

    cConf.set(Constants.Dataset.Executor.ADDRESS, ""localhost"");
    cConf.setInt(Constants.Dataset.Executor.PORT, Networks.getRandomPort());

    cConf.set(Constants.CFG_LOCAL_DATA_DIR, tmpFolder.newFolder().getAbsolutePath());

    Injector injector = Guice.createInjector(
      new ConfigModule(cConf, hConf),
      new IOModule(),
      new ZKClientModule(),
      new KafkaClientModule(),
      new InMemoryDiscoveryModule(),
      new NonCustomLocationUnitTestModule(),
      new DataFabricModules().getInMemoryModules(),
      new DataSetsModules().getStandaloneModules(),
      new DataSetServiceModules().getInMemoryModules(),
      new TransactionMetricsModule(),
      new ExploreClientModule(),
      new LocalLogAppenderModule(),
      new NamespaceAdminTestModule(),
      new AuthorizationTestModule(),
      new AuthorizationEnforcementModule().getInMemoryModules(),
      new AuthenticationContextModules().getMasterModule(),
      new AbstractModule() {
        @Override
        protected void configure() {
          bind(UGIProvider.class).to(UnsupportedUGIProvider.class);
          bind(OwnerAdmin.class).to(NoOpOwnerAdmin.class);
          bind(MetadataServiceClient.class).to(NoOpMetadataServiceClient.class);
        }
      });

    TransactionManager txManager = injector.getInstance(TransactionManager.class);
    txManager.startAndWait();
    StructuredTableRegistry structuredTableRegistry = injector.getInstance(StructuredTableRegistry.class);
    structuredTableRegistry.initialize();
    StoreDefinition.createAllTables(injector.getInstance(StructuredTableAdmin.class), structuredTableRegistry);

    DatasetOpExecutorService opExecutorService = injector.getInstance(DatasetOpExecutorService.class);
    opExecutorService.startAndWait();

    // Start the logging before starting the service.
    LoggingContextAccessor.setLoggingContext(new WorkerLoggingContext(""TRL_ACCT_1"", ""APP_1"", ""WORKER_1"",
                                                                      ""RUN"", ""INSTANCE""));
    String logBaseDir = ""trl-log/log_files_"" + new Random(System.currentTimeMillis()).nextLong();

    cConf.set(LoggingConfiguration.LOG_BASE_DIR, logBaseDir);
    cConf.setInt(LoggingConfiguration.LOG_MAX_FILE_SIZE_BYTES, 20 * 1024);
    final LogAppender appender = injector.getInstance(LocalLogAppender.class);
    new LogAppenderInitializer(appender).initialize(""TestResilientLogging"");

    int failureMsgCount = 3;
    final CountDownLatch failureLatch = new CountDownLatch(failureMsgCount);
    LoggerContext loggerContext = (LoggerContext) LoggerFactory.getILoggerFactory();
    loggerContext.getStatusManager().add(new StatusListener() {
      @Override
      public void addStatusEvent(Status status) {
        if (status.getLevel() != Status.ERROR || status.getOrigin() != appender) {
          return;
        }
        Throwable cause = status.getThrowable();
        if (cause != null) {
          Throwable rootCause = Throwables.getRootCause(cause);
          if (rootCause instanceof ServiceUnavailableException) {
            String serviceName = ((ServiceUnavailableException) rootCause).getServiceName();
            if (Constants.Service.DATASET_MANAGER.equals(serviceName)) {
              failureLatch.countDown();
            }
          }
        }
      }
"
"  @Test
  public void testMDC() {
    LoggingContext context = new TestLoggingContext(""namespace"", ""app"", ""run"", ""instance"");

    // Put an entry in the user mdc. It shouldn't override what's in the system tags.
    Map<String, String> userMDC = new HashMap<>();
    userMDC.put(Constants.Logging.TAG_APPLICATION_ID, ""userApp"");

    Map<String, String> mdc = new LoggingContextMDC(context.getSystemTagsAsString(), userMDC);

    Assert.assertEquals(4, mdc.size());

    Map<String, String> copiedMDC = new HashMap<>();
    for (Map.Entry<String, String> entry : mdc.entrySet()) {
      copiedMDC.put(entry.getKey(), entry.getValue());
    }

    Assert.assertEquals(4, copiedMDC.size());
    Assert.assertEquals(""namespace"", copiedMDC.get(Constants.Logging.TAG_NAMESPACE_ID));
    Assert.assertEquals(""app"", copiedMDC.get(Constants.Logging.TAG_APPLICATION_ID));
    Assert.assertEquals(""run"", copiedMDC.get(Constants.Logging.TAG_RUN_ID));
    Assert.assertEquals(""instance"", copiedMDC.get(Constants.Logging.TAG_INSTANCE_ID));

    // Should be able to set user property
    mdc.put(""user"", ""test"");
    Assert.assertEquals(5, mdc.size());
    Assert.assertEquals(5, mdc.entrySet().size());

    // This should fail with exception
    try {
      mdc.put(Constants.Logging.TAG_APPLICATION_ID, ""newApp"");
      Assert.fail();
    } catch (IllegalArgumentException e) {
      // expected
    }
  }
"
"  @Test
  public void testReset() {
    Cancellable cancellable = LoggingContextAccessor.setLoggingContext(
      new GenericLoggingContext(OLD_NS, OLD_APP, OLD_ENTITY));
    Assert.assertEquals(MDC.get(NamespaceLoggingContext.TAG_NAMESPACE_ID), OLD_NS);
    Assert.assertEquals(MDC.get(ApplicationLoggingContext.TAG_APPLICATION_ID), OLD_APP);
    Assert.assertEquals(MDC.get(GenericLoggingContext.TAG_ENTITY_ID), OLD_ENTITY);

    final Cancellable cancellable2 =
      LoggingContextAccessor.setLoggingContext(new GenericLoggingContext(NS, APP, ENTITY));

    Assert.assertEquals(MDC.get(NamespaceLoggingContext.TAG_NAMESPACE_ID), NS);
    Assert.assertEquals(MDC.get(ApplicationLoggingContext.TAG_APPLICATION_ID), APP);
    Assert.assertEquals(MDC.get(GenericLoggingContext.TAG_ENTITY_ID), ENTITY);

    // Verify a different thread cannot change context
    Thread thread = new Thread(new Runnable() {
      @Override
      public void run() {
        cancellable2.cancel();
      }
"
"  @Test (timeout = 10000L)
  public void testConfigUpdate() throws Exception {
    File logbackFile = createLogbackFile(new File(TEMP_FOLDER.newFolder(), ""logback.xml""), """");

    // Create a logger context from the generated logback.xml file
    LoggerContext loggerContext = (LoggerContext) LoggerFactory.getILoggerFactory();
    loggerContext.reset();

    JoranConfigurator configurator = new JoranConfigurator();
    configurator.setContext(loggerContext);
    configurator.doConfigure(logbackFile);

    TestLogAppender testAppender = new TestLogAppender();
    testAppender.setName(""TestAppender"");
    LogAppenderInitializer initializer = new LogAppenderInitializer(testAppender);
    initializer.initialize();

    LoggingContextAccessor.setLoggingContext(new TestLoggingContext(""ns"", ""app"", ""run"", ""instance""));

    Logger logger = loggerContext.getLogger(LogAppenderInitializerTest.class);
    logger.info(""Testing"");

    Assert.assertEquals(""Testing"", testAppender.getLastMessage());

    // Update the logback file
    createLogbackFile(logbackFile, ""<logger name=\""io.cdap.cdap\"" level=\""INFO\"" />"");

    // Wait till the test appender stop() is called. This will happen when logback detected the changes.
    while (!testAppender.isStoppedOnce()) {
      // We need to keep logging because there is some internal thresold in logback implementation to trigger the
      // config reload thread.
      logger.info(""Waiting stop"");
      TimeUnit.MILLISECONDS.sleep(150);
      // Update the last modified time of the logback file to make sure logback can pick it with.
      logbackFile.setLastModified(System.currentTimeMillis());
    }

    // The appender should get automatically started again
    Tasks.waitFor(true, testAppender::isStarted, 5, TimeUnit.SECONDS);
    logger.info(""Reattached"");

    Assert.assertEquals(""Reattached"", testAppender.getLastMessage());
    loggerContext.stop();
  }
"
"  @Test
  public void testTmsLogAppender() throws Exception {
    // setup TMSLogAppender and log messages to it
    LogAppenderInitializer logAppenderInitializer = new LogAppenderInitializer(tmsLogAppender);
    logAppenderInitializer.initialize(""TestTMSLogging"");

    Logger logger = LoggerFactory.getLogger(""TestTMSLogging"");
    LoggingTester loggingTester = new LoggingTester();

    LoggingContext loggingContext = new MapReduceLoggingContext(""TKL_NS_1"", ""APP_1"", ""MR_1"", ""RUN1"");
    loggingTester.generateLogs(logger, loggingContext);

    logAppenderInitializer.close();

    // fetch and deserialize all the logs from TMS
    LoggingEventSerializer loggingEventSerializer = new LoggingEventSerializer();

    Map<Integer, List<ILoggingEvent>> partitionedFetchedLogs = new HashMap<>();
    int totalFetchedLogs = 0;

    for (Map.Entry<Integer, TopicId> topicId : topicIds.entrySet()) {
      List<ILoggingEvent> fetchedLogs = new ArrayList<>();
      MessageFetcher messageFetcher = client.prepareFetch(topicId.getValue());
      try (CloseableIterator<RawMessage> messages = messageFetcher.fetch()) {
        while (messages.hasNext()) {
          RawMessage message = messages.next();
          ILoggingEvent iLoggingEvent = loggingEventSerializer.fromBytes(ByteBuffer.wrap(message.getPayload()));
          fetchedLogs.add(iLoggingEvent);
        }
      }

      totalFetchedLogs += fetchedLogs.size();
      partitionedFetchedLogs.put(topicId.getKey(), fetchedLogs);
    }

    // LoggingTester emits 240 logs in total
    Assert.assertEquals(240, totalFetchedLogs);

    // Read the partition that our LoggingContext maps to and filter the logs in there to the logs that correspond
    // to our LoggingContext.
    LogPartitionType logPartitionType =
            LogPartitionType.valueOf(cConf.get(Constants.Logging.LOG_PUBLISH_PARTITION_KEY).toUpperCase());
    String partitionKey = logPartitionType.getPartitionKey(loggingContext);
    int partition = TMSLogAppender.partition(partitionKey, cConf.getInt(Constants.Logging.NUM_PARTITIONS));
    Filter logFilter = LoggingContextHelper.createFilter(loggingContext);

    List<ILoggingEvent> filteredLogs =
            partitionedFetchedLogs.get(partition).stream().filter(logFilter::match).collect(Collectors.toList());

    // LoggingTester emits 60 logs with the given LoggingContext
    Assert.assertEquals(60, filteredLogs.size());

    for (int i = 0; i < filteredLogs.size(); i++) {
      ILoggingEvent loggingEvent = filteredLogs.get(i);
      Assert.assertEquals(String.format(""Test log message %s arg1 arg2"", i), loggingEvent.getFormattedMessage());
    }
  }
"
"  @Test
  public void testRollingLocationLogAppender() throws Exception {
    // assume SLF4J is bound to logback in the current environment
    AppenderContext appenderContext = new LocalAppenderContext(injector.getInstance(TransactionRunner.class),
                                                               injector.getInstance(LocationFactory.class),
                                                               new NoOpMetricsCollectionService());

    JoranConfigurator configurator = new JoranConfigurator();
    configurator.setContext(appenderContext);
    // Call context.reset() to clear any previous configuration, e.g. default
    // configuration. For multi-step configuration, omit calling context.reset().
    appenderContext.reset();

    configurator.doConfigure(getClass().getResourceAsStream(""/rolling-appender-logback-test.xml""));
    StatusPrinter.printInCaseOfErrorsOrWarnings(appenderContext);

    RollingLocationLogAppender rollingAppender =
      (RollingLocationLogAppender) appenderContext.getLogger(RollingLocationLogAppenderTest.class)
        .getAppender(""rollingAppender"");

    addTagsToMdc(""testNamespace"", ""testApp"");
    Logger logger = appenderContext.getLogger(RollingLocationLogAppenderTest.class);
    ingestLogs(logger, 5);
    Map<LocationIdentifier, LocationOutputStream> activeFiles = rollingAppender.getLocationManager()
      .getActiveLocations();
    Assert.assertEquals(1, activeFiles.size());
    verifyFileOutput(activeFiles, 5);

    // different program should go to different directory
    addTagsToMdc(""testNamespace"", ""testApp1"");
    ingestLogs(logger, 5);
    activeFiles = rollingAppender.getLocationManager().getActiveLocations();
    Assert.assertEquals(2, activeFiles.size());
    verifyFileOutput(activeFiles, 5);

    // different program should go to different directory because namespace is different
    addTagsToMdc(""testNamespace1"", ""testApp1"");
    ingestLogs(logger, 5);
    activeFiles = rollingAppender.getLocationManager().getActiveLocations();
    Assert.assertEquals(3, activeFiles.size());
    verifyFileOutput(activeFiles, 5);
  }
"
"  @Test
  public void testRollOver() throws Exception {
    // assume SLF4J is bound to logback in the current environment
    AppenderContext appenderContext = new LocalAppenderContext(injector.getInstance(TransactionRunner.class),
                                                               injector.getInstance(LocationFactory.class),
                                                               new NoOpMetricsCollectionService());

    JoranConfigurator configurator = new JoranConfigurator();
    configurator.setContext(appenderContext);
    // Call context.reset() to clear any previous configuration, e.g. default
    // configuration. For multi-step configuration, omit calling context.reset().
    appenderContext.reset();

    configurator.doConfigure(getClass().getResourceAsStream(""/rolling-appender-logback-test.xml""));
    StatusPrinter.printInCaseOfErrorsOrWarnings(appenderContext);

    RollingLocationLogAppender rollingAppender =
      (RollingLocationLogAppender) appenderContext.getLogger(RollingLocationLogAppenderTest.class)
        .getAppender(""rollingAppender"");

    addTagsToMdc(""testNs"", ""testApp"");
    Logger logger = appenderContext.getLogger(RollingLocationLogAppenderTest.class);
    ingestLogs(logger, 20000);
    Map<LocationIdentifier, LocationOutputStream> activeFiles = rollingAppender.getLocationManager()
      .getActiveLocations();
    Assert.assertEquals(1, activeFiles.size());
    LocationOutputStream locationOutputStream = activeFiles.get(new LocationIdentifier(""testNs"", ""testApp""));
    Location parentDir = Locations.getParent(locationOutputStream.getLocation());
    Assert.assertEquals(10, parentDir.list().size());

    // different program should go to different directory
    addTagsToMdc(""testNs"", ""testApp1"");
    ingestLogs(logger, 20000);
    activeFiles = rollingAppender.getLocationManager().getActiveLocations();
    Assert.assertEquals(2, activeFiles.size());
    locationOutputStream = activeFiles.get(new LocationIdentifier(""testNs"", ""testApp1""));
    parentDir = Locations.getParent(locationOutputStream.getLocation());
    Assert.assertEquals(10, parentDir.list().size());

    // different program should go to different directory because namespace is different
    addTagsToMdc(""testNs1"", ""testApp1"");
    ingestLogs(logger, 20000);
    activeFiles = rollingAppender.getLocationManager().getActiveLocations();
    Assert.assertEquals(3, activeFiles.size());
    locationOutputStream = activeFiles.get(new LocationIdentifier(""testNs1"", ""testApp1""));
    parentDir = Locations.getParent(locationOutputStream.getLocation());
    Assert.assertEquals(10, parentDir.list().size());
  }
"
"  @Test
  public void testFileClose() throws Exception {
    // assume SLF4J is bound to logback in the current environment
    AppenderContext appenderContext = new LocalAppenderContext(injector.getInstance(TransactionRunner.class),
                                                               injector.getInstance(LocationFactory.class),
                                                               new NoOpMetricsCollectionService());

    JoranConfigurator configurator = new JoranConfigurator();
    configurator.setContext(appenderContext);
    // Call context.reset() to clear any previous configuration, e.g. default
    // configuration. For multi-step configuration, omit calling context.reset().
    appenderContext.reset();

    configurator.doConfigure(getClass().getResourceAsStream(""/rolling-appender-logback-test.xml""));
    StatusPrinter.printInCaseOfErrorsOrWarnings(appenderContext);

    RollingLocationLogAppender rollingAppender =
      (RollingLocationLogAppender) appenderContext.getLogger(RollingLocationLogAppenderTest.class)
        .getAppender(""rollingAppender"");

    addTagsToMdc(""testNs"", ""testApp"");
    Logger logger = appenderContext.getLogger(RollingLocationLogAppenderTest.class);
    ingestLogs(logger, 20);

    // wait for 500 ms so that file is eligible for closing
    Thread.sleep(500);
    // flush to make sure file is closed
    rollingAppender.flush();
    Assert.assertEquals(0, rollingAppender.getLocationManager().getActiveLocations().size());
  }
"
"  @Test
  public void test() throws Exception {
    MetricsAdminSubscriberService adminService = injector.getInstance(MetricsAdminSubscriberService.class);
    adminService.startAndWait();

    // publish a metrics
    MetricsContext metricsContext = metricsCollectionService.getContext(
      Collections.singletonMap(Constants.Metrics.Tag.NAMESPACE, NamespaceId.SYSTEM.getNamespace()));
    metricsContext.increment(""test.increment"", 10L);
    metricsContext.gauge(""test.gauge"", 20L);

    MetricsSystemClient systemClient = injector.getInstance(RemoteMetricsSystemClient.class);

    // Search for metrics names
    Tasks.waitFor(true, () -> {
      Set<String> names = new HashSet<>(systemClient.search(metricsContext.getTags()));
      return names.contains(""system.test.increment"") && names.contains(""system.test.gauge"");
    }, 10, TimeUnit.SECONDS, 1, TimeUnit.SECONDS);

    // Query for metrics values
    Tasks.waitFor(true, () -> {
      Collection<MetricTimeSeries> values = systemClient.query(metricsContext.getTags(),
                                                               Arrays.asList(""system.test.increment"",
                                                                             ""system.test.gauge""));
      // Find and match the values for the increment and gauge
      boolean incMatched = values.stream()
        .filter(timeSeries -> timeSeries.getMetricName().equals(""system.test.increment""))
        .flatMap(timeSeries -> timeSeries.getTimeValues().stream())
        .findFirst()
        .filter(timeValue -> timeValue.getValue() == 10L)
        .isPresent();

      boolean gaugeMatched = values.stream()
        .filter(timeSeries -> timeSeries.getMetricName().equals(""system.test.gauge""))
        .flatMap(timeSeries -> timeSeries.getTimeValues().stream())
        .findFirst()
        .filter(timeValue -> timeValue.getValue() == 20L)
        .isPresent();

      return incMatched && gaugeMatched;
    }, 10, TimeUnit.SECONDS, 1, TimeUnit.SECONDS);

    // Emit more metrics
    metricsContext.increment(""test.increment"", 40L);
    metricsContext.gauge(""test.gauge"", 40L);

    // Query for metrics values. Should see the latest aggregates
    Tasks.waitFor(true, () -> {
      Collection<MetricTimeSeries> values = systemClient.query(metricsContext.getTags(),
                                                               Arrays.asList(""system.test.increment"",
                                                                             ""system.test.gauge""));
      // Find and match the values for the increment and gauge
      boolean incMatched = values.stream()
        .filter(timeSeries -> timeSeries.getMetricName().equals(""system.test.increment""))
        .flatMap(timeSeries -> timeSeries.getTimeValues().stream())
        .findFirst()
        .filter(timeValue -> timeValue.getValue() == 50L)
        .isPresent();

      boolean gaugeMatched = values.stream()
        .filter(timeSeries -> timeSeries.getMetricName().equals(""system.test.gauge""))
        .flatMap(timeSeries -> timeSeries.getTimeValues().stream())
        .findFirst()
        .filter(timeValue -> timeValue.getValue() == 40L)
        .isPresent();

      return incMatched && gaugeMatched;
    }, 10, TimeUnit.SECONDS, 1, TimeUnit.SECONDS);


    // Delete the increment metrics
    systemClient.delete(new MetricDeleteQuery(0, Integer.MAX_VALUE,
                                              Collections.emptySet(),
                                              metricsContext.getTags(),
                                              new ArrayList<>(metricsContext.getTags().keySet())));

    Tasks.waitFor(true, () -> {
      Collection<MetricTimeSeries> values = systemClient.query(metricsContext.getTags(),
                                                               Arrays.asList(""system.test.increment"",
                                                                             ""system.test.gauge""));
      // increment should be missing
      boolean foundInc = values.stream()
        .anyMatch(timeSeries -> timeSeries.getMetricName().equals(""system.test.increment""));

      // Find and match the values for gauge
      boolean foundGauge = values.stream()
        .anyMatch(timeSeries -> timeSeries.getMetricName().equals(""system.test.gauge""));

      return !foundInc && !foundGauge;
    }, 1000, TimeUnit.SECONDS, 1, TimeUnit.SECONDS);

    adminService.stopAndWait();
  }
"
"  @Test
  public void persistMetricsTests() throws Exception {

    injector.getInstance(TransactionManager.class).startAndWait();
    StructuredTableRegistry structuredTableRegistry = injector.getInstance(StructuredTableRegistry.class);
    structuredTableRegistry.initialize();
    StoreDefinition.createAllTables(injector.getInstance(StructuredTableAdmin.class), structuredTableRegistry);
    injector.getInstance(DatasetOpExecutorService.class).startAndWait();
    injector.getInstance(DatasetService.class).startAndWait();

    Set<Integer> partitions = IntStream.range(0, cConf.getInt(Constants.Metrics.MESSAGING_TOPIC_NUM))
      .boxed().collect(Collectors.toSet());

    long startTime = TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis());

    for (int iteration = 0; iteration < 50; iteration++) {
      // First publish all metrics before MessagingMetricsProcessorManagerService starts, so that fetchers of
      // different topics
      // will fetch metrics concurrently.
      for (int i = 0; i < 50; i++) {
        // TOPIC_PREFIX + (i % PARTITION_SIZE) decides which topic the metric is published to
        publishMessagingMetrics(i, startTime, METRICS_CONTEXT, expected, """", MetricType.COUNTER);
      }
      for (int i = 50; i < 100; i++) {
        // TOPIC_PREFIX + (i % PARTITION_SIZE) decides which topic the metric is published to
        publishMessagingMetrics(i, startTime, METRICS_CONTEXT, expected, """", MetricType.GAUGE);
      }

      final MockMetricStore metricStore = new MockMetricStore();
      // Create new MessagingMetricsProcessorManagerService instance every time because the same instance cannot be
      // started
      // again after it's stopped
      MessagingMetricsProcessorManagerService messagingMetricsProcessorManagerService =
        new MessagingMetricsProcessorManagerService(cConf, injector.getInstance(MetricDatasetFactory.class),
                                                    messagingService,
                                                    injector.getInstance(SchemaGenerator.class),
                                                    injector.getInstance(DatumReaderFactory.class), metricStore,
                                                    injector.getInstance(MetricsWriterProvider.class),
                                                    partitions, new NoopMetricsContext(), 50, 0);
      messagingMetricsProcessorManagerService.startAndWait();

      // Wait for the 1 aggregated counter metric (with value 50) and 50 gauge metrics to be stored in the metricStore
      Tasks.waitFor(51, () -> metricStore.getAllMetrics().size(), 15, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);

      assertMetricsResult(expected, metricStore.getAllMetrics());

      // validate metrics processor metrics
      // 50 counter and 50 gauge metrics are emitted in each iteration above
      Tasks.waitFor(100L, () -> metricStore.getMetricsProcessedByMetricsProcessor(),
                    15, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);

      // publish a dummy metric
      // this is to force the metrics processor to publish delay metrics for all the topics
      publishMessagingMetrics(100, startTime, METRICS_CONTEXT, expected, """", MetricType.GAUGE);
      // validate the newly published metric
      Tasks.waitFor(101L, () -> metricStore.getMetricsProcessedByMetricsProcessor(),
                    15, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);

      // in MessagingMetricsProcessorManagerService, before persisting the metrics and topic metas, a copy of the
      // topic metas
      // containing the metrics processor delay metrics is made before making a copy of metric values.
      // Therefore, there can be a very small chance where all metric values are persisted but the corresponding
      // topic metas are not yet persisted. Wait for all topic metas to be persisted
      Tasks.waitFor(true, metricStore::isMetricsProcessorDelayEmitted, 15, TimeUnit.SECONDS);

      // Clear metricStore and expected results for the next iteration
      metricStore.deleteAll();
      expected.clear();
      // Stop messagingMetricsProcessorManagerService
      messagingMetricsProcessorManagerService.stopAndWait();
    }
  }
"
"  @Test
  public void testMetricsProcessor() throws Exception {
    injector.getInstance(TransactionManager.class).startAndWait();
    StructuredTableRegistry structuredTableRegistry = injector.getInstance(StructuredTableRegistry.class);
    structuredTableRegistry.initialize();
    StoreDefinition.createAllTables(injector.getInstance(StructuredTableAdmin.class), structuredTableRegistry);
    injector.getInstance(DatasetOpExecutorService.class).startAndWait();
    injector.getInstance(DatasetService.class).startAndWait();

    final MetricStore metricStore = injector.getInstance(MetricStore.class);

    Set<Integer> partitions = new HashSet<>();
    for (int i = 0; i < cConf.getInt(Constants.Metrics.MESSAGING_TOPIC_NUM); i++) {
      partitions.add(i);
    }

    // Start KafkaMetricsProcessorService after metrics are published to Kafka

    // Intentionally set queue size to a small value, so that MessagingMetricsProcessorManagerService
    // internally can persist metrics when more messages are to be fetched
    MessagingMetricsProcessorManagerService messagingMetricsProcessorManagerService =
      new MessagingMetricsProcessorManagerService(cConf, injector.getInstance(MetricDatasetFactory.class),
                                                  messagingService, injector.getInstance(SchemaGenerator.class),
                                                  injector.getInstance(DatumReaderFactory.class),
                                                  metricStore, injector.getInstance(MetricsWriterProvider.class),
                                                  partitions, new NoopMetricsContext(), 50, 0);
    messagingMetricsProcessorManagerService.startAndWait();

    long startTime = TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis());
    // Publish metrics with messaging service and record expected metrics
    for (int i = 10; i < 20; i++) {
      publishMessagingMetrics(i, startTime, METRICS_CONTEXT, expected, SYSTEM_METRIC_PREFIX, MetricType.COUNTER);
    }

    Thread.sleep(500);
    // Stop and restart messagingMetricsProcessorManagerService
    messagingMetricsProcessorManagerService.stopAndWait();
    // Intentionally set queue size to a large value, so that MessagingMetricsProcessorManagerService
    // internally only persists metrics during terminating.
    messagingMetricsProcessorManagerService =
      new MessagingMetricsProcessorManagerService(cConf, injector.getInstance(MetricDatasetFactory.class),
                                                  messagingService, injector.getInstance(SchemaGenerator.class),
                                                  injector.getInstance(DatumReaderFactory.class),
                                                  metricStore, injector.getInstance(MetricsWriterProvider.class),
                                                  partitions, new NoopMetricsContext(), 50, 0);
    messagingMetricsProcessorManagerService.startAndWait();

    // Publish metrics after MessagingMetricsProcessorManagerService restarts and record expected metrics
    for (int i = 20; i < 30; i++) {
      publishMessagingMetrics(i, startTime, METRICS_CONTEXT, expected, SYSTEM_METRIC_PREFIX, MetricType.GAUGE);
    }

    final List<String> missingMetricNames = new ArrayList<>();
    // Wait until all expected metrics can be queried from the metric store. If not all expected metrics
    // are retrieved when timeout occurs, print out the missing metrics
    try {
      Tasks.waitFor(true, new Callable<Boolean>() {
        @Override
        public Boolean call() throws Exception {
          return canQueryAllMetrics(metricStore, METRICS_CONTEXT, expected, missingMetricNames);
        }
"
"  @Test
  public void testGetResolution() {
    MetricsQueryHelper helper = new MetricsQueryHelper(null, CConfiguration.create());
    try {
      // test  start > end time
      helper.getResolution(""auto"", 10000L, 100L);
      Assert.fail();
    } catch (IllegalArgumentException e) {
      // expected
    }

    try {
      // test resolution is auto, but not both start and end time are provided
      helper.getResolution(""auto"", null, 200L);
      Assert.fail();
    } catch (IllegalArgumentException e) {
      // expected
    }

    try {
      // test resolution is auto, but not both start and end time are provided
      helper.getResolution(""auto"", 200L, null);
      Assert.fail();
    } catch (IllegalArgumentException e) {
      // expected
    }

    try {
      // test non-existing resolution
      helper.getResolution(""6s"", 400L, 600L);
      Assert.fail();
    } catch (IllegalArgumentException e) {
      // expected
    }

    // test specific resolution
    Assert.assertEquals(1, helper.getResolution(""1s"", 100L, 10000L).intValue());
    Assert.assertEquals(60, helper.getResolution(""1m"", 1000L, 100000L).intValue());
    Assert.assertEquals(3600, helper.getResolution(""1h"", 100L, 10000L).intValue());
    Assert.assertEquals(60, helper.getResolution(""60s"", 100L, 10000L).intValue());

    // test resolution is auto
    // if 0 < ts diff <= 600, second resolution will be used
    Assert.assertEquals(1, helper.getResolution(""auto"", 0L, 300L).intValue());
    Assert.assertEquals(1, helper.getResolution(""auto"", 10000L, 10300L).intValue());
    Assert.assertEquals(1, helper.getResolution(""auto"", 0L, 600L).intValue());
    Assert.assertEquals(1, helper.getResolution(""auto"", 1000L, 1600L).intValue());

    // if 600 < ts diff <= 36000, minute resolution will be used
    Assert.assertEquals(60, helper.getResolution(""auto"", 0L, 601L).intValue());
    Assert.assertEquals(60, helper.getResolution(""auto"", 10000L, 10601L).intValue());
    Assert.assertEquals(60, helper.getResolution(""auto"", 0L, 36000L).intValue());
    Assert.assertEquals(60, helper.getResolution(""auto"", 10000L, 46000L).intValue());

    // if ts > 36000, hour resolution will be used
    Assert.assertEquals(3600, helper.getResolution(""auto"", 0L, 36001L).intValue());
    Assert.assertEquals(3600, helper.getResolution(""auto"", 1000L, 10000000L).intValue());

    // if resolution is null, and both start and end time provided, the logic should be same as auto
    Assert.assertEquals(1, helper.getResolution(null, 0L, 300L).intValue());
    Assert.assertEquals(1, helper.getResolution(null, 10000L, 10300L).intValue());
    Assert.assertEquals(1, helper.getResolution(null, 0L, 600L).intValue());
    Assert.assertEquals(1, helper.getResolution(null, 1000L, 1600L).intValue());
    Assert.assertEquals(60, helper.getResolution(null, 0L, 601L).intValue());
    Assert.assertEquals(60, helper.getResolution(null, 10000L, 10601L).intValue());
    Assert.assertEquals(60, helper.getResolution(null, 0L, 36000L).intValue());
    Assert.assertEquals(60, helper.getResolution(null, 10000L, 46000L).intValue());
    Assert.assertEquals(3600, helper.getResolution(null, 0L, 36001L).intValue());
    Assert.assertEquals(3600, helper.getResolution(null, 1000L, 10000000L).intValue());

    // if resolution is null, and either timestamp is not specified, minimum resolution will be used
    Assert.assertEquals(1, helper.getResolution(null, 0L, null).intValue());
    Assert.assertEquals(1, helper.getResolution(null, null, 10000000L).intValue());
  }
"
"  @Test
  public void testEquality() {
    TimeseriesId id1 = new TimeseriesId(""app.f.flow.flowlet.0"", ""process.events"", null, ""0"");
    TimeseriesId id2 = new TimeseriesId(""app.f.flow.flowlet.0"", ""process.events"", null, ""0"");
    Assert.assertTrue(id1.equals(id2));
    Assert.assertTrue(id2.equals(id1));
    Assert.assertEquals(id1.hashCode(), id2.hashCode());

    id1 = new TimeseriesId(""app.f.flow.flowlet.0"", ""process.events"", ""tag1"", ""0"");
    id2 = new TimeseriesId(""app.f.flow.flowlet.0"", ""process.events"", ""tag1"", ""0"");
    Assert.assertTrue(id1.equals(id2));
    Assert.assertTrue(id2.equals(id1));
    Assert.assertEquals(id1.hashCode(), id2.hashCode());
  }
"
"  @Test
  public void testPublish() throws InterruptedException {
    final BlockingQueue<MetricValues> published = new LinkedBlockingQueue<>();

    AggregatedMetricsCollectionService service = new AggregatedMetricsCollectionService(1000L) {
      @Override
      protected void publish(Iterator<MetricValues> metrics) {
        Iterators.addAll(published, metrics);
      }
    };

    service.startAndWait();

    // non-empty tags.
    final Map<String, String> baseTags = ImmutableMap.of(Constants.Metrics.Tag.NAMESPACE, NAMESPACE,
                                                         Constants.Metrics.Tag.APP, APP,
                                                         Constants.Metrics.Tag.SERVICE, SERVICE,
                                                         Constants.Metrics.Tag.RUN_ID, RUNID);

    try {
      // The first section tests with empty tags.
      // Publish couple metrics with empty tags, they should be aggregated.
      service.getContext(EMPTY_TAGS).increment(METRIC, Integer.MAX_VALUE);
      service.getContext(EMPTY_TAGS).increment(METRIC, 2);
      service.getContext(EMPTY_TAGS).increment(METRIC, 3);
      service.getContext(EMPTY_TAGS).increment(METRIC, 4);

      verifyCounterMetricsValue(published, ImmutableMap.of(0, ImmutableMap.of(METRIC, 9L + Integer.MAX_VALUE)));

      // No publishing for 0 value metrics
      Assert.assertNull(published.poll(3, TimeUnit.SECONDS));

      //update the metrics multiple times with gauge.
      service.getContext(EMPTY_TAGS).gauge(GAUGE_METRIC, 1);
      service.getContext(EMPTY_TAGS).gauge(GAUGE_METRIC, 2);
      service.getContext(EMPTY_TAGS).gauge(GAUGE_METRIC, 3);

      // gauge just updates the value, so polling should return the most recent value written
      verifyGaugeMetricsValue(published, ImmutableMap.of(0, 3L));

      // define collectors for non-empty tags
      MetricsContext baseCollector = service.getContext(baseTags);
      MetricsContext metricsContext = baseCollector.childContext(Constants.Metrics.Tag.HANDLER, HANDLER)
        .childContext(Constants.Metrics.Tag.INSTANCE_ID, INSTANCE);

      // increment metrics for various collectors
      baseCollector.increment(METRIC, Integer.MAX_VALUE);
      metricsContext.increment(METRIC, 5);
      baseCollector.increment(METRIC, 10);
      baseCollector.increment(METRIC, 3);
      metricsContext.increment(METRIC, 2);
      metricsContext.increment(METRIC, 4);
      metricsContext.increment(METRIC, 3);
      metricsContext.increment(METRIC, 1);

      // there are two collectors, verify their metrics values
      verifyCounterMetricsValue(published, ImmutableMap.of(4, ImmutableMap.of(METRIC, 13L + Integer.MAX_VALUE),
                                                           6, ImmutableMap.of(METRIC, 15L)));

      // No publishing for 0 value metrics
      Assert.assertNull(published.poll(3, TimeUnit.SECONDS));

      // gauge metrics for various collectors
      baseCollector.gauge(GAUGE_METRIC, Integer.MAX_VALUE);
      baseCollector.gauge(GAUGE_METRIC, 3);
      metricsContext.gauge(GAUGE_METRIC, 6);
      metricsContext.gauge(GAUGE_METRIC, 2);
      baseCollector.gauge(GAUGE_METRIC, 1);
      metricsContext.gauge(GAUGE_METRIC, Integer.MAX_VALUE);

      // gauge just updates the value, so polling should return the most recent value written
      verifyGaugeMetricsValue(published, ImmutableMap.of(4, 1L, 6, (long) Integer.MAX_VALUE));

      metricsContext.gauge(GAUGE_METRIC, 0);
      verifyCounterMetricsValue(published, ImmutableMap.of(6, ImmutableMap.of(GAUGE_METRIC, 0L)));
    } finally {
      service.stopAndWait();
    }
  }
"
"  @Test
  public void testMessagingPublish() throws TopicNotFoundException {

    MetricsCollectionService collectionService = new MessagingMetricsCollectionService(CConfiguration.create(),
                                                                                       messagingService,
                                                                                       recordWriter);
    collectionService.startAndWait();

    // publish metrics for different context
    for (int i = 1; i <= 3; i++) {
      collectionService.getContext(ImmutableMap.of(""tag"", """" + i)).increment(""processed"", i);
    }

    collectionService.stopAndWait();

    // <Context, metricName, value>
    Table<String, String, Long> expected = HashBasedTable.create();
    expected.put(""tag.1"", ""processed"", 1L);
    expected.put(""tag.2"", ""processed"", 2L);
    expected.put(""tag.3"", ""processed"", 3L);

    ReflectionDatumReader<MetricValues> recordReader = new ReflectionDatumReader<>(schema, metricValueType);
    assertMetricsFromMessaging(schema, recordReader, expected);
  }
"
"  @Test
  public void testTypeHandlerRequests() throws Exception {
    Assert.assertEquals(""listModules"", doRequest(""/namespaces/myspace/data/modules"", ""GET""));
    Assert.assertEquals(""post:myModule"", doRequest(""/namespaces/myspace/data/modules/myModule"", ""POST""));
    Assert.assertEquals(""delete:myModule"", doRequest(""/namespaces/myspace/data/modules/myModule"", ""DELETE""));
    Assert.assertEquals(""get:myModule"", doRequest(""/namespaces/myspace/data/modules/myModule"", ""GET""));
    Assert.assertEquals(""listTypes"", doRequest(""/namespaces/myspace/data/types"", ""GET""));
    Assert.assertEquals(""getType:myType"", doRequest(""/namespaces/myspace/data/types/myType"", ""GET""));
  }
"
"  @Test
  public void testInstanceHandlerRequests() throws Exception {
    Assert.assertEquals(""list"", doRequest(""/namespaces/myspace/data/datasets"", ""GET""));
    Assert.assertEquals(""post:myInstance"",
                        doRequest(""/namespaces/myspace/data/datasets/myInstance"", ""POST""));
    Assert.assertEquals(""delete:myInstance"",
                        doRequest(""/namespaces/myspace/data/datasets/myInstance"", ""DELETE""));
    Assert.assertEquals(""get:myInstance"",
                        doRequest(""/namespaces/myspace/data/datasets/myInstance"", ""GET""));
  }
"
"  @Test
  public void testCorrectNumberInClassPath() throws Exception {
    Assert.assertEquals(ExpectedNumberOfAuditPolicyPaths.EXPECTED_PATH_NUMBER, AUDIT_LOOK_UP.getNumberOfPaths());
  }
"
"  @Test
  public void testDataFabricEndpoints() throws Exception {
    // endpoints from DatasetInstanceHandler
    assertContent(""/v3/namespaces/default/data/datasets/myDataset"", DEFAULT_AUDIT);
    // endpoints from DatasetTypeHandler
    assertContent(""/v3/namespaces/default/data/modules/myModule"",
                  new AuditLogConfig(HttpMethod.PUT, false, false, ImmutableList.of(""X-Class-Name"")));
  }
"
"  @Test
  public void testAppFabricEndpoints() throws Exception {
    // endpoints from AppLifecycleHttpHandler
    assertContent(""/v3/namespaces/default/apps/myApp"", DEFAULT_AUDIT);
    assertContent(""/v3/namespaces/default/apps"",
                  new AuditLogConfig(HttpMethod.POST, false, true,
                                     ImmutableList.of(AbstractAppFabricHttpHandler.ARCHIVE_NAME_HEADER,
                                                       AbstractAppFabricHttpHandler.APP_CONFIG_HEADER,
                                                       AbstractAppFabricHttpHandler.PRINCIPAL_HEADER,
                                                       AbstractAppFabricHttpHandler.SCHEDULES_HEADER)));
    // endpoints from ArtifactHttpHandler
    assertContent(""/v3/namespaces/default/artifacts/myArtifact/versions/1.0/properties"", DEFAULT_AUDIT);
    assertContent(""/v3/namespaces/default/artifacts/myArtifact"",
                  new AuditLogConfig(HttpMethod.POST, false, false,
                                     ImmutableList.of(""Artifact-Version"", ""Artifact-Extends"", ""Artifact-Plugins"")));
    // endpoints from AuthorizationHandler
    assertContent(""/v3/security/authorization/privileges/grant"",
                  new AuditLogConfig(HttpMethod.POST, true, false, EMPTY_HEADERS));
    // endpoints from ConsoleSettingsHttpHandler
    assertContent(""/v3/configuration/user/"", DEFAULT_AUDIT);
    // endpoints from MetadataHttpHandler
    assertContent(""/v3/namespaces/default/apps/app1/metadata/properties"",
                  new AuditLogConfig(HttpMethod.POST, true, false, EMPTY_HEADERS));
    // endpoints from MonitorHttpHandler
    assertContent(""/v3/system/services/appfabric/instances"", DEFAULT_AUDIT);
    // endpoints from NamespaceHttpHandler
    assertContent(""/v3/namespaces/default"", DEFAULT_AUDIT);
    // endpoints from PreferencesHttpHandler
    assertContent(""/v3/preferences"", DEFAULT_AUDIT);
    // endpoints from ProgramLifecycleHttpHandler
    assertContent(""/v3/namespaces/default/stop"", new AuditLogConfig(HttpMethod.POST, true, true, EMPTY_HEADERS));
    // endpoints from SecureStoreHandler
    assertContent(""/v3/namespaces/default/securekeys/myKey"", DEFAULT_AUDIT);
    // endpoints from TransactionHttpHandler
    assertContent(""/v3/transactions/invalid/remove/until"",
                  new AuditLogConfig(HttpMethod.POST, true, false, EMPTY_HEADERS));
  }
"
"  @Test
  public void testExploreEndpoints() throws Exception {
    // endpoints from ExploreExecutorHttpHandler
    assertContent(""/v3/namespaces/default/data/explore/datasets/myDataset/update"",
                  new AuditLogConfig(HttpMethod.POST, true, false, EMPTY_HEADERS));
    // endpoints from NamespacedExploreMetadataHttpHandler
    assertContent(""/v3/namespaces/default/data/explore/jdbc/tables"",
                  new AuditLogConfig(HttpMethod.POST, true, false, EMPTY_HEADERS));
    // endpoints from NamespacedExploreQueryExecutorHttpHandler
    assertContent(""/v3/namespaces/default/data/explore/queries"",
                  new AuditLogConfig(HttpMethod.POST, true, false, EMPTY_HEADERS));
  }
"
"  @Test
  public void testRouterAuthBypass() throws Exception {
    // mock token validator passes for any token other than ""Bearer failme""
    testGet(200, ""hello"", ""/v1/echo/hello"", ImmutableMap.of(""Authorization"", ""Bearer x""));
    // so this should fail
    testGet(401, null, ""/v1/echo/hello"");
    testGet(401, null, ""/v1/echo/hello"", ImmutableMap.of(""Authorization"", ""Bearer failme""));
    // but /v1/echo/dontfail is configured to bypass auth
    testGet(200, ""dontfail"", ""/v1/echo/dontfail"", ImmutableMap.of(""Authorization"", ""Bearer failme""));
    // it only bypasses on exact match, not prefix match
    testGet(401, null, ""/v1/echo/dontfailme"", ImmutableMap.of(""Authorization"", ""Bearer failme""));

    // /v1/repeat is configured to bypass auth validation, on prefix match
    testGet(200, ""hello"", ""/v1/repeat/hello"");
    testGet(200, ""hello"", ""/v1/repeat/hello"", ImmutableMap.of(""Authorization"", ""Bearer x""));
    testGet(200, ""hello"", ""/v1/repeat/hello"", ImmutableMap.of(""Authorization"", ""Bearer failme""));
    // even with a token that fails validation, we get the correct status code 404
    testGet(404, null, ""/v1/repeat/dontfail/me"", ImmutableMap.of(""Authorization"", ""Bearer failme""));
  }
"
"  @Test
  public void testAuditLog() throws IOException {
    HttpURLConnection urlConn = createURLConnection(""/get"", HttpMethod.GET);
    Assert.assertEquals(200, urlConn.getResponseCode());
    urlConn.getInputStream().close();

    urlConn = createURLConnection(""/put"", HttpMethod.PUT);
    urlConn.getOutputStream().write(""Test Put"".getBytes(StandardCharsets.UTF_8));
    Assert.assertEquals(200, urlConn.getResponseCode());
    Assert.assertEquals(""Test Put"", new String(ByteStreams.toByteArray(urlConn.getInputStream()), ""UTF-8""));
    urlConn.getInputStream().close();

    urlConn = createURLConnection(""/post"", HttpMethod.POST);
    urlConn.getOutputStream().write(""Test Post"".getBytes(StandardCharsets.UTF_8));
    Assert.assertEquals(200, urlConn.getResponseCode());
    Assert.assertEquals(""Test Post"", new String(ByteStreams.toByteArray(urlConn.getInputStream()), ""UTF-8""));
    urlConn.getInputStream().close();

    urlConn = createURLConnection(""/postHeaders"", HttpMethod.POST);
    urlConn.setRequestProperty(""user-id"", ""cdap"");
    urlConn.getOutputStream().write(""Post Headers"".getBytes(StandardCharsets.UTF_8));
    Assert.assertEquals(200, urlConn.getResponseCode());
    Assert.assertEquals(""Post Headers"", new String(ByteStreams.toByteArray(urlConn.getInputStream()), ""UTF-8""));
    urlConn.getInputStream().close();

    List<String> loggedMessages = TestLogAppender.INSTANCE.getLoggedMessages();
    Assert.assertEquals(4, loggedMessages.size());

    Assert.assertTrue(loggedMessages.get(0).endsWith(""\""GET /get HTTP/1.1\"" - - 200 0 -""));
    Assert.assertTrue(loggedMessages.get(1).endsWith(""\""PUT /put HTTP/1.1\"" - Test Put 200 8 -""));
    Assert.assertTrue(loggedMessages.get(2).endsWith(""\""POST /post HTTP/1.1\"" - Test Post 200 9 Test Post""));
    Assert.assertTrue(
      loggedMessages.get(3).endsWith(""\""POST /postHeaders HTTP/1.1\"" {user-id=cdap} Post Headers 200 12 Post Headers""));
  }
"
"  @Test
  public void testGuiceInjection() {
    CConfiguration cConf = CConfiguration.create();

    Injector injector = RouterMain.createGuiceInjector(cConf);
    Assert.assertNotNull(injector);

    NettyRouter router = injector.getInstance(NettyRouter.class);
    Assert.assertNotNull(router);
  }
"
"  @Test
  public void testChunkRequestSuccess() throws Exception {

    AsyncHttpClientConfig.Builder configBuilder = new AsyncHttpClientConfig.Builder();

    final AsyncHttpClient asyncHttpClient = new AsyncHttpClient(
      new NettyAsyncHttpProvider(configBuilder.build()),
      configBuilder.build());

    byte [] requestBody = generatePostData();
    InetSocketAddress address = ROUTER.getRouterAddress();
    final Request request = new RequestBuilder(""POST"")
      .setUrl(String.format(""http://%s:%d%s"", address.getHostName(), address.getPort(), ""/v1/upload""))
      .setContentLength(requestBody.length)
      .setBody(new ByteEntityWriter(requestBody))
      .build();

    final ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream();
    Future<Void> future = asyncHttpClient.executeRequest(request, new AsyncCompletionHandler<Void>() {
      @Override
      public Void onCompleted(Response response) {
        return null;
      }
"
"  @Test
  public void testDeployNTimes() throws Exception {
    // regression tests for race condition during multiple deploys.
    deploy(100);
  }
"
"  @Test
  public void testHttpPipelining() throws Exception {
    final BlockingQueue<HttpResponseStatus> responseStatuses = new LinkedBlockingQueue<>();
    EventLoopGroup eventGroup = new NioEventLoopGroup();

    Bootstrap bootstrap = new Bootstrap()
      .channel(NioSocketChannel.class)
      .group(eventGroup)
      .handler(new ChannelInitializer<SocketChannel>() {
        @Override
        protected void initChannel(SocketChannel ch) {
          ChannelPipeline pipeline = ch.pipeline();
          pipeline.addLast(""codec"", new HttpClientCodec());
          pipeline.addLast(""aggregator"", new HttpObjectAggregator(1048576));
          pipeline.addLast(""handler"", new ChannelInboundHandlerAdapter() {
            @Override
            public void channelRead(ChannelHandlerContext ctx, Object msg) {
              if (msg instanceof HttpResponse) {
                responseStatuses.add(((HttpResponse) msg).status());
              }
              ReferenceCountUtil.release(msg);
            }
"
"  @Test
  public void testEmptyAnnounceAddressURLsConfig() throws Exception {
    HttpRouterService routerService = new AuthServerAnnounceTest.HttpRouterService(HOSTNAME, DISCOVERY_SERVICE);
    routerService.startUp();
    try {
      Assert.assertEquals(Collections.EMPTY_LIST, getAuthURI(routerService));
    } finally {
      routerService.shutDown();
    }
  }
"
"  @Test
  public void testAnnounceURLsConfig() throws Exception {
    HttpRouterService routerService = new AuthServerAnnounceTest.HttpRouterService(HOSTNAME, DISCOVERY_SERVICE);
    routerService.cConf.set(Constants.Security.AUTH_SERVER_ANNOUNCE_URLS, ANNOUNCE_URLS);
    routerService.startUp();
    try {
      List<String> expected = Stream.of(ANNOUNCE_URLS.split("",""))
        .map(url -> String.format(""%s/%s"", url, GrantAccessToken.Paths.GET_TOKEN))
        .collect(Collectors.toList());
      Assert.assertEquals(expected, getAuthURI(routerService));
    } finally {
      routerService.shutDown();
    }
  }
"
"  @Test
  public void testRouterSync() throws Exception {
    testSync(25);
    // sticky endpoint strategy used so the sum should be 25
    Assert.assertEquals(25, defaultServer1.getNumRequests() + defaultServer2.getNumRequests());
  }
"
"  @Test
  public void testRouterAsync() throws Exception {
    int numElements = 123;
    AsyncHttpClientConfig.Builder configBuilder = new AsyncHttpClientConfig.Builder();

    final AsyncHttpClient asyncHttpClient = new AsyncHttpClient(
      new NettyAsyncHttpProvider(configBuilder.build()),
      configBuilder.build());

    final CountDownLatch latch = new CountDownLatch(numElements);
    final AtomicInteger numSuccessfulRequests = new AtomicInteger(0);
    for (int i = 0; i < numElements; ++i) {
      final int elem = i;
      final Request request = new RequestBuilder(""GET"")
        .setUrl(resolveURI(String.format(""%s/%s-%d"", ""/v1/echo"", ""async"", i)))
        .build();
      asyncHttpClient.executeRequest(request, new AsyncCompletionHandler<Void>() {
        @Override
        public Void onCompleted(Response response) throws Exception {
          latch.countDown();
          Assert.assertEquals(HttpResponseStatus.OK.code(), response.getStatusCode());
          String responseBody = response.getResponseBody();
          LOG.trace(""Got response {}"", responseBody);
          Assert.assertEquals(""async-"" + elem, responseBody);
          numSuccessfulRequests.incrementAndGet();
          return null;
        }
"
"  @Test
  public void testRouterOneServerDown() throws Exception {
    // Bring down defaultServer1
    defaultServer1.cancelRegistration();

    testSync(25);
    Assert.assertEquals(0, defaultServer1.getNumRequests());
    Assert.assertTrue(defaultServer2.getNumRequests() > 0);

    defaultServer1.registerServer();
  }
"
"  @Test
  public void testRouterAllServersDown() throws Exception {
    // Bring down all servers
    defaultServer1.cancelRegistration();
    defaultServer2.cancelRegistration();

    testSyncServiceUnavailable();
    Assert.assertEquals(0, defaultServer1.getNumRequests());
    Assert.assertEquals(0, defaultServer2.getNumRequests());

    defaultServer1.registerServer();
    defaultServer2.registerServer();
  }
"
"  @Test
  public void testHostForward() throws Exception {
    // Test defaultService
    HttpResponse response = get(resolveURI(String.format(""%s/%s"", ""/v1/ping"", ""sync"")));
    Assert.assertEquals(HttpResponseStatus.OK.code(), response.getStatusLine().getStatusCode());
    Assert.assertEquals(APP_FABRIC_SERVICE, EntityUtils.toString(response.getEntity()));
  }
"
"  @Test
  public void testUpload() throws Exception {
    AsyncHttpClientConfig.Builder configBuilder = new AsyncHttpClientConfig.Builder();

    final AsyncHttpClient asyncHttpClient = new AsyncHttpClient(
      new NettyAsyncHttpProvider(configBuilder.build()),
      configBuilder.build());

    byte [] requestBody = generatePostData();
    final Request request = new RequestBuilder(""POST"")
      .setUrl(resolveURI(""/v1/upload""))
      .setContentLength(requestBody.length)
      .setBody(new ByteEntityWriter(requestBody))
      .build();

    final ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream();
    Future<Void> future = asyncHttpClient.executeRequest(request, new AsyncCompletionHandler<Void>() {
      @Override
      public Void onCompleted(Response response) {
        return null;
      }
"
"  @Test
  public void testConnectionClose() throws Exception {
    URL[] urls = new URL[] {
      new URL(resolveURI(""/abc/v1/status"")),
      new URL(resolveURI(""/def/v1/status""))
    };

    // Make bunch of requests to one service to 2 difference urls, with the first one keep-alive, second one not.
    // This make router creates two backend service connections on the same inbound connection
    // This is to verify on the close of the second one, it won't close the the inbound if there is an
    // in-flight request happening already (if reached another round of the following for-loop).
    int times = 1000;
    boolean keepAlive = true;
    for (int i = 0; i < times; i++) {
      HttpURLConnection urlConn = openURL(urls[i % urls.length]);
      try {
        urlConn.setRequestProperty(HttpHeaderNames.CONNECTION.toString(),
                                   (keepAlive ? HttpHeaderValues.KEEP_ALIVE : HttpHeaderValues.CLOSE).toString());
        Assert.assertEquals(HttpURLConnection.HTTP_OK, urlConn.getResponseCode());
      } finally {
        urlConn.getInputStream().close();
        keepAlive = !keepAlive;
        urlConn.disconnect();
      }
    }

    Assert.assertEquals(times, defaultServer1.getNumRequests() + defaultServer2.getNumRequests());
  }
"
"  @Test
  public void testConnectionIdleTimeout() throws Exception {
    // Only use server1
    defaultServer2.cancelRegistration();

    String path = ""/v2/ping"";
    URI uri = new URI(resolveURI(path));
    Socket socket = getSocketFactory().createSocket(uri.getHost(), uri.getPort());
    PrintWriter out = new PrintWriter(socket.getOutputStream(), true);
    InputStream inputStream = socket.getInputStream();

    // make a request
    String firstLine = makeRequest(uri, out, inputStream);
    Assert.assertEquals(""HTTP/1.1 200 OK"", firstLine);

    // sleep for 500 ms below the configured idle timeout; the connection on server side should not get closed by then
    // Hence it should be reusing the same server side connection
    TimeUnit.MILLISECONDS.sleep(TimeUnit.SECONDS.toMillis(CONNECTION_IDLE_TIMEOUT_SECS) - 500);
    firstLine = makeRequest(uri, out, inputStream);
    Assert.assertEquals(""HTTP/1.1 200 OK"", firstLine);

    // sleep for 500 ms over the configured idle timeout; the connection on server side should get closed by then
    // Hence it should be create a new server side connection
    TimeUnit.MILLISECONDS.sleep(TimeUnit.SECONDS.toMillis(CONNECTION_IDLE_TIMEOUT_SECS) + 500);
    // Due to timeout the client connection will be closed, and hence this request should not go to the server
    firstLine = makeRequest(uri, out, inputStream);
    Assert.assertEquals(""HTTP/1.1 200 OK"", firstLine);

    // assert that the connection is closed on the server side
    Assert.assertEquals(3, defaultServer1.getNumRequests());
    Assert.assertEquals(2, defaultServer1.getNumConnectionsOpened());
    Assert.assertEquals(1, defaultServer1.getNumConnectionsClosed());
  }
"
"  @Test
  public void testConnectionIdleTimeoutWithMultipleServers() throws Exception {
    defaultServer2.cancelRegistration();

    URL url = new URL(resolveURI(""/v2/ping""));
    HttpURLConnection urlConnection = openURL(url);
    Assert.assertEquals(200, urlConnection.getResponseCode());
    urlConnection.getInputStream().close();
    urlConnection.disconnect();

    // requests past this point will go to defaultServer2
    defaultServer1.cancelRegistration();
    defaultServer2.registerServer();

    for (int i = 0; i < 4; i++) {
      // this is an assumption that CONNECTION_IDLE_TIMEOUT_SECS is more than 1 second
      TimeUnit.SECONDS.sleep(1);
      url = new URL(resolveURI(""/v1/ping/"" + i));
      urlConnection = openURL(url);
      Assert.assertEquals(200, urlConnection.getResponseCode());
      urlConnection.getInputStream().close();
      urlConnection.disconnect();
    }

    // for the past 4 seconds, we've been making requests to defaultServer2; therefore, defaultServer1 will have closed
    // its single connection
    Assert.assertEquals(1, defaultServer1.getNumConnectionsOpened());
    Assert.assertEquals(1, defaultServer1.getNumConnectionsClosed());

    // however, the connection to defaultServer2 is not timed out, because we've been making requests to it
    Assert.assertEquals(1, defaultServer2.getNumConnectionsOpened());
    Assert.assertEquals(0, defaultServer2.getNumConnectionsClosed());

    defaultServer2.registerServer();
    defaultServer1.cancelRegistration();
    url = new URL(resolveURI(""/v2/ping""));
    urlConnection = openURL(url);
    Assert.assertEquals(200, urlConnection.getResponseCode());
    urlConnection.getInputStream().close();
    urlConnection.disconnect();
  }
"
"  @Test (timeout = 5000L)
  public void testExpectContinue() throws Exception {
    URL url = new URL(resolveURI(""/v2/upload""));
    HttpURLConnection urlConn = openURL(url);
    urlConn.setRequestMethod(""POST"");
    urlConn.setRequestProperty(HttpHeaderNames.EXPECT.toString(), HttpHeaderValues.CONTINUE.toString());
    urlConn.setDoOutput(true);

    // Forces sending small chunks to have the netty server receives multiple chunks
    urlConn.setChunkedStreamingMode(10);
    String msg = Strings.repeat(""Message"", 100);
    urlConn.getOutputStream().write(msg.getBytes(StandardCharsets.UTF_8));

    Assert.assertEquals(200, urlConn.getResponseCode());
    String result = new String(ByteStreams.toByteArray(urlConn.getInputStream()), StandardCharsets.UTF_8);
    Assert.assertEquals(msg, result);
  }
"
"    @Test
    public void testGetFullTableName() {
        MatcherAssert.assertThat(
                Schemas.getFullTableName(TABLE_NAME, NAMESPACE),
                Matchers.equalTo(NAMESPACE.getName() + ""."" + TABLE_NAME));
    }
"
"    @Test
    public void testGetFullTableNameLegacy() {
        MatcherAssert.assertThat(
                Schemas.getFullTableName(TABLE_NAME, Namespace.create(""met"")),
                Matchers.equalTo(TABLE_NAME)
        );
    }
"
"    @Test
    public void testGetFullTableNameEmptyNamespace() {
        MatcherAssert.assertThat(
                Schemas.getFullTableName(TABLE_NAME, Namespace.EMPTY_NAMESPACE),
                Matchers.equalTo(TABLE_NAME)
        );
    }
"
"    @Test
    public void testCreateTable() {
        mockery.checking(new Expectations(){{
            oneOf(kvs).createTables(with(tableMapContainsEntry(TABLE_REF, getSimpleTableDefinitionAsBytes(TABLE_REF))));
        }});
        Schemas.createTable(kvs, TABLE_REF, getSimpleTableDefinition(TABLE_REF));
    }
"
"    @Test
    public void testCreateTables() {
        TableReference tableName1 = TableReference.createWithEmptyNamespace(TABLE_NAME + ""1"");
        TableReference tableName2 = TableReference.createWithEmptyNamespace(TABLE_NAME + ""2"");
        mockery.checking(new Expectations(){{
            oneOf(kvs).createTables(with(tableMapContainsEntry(tableName1, getSimpleTableDefinitionAsBytes(tableName1))));
            oneOf(kvs).createTables(with(tableMapContainsEntry(tableName2, getSimpleTableDefinitionAsBytes(tableName2))));
        }});
        Map<TableReference, TableDefinition> tables = Maps.newHashMap();
        tables.put(tableName1, getSimpleTableDefinition(tableName1));
        tables.put(tableName2, getSimpleTableDefinition(tableName2));
        Schemas.createTables(kvs, tables);
    }
"
"    @Test
    public void testDeleteTable() {
        mockery.checking(new Expectations(){{
            oneOf(kvs).dropTable(with(equal(TABLE_REF)));
        }});
        Schemas.deleteTable(kvs, TABLE_REF);
    }
"
"    @Test
    public void testDeleteTablesForSweepSchema() {
        Set<TableReference> allTableNames = Sets.newHashSet();
        allTableNames.add(TableReference.createFromFullyQualifiedName(""sweep.progress""));
        allTableNames.add(TableReference.createFromFullyQualifiedName(""sweep.priority""));

        mockery.checking(new Expectations(){{
            oneOf(kvs).getAllTableNames(); will(returnValue(allTableNames));
            oneOf(kvs).dropTables(allTableNames);
            oneOf(kvs).getAllTableNames();
        }});
        Schemas.deleteTablesAndIndexes(SweepSchema.INSTANCE.getLatestSchema(), kvs);
    }
"
"    @Test
    public void testWritePerf() throws ExecutionException, InterruptedException {
        final long startTime = System.currentTimeMillis();
        final Future<Pair<Long, Set<byte[]>>>
            f1 = submitWriteJob(0, BATCH_SIZE / 4),
            f2 = submitWriteJob(BATCH_SIZE / 4, BATCH_SIZE / 2),
            f3 = submitWriteJob(BATCH_SIZE / 2, 3 * BATCH_SIZE / 4),
            f4 = submitWriteJob(3 * BATCH_SIZE / 4, BATCH_SIZE);
        final long rawBytes = f1.get().lhSide
                              + f2.get().lhSide
                              + f3.get().lhSide
                              + f4.get().lhSide;
        final long elapsedTime = System.currentTimeMillis() - startTime;
        final double elapsedSeconds = elapsedTime / 1000.0;
        final double megs = rawBytes / (1024.0 * 1024.0);
        System.out.println(""MB = "" + megs);
        System.out.println(""MB/s = "" + (megs/elapsedSeconds));
    }
"
"    @Test
    public void testCreate() {
        TableReference otherTable = TableReference.createWithEmptyNamespace(""yodog"");
        db.createTable(TABLE, AtlasDbConstants.EMPTY_TABLE_METADATA);
        db.createTable(otherTable, AtlasDbConstants.EMPTY_TABLE_METADATA);
        db.createTable(TRANSACTION_TABLE, AtlasDbConstants.EMPTY_TABLE_METADATA);
        assertEquals(ImmutableSet.of(TABLE, otherTable, TRANSACTION_TABLE),
                db.getAllTableNames());
    }
"
"    @Test
    public void testReadNoExist() {
        final Cell cell = Cell.create(""r1"".getBytes(), COMMIT_TS_COLUMN);
        final Map<Cell, Value> res = db.get(TABLE, ImmutableMap.of(cell, 1L));
        assertTrue(res.isEmpty());
    }
"
"    @Test
    public void testReadGood() {
        final Cell cell = Cell.create(""r1"".getBytes(), ""2"".getBytes());
        db.put(TABLE, ImmutableMap.of(cell, ""v1"".getBytes()), 1);
        final Map<Cell, Value> res = db.get(TABLE, ImmutableMap.of(cell, 2L));
        assertEquals(1, res.size());
        final Value value = res.get(cell);
        assertEquals(1, value.getTimestamp());
        assertEquals(""v1"", new String(value.getContents()));
    }
"
"    @Test
    public void testReadGood2() {
        final Cell cell = Cell.create(""r1"".getBytes(), ""2"".getBytes());
        final Cell cell2 = Cell.create(""r"".getBytes(), ""12"".getBytes());
        db.put(TABLE, ImmutableMap.of(cell, ""v1"".getBytes()), 1000);
        db.put(TABLE, ImmutableMap.of(cell2, ""v2"".getBytes()), 1000);
        final Map<Cell, Value> res = db.get(TABLE, ImmutableMap.of(cell, 1001L));
        final Value value = res.get(cell);
        assertEquals(1000, value.getTimestamp());
        assertEquals(""v1"", new String(value.getContents()));
    }
"
"    @Test
    public void testReadGood3() {
        final Cell cell = Cell.create(""r1"".getBytes(), COMMIT_TS_COLUMN);
        db.put(TABLE, ImmutableMap.of(cell, ""v1"".getBytes()), Long.MAX_VALUE - 3);
        final Map<Cell, Value> res = db.get(TABLE, ImmutableMap.of(cell, Long.MAX_VALUE - 2));
        final Value value = res.get(cell);
        assertEquals(Long.MAX_VALUE - 3, value.getTimestamp());
        assertEquals(""v1"", new String(value.getContents()));
    }
"
"    @Test
    public void testReadGood4() {
        final Cell cell = Cell.create(""r,1"".getBytes(), "",c,1,"".getBytes());
        db.put(TABLE, ImmutableMap.of(cell, ""v,1"".getBytes()), 1);
        final Map<Cell, Value> res = db.get(TABLE, ImmutableMap.of(cell, 2L));
        final Value value = res.get(cell);
        assertEquals(1, value.getTimestamp());
        assertEquals(""v,1"", new String(value.getContents()));
    }
"
"    @Test
    public void testReadBeforeTime() {
        final Cell cell = Cell.create(""r1"".getBytes(), COMMIT_TS_COLUMN);
        db.put(TABLE, ImmutableMap.of(cell, ""v1"".getBytes()), 2);
        final Map<Cell, Value> res = db.get(TABLE, ImmutableMap.of(cell, 2L));
        assertTrue(res.isEmpty());
    }
"
"    @Test
    public void testGetRow() {
        final Cell cell = Cell.create(""r1"".getBytes(), ""c1"".getBytes());
        final Cell cell2 = Cell.create(""r1"".getBytes(), ""c2"".getBytes());
        db.put(TABLE, ImmutableMap.of(cell, ""v1"".getBytes()), 2);
        db.put(TABLE, ImmutableMap.of(cell2, ""v2"".getBytes()), 2);
        final Map<Cell, Value> rows = db.getRows(TABLE, ImmutableList.of(""r1"".getBytes()), ColumnSelection.all(), 3);
        assertEquals(2, rows.size());
    }
"
"    @Test
    public void testGetRange() {
        final Cell cell = Cell.create(""r1"".getBytes(), ""c1"".getBytes());
        final Cell cell2 = Cell.create(""r1"".getBytes(), ""c2"".getBytes());
        final Cell cell3 = Cell.create(""r2"".getBytes(), ""c2"".getBytes());
        db.put(TABLE, ImmutableMap.of(cell, ""v1"".getBytes()), 2);
        db.put(TABLE, ImmutableMap.of(cell2, ""v2"".getBytes()), 2);
        db.put(TABLE, ImmutableMap.of(cell3, ""v3"".getBytes()), 4);
        final RangeRequest range = RangeRequest.builder().endRowExclusive(""r2"".getBytes()).build();
        final ClosableIterator<? extends RowResult<Value>> it = db.getRange(TABLE, range, 10);
        try {
            final List<RowResult<Value>> list = Lists.newArrayList();
            Iterators.addAll(list, it);
            assertEquals(1, list.size());
            final Map<Cell, Value> rows = db.getRows(TABLE, ImmutableList.of(""r1"".getBytes()), ColumnSelection.all(), 3);
            assertEquals(2, rows.size());
            final RowResult<Value> row = list.iterator().next();
            final Map<Cell, Value> cellsFromRow = putAll(Maps.<Cell, Value>newHashMap(), row.getCells());
            assertEquals(rows, cellsFromRow);
        } finally {
            it.close();
        }
    }
"
"    @Test
    public void testGetRange2() {
        final Cell cell = Cell.create("",r,1"".getBytes(), "",c,1,"".getBytes());
        db.put(TABLE, ImmutableMap.of(cell, ""v1"".getBytes()), 2);
        final RangeRequest range = RangeRequest.builder().build();
        final ClosableIterator<RowResult<Value>> it = db.getRange(TABLE, range, 10);
        try {
            final List<RowResult<Value>> list = Lists.newArrayList();
            Iterators.addAll(list, it);
            assertEquals(1, list.size());
            final RowResult<Value> row = list.iterator().next();
            final Map<Cell, Value> cellsFromRow = putAll(Maps.<Cell, Value>newHashMap(), row.getCells());
            final Map<Cell, Value> rows = db.getRows(TABLE, ImmutableList.of("",r,1"".getBytes()), ColumnSelection.all(), 3);
            assertEquals(rows, cellsFromRow);
        } finally {
            it.close();
        }
    }
"
"    @Test
    public void testGetRowCellOverlap() {
        final Cell cell = Cell.create(""12"".getBytes(), ""34"".getBytes());
        final Cell cell2 = Cell.create(""1"".getBytes(), ""23"".getBytes());
        db.put(TABLE, ImmutableMap.of(cell, ""v1"".getBytes()), 2);
        db.put(TABLE, ImmutableMap.of(cell2, ""v2"".getBytes()), 2);
        final Map<Cell, Value> rows = db.getRows(TABLE, ImmutableList.of(""12"".getBytes()), ColumnSelection.all(), 3);
        assertEquals(1, rows.size());
    }
"
"    @Test
    public void testGetRangeCellOverlap() {
        final Cell cell = Cell.create(""12"".getBytes(), ""34"".getBytes());
        final Cell cell2 = Cell.create(""1"".getBytes(), ""235"".getBytes());
        db.put(TABLE, ImmutableMap.of(cell, ""v1"".getBytes()), 2);
        db.put(TABLE, ImmutableMap.of(cell2, ""v2"".getBytes()), 2);
        ClosableIterator<? extends RowResult<Value>> it = db.getRange(TABLE, RangeRequest.builder().build(), 3);
        try {
            assertEquals(2, Iterators.size(it));
        } finally {
            it.close();
        }
        it = db.getRange(TABLE, RangeRequest.builder().endRowExclusive(""12"".getBytes()).build(), 3);
        try {
            assertEquals(1, Iterators.size(it));
        } finally {
            it.close();
        }
        it = db.getRange(TABLE, RangeRequest.builder().startRowInclusive(""12"".getBytes()).build(), 3);
        try {
            assertEquals(1, Iterators.size(it));
        } finally {
            it.close();
        }
    }
"
"    @Test
    public void testGetRangeCellOverlap2() {
        final Cell cell = Cell.create(""1"".getBytes(), ""1"".getBytes());
        final Cell cell2 = Cell.create(""12"".getBytes(), ""0"".getBytes());
        final Cell cell3 = Cell.create(""1"".getBytes(), ""3"".getBytes());
        db.put(TABLE, ImmutableMap.of(cell, ""v1"".getBytes()), 2);
        db.put(TABLE, ImmutableMap.of(cell2, ""v2"".getBytes()), 2);
        db.put(TABLE, ImmutableMap.of(cell3, ""v3"".getBytes()), 2);
        final ClosableIterator<? extends RowResult<Value>> it = db.getRange(TABLE, RangeRequest.builder().build(), 3);
        try {
            assertEquals(2, Iterators.size(it));
        } finally {
            it.close();
        }
    }
"
"    @Test
    public void testDoubleWriteToTransactionTable() {
        db.createTable(TRANSACTION_TABLE, AtlasDbConstants.EMPTY_TABLE_METADATA);
        final Cell cell = Cell.create(""r1"".getBytes(), COMMIT_TS_COLUMN);
        db.putUnlessExists(TRANSACTION_TABLE, ImmutableMap.of(cell, ""v1"".getBytes()));
        try {
            db.putUnlessExists(TRANSACTION_TABLE, ImmutableMap.of(cell, ""v2"".getBytes()));
            fail();
        } catch (KeyAlreadyExistsException e) {
            // expected
        }
        final Map<Cell, Value> res = db.get(TRANSACTION_TABLE, ImmutableMap.of(cell, 1L));
        final Value value = res.get(cell);
        assertEquals(0L, value.getTimestamp());
        assertEquals(""v1"", new String(value.getContents()));
    }
"
"    @Test
    public void testMetadata() {
        db.putMetadataForTable(TABLE, ""yoyo"".getBytes());
        final byte[] meta = db.getMetadataForTable(TABLE);
        assertEquals(""yoyo"", new String(meta));
    }
"
"    @Test
    public void testCreateTables() {
        db.putMetadataForTable(TABLE, ""yoyo"".getBytes());
        final byte[] meta = db.getMetadataForTable(TABLE);
        assertEquals(""yoyo"", new String(meta));
    }
"
"    @Test
    public void testLockFile() {
        try {
            RocksDbKeyValueService db2 = RocksDbKeyValueService.create(""testdb""); // tempted to make IBM DB2 joke
            assertTrue(""RocksDBKVS should protect against concurrent instances with a lock"", false);
        } catch (RuntimeException e) {
            assertTrue(""Unknown exception type thrown; expected IOException when two RocksDBs are pointed at same directory"", e.getCause() instanceof IOException);
        }
    }
"
"    @Test
    public void testProperCaseWord() throws Exception {
        String[] words = new String[] { ""AA102"", ""nw"", ""dog"", ""daVID CHiu"", ""yu-gi-oh rules"" };
        String[] results = new String[] { ""AA102"", ""Nw"", ""Dog"", ""David chiu"", ""Yu-gi-oh rules"" };
        for (int i=0; i < words.length; i++) {
            String result = TextUtils.properCaseWord(words[i]);
            assertEquals(results[i], result);
        }
    }
"
"    @Test
    public void testProperCaseWords() throws Exception {
        String[] words = new String[] { ""AA102"", ""nw"", ""dog"", ""daVID CHiu"", ""yu-gi-oh rules"",
                ""b.j. penn the great,shawn sherk""};
        String[] results = new String[] { ""AA102"", ""Nw"", ""Dog"", ""David Chiu"", ""Yu-Gi-Oh Rules"",
                ""B.J. Penn The Great,Shawn Sherk""};
        for (int i=0; i < words.length; i++) {
            String result = TextUtils.properCaseWords(words[i]);
            assertEquals(results[i], result);
        }
    }
"
"    @Test
    public void testPluralization() throws Exception {
        assertEquals("""", TextUtils.pluralize(null));
        assertEquals("""", TextUtils.pluralize(""""));
        assertEquals(""dogs"", TextUtils.pluralize(""dog""));
        assertEquals(""keywords"", TextUtils.pluralize(""keywords""));
    }
"
"    @Test
    public void testStringForValue() throws Exception {
        assertEquals(""0"", TextUtils.getStringForValue(0.0));
        assertEquals(""5"", TextUtils.getStringForValue(5.0));
        assertEquals(""100"", TextUtils.getStringForValue(100.0));
        assertEquals(""9.2k"", TextUtils.getStringForValue(9204.0));
        assertEquals(""9.5k"", TextUtils.getStringForValue(9499.0));
        assertEquals(""10.0k"", TextUtils.getStringForValue(9999.0));
        assertEquals(""100k"", TextUtils.getStringForValue(99999.0));
        assertEquals(""100k"", TextUtils.getStringForValue(100000.0));
        assertEquals(""1.0M"", TextUtils.getStringForValue(1000000.0));
        assertEquals(""1.5M"", TextUtils.getStringForValue(1499999.0));
        assertEquals(""10M"", TextUtils.getStringForValue(10000000.0));
        assertEquals(""15M"", TextUtils.getStringForValue(14999999.0));
        assertEquals(""100M"", TextUtils.getStringForValue(100000000.0));
        assertEquals(""150M"", TextUtils.getStringForValue(149999990.0));
        assertEquals(""1.0B"", TextUtils.getStringForValue(1000000000.0));
        assertEquals(""1.5B"", TextUtils.getStringForValue(1499999900.0));
        assertEquals(""10B"", TextUtils.getStringForValue(10000000000.0));
        assertEquals(""15B"", TextUtils.getStringForValue(14999999000.0));
        assertEquals(""100B"", TextUtils.getStringForValue(100000000000.0));
        assertEquals(""150B"", TextUtils.getStringForValue(149999990000.0));
        assertEquals(""1.0T"", TextUtils.getStringForValue(1000000000000.0));
        assertEquals(""1.5T"", TextUtils.getStringForValue(1499999900000.0));
        assertEquals(""10T"", TextUtils.getStringForValue(10000000000000.0));
        assertEquals(""15T"", TextUtils.getStringForValue(14999999000000.0));
        assertEquals(""100T"", TextUtils.getStringForValue(100000000000000.0));
        assertEquals(""150T"", TextUtils.getStringForValue(149999990000000.0));
        assertEquals(""1.0e+15"", TextUtils.getStringForValue(1000000000000000.0));
        assertEquals(""1.5e+15"", TextUtils.getStringForValue(1499999900000000.0));

        assertEquals(""-5"", TextUtils.getStringForValue(-5.0));
        assertEquals(""-100"", TextUtils.getStringForValue(-100.0));
        assertEquals(""-9.2k"", TextUtils.getStringForValue(-9204.0));
        assertEquals(""-9.5k"", TextUtils.getStringForValue(-9499.0));
        assertEquals(""-10.0k"", TextUtils.getStringForValue(-9999.0));
        assertEquals(""-100k"", TextUtils.getStringForValue(-99999.0));
        assertEquals(""-100k"", TextUtils.getStringForValue(-100000.0));
        assertEquals(""-1.0M"", TextUtils.getStringForValue(-1000000.0));
        assertEquals(""-1.5M"", TextUtils.getStringForValue(-1499999.0));
        assertEquals(""-10M"", TextUtils.getStringForValue(-10000000.0));
        assertEquals(""-15M"", TextUtils.getStringForValue(-14999999.0));
        assertEquals(""-100M"", TextUtils.getStringForValue(-100000000.0));
        assertEquals(""-150M"", TextUtils.getStringForValue(-149999990.0));
        assertEquals(""-1.0B"", TextUtils.getStringForValue(-1000000000.0));
        assertEquals(""-1.5B"", TextUtils.getStringForValue(-1499999900.0));
        assertEquals(""-1.5B"", TextUtils.getStringForValue(-1500000001.0));
        assertEquals(""-10B"", TextUtils.getStringForValue(-10000000000.0));
        assertEquals(""-15B"", TextUtils.getStringForValue(-14999999000.0));
        assertEquals(""-100B"", TextUtils.getStringForValue(-100000000000.0));
        assertEquals(""-150B"", TextUtils.getStringForValue(-149999990000.0));
        assertEquals(""-1.0T"", TextUtils.getStringForValue(-1000000000000.0));
        assertEquals(""-1.5T"", TextUtils.getStringForValue(-1499999900000.0));
        assertEquals(""-10T"", TextUtils.getStringForValue(-10000000000000.0));
        assertEquals(""-15T"", TextUtils.getStringForValue(-14999999000000.0));
        assertEquals(""-100T"", TextUtils.getStringForValue(-100000000000000.0));
        assertEquals(""-150T"", TextUtils.getStringForValue(-149999990000000.0));
        assertEquals(""-1.0e+15"", TextUtils.getStringForValue(-1000000000000000.0));
        assertEquals(""-1.5e+15"", TextUtils.getStringForValue(-1499999900000000.0));
    }
"
"    @Test
    public void testRemoveAllWhitespace() {
        String before = ""  \r\n\n\r  \t FOOOooo\r\n\n\n\r\t\r o   \n"";
        String after = TextUtils.removeAllWhitespace(before);
        assertEquals(""FOOOoooo"", after);
    }
"
"    @Test
    public void testTruncateStringToCharLength() {
        String string = ""abcde"";
        assertEquals(string, TextUtils.truncateStringToCharLength(string, 5, ""...""));
        assertEquals(string, TextUtils.truncateStringToCharLength(string, 5, """"));
        assertEquals(string, TextUtils.truncateStringToCharLength(string, 6, ""...""));
        assertEquals(string, TextUtils.truncateStringToCharLength(string, 6, """"));
        assertEquals(""a..."", TextUtils.truncateStringToCharLength(string, 4, ""...""));
        assertEquals(""abcd"", TextUtils.truncateStringToCharLength(string, 4, """"));
    }
"
"    @Test
    public void testTruncateLabelString() {
        // TODO(nackner): Add in more tests with UTF-8 characters, but they won't play nice with the
        // build or people's Eclipse clients even if commented out.
        String string = ""abcde"";
        assertEquals(string, TextUtils.truncateLabelString(string, 5));
        assertEquals(string, TextUtils.truncateLabelString(string, 6, ""...""));
        assertEquals(""a..."", TextUtils.truncateLabelString(string, 4, ""...""));
    }
"
"    @Test
    public void testPropertiesToXML()
    {
        // simple string kv pair
        Properties p = new Properties();
        p.setProperty(""MY_CONFIG_KEY"", ""MY_CONFIG_VALUE"");
        String propertiesAsXML = TextUtils.storePropertiesToXMLString(p);
        assertNotNull(propertiesAsXML);
        p = TextUtils.loadPropertiesFromXMLString(propertiesAsXML);
        assertNotNull(p.getProperty(""MY_CONFIG_KEY""));
        assertEquals(""MY_CONFIG_VALUE"", p.getProperty(""MY_CONFIG_KEY""));

        // embedded config
        Properties pComplex = new Properties();
        pComplex.setProperty(""MY_SUB_CONFIG"", TextUtils.storePropertiesToXMLString(p));
        propertiesAsXML = TextUtils.storePropertiesToXMLString(pComplex);
        assertNotNull(propertiesAsXML);
        pComplex = TextUtils.loadPropertiesFromXMLString(propertiesAsXML);
        p = TextUtils.loadPropertiesFromXMLString(pComplex.getProperty(""MY_SUB_CONFIG""));
        assertNotNull(p.getProperty(""MY_CONFIG_KEY""));
        assertEquals(""MY_CONFIG_VALUE"", p.getProperty(""MY_CONFIG_KEY""));
    }
"
"    @Test
    public void testMaximalPrefix() throws Exception{
        ArrayList<String> list1 = new ArrayList<String>(7);
        list1.add(""abcdef"");
        list1.add(""abcdefg"");
        list1.add(""abcdefgh"");
        list1.add(""abcd"");
        list1.add(""abcdefl58a"");
        list1.add(""abcdeeeeee"");
        list1.add(""abcde888"");
        assertEquals(""Wrong maximal prefix"",""abcd"",TextUtils.findMaximalPrefix(list1));

        list1.clear();
        assertEquals(""Should be empty string"","""",TextUtils.findMaximalPrefix(list1));
        assertEquals(""Should be empty string"","""",TextUtils.findMaximalPrefix(null));

        list1.add(""abcd"");
        assertEquals(""Should be abcd"",""abcd"",TextUtils.findMaximalPrefix(list1));
        list1.add(""efgh"");
        list1.add(""ifht"");

        assertEquals(""Should be empty string"","""",TextUtils.findMaximalPrefix(list1));
    }
"
"    @Test
    public void testHexConverter(){
        byte[] bytes = new byte[]{(byte)255, (byte)255, 0, 0};
        System.out.println(Arrays.toString(bytes) +"" -> 0x"" + TextUtils.byteArrayToHexString(bytes));
        assertEquals(""ffff0000"", TextUtils.byteArrayToHexString(bytes));
    }
"
"    @Test
    public void testParseDate()
    {
        helperTestParseDate(new Date());
    }
"
"    @Test
    public void testParseDateFeb29() {
        helperTestParseDate(new Date(2012 - 1900, 1, 29));
    }
"
"    @Test
    public void testcleanUTF8String() throws Exception {
        String cleanString = ""Hello World"";
        String dirtyString = ""Hello\u0007World"";

        String cleanedString = TextUtils.cleanUTF8String(dirtyString);
        assertEquals(cleanString, cleanedString);
        assertEquals(cleanString, TextUtils.cleanUTF8String(cleanString));
    }
"
"    @Test
    public void testHashString() throws Exception {
        String testStr = null;
        long hash = TextUtils.hashString(testStr);
        assertEquals(0, hash);

        testStr = ""Allen cheats at Race for the Galaxy."";
        hash = TextUtils.hashString(testStr);
        assertEquals(1133932183, hash);
    }
"
"    @Test
    public void testEncodeDecodeStringUTF8() throws Exception {
        String str = ""THIS IS A \u1234 TEST STRING"";
        byte[] bytes = TextUtils.convertStringToBytesUtf8(str);
        assertTrue(Arrays.equals(str.getBytes(""UTF-8""), bytes));
        assertEquals(str, TextUtils.convertBytesToStringUtf8(bytes));
    }
"
"    @Test
    public void testEscapeHtmlBasic() {
        String input1 = ""\""A\"" \""b\""; 1 < 2 && 3 > 2"";
        String output1 = ""&quot;A&quot; &quot;b&quot;; 1 &lt; 2 &amp;&amp; 3 &gt; 2"";
        assertTrue(output1.equals(TextUtils.escapeHtml(input1)));
    }
"
"    @Test
    public void testEscapeHtmlWhitespaceHandling() {
        String input2 = ""a b  c   d    e"";
        String output2 = ""a b &nbsp;c &nbsp; d &nbsp; &nbsp;e"";
        assertTrue(output2.equals(TextUtils.escapeHtml(input2)));

        String input3 = ""line 1\nline 2 \n\n line4"";
        String output3f = ""line 1<br/>line 2 <br/><br/> line4"";
        String output3t = ""line 1line 2  line4"";
        assertTrue(output3f.equals(TextUtils.escapeHtml(input3, false)));
        assertTrue(output3t.equals(TextUtils.escapeHtml(input3, true)));
    }
"
"    @Test
    public void testEscapeHtmlTwoByteUnicode() {
        assertTrue(""&#192;"".equals(TextUtils.escapeHtml(u00C0)));
        assertTrue(""&#256;"".equals(TextUtils.escapeHtml(u0100)));
        assertTrue(""&#288;"".equals(TextUtils.escapeHtml(u0120)));
    }
"
"    @Test
    public void testGetRowColumnSelection() {
        Cell cell1 = Cell.create(PtBytes.toBytes(""row""), PtBytes.toBytes(""col1""));
        Cell cell2 = Cell.create(PtBytes.toBytes(""row""), PtBytes.toBytes(""col2""));
        Cell cell3 = Cell.create(PtBytes.toBytes(""row""), PtBytes.toBytes(""col3""));
        byte[] val = PtBytes.toBytes(""val"");

        keyValueService.put(TEST_TABLE, ImmutableMap.of(cell1, val, cell2, val, cell3, val), 0);

        Map<Cell, Value> rows1 = keyValueService.getRows(
                TEST_TABLE,
                ImmutableSet.of(cell1.getRowName()),
                ColumnSelection.all(),
                1);
        Assert.assertEquals(ImmutableSet.of(cell1, cell2, cell3), rows1.keySet());

        Map<Cell, Value> rows2 = keyValueService.getRows(
                TEST_TABLE,
                ImmutableSet.of(cell1.getRowName()),
                ColumnSelection.create(ImmutableList.of(cell1.getColumnName())),
                1);
        assertEquals(ImmutableSet.of(cell1), rows2.keySet());

        Map<Cell, Value> rows3 = keyValueService.getRows(
                TEST_TABLE,
                ImmutableSet.of(cell1.getRowName()),
                ColumnSelection.create(ImmutableList.of(cell1.getColumnName(), cell3.getColumnName())),
                1);
        assertEquals(ImmutableSet.of(cell1, cell3), rows3.keySet());
        Map<Cell, Value> rows4 = keyValueService.getRows(
                TEST_TABLE,
                ImmutableSet.of(cell1.getRowName()),
                ColumnSelection.create(ImmutableList.<byte[]>of()),
                1);

        // This has changed recently - now empty column set means
        // that all columns are selected.
        assertEquals(ImmutableSet.of(cell1, cell2, cell3), rows4.keySet());
    }
"
"    @Test
    public void testGetRowsAllColumns() {
        putTestDataForSingleTimestamp();
        Map<Cell, Value> values = keyValueService.getRows(TEST_TABLE,
                                                          Arrays.asList(row1, row2),
                                                          ColumnSelection.all(),
                                                          TEST_TIMESTAMP + 1);
        assertEquals(4, values.size());
        assertEquals(null, values.get(Cell.create(row1, column1)));
        assertArrayEquals(value10, values.get(Cell.create(row1, column0)).getContents());
        assertArrayEquals(value12, values.get(Cell.create(row1, column2)).getContents());
        assertArrayEquals(value21, values.get(Cell.create(row2, column1)).getContents());
        assertArrayEquals(value22, values.get(Cell.create(row2, column2)).getContents());
    }
"
"    @Test
    public void testGetRowsWhenMultipleVersions() {
        putTestDataForMultipleTimestamps();
        Map<Cell, Value> result = keyValueService.getRows(
                TEST_TABLE,
                ImmutableSet.of(row0),
                ColumnSelection.all(),
                TEST_TIMESTAMP + 1);
        assertEquals(1, result.size());
        assertTrue(result.containsKey(Cell.create(row0, column0)));
        assertTrue(result.containsValue(Value.create(value0_t0, TEST_TIMESTAMP)));

        result = keyValueService.getRows(
                TEST_TABLE,
                ImmutableSet.of(row0),
                ColumnSelection.all(),
                TEST_TIMESTAMP + 2);
        assertEquals(1, result.size());
        assertTrue(result.containsKey(Cell.create(row0, column0)));
        assertTrue(result.containsValue(Value.create(value0_t1, TEST_TIMESTAMP + 1)));
    }
"
"    @Test
    public void testGetRowsWhenMultipleVersionsAndColumnsSelected() {
        putTestDataForMultipleTimestamps();
        Map<Cell, Value> result = keyValueService.getRows(
                TEST_TABLE,
                ImmutableSet.of(row0),
                ColumnSelection.create(ImmutableSet.of(column0)),
                TEST_TIMESTAMP + 1);
        assertEquals(1, result.size());
        assertTrue(result.containsKey(Cell.create(row0, column0)));
        assertTrue(result.containsValue(Value.create(value0_t0, TEST_TIMESTAMP)));

        result = keyValueService.getRows(
                TEST_TABLE,
                ImmutableSet.of(row0),
                ColumnSelection.create(ImmutableSet.of(column0)),
                TEST_TIMESTAMP + 2);
        assertEquals(1, result.size());
        assertTrue(result.containsKey(Cell.create(row0, column0)));
        assertTrue(result.containsValue(Value.create(value0_t1, TEST_TIMESTAMP + 1)));
    }
"
"    @Test
    public void testGetWhenMultipleVersions() {
        putTestDataForMultipleTimestamps();
        Cell cell = Cell.create(row0, column0);
        Value val0 = Value.create(value0_t0, TEST_TIMESTAMP);
        Value val1 = Value.create(value0_t1, TEST_TIMESTAMP + 1);

        assertTrue(keyValueService.get(TEST_TABLE, ImmutableMap.of(cell, TEST_TIMESTAMP)).isEmpty());

        Map<Cell, Value> result = keyValueService.get(
                TEST_TABLE,
                ImmutableMap.of(cell, TEST_TIMESTAMP + 1));
        assertTrue(result.containsKey(cell));
        assertEquals(1, result.size());
        assertTrue(result.containsValue(val0));

        result = keyValueService.get(TEST_TABLE, ImmutableMap.of(cell, TEST_TIMESTAMP + 2));

        assertEquals(1, result.size());
        assertTrue(result.containsKey(cell));
        assertTrue(result.containsValue(val1));

        result = keyValueService.get(TEST_TABLE, ImmutableMap.of(cell, TEST_TIMESTAMP + 3));

        assertEquals(1, result.size());
        assertTrue(result.containsKey(cell));
        assertTrue(result.containsValue(val1));
    }
"
"    @Test
    public void testGetRowsWithSelectedColumns() {
        putTestDataForSingleTimestamp();
        ColumnSelection columns1and2 = ColumnSelection.create(Arrays.asList(column1, column2));
        Map<Cell, Value> values = keyValueService.getRows(TEST_TABLE,
                                                          Arrays.asList(row1, row2),
                                                          columns1and2,
                                                          TEST_TIMESTAMP + 1);
        assertEquals(3, values.size());
        assertEquals(null, values.get(Cell.create(row1, column0)));
        assertArrayEquals(value12, values.get(Cell.create(row1, column2)).getContents());
        assertArrayEquals(value21, values.get(Cell.create(row2, column1)).getContents());
        assertArrayEquals(value22, values.get(Cell.create(row2, column2)).getContents());
    }
"
"    @Test
    public void testGetLatestTimestamps() {
        putTestDataForMultipleTimestamps();
        Map<Cell, Long> timestamps = keyValueService.getLatestTimestamps(TEST_TABLE,
                ImmutableMap.of(Cell.create(row0, column0), TEST_TIMESTAMP + 2));
        assertTrue(""Incorrect number of values returned."", timestamps.size() == 1);
        assertEquals(""Incorrect value returned."", new Long(TEST_TIMESTAMP + 1),
                timestamps.get(Cell.create(row0, column0)));
    }
"
"    @Test
    public void testGetWithMultipleVersions() {
        putTestDataForMultipleTimestamps();
        Map<Cell, Value> values = keyValueService.get(TEST_TABLE,
                ImmutableMap.of(Cell.create(row0, column0), TEST_TIMESTAMP + 2));
        assertTrue(""Incorrect number of values returned."", values.size() == 1);
        assertEquals(""Incorrect value returned."", Value.create(value0_t1, TEST_TIMESTAMP + 1),
                values.get(Cell.create(row0, column0)));
    }
"
"    @Test
    public void testGetAllTableNames() {
        final TableReference anotherTable = TableReference.createWithEmptyNamespace(""AnotherTable"");
        assertEquals(1, keyValueService.getAllTableNames().size());
        assertEquals(TEST_TABLE, keyValueService.getAllTableNames().iterator().next());
        keyValueService.createTable(anotherTable, AtlasDbConstants.GENERIC_TABLE_METADATA);
        assertEquals(2, keyValueService.getAllTableNames().size());
        assertTrue(keyValueService.getAllTableNames().contains(anotherTable));
        assertTrue(keyValueService.getAllTableNames().contains(TEST_TABLE));
        keyValueService.dropTable(anotherTable);
        assertEquals(1, keyValueService.getAllTableNames().size());
        assertEquals(TEST_TABLE, keyValueService.getAllTableNames().iterator().next());
    }
"
"    @Test
    public void testTableMetadata() {
        assertEquals(AtlasDbConstants.GENERIC_TABLE_METADATA.length, keyValueService.getMetadataForTable(TEST_TABLE).length);
        keyValueService.putMetadataForTable(TEST_TABLE, ArrayUtils.EMPTY_BYTE_ARRAY);
        assertEquals(0, keyValueService.getMetadataForTable(TEST_TABLE).length);
        keyValueService.putMetadataForTable(TEST_TABLE, metadata0);
        assertTrue(Arrays.equals(metadata0, keyValueService.getMetadataForTable(TEST_TABLE)));
    }
"
"    @Test
    public void testGetRange() {
        testGetRange(reverseRangesSupported());
    }
"
"    @Test
    public void testGetAllTimestamps() {
        putTestDataForMultipleTimestamps();
        final Cell cell = Cell.create(row0, column0);
        final Set<Cell> cellSet = ImmutableSet.of(cell);
        Multimap<Cell, Long> timestamps = keyValueService.getAllTimestamps(
                TEST_TABLE,
                cellSet,
                TEST_TIMESTAMP);
        assertEquals(0, timestamps.size());

        timestamps = keyValueService.getAllTimestamps(TEST_TABLE, cellSet, TEST_TIMESTAMP + 1);
        assertEquals(1, timestamps.size());
        assertTrue(timestamps.containsEntry(cell, TEST_TIMESTAMP));

        timestamps = keyValueService.getAllTimestamps(TEST_TABLE, cellSet, TEST_TIMESTAMP + 2);
        assertEquals(2, timestamps.size());
        assertTrue(timestamps.containsEntry(cell, TEST_TIMESTAMP));
        assertTrue(timestamps.containsEntry(cell, TEST_TIMESTAMP + 1));

        assertEquals(
                timestamps,
                keyValueService.getAllTimestamps(TEST_TABLE, cellSet, TEST_TIMESTAMP + 3));
    }
"
"    @Test
    public void testDelete() {
        putTestDataForSingleTimestamp();
        assertEquals(3, Iterators.size(keyValueService.getRange(
                TEST_TABLE,
                RangeRequest.all(),
                TEST_TIMESTAMP + 1)));
        keyValueService.delete(
                TEST_TABLE,
                ImmutableMultimap.of(Cell.create(row0, column0), TEST_TIMESTAMP));
        assertEquals(3, Iterators.size(keyValueService.getRange(
                TEST_TABLE,
                RangeRequest.all(),
                TEST_TIMESTAMP + 1)));
        keyValueService.delete(
                TEST_TABLE,
                ImmutableMultimap.of(Cell.create(row0, column1), TEST_TIMESTAMP));
        assertEquals(2, Iterators.size(keyValueService.getRange(
                TEST_TABLE,
                RangeRequest.all(),
                TEST_TIMESTAMP + 1)));
        keyValueService.delete(
                TEST_TABLE,
                ImmutableMultimap.of(Cell.create(row1, column0), TEST_TIMESTAMP));
        assertEquals(2, Iterators.size(keyValueService.getRange(
                TEST_TABLE,
                RangeRequest.all(),
                TEST_TIMESTAMP + 1)));
        keyValueService.delete(
                TEST_TABLE,
                ImmutableMultimap.of(Cell.create(row1, column2), TEST_TIMESTAMP));
        assertEquals(1, Iterators.size(keyValueService.getRange(
                TEST_TABLE,
                RangeRequest.all(),
                TEST_TIMESTAMP + 1)));
    }
"
"    @Test
    public void testDeleteMultipleVersions() {
        putTestDataForMultipleTimestamps();
        Cell cell = Cell.create(row0, column0);
        ClosableIterator<RowResult<Value>> result = keyValueService.getRange(
                TEST_TABLE,
                RangeRequest.all(),
                TEST_TIMESTAMP + 1);
        assertTrue(result.hasNext());

        keyValueService.delete(TEST_TABLE, ImmutableMultimap.of(cell, TEST_TIMESTAMP));

        result = keyValueService.getRange(TEST_TABLE, RangeRequest.all(), TEST_TIMESTAMP + 1);
        assertTrue(!result.hasNext());

        result = keyValueService.getRange(TEST_TABLE, RangeRequest.all(), TEST_TIMESTAMP + 2);
        assertTrue(result.hasNext());
    }
"
"    @Test
    public void testPutWithTimestamps() {
        putTestDataForMultipleTimestamps();
        final Cell cell = Cell.create(row0, column0);
        final Value val1 = Value.create(value0_t1, TEST_TIMESTAMP + 1);
        final Value val5 = Value.create(value0_t5, TEST_TIMESTAMP + 5);
        keyValueService.putWithTimestamps(TEST_TABLE, ImmutableMultimap.of(cell, val5));
        assertEquals(
                val5,
                keyValueService.get(TEST_TABLE, ImmutableMap.of(cell, TEST_TIMESTAMP + 6)).get(cell));
        assertEquals(
                val1,
                keyValueService.get(TEST_TABLE, ImmutableMap.of(cell, TEST_TIMESTAMP + 5)).get(cell));
        keyValueService.delete(TEST_TABLE, ImmutableMultimap.of(cell, TEST_TIMESTAMP + 5));
    }
"
"    @Test
    public void testGetRangeWithHistory() {
        testGetRangeWithHistory(false);
        if (reverseRangesSupported()) {
            testGetRangeWithHistory(true);
        }
    }
"
"    @Test
    public void testGetRangeWithTimestamps() {
        testGetRangeWithTimestamps(false);
        if (reverseRangesSupported()) {
            testGetRangeWithTimestamps(true);
        }
    }
"
"    @Test
    public void testKeyAlreadyExists() {
        // Test that it does not throw some random exceptions
        putTestDataForSingleTimestamp();
        try {
            putTestDataForSingleTimestamp();
            // Legal
        } catch (KeyAlreadyExistsException e) {
            Assert.fail(""Must not throw when overwriting with same value!"");
        }

        keyValueService.putWithTimestamps(
                TEST_TABLE,
                ImmutableMultimap.of(
                        Cell.create(row0, column0),
                        Value.create(value00, TEST_TIMESTAMP + 1)));
        try {
            keyValueService.putWithTimestamps(
                    TEST_TABLE,
                    ImmutableMultimap.of(
                            Cell.create(row0, column0),
                            Value.create(value00, TEST_TIMESTAMP + 1)));
            // Legal
        } catch (KeyAlreadyExistsException e) {
            Assert.fail(""Must not throw when overwriting with same value!"");
        }

        try {
            keyValueService.putWithTimestamps(TEST_TABLE, ImmutableMultimap.of(Cell.create(row0, column0), Value.create(value01, TEST_TIMESTAMP + 1)));
            // Legal
        } catch (KeyAlreadyExistsException e) {
            // Legal
        }

        // The first try might not throw as putUnlessExists must only be exclusive with other putUnlessExists.
        try {
            keyValueService.putUnlessExists(TEST_TABLE, ImmutableMap.of(Cell.create(row0, column0), value00));
            // Legal
        } catch (KeyAlreadyExistsException e) {
            // Legal
        }

        try {
            keyValueService.putUnlessExists(TEST_TABLE, ImmutableMap.of(Cell.create(row0, column0), value00));
            Assert.fail(""putUnlessExists must throw when overwriting the same cell!"");
        } catch (KeyAlreadyExistsException e) {
            // Legal
        }
    }
"
"    @Test
    public void testAddGCSentinelValues() {
        putTestDataForMultipleTimestamps();
        Cell cell = Cell.create(row0, column0);

        Multimap<Cell, Long> timestampsBefore = keyValueService.getAllTimestamps(TEST_TABLE, ImmutableSet.of(cell), Long.MAX_VALUE);
        assertEquals(2, timestampsBefore.size());
        assertTrue(!timestampsBefore.containsEntry(cell, Value.INVALID_VALUE_TIMESTAMP));

        keyValueService.addGarbageCollectionSentinelValues(TEST_TABLE, ImmutableSet.of(cell));

        Multimap<Cell, Long> timestampsAfter1 = keyValueService.getAllTimestamps(TEST_TABLE, ImmutableSet.of(cell), Long.MAX_VALUE);
        assertEquals(3, timestampsAfter1.size());
        assertTrue(timestampsAfter1.containsEntry(cell, Value.INVALID_VALUE_TIMESTAMP));

        keyValueService.addGarbageCollectionSentinelValues(TEST_TABLE, ImmutableSet.of(cell));

        Multimap<Cell, Long> timestampsAfter2 = keyValueService.getAllTimestamps(TEST_TABLE, ImmutableSet.of(cell), Long.MAX_VALUE);
        assertEquals(3, timestampsAfter2.size());
        assertTrue(timestampsAfter2.containsEntry(cell, Value.INVALID_VALUE_TIMESTAMP));
    }
"
"    @Test
    public void testGetRangeThrowsOnError() {
        try {
            keyValueService.getRange(TEST_NONEXISTING_TABLE, RangeRequest.all(), Long.MAX_VALUE).hasNext();
            Assert.fail(""getRange must throw on failure"");
        } catch (RuntimeException e) {
            // Expected
        }
    }
"
"    @Test
    public void testGetRangeWithHistoryThrowsOnError() {
        try {
            keyValueService.getRangeWithHistory(TEST_NONEXISTING_TABLE, RangeRequest.all(), Long.MAX_VALUE).hasNext();
            Assert.fail(""getRangeWithHistory must throw on failure"");
        } catch (RuntimeException e) {
            // Expected
        }
    }
"
"    @Test
    public void testGetRangeOfTimestampsThrowsOnError() {
        try {
            keyValueService.getRangeOfTimestamps(TEST_NONEXISTING_TABLE, RangeRequest.all(), Long.MAX_VALUE).hasNext();
            Assert.fail(""getRangeOfTimestamps must throw on failure"");
        } catch (RuntimeException e) {
            // Expected
        }
    }
"
"    @Test
    public void testCannotModifyValuesAfterWrite() {
        byte[] data = new byte[1];
        byte[] unmodifiedData = Arrays.copyOf(data, data.length);

        Cell cell = Cell.create(row0, column0);

        Value val = Value.create(data, TEST_TIMESTAMP + 1);

        keyValueService.putWithTimestamps(TEST_TABLE, ImmutableMultimap.of(cell, val));

        data[0] = (byte) 50;

        assertThat(keyValueService.get(TEST_TABLE, ImmutableMap.of(cell, TEST_TIMESTAMP + 3)).get(cell).getContents(),
                is(unmodifiedData));

        keyValueService.delete(TEST_TABLE, ImmutableMultimap.of(cell, TEST_TIMESTAMP + 1));
    }
"
"    @Test
    public void testClassicWriteSkew() {
        Transaction t0 = startTransaction();
        put(t0, ""row1"", ""col1"", ""100"");
        put(t0, ""row2"", ""col1"", ""100"");
        t0.commit();

        Transaction t1 = startTransaction();
        Transaction t2 = startTransaction();
        withdrawMoney(t1, true, false);
        withdrawMoney(t2, false, false);

        t1.commit();
        try {
            t2.commit();
            fail();
        } catch (TransactionSerializableConflictException e) {
            // this is expectecd to throw because it is a write skew
        }
    }
"
"    @Test
    public void testClassicWriteSkew2() {
        Transaction t0 = startTransaction();
        put(t0, ""row1"", ""col1"", ""100"");
        put(t0, ""row2"", ""col1"", ""100"");
        t0.commit();

        Transaction t1 = startTransaction();
        Transaction t2 = startTransaction();
        withdrawMoney(t1, true, false);
        withdrawMoney(t2, false, false);

        t2.commit();
        try {
            t1.commit();
            fail();
        } catch (TransactionSerializableConflictException e) {
            // this is expectecd to throw because it is a write skew
        }
    }
"
"    @Test(expected=TransactionFailedRetriableException.class)
    public void testConcurrentWriteSkew() throws InterruptedException, BrokenBarrierException {
        Transaction t0 = startTransaction();
        put(t0, ""row1"", ""col1"", ""100"");
        put(t0, ""row2"", ""col1"", ""100"");
        t0.commit();

        final CyclicBarrier barrier = new CyclicBarrier(2);

        final Transaction t1 = startTransaction();
        ExecutorService exec = PTExecutors.newCachedThreadPool();
        Future<?> f = exec.submit( new Callable<Void>() {
            @Override
            public Void call() throws Exception {
                withdrawMoney(t1, true, false);
                barrier.await();
                t1.commit();
                return null;
            }
"
"    @Test
    public void testClassicWriteSkewCell() {
        Transaction t0 = startTransaction();
        put(t0, ""row1"", ""col1"", ""100"");
        put(t0, ""row2"", ""col1"", ""100"");
        t0.commit();

        Transaction t1 = startTransaction();
        Transaction t2 = startTransaction();
        withdrawMoney(t1, true, true);
        withdrawMoney(t2, false, true);

        t1.commit();
        try {
            t2.commit();
            fail();
        } catch (TransactionSerializableConflictException e) {
            // this is expectecd to throw because it is a write skew
        }
    }
"
"    @Test
    public void testClassicWriteSkew2Cell() {
        Transaction t0 = startTransaction();
        put(t0, ""row1"", ""col1"", ""100"");
        put(t0, ""row2"", ""col1"", ""100"");
        t0.commit();

        Transaction t1 = startTransaction();
        Transaction t2 = startTransaction();
        withdrawMoney(t1, true, true);
        withdrawMoney(t2, false, true);

        t2.commit();
        try {
            t1.commit();
            fail();
        } catch (TransactionSerializableConflictException e) {
            // this is expectecd to throw because it is a write skew
        }
    }
"
"    @Test(expected=TransactionFailedRetriableException.class)
    public void testConcurrentWriteSkewCell() throws InterruptedException, BrokenBarrierException {
        Transaction t0 = startTransaction();
        put(t0, ""row1"", ""col1"", ""100"");
        put(t0, ""row2"", ""col1"", ""100"");
        t0.commit();

        final CyclicBarrier barrier = new CyclicBarrier(2);

        final Transaction t1 = startTransaction();
        ExecutorService exec = PTExecutors.newCachedThreadPool();
        Future<?> f = exec.submit( new Callable<Void>() {
            @Override
            public Void call() throws Exception {
                withdrawMoney(t1, true, true);
                barrier.await();
                t1.commit();
                return null;
            }
"
"    @Test
    public void testCycleWithReadOnly() {
        // readOnly has a r/w dep on t2 and t2 has a r/w on t1 and t1 has a w/r dep on readOnly
        // This creates a cycle that is valid under SI, but not SSI
        // The main issue is that readOnly reads an invalid state of the world. because it reads the updated value of
        // t1, but the old value of t2.

        String initialValue = ""100"";
        String newValue = ""101"";
        Transaction t0 = startTransaction();
        put(t0, ""row1"", ""col1"", initialValue);
        put(t0, ""row2"", ""col1"", initialValue);
        t0.commit();

        Transaction t1 = startTransaction();
        put(t1, ""row1"", ""col1"", newValue);
        Transaction t2 = startTransaction();
        String row1Get = get(t2, ""row1"", ""col1"");
        assertEquals(initialValue, row1Get);
        put(t2, ""row2"", ""col1"", row1Get);

        t1.commit();
        Transaction readOnly = startTransaction();
        assertEquals(newValue, get(readOnly, ""row1"", ""col1""));
        assertEquals(initialValue, get(readOnly, ""row2"", ""col1""));

        try {
            t2.commit();
            fail();
        } catch (TransactionSerializableConflictException e) {
            // this is expectecd to throw because it is a write skew
        }
    }
"
"    @Test
    public void testLargerCycleWithReadOnly() {
        String initialValue = ""100"";
        String newValue = ""101"";
        String newValue2 = ""102"";
        Transaction t0 = startTransaction();
        put(t0, ""row1"", ""col1"", initialValue);
        put(t0, ""row2"", ""col1"", initialValue);
        t0.commit();

        Transaction t1 = startTransaction();
        put(t1, ""row1"", ""col1"", newValue);
        Transaction t2 = startTransaction();
        String row1Get = get(t2, ""row1"", ""col1"");
        assertEquals(initialValue, row1Get);
        put(t2, ""row2"", ""col1"", row1Get);

        t1.commit();
        Transaction t3 = startTransaction();
        put(t3, ""row1"", ""col1"", newValue2);
        t3.commit();
        Transaction readOnly = startTransaction();
        assertEquals(newValue2, get(readOnly, ""row1"", ""col1""));
        assertEquals(initialValue, get(readOnly, ""row2"", ""col1""));

        try {
            t2.commit();
            fail();
        } catch (TransactionSerializableConflictException e) {
            // this is expectecd to throw because it is a write skew
        }
    }
"
"    @Test
    public void testNonPhantomRead() {
        String initialValue = ""100"";
        Transaction t0 = startTransaction();
        put(t0, ""row1"", ""col1"", initialValue);
        put(t0, ""row2"", ""col1"", initialValue);
        t0.commit();

        Transaction t1 = startTransaction();
        RowResult<byte[]> first = BatchingVisitables.getFirst(t1.getRange(TEST_TABLE, RangeRequest.builder().build()));
        put(t1, ""row22"", ""col1"", initialValue);

        Transaction t2 = startTransaction();
        put(t2, ""row11"", ""col1"", initialValue);
        t2.commit();

        t1.commit();
    }
"
"    @Test
    public void testPhantomReadFail() {
        String initialValue = ""100"";
        Transaction t0 = startTransaction();
        put(t0, ""row1"", ""col1"", initialValue);
        put(t0, ""row2"", ""col1"", initialValue);
        t0.commit();

        Transaction t1 = startTransaction();
        RowResult<byte[]> first = BatchingVisitables.getFirst(t1.getRange(TEST_TABLE, RangeRequest.builder().build()));
        put(t1, ""row22"", ""col1"", initialValue);

        Transaction t2 = startTransaction();
        put(t2, ""row0"", ""col1"", initialValue);
        t2.commit();

        try {
            t1.commit();
            fail();
        } catch (TransactionSerializableConflictException e) {
            // this is expectecd to throw because it is a write skew
        }
    }
"
"    @Test
    public void testPhantomReadFail2() {
        String initialValue = ""100"";
        Transaction t0 = startTransaction();
        put(t0, ""row1"", ""col1"", initialValue);
        put(t0, ""row2"", ""col1"", initialValue);
        t0.commit();

        Transaction t1 = startTransaction();
        BatchingVisitables.copyToList(t1.getRange(TEST_TABLE, RangeRequest.builder().build()));
        put(t1, ""row22"", ""col1"", initialValue);

        Transaction t2 = startTransaction();
        put(t2, ""row3"", ""col1"", initialValue);
        t2.commit();

        try {
            t1.commit();
            fail();
        } catch (TransactionSerializableConflictException e) {
            // this is expectecd to throw because it is a write skew
        }
    }
"
"    @Test
    public void testCellReadWriteFailure() {
        String initialValue = ""100"";
        Transaction t0 = startTransaction();
        put(t0, ""row1"", ""col1"", initialValue);
        put(t0, ""row2"", ""col1"", initialValue);
        t0.commit();

        Transaction t1 = startTransaction();
        BatchingVisitables.copyToList(t1.getRange(TEST_TABLE, RangeRequest.builder().build()));
        put(t1, ""row22"", ""col1"", initialValue);

        Transaction t2 = startTransaction();
        put(t2, ""row3"", ""col1"", initialValue);
        t2.commit();

        try {
            t1.commit();
            fail();
        } catch (TransactionSerializableConflictException e) {
            // this is expectecd to throw because it is a write skew
        }
    }
"
"    @Test
    public void testCellReadWriteFailure2() {
        String initialValue = ""100"";
        Transaction t0 = startTransaction();
        put(t0, ""row1"", ""col1"", initialValue);
        put(t0, ""row2"", ""col1"", initialValue);
        t0.commit();

        Transaction t1 = startTransaction();
        BatchingVisitables.copyToList(t1.getRange(TEST_TABLE, RangeRequest.builder().build()));
        put(t1, ""row22"", ""col1"", initialValue);

        Transaction t2 = startTransaction();
        put(t2, ""row2"", ""col1"", ""101"");
        t2.commit();

        try {
            t1.commit();
            fail();
        } catch (TransactionSerializableConflictException e) {
            // this is expectecd to throw because it is a write skew
        }
    }
"
"    @Test
    public void testColumnSelection() {
        String initialValue = ""100"";
        Transaction t0 = startTransaction();
        put(t0, ""row1"", ""col1"", initialValue);
        put(t0, ""row1"", ""col2"", initialValue);
        put(t0, ""row2"", ""col1"", initialValue);
        t0.commit();

        Transaction t1 = startTransaction();
        BatchingVisitables.copyToList(t1.getRange(TEST_TABLE, RangeRequest.builder().retainColumns(ImmutableList.of(PtBytes.toBytes(""col1""))).build()));
        get(t1, ""row1"", ""col2"");

        // We need to do at least one put so we don't get caught by the read only code path
        put(t1, ""row22"", ""col2"", initialValue);

        t1.commit();
    }
"
"    @Test
    public void testColumnSelection2() {
        String initialValue = ""100"";
        Transaction t0 = startTransaction();
        put(t0, ""row1"", ""col1"", initialValue);
        put(t0, ""row1"", ""col2"", initialValue);
        put(t0, ""row2"", ""col1"", initialValue);
        t0.commit();

        Transaction t1 = startTransaction();
        BatchingVisitables.copyToList(t1.getRange(TEST_TABLE, RangeRequest.builder().retainColumns(ImmutableList.of(PtBytes.toBytes(""col1""))).build()));
        BatchingVisitables.copyToList(t1.getRange(TEST_TABLE, RangeRequest.builder().retainColumns(ImmutableList.of(PtBytes.toBytes(""col2""))).build()));

        // We need to do at least one put so we don't get caught by the read only code path
        put(t1, ""row22"", ""col2"", initialValue);

        t1.commit();
    }
"
"    @Test
    public void testBigValue() {
        byte[] bytes = new byte[64*1024];
        new Random().nextBytes(bytes);
        String encodeHexString = BaseEncoding.base16().lowerCase().encode(bytes);
        putDirect(""row1"", ""col1"", encodeHexString, 0);
        Pair<String, Long> pair = getDirect(""row1"", ""col1"", 1);
        Assert.assertEquals(0L, (long)pair.getRhSide());
        assertEquals(encodeHexString, pair.getLhSide());
    }
"
"    @Test
    public void testSpecialValues() {
        String eight = ""00000000"";
        String sixteen = eight + eight;
        putDirect(""row1"", ""col1"", eight, 0);
        putDirect(""row2"", ""col1"", sixteen, 0);
        Pair<String, Long> direct1 = getDirect(""row1"", ""col1"", 1);
        assertEquals(eight, direct1.lhSide);
        Pair<String, Long> direct2 = getDirect(""row2"", ""col1"", 1);
        assertEquals(sixteen, direct2.lhSide);
    }
"
"    @Test
    public void testKeyValueRows() {
        putDirect(""row1"", ""col1"", ""v1"", 0);
        Pair<String, Long> pair = getDirect(""row1"", ""col1"", 1);
        assertEquals(0L, (long)pair.getRhSide());
        assertEquals(""v1"", pair.getLhSide());

        putDirect(""row1"", ""col1"", ""v2"", 2);
        pair = getDirect(""row1"", ""col1"", 2);
        assertEquals(0L, (long)pair.getRhSide());
        assertEquals(""v1"", pair.getLhSide());

        pair = getDirect(""row1"", ""col1"", 3);
        assertEquals(2L, (long)pair.getRhSide());
        assertEquals(""v2"", pair.getLhSide());
    }
"
"    @Test
    public void testPrimaryKeyViolation() {
        Cell cell = Cell.create(""r1"".getBytes(), TransactionConstants.COMMIT_TS_COLUMN);
        keyValueService.putUnlessExists(TransactionConstants.TRANSACTION_TABLE,
            ImmutableMap.of(cell, ""v1"".getBytes()));
        try {
            keyValueService.putUnlessExists(TransactionConstants.TRANSACTION_TABLE,
                ImmutableMap.of(cell, ""v2"".getBytes()));
            fail();
        } catch (KeyAlreadyExistsException e) {
            //expected
        }
    }
"
"    @Test
    public void testEmptyValue() {
        putDirect(""row1"", ""col1"", ""v1"", 0);
        Pair<String, Long> pair = getDirect(""row1"", ""col1"", 1);
        assertEquals(0L, (long)pair.getRhSide());
        assertEquals(""v1"", pair.getLhSide());

        putDirect(""row1"", ""col1"", """", 2);
        pair = getDirect(""row1"", ""col1"", 2);
        assertEquals(0L, (long)pair.getRhSide());
        assertEquals(""v1"", pair.getLhSide());

        pair = getDirect(""row1"", ""col1"", 3);
        assertEquals(2L, (long)pair.getRhSide());
        assertEquals("""", pair.getLhSide());
    }
"
"    @Test
    public void testKeyValueRange() {
        putDirect(""row1"", ""col1"", ""v1"", 0);
        putDirect(""row1"", ""col2"", ""v2"", 2);
        putDirect(""row1"", ""col4"", ""v5"", 3);
        putDirect(""row1a"", ""col4"", ""v5"", 100);
        putDirect(""row2"", ""col2"", ""v3"", 1);
        putDirect(""row2"", ""col4"", ""v4"", 6);

        ImmutableList<RowResult<Value>> list = ImmutableList.copyOf(keyValueService.getRange(TEST_TABLE, RangeRequest.builder().build(), 1));
        assertEquals(1, list.size());
        RowResult<Value> row = list.iterator().next();
        assertEquals(1, row.getColumns().size());

        list = ImmutableList.copyOf(keyValueService.getRange(TEST_TABLE, RangeRequest.builder().build(), 2));
        assertEquals(2, list.size());
        row = list.iterator().next();
        assertEquals(1, row.getColumns().size());

        list = ImmutableList.copyOf(keyValueService.getRange(TEST_TABLE, RangeRequest.builder().build(), 3));
        assertEquals(2, list.size());
        row = list.iterator().next();
        assertEquals(2, row.getColumns().size());

        list = ImmutableList.copyOf(keyValueService.getRange(TEST_TABLE, RangeRequest.builder().endRowExclusive(PtBytes.toBytes(""row2"")).build(), 3));
        assertEquals(1, list.size());
        row = list.iterator().next();
        assertEquals(2, row.getColumns().size());

        list = ImmutableList.copyOf(keyValueService.getRange(TEST_TABLE, RangeRequest.builder().startRowInclusive(PtBytes.toBytes(""row1a"")).build(), 3));
        assertEquals(1, list.size());
        row = list.iterator().next();
        assertEquals(1, row.getColumns().size());
    }
"
"    @Test
    public void testKeyValueEmptyRange() {
        putDirect(""row1"", ""col1"", ""v1"", 0);

        byte[] rowBytes = PtBytes.toBytes(""row1"");
        ImmutableList<RowResult<Value>> list = ImmutableList.copyOf(keyValueService.getRange(TEST_TABLE, RangeRequest.builder().startRowInclusive(rowBytes).endRowExclusive(rowBytes).build(), 1));
        assertTrue(list.isEmpty());
    }
"
"    @Test
    public void testKeyValueRangeColumnSelection() {
        putDirect(""row1"", ""col1"", ""v1"", 0);
        putDirect(""row1"", ""col2"", ""v2"", 2);
        putDirect(""row1"", ""col4"", ""v5"", 3);
        putDirect(""row1a"", ""col4"", ""v5"", 100);
        putDirect(""row2"", ""col2"", ""v3"", 1);
        putDirect(""row2"", ""col4"", ""v4"", 6);

        List<byte[]> selectedColumns = ImmutableList.of(PtBytes.toBytes(""col2""));
        RangeRequest simpleRange = RangeRequest.builder().retainColumns(ColumnSelection.create(selectedColumns)).build();
        ImmutableList<RowResult<Value>> list = ImmutableList.copyOf(keyValueService.getRange(TEST_TABLE, simpleRange, 1));
        assertEquals(0, list.size());

        list = ImmutableList.copyOf(keyValueService.getRange(TEST_TABLE, simpleRange, 2));
        assertEquals(1, list.size());
        RowResult<Value> row = list.iterator().next();
        assertEquals(1, row.getColumns().size());

        list = ImmutableList.copyOf(keyValueService.getRange(TEST_TABLE, simpleRange, 3));
        assertEquals(2, list.size());
        row = list.iterator().next();
        assertEquals(1, row.getColumns().size());

        list = ImmutableList.copyOf(keyValueService.getRange(TEST_TABLE, simpleRange.getBuilder().endRowExclusive(PtBytes.toBytes(""row2"")).build(), 3));
        assertEquals(1, list.size());
        row = list.iterator().next();
        assertEquals(1, row.getColumns().size());

        list = ImmutableList.copyOf(keyValueService.getRange(TEST_TABLE, simpleRange.getBuilder().startRowInclusive(PtBytes.toBytes(""row1a"")).build(), 3));
        assertEquals(1, list.size());
        row = list.iterator().next();
        assertEquals(1, row.getColumns().size());
    }
"
"    @Test
    public void testKeyValueRangeWithDeletes() {
        putDirect(""row1"", ""col1"", """", 0);

        ImmutableList<RowResult<Value>> list = ImmutableList.copyOf(keyValueService.getRange(TEST_TABLE, RangeRequest.builder().build(), 1));
        assertEquals(1, list.size());
        RowResult<Value> row = list.iterator().next();
        assertEquals(1, row.getColumns().size());
    }
"
"    @Test
    public void testKeyValueRanges() {
        putDirect(""row1"", ""col1"", """", 0);
        putDirect(""row2"", ""col1"", """", 0);
        putDirect(""row2"", ""col2"", """", 0);

        Map<RangeRequest, TokenBackedBasicResultsPage<RowResult<Value>, byte[]>> ranges = keyValueService.getFirstBatchForRanges(TEST_TABLE, ImmutableList.of(RangeRequest.builder().build(), RangeRequest.builder().build()), 1);
        assertTrue(ranges.size() >= 1);
    }
"
"    @Test
    public void testKeyValueRanges2() {
        putDirect(""row1"", ""col1"", """", 0);
        putDirect(""row2"", ""col1"", """", 0);
        putDirect(""row2"", ""col2"", """", 0);

        final RangeRequest allRange = RangeRequest.builder().build();
        final RangeRequest oneRange = RangeRequest.builder().startRowInclusive(""row2"".getBytes()).build();
        final RangeRequest allRangeBatch = RangeRequest.builder().batchHint(3).build();
        Map<RangeRequest, TokenBackedBasicResultsPage<RowResult<Value>, byte[]>> ranges = keyValueService.getFirstBatchForRanges(TEST_TABLE, ImmutableList.of(allRange, oneRange, allRangeBatch), 1);
        assertTrue(ranges.get(allRange).getResults().size()>=1);
        assertEquals(2, ranges.get(allRangeBatch).getResults().size());
        assertFalse(ranges.get(allRangeBatch).moreResultsAvailable());
        assertEquals(1, ranges.get(oneRange).getResults().size());
    }
"
"    @Test
    public void testKeyValueRangesMany3() {
        putDirect(""row1"", ""col1"", """", 0);
        putDirect(""row2"", ""col1"", """", 0);
        putDirect(""row2"", ""col2"", """", 0);

        RangeRequest allRange = RangeRequest.builder().prefixRange(""row1"".getBytes()).batchHint(3).build();
        for (int i = 0 ; i < 1000 ; i++) {
            ClosableIterator<RowResult<Value>> range = keyValueService.getRange(TEST_TABLE, allRange, 1);
            ImmutableList<RowResult<Value>> list = ImmutableList.copyOf(range);
            assertEquals(1, list.size());
        }
    }
"
"    @Test
    public void testKeyValueRangeReverse() {
        if (!supportsReverse()) {
            return;
        }
        putDirect(""row1"", ""col1"", """", 0);
        putDirect(""row2"", ""col1"", """", 0);
        putDirect(""row2"", ""col2"", """", 0);

        RangeRequest allRange = RangeRequest.reverseBuilder().batchHint(3).build();
        ClosableIterator<RowResult<Value>> range = keyValueService.getRange(TEST_TABLE, allRange, 1);
        ImmutableList<RowResult<Value>> list = ImmutableList.copyOf(range);
        assertEquals(2, list.size());
        assertEquals(""row2"", PtBytes.toString(list.iterator().next().getRowName()));
    }
"
"    @Test
    public void testRangePagingBatches() {
        int totalPuts = 101;
        for (int i = 0 ; i < totalPuts ; i++) {
            putDirect(""row""+i, ""col1"", ""v1"", 0);
        }

        Map<RangeRequest, TokenBackedBasicResultsPage<RowResult<Value>, byte[]>> ranges = keyValueService.getFirstBatchForRanges(TEST_TABLE, Iterables.limit(Iterables.cycle(RangeRequest.builder().batchHint(1000).build()), 100), 1);
        assertEquals(1, ranges.keySet().size());
        assertEquals(totalPuts, ranges.values().iterator().next().getResults().size());
    }
"
"    @Test
    public void testRangePagingBatchesReverse() {
        if (!supportsReverse()) {
            return;
        }
        int totalPuts = 101;
        for (int i = 0 ; i < totalPuts ; i++) {
            putDirect(""row""+i, ""col1"", ""v1"", 0);
        }

        Map<RangeRequest, TokenBackedBasicResultsPage<RowResult<Value>, byte[]>> ranges = keyValueService.getFirstBatchForRanges(TEST_TABLE, Iterables.limit(Iterables.cycle(RangeRequest.reverseBuilder().batchHint(1000).build()), 100), 1);
        assertEquals(1, ranges.keySet().size());
        assertEquals(totalPuts, ranges.values().iterator().next().getResults().size());
    }
"
"    @Test
    public void testRangePagingBatchSizeOne() {
        int totalPuts = 100;
        for (int i = 0 ; i < totalPuts ; i++) {
            putDirect(""row""+i, ""col1"", ""v1"", 0);
        }

        RangeRequest rangeRequest = RangeRequest.builder().batchHint(1).build();
        Map<RangeRequest, TokenBackedBasicResultsPage<RowResult<Value>, byte[]>> ranges = keyValueService.getFirstBatchForRanges(TEST_TABLE, Iterables.limit(Iterables.cycle(rangeRequest), 100), 1);
        assertEquals(1, ranges.keySet().size());
        assertEquals(1, ranges.values().iterator().next().getResults().size());
        assertEquals(""row0"", PtBytes.toString(ranges.values().iterator().next().getResults().iterator().next().getRowName()));
    }
"
"    @Test
    public void testRangePagingBatchSizeOneReverse() {
        if (!supportsReverse()) {
            return;
        }
        int totalPuts = 100;
        for (int i = 0 ; i < totalPuts ; i++) {
            putDirect(""row""+i, ""col1"", ""v1"", 0);
        }

        RangeRequest rangeRequest = RangeRequest.reverseBuilder().batchHint(1).build();
        Map<RangeRequest, TokenBackedBasicResultsPage<RowResult<Value>, byte[]>> ranges = keyValueService.getFirstBatchForRanges(TEST_TABLE, Iterables.limit(Iterables.cycle(rangeRequest), 100), 1);
        assertEquals(1, ranges.keySet().size());
        assertEquals(1, ranges.values().iterator().next().getResults().size());
        assertEquals(""row99"", PtBytes.toString(ranges.values().iterator().next().getResults().iterator().next().getRowName()));
    }
"
"    @Test
    public void testRangePageBatchSizeOne() {
        RangeRequest rangeRequest = RangeRequest.builder().batchHint(1).build();
        Map<RangeRequest, TokenBackedBasicResultsPage<RowResult<Value>, byte[]>> ranges = keyValueService.getFirstBatchForRanges(TEST_TABLE, Collections.singleton(rangeRequest), 1);
        assertEquals(1, ranges.keySet().size());
        assertEquals(0, ranges.values().iterator().next().getResults().size());
        assertEquals(false, ranges.values().iterator().next().moreResultsAvailable());
    }
"
"    @Test
    public void testRangeAfterTimestmap() {
        putDirect(""row1"", ""col2"", """", 5);
        putDirect(""row2"", ""col2"", """", 0);
        RangeRequest rangeRequest = RangeRequest.builder().batchHint(1).build();
        Map<RangeRequest, TokenBackedBasicResultsPage<RowResult<Value>, byte[]>> ranges = keyValueService.getFirstBatchForRanges(TEST_TABLE, Collections.singleton(rangeRequest), 1);
        assertEquals(1, ranges.keySet().size());
        TokenBackedBasicResultsPage<RowResult<Value>, byte[]> page = ranges.values().iterator().next();
        assertTrue(!page.getResults().isEmpty() || page.moreResultsAvailable());
    }
"
"    @Test(timeOut = 5000)
    public void testLoadArguments() throws Exception {
        PerformanceClient client = new PerformanceClient();

        // ""--proxy-url"" has the highest priority
        PerformanceClient.Arguments arguments = client.loadArguments(
                getArgs(""ws://broker0.pulsar.apache.org:8080/"", ""./src/test/resources/websocket_client1.conf""));
        assertEquals(arguments.proxyURL, ""ws://broker0.pulsar.apache.org:8080/"");

        // ""webSocketServiceUrl"" written in the conf file has the second priority
        arguments = client.loadArguments(getArgs(null, ""./src/test/resources/websocket_client1.conf""));
        assertEquals(arguments.proxyURL, ""ws://broker1.pulsar.apache.org:8080/"");

        // ""webServiceUrl"" written in the conf file has the third priority
        arguments = client.loadArguments(getArgs(null, ""./src/test/resources/websocket_client2.conf""));
        assertEquals(arguments.proxyURL, ""ws://broker2.pulsar.apache.org:8080/"");

        // ""serviceUrl"" written in the conf file has the fourth priority
        arguments = client.loadArguments(getArgs(null, ""./src/test/resources/websocket_client3.conf""));
        assertEquals(arguments.proxyURL, ""wss://broker3.pulsar.apache.org:8443/"");

        // The default value is ""ws://localhost:8080/""
        arguments = client.loadArguments(getArgs(null, null));
        assertEquals(arguments.proxyURL, ""ws://localhost:8080/"");

        // If the URL does not end with ""/"", it will be added
        arguments = client.loadArguments(getArgs(""ws://broker0.pulsar.apache.org:8080"", null));
        assertEquals(arguments.proxyURL, ""ws://broker0.pulsar.apache.org:8080/"");
    }
"
"    @Test
    public void testGenerateDocumention() throws Exception {
        CmdGenerateDocumentation.main(new String[]{});
    }
"
"    @Test
    public void testSpecifyModuleName() throws Exception {
        String[] args = new String[]{""-n"", ""produce"", ""-n"", ""consume""};
        CmdGenerateDocumentation.main(args);
    }
"
"    @Test(timeOut = 20000)
    public void testMsgKey() throws Exception {
        String argString = ""%s -r 10 -u %s -m 500"";
        String topic = testTopic + UUID.randomUUID().toString();
        String args = String.format(argString, topic, pulsar.getBrokerServiceUrl());
        Thread thread = new Thread(() -> {
            try {
                PerformanceProducer.main(args.split("" ""));
            } catch (Exception e) {
                e.printStackTrace();
            }
        });
        thread.start();
        Consumer<byte[]> consumer1 = pulsarClient.newConsumer().topic(topic).subscriptionName(""sub-1"")
                .subscriptionType(SubscriptionType.Key_Shared).subscribe();
        Consumer<byte[]> consumer2 = pulsarClient.newConsumer().topic(topic).subscriptionName(""sub-1"")
                .subscriptionType(SubscriptionType.Key_Shared).subscribe();

        int count1 = 0;
        int count2 = 0;
        for (int i = 0; i < 10; i++) {
            Message<byte[]> message = consumer1.receive(1, TimeUnit.SECONDS);
            if (message == null) {
                break;
            }
            count1++;
            consumer1.acknowledge(message);
        }
        for (int i = 0; i < 10; i++) {
            Message<byte[]> message = consumer2.receive(1, TimeUnit.SECONDS);
            if (message == null) {
                break;
            }
            count2++;
            consumer2.acknowledge(message);
        }
        //in key_share mode, only one consumer can get msg
        Assert.assertTrue(count1 == 0 || count2 == 0);

        consumer1.close();
        consumer2.close();
        thread.interrupt();
        while (thread.isAlive()) {
            Thread.sleep(1000);
        }

        //use msg key generator,so every consumer can get msg
        String newArgString = ""%s -r 10 -u %s -m 500 -mk autoIncrement"";
        String topic2 = testTopic + UUID.randomUUID().toString();
        String newArgs = String.format(newArgString, topic2, pulsar.getBrokerServiceUrl());
        Thread thread2 = new Thread(() -> {
            try {
                PerformanceProducer.main(newArgs.split("" ""));
            } catch (Exception e) {
                e.printStackTrace();
            }
        });
        thread2.start();

        Consumer newConsumer1 = pulsarClient.newConsumer().topic(topic2).subscriptionName(""sub-2"")
                .subscriptionType(SubscriptionType.Key_Shared).subscribe();
        Consumer newConsumer2 = pulsarClient.newConsumer().topic(topic2).subscriptionName(""sub-2"")
                .subscriptionType(SubscriptionType.Key_Shared).subscribe();
        count1 = 0;
        count2 = 0;
        for (int i = 0; i < 10; i++) {
            Message<byte[]> message = newConsumer1.receive(1, TimeUnit.SECONDS);
            if (message == null) {
                break;
            }
            count1++;
            newConsumer1.acknowledge(message);
        }
        for (int i = 0; i < 10; i++) {
            Message<byte[]> message = newConsumer2.receive(1, TimeUnit.SECONDS);
            if (message == null) {
                break;
            }
            count2++;
            newConsumer2.acknowledge(message);
        }

        Assert.assertTrue(count1 > 0 && count2 > 0);
        thread2.interrupt();
        newConsumer1.close();
        newConsumer2.close();
    }
"
"    @Test(timeOut = 20000)
    public void testCreatePartitions() throws Exception {
        String argString = ""%s -r 10 -u %s -au %s -m 5 -np 10"";
        String topic = testTopic + UUID.randomUUID().toString();
        String args = String.format(argString, topic, pulsar.getBrokerServiceUrl(), pulsar.getWebServiceAddress());
        Thread thread = new Thread(() -> {
            try {
                PerformanceProducer.main(args.split("" ""));
            } catch (Exception e) {
                e.printStackTrace();
            }
        });
        thread.start();
        thread.join();
        Assert.assertEquals(10, pulsar.getAdminClient().topics().getPartitionedTopicMetadata(topic).partitions);
    }
"
"    @Test
    public void testNotExistIMessageFormatter() {
        IMessageFormatter msgFormatter = PerformanceProducer.getMessageFormatter(""org.apache.pulsar.testclient.NonExistentFormatter"");
        Assert.assertNull(msgFormatter);
    }
"
"    @Test
    public void testDefaultIMessageFormatter() {
        IMessageFormatter msgFormatter = PerformanceProducer.getMessageFormatter(""org.apache.pulsar.testclient.DefaultMessageFormatter"");
        Assert.assertTrue(msgFormatter instanceof DefaultMessageFormatter);
    }
"
"    @Test
    public void testFormatMessage() {
        String producerName = ""producer-1"";
        long msgId = 3;
        byte[] message = ""{ \""producer\"": \""%p\"", \""msgId\"": %i, \""nanoTime\"": %t, \""float1\"": %5.2f, \""float2\"": %-5.2f, \""long1\"": %12l, \""long2\"": %l, \""int1\"": %d, \""int2\"": %1d , \""long3\"": %5l,  \""str\"": \""%5s\"" }"".getBytes();
        byte[] formatted = new DefaultMessageFormatter().formatMessage(producerName, msgId, message);
        String jsonString = new String(formatted, StandardCharsets.UTF_8);

        ObjectMapper objectMapper = new ObjectMapper();

        JsonNode obj = null;
        try {
            obj = objectMapper.readValue(jsonString, JsonNode.class);

        } catch(Exception jpe) {
            Assert.fail(""Exception parsing json"");
        }

        String prod = obj.get(""producer"").asText();
        int mid = obj.get(""msgId"").asInt();
        long nt = obj.get(""nanoTime"").asLong();
        float f1 = obj.get(""float1"").floatValue();
        float f2 = obj.get(""float2"").floatValue();
        long l1 = obj.get(""long1"").asLong();
        long l2 = obj.get(""long2"").asLong();
        long i1 = obj.get(""int1"").asInt();
        long i2 = obj.get(""int2"").asInt();
        String str = obj.get(""str"").asText();
        long l3 = obj.get(""long3"").asLong();
        Assert.assertEquals(producerName, prod);
        Assert.assertEquals(msgId, mid);
        Assert.assertTrue( nt > 0);
        Assert.assertNotEquals(f1, f2);
        Assert.assertNotEquals(l1, l2);
        Assert.assertNotEquals(i1, i2);
        Assert.assertTrue(l3 > 0);
        Assert.assertTrue(l3 <= 99999);
        Assert.assertTrue(i2 < 10);
        Assert.assertTrue(0 < i2, ""i2 was "" + i2);
        Assert.assertTrue(f2 < 100000);
        Assert.assertTrue( -100000 < f2);

    }
"
"    @Test
    public void TestGetTableId() throws Exception {
        String tableName = ""TestGetTableId"";

        sqliteUtils.createTable(
            ""CREATE TABLE "" + tableName + ""("" +
                ""    firstName  TEXT,"" +
                ""    lastName  TEXT,"" +
                ""    age INTEGER,"" +
                ""    bool  NUMERIC,"" +
                ""    byte  INTEGER,"" +
                ""    short INTEGER NULL,"" +
                ""    long INTEGER,"" +
                ""    float NUMERIC,"" +
                ""    double NUMERIC,"" +
                ""    bytes BLOB, "" +
                ""PRIMARY KEY (firstName, lastName));""
        );

        Connection connection = sqliteUtils.getConnection();

        // Test getTableId
        log.info(""verify getTableId"");
        TableId id = JdbcUtils.getTableId(connection, tableName);
        Assert.assertEquals(id.getTableName(), tableName);

        // Test get getTableDefinition
        log.info(""verify getTableDefinition"");
        List<String> keyList = Lists.newArrayList();
        keyList.add(""firstName"");
        keyList.add(""lastName"");
        List<String> nonKeyList = Lists.newArrayList();
        nonKeyList.add(""age"");
        nonKeyList.add(""long"");
        TableDefinition table = JdbcUtils.getTableDefinition(connection, id, keyList, nonKeyList);
        Assert.assertEquals(table.getColumns().get(0).getName(), ""firstName"");
        Assert.assertEquals(table.getColumns().get(0).getTypeName(), ""TEXT"");
        Assert.assertEquals(table.getColumns().get(2).getName(), ""age"");
        Assert.assertEquals(table.getColumns().get(2).getTypeName(), ""INTEGER"");
        Assert.assertEquals(table.getColumns().get(7).getName(), ""float"");
        Assert.assertEquals(table.getColumns().get(7).getTypeName(), ""NUMERIC"");
        Assert.assertEquals(table.getKeyColumns().get(0).getName(), ""firstName"");
        Assert.assertEquals(table.getKeyColumns().get(0).getTypeName(), ""TEXT"");
        Assert.assertEquals(table.getKeyColumns().get(1).getName(), ""lastName"");
        Assert.assertEquals(table.getKeyColumns().get(1).getTypeName(), ""TEXT"");
        Assert.assertEquals(table.getNonKeyColumns().get(0).getName(), ""age"");
        Assert.assertEquals(table.getNonKeyColumns().get(0).getTypeName(), ""INTEGER"");
        Assert.assertEquals(table.getNonKeyColumns().get(1).getName(), ""long"");
        Assert.assertEquals(table.getNonKeyColumns().get(1).getTypeName(), ""INTEGER"");
        // Test get getTableDefinition
        log.info(""verify buildInsertSql"");
        String expctedInsertStatement = ""INSERT INTO "" + tableName +
            ""(firstName,lastName,age,bool,byte,short,long,float,double,bytes)"" +
            "" VALUES(?,?,?,?,?,?,?,?,?,?)"";
        String insertStatement = JdbcUtils.buildInsertSql(table);
        Assert.assertEquals(insertStatement, expctedInsertStatement);
        log.info(""verify buildUpdateSql"");
        String expectedUpdateStatement = ""UPDATE "" + tableName +
                "" SET age=? ,long=?  WHERE firstName=? AND lastName=?"";
        String updateStatement = JdbcUtils.buildUpdateSql(table);
        Assert.assertEquals(updateStatement, expectedUpdateStatement);
        log.info(""verify buildDeleteSql"");
        String expectedDeleteStatement = ""DELETE FROM "" + tableName +
                "" WHERE firstName=? AND lastName=?"";
        String deleteStatement = JdbcUtils.buildDeleteSql(table);
        Assert.assertEquals(deleteStatement, expectedDeleteStatement);
    }
"
"    @Test
    public void TestInsertAction() throws Exception {
        testOpenAndWriteSink(ImmutableMap.of(""ACTION"", ""INSERT""));
    }
"
"    @Test
    public void TestNoAction() throws Exception {
        testOpenAndWriteSink(ImmutableMap.of());
    }
"
"    @Test
    public void TestNoActionNullValue() throws Exception {
        testOpenAndWriteSinkNullValue(ImmutableMap.of(""ACTION"", ""INSERT""));
    }
"
"    @Test
    public void TestNoActionNullValueJson() throws Exception {
        testOpenAndWriteSinkNullValueJson(ImmutableMap.of(""ACTION"", ""INSERT""));
    }
"
"    @Test
    public void TestNoActionJson() throws Exception {
        testOpenAndWriteSinkJson(ImmutableMap.of(""ACTION"", ""INSERT""));
    }
"
"    @Test
    public void TestUnknownAction() throws Exception {
        Record<GenericRecord> recordRecord = mock(Record.class);
        when(recordRecord.getProperties()).thenReturn(ImmutableMap.of(""ACTION"", ""UNKNOWN""));
        CompletableFuture<Void> future = new CompletableFuture<>();
        doAnswer(a -> future.complete(null)).when(recordRecord).fail();
        jdbcSink.write(recordRecord);
        future.get(1, TimeUnit.SECONDS);
    }
"
"    @Test
    public void TestUpdateAction() throws Exception {

        AvroSchema<Foo> schema = AvroSchema.of(SchemaDefinition.<Foo>builder().withPojo(Foo.class).build());

        Foo updateObj = new Foo();
        updateObj.setField1(""ValueOfField3"");
        updateObj.setField2(""ValueOfField3"");
        updateObj.setField3(4);

        byte[] updateBytes = schema.encode(updateObj);
        Message<GenericRecord> updateMessage = mock(MessageImpl.class);
        CompletableFuture<Void> future = new CompletableFuture<>();
        Record<GenericRecord> updateRecord = PulsarRecord.<GenericRecord>builder()
                .message(updateMessage)
                .topicName(""fake_topic_name"")
                .ackFunction(() -> future.complete(null))
                .build();

        GenericSchema<GenericRecord> updateGenericAvroSchema;
        updateGenericAvroSchema = new GenericAvroSchema(schema.getSchemaInfo());

        Map<String, String> updateProperties = Maps.newHashMap();
        updateProperties.put(""ACTION"", ""UPDATE"");
        when(updateMessage.getValue()).thenReturn(updateGenericAvroSchema.decode(updateBytes));
        when(updateMessage.getProperties()).thenReturn(updateProperties);
        log.info(""foo:{}, Message.getValue: {}, record.getValue: {}"",
                updateObj.toString(),
                updateMessage.getValue().toString(),
                updateRecord.getValue().toString());

        jdbcSink.write(updateRecord);
        future.get(1, TimeUnit.SECONDS);

        // value has been written to db, read it out and verify.
        String updateQuerySql = ""SELECT * FROM "" + tableName + "" WHERE field3=4"";
        sqliteUtils.select(updateQuerySql, (resultSet) -> {
            Assert.assertEquals(updateObj.getField1(), resultSet.getString(1));
            Assert.assertEquals(updateObj.getField2(), resultSet.getString(2));
            Assert.assertEquals(updateObj.getField3(), resultSet.getInt(3));
        });
    }
"
"    @Test
    public void TestDeleteAction() throws Exception {

        AvroSchema<Foo> schema = AvroSchema.of(SchemaDefinition.<Foo>builder().withPojo(Foo.class).build());

        Foo deleteObj = new Foo();
        deleteObj.setField3(5);

        byte[] deleteBytes = schema.encode(deleteObj);
        Message<GenericRecord> deleteMessage = mock(MessageImpl.class);
        CompletableFuture<Void> future = new CompletableFuture<>();
        Record<GenericRecord> deleteRecord = PulsarRecord.<GenericRecord>builder()
                .message(deleteMessage)
                .topicName(""fake_topic_name"")
                .ackFunction(() -> future.complete(null))
                .build();

        GenericSchema<GenericRecord> deleteGenericAvroSchema = new GenericAvroSchema(schema.getSchemaInfo());

        Map<String, String> deleteProperties = Maps.newHashMap();
        deleteProperties.put(""ACTION"", ""DELETE"");
        when(deleteMessage.getValue()).thenReturn(deleteGenericAvroSchema.decode(deleteBytes));
        when(deleteMessage.getProperties()).thenReturn(deleteProperties);
        log.info(""foo:{}, Message.getValue: {}, record.getValue: {}"",
                deleteObj.toString(),
                deleteMessage.getValue().toString(),
                deleteRecord.getValue().toString());

        jdbcSink.write(deleteRecord);
        future.get(1, TimeUnit.SECONDS);

        // value has been written to db, read it out and verify.
        String deleteQuerySql = ""SELECT * FROM "" + tableName + "" WHERE field3=5"";
        Assert.assertEquals(sqliteUtils.select(deleteQuerySql, (resultSet) -> {}), 0);
    }
"
"    @Test
    public void testDispoableChannel() throws Exception {
        String agentName = ""agent1"";
        Map<String, String> properties = getPropertiesForChannel(agentName,
                DisposableChannel.class.getName());
        MemoryConfigurationProvider provider =
                new MemoryConfigurationProvider(agentName, properties);
        MaterializedConfiguration config1 = provider.getConfiguration();
        Channel channel1 = config1.getChannels().values().iterator().next();
        assertTrue(channel1 instanceof DisposableChannel);
        MaterializedConfiguration config2 = provider.getConfiguration();
        Channel channel2 = config2.getChannels().values().iterator().next();
        assertTrue(channel2 instanceof DisposableChannel);
        assertNotSame(channel1, channel2);
    }
"
"    @Test
    public void testReusableChannel() throws Exception {
        String agentName = ""agent1"";
        Map<String, String> properties = getPropertiesForChannel(agentName,
                RecyclableChannel.class.getName());
        MemoryConfigurationProvider provider =
                new MemoryConfigurationProvider(agentName, properties);

        MaterializedConfiguration config1 = provider.getConfiguration();
        Channel channel1 = config1.getChannels().values().iterator().next();
        assertTrue(channel1 instanceof RecyclableChannel);

        MaterializedConfiguration config2 = provider.getConfiguration();
        Channel channel2 = config2.getChannels().values().iterator().next();
        assertTrue(channel2 instanceof RecyclableChannel);

        assertSame(channel1, channel2);
    }
"
"    @Test
    public void testUnspecifiedChannel() throws Exception {
        String agentName = ""agent1"";
        Map<String, String> properties = getPropertiesForChannel(agentName,
                UnspecifiedChannel.class.getName());
        MemoryConfigurationProvider provider =
                new MemoryConfigurationProvider(agentName, properties);

        MaterializedConfiguration config1 = provider.getConfiguration();
        Channel channel1 = config1.getChannels().values().iterator().next();
        assertTrue(channel1 instanceof UnspecifiedChannel);

        MaterializedConfiguration config2 = provider.getConfiguration();
        Channel channel2 = config2.getChannels().values().iterator().next();
        assertTrue(channel2 instanceof UnspecifiedChannel);

        assertSame(channel1, channel2);
    }
"
"    @Test
    public void testReusableChannelNotReusedLater() throws Exception {
        String agentName = ""agent1"";
        Map<String, String> propertiesReusable = getPropertiesForChannel(agentName,
                RecyclableChannel.class
                        .getName());
        Map<String, String> propertiesDispoable = getPropertiesForChannel(agentName,
                DisposableChannel.class
                        .getName());
        MemoryConfigurationProvider provider =
                new MemoryConfigurationProvider(agentName, propertiesReusable);
        MaterializedConfiguration config1 = provider.getConfiguration();
        Channel channel1 = config1.getChannels().values().iterator().next();
        assertTrue(channel1 instanceof RecyclableChannel);

        provider.setProperties(propertiesDispoable);
        MaterializedConfiguration config2 = provider.getConfiguration();
        Channel channel2 = config2.getChannels().values().iterator().next();
        assertTrue(channel2 instanceof DisposableChannel);

        provider.setProperties(propertiesReusable);
        MaterializedConfiguration config3 = provider.getConfiguration();
        Channel channel3 = config3.getChannels().values().iterator().next();
        assertTrue(channel3 instanceof RecyclableChannel);

        assertNotSame(channel1, channel3);
    }
"
"    @Test
    public void testSourceThrowsExceptionDuringConfiguration() throws Exception {
        String agentName = ""agent1"";
        String sourceType = UnconfigurableSource.class.getName();
        String channelType = ""memory"";
        String sinkType = ""null"";
        Map<String, String> properties = getProperties(agentName, sourceType,
                channelType, sinkType);
        MemoryConfigurationProvider provider =
                new MemoryConfigurationProvider(agentName, properties);
        MaterializedConfiguration config = provider.getConfiguration();
        assertEquals(config.getSourceRunners().size(), 0);
        assertEquals(config.getChannels().size(), 1);
        assertEquals(config.getSinkRunners().size(), 1);
    }
"
"    @Test
    public void testChannelThrowsExceptionDuringConfiguration() throws Exception {
        String agentName = ""agent1"";
        String sourceType = ""seq"";
        String channelType = UnconfigurableChannel.class.getName();
        String sinkType = ""null"";
        Map<String, String> properties = getProperties(agentName, sourceType,
                channelType, sinkType);
        MemoryConfigurationProvider provider =
                new MemoryConfigurationProvider(agentName, properties);
        MaterializedConfiguration config = provider.getConfiguration();
        assertEquals(config.getSourceRunners().size(), 0);
        assertEquals(config.getChannels().size(), 0);
        assertEquals(config.getSinkRunners().size(), 0);
    }
"
"    @Test
    public void testSinkThrowsExceptionDuringConfiguration() throws Exception {
        String agentName = ""agent1"";
        String sourceType = ""seq"";
        String channelType = ""memory"";
        String sinkType = UnconfigurableSink.class.getName();
        Map<String, String> properties = getProperties(agentName, sourceType,
                channelType, sinkType);
        MemoryConfigurationProvider provider =
                new MemoryConfigurationProvider(agentName, properties);
        MaterializedConfiguration config = provider.getConfiguration();
        assertEquals(config.getSourceRunners().size(), 1);
        assertEquals(config.getChannels().size(), 1);
        assertEquals(config.getSinkRunners().size(), 0);
    }
"
"    @Test
    public void testSourceAndSinkThrowExceptionDuringConfiguration()
            throws Exception {
        String agentName = ""agent1"";
        String sourceType = UnconfigurableSource.class.getName();
        String channelType = ""memory"";
        String sinkType = UnconfigurableSink.class.getName();
        Map<String, String> properties = getProperties(agentName, sourceType,
                channelType, sinkType);
        MemoryConfigurationProvider provider =
                new MemoryConfigurationProvider(agentName, properties);
        MaterializedConfiguration config = provider.getConfiguration();
        assertEquals(config.getSourceRunners().size(), 0);
        assertEquals(config.getChannels().size(), 0);
        assertEquals(config.getSinkRunners().size(), 0);
    }
"
"    @Test
    public void testSinkSourceMismatchDuringConfiguration() throws Exception {
        String agentName = ""agent1"";
        String sourceType = ""seq"";
        String channelType = ""memory"";
        String sinkType = ""avro"";
        Map<String, String> properties = getProperties(agentName, sourceType,
                channelType, sinkType);
        properties.put(agentName + "".channels.channel1.capacity"", ""1000"");
        properties.put(agentName + "".channels.channel1.transactionCapacity"", ""1000"");
        properties.put(agentName + "".sources.source1.batchSize"", ""1000"");
        properties.put(agentName + "".sinks.sink1.batch-size"", ""1000"");
        properties.put(agentName + "".sinks.sink1.hostname"", ""10.10.10.10"");
        properties.put(agentName + "".sinks.sink1.port"", ""1010"");

        MemoryConfigurationProvider provider =
                new MemoryConfigurationProvider(agentName, properties);
        MaterializedConfiguration config = provider.getConfiguration();
        assertEquals(config.getSourceRunners().size(), 1);
        assertEquals(config.getChannels().size(), 1);
        assertEquals(config.getSinkRunners().size(), 1);

        properties.put(agentName + "".sources.source1.batchSize"", ""1001"");
        properties.put(agentName + "".sinks.sink1.batch-size"", ""1000"");

        provider = new MemoryConfigurationProvider(agentName, properties);
        config = provider.getConfiguration();
        assertEquals(config.getSourceRunners().size(), 0);
        assertEquals(config.getChannels().size(), 1);
        assertEquals(config.getSinkRunners().size(), 1);

        properties.put(agentName + "".sources.source1.batchSize"", ""1000"");
        properties.put(agentName + "".sinks.sink1.batch-size"", ""1001"");

        provider = new MemoryConfigurationProvider(agentName, properties);
        config = provider.getConfiguration();
        assertEquals(config.getSourceRunners().size(), 1);
        assertEquals(config.getChannels().size(), 1);
        assertEquals(config.getSinkRunners().size(), 0);

        properties.put(agentName + "".sources.source1.batchSize"", ""1001"");
        properties.put(agentName + "".sinks.sink1.batch-size"", ""1001"");

        provider = new MemoryConfigurationProvider(agentName, properties);
        config = provider.getConfiguration();
        assertEquals(config.getSourceRunners().size(), 0);
        assertEquals(config.getChannels().size(), 0);
        assertEquals(config.getSinkRunners().size(), 0);
    }
"
"    @Test(enabled = false)
    public void testPolling() throws Exception {

        // let first event fire
        Thread.sleep(2000L);

        final List<MaterializedConfiguration> events = Lists.newArrayList();

        Object eventHandler = new Object() {
            @Subscribe
            public synchronized void handleConfigurationEvent(MaterializedConfiguration event) {
                events.add(event);
            }
        };
        eventBus.register(eventHandler);
        configFile.setLastModified(System.currentTimeMillis());

        // now wait for second event to fire
        Thread.sleep(2000L);

        Assert.assertEquals(events.size(), 1, String.valueOf(events));

        MaterializedConfiguration materializedConfiguration = events.remove(0);

        Assert.assertEquals(materializedConfiguration.getSourceRunners().size(),1);
        Assert.assertEquals(materializedConfiguration.getSinkRunners().size(), 1);
        Assert.assertEquals(materializedConfiguration.getChannels().size(), 1);


    }
"
"    @Test
    public void testPolling() throws Exception {
        es.awaitEvent();
        es.reset();

        FlumeConfiguration fc = cp.getFlumeConfiguration();
        Assert.assertTrue(fc.getConfigurationErrors().isEmpty());
        AgentConfiguration ac = fc.getConfigurationFor(AGENT_NAME);
        Assert.assertNull(ac);

        addData();
        es.awaitEvent();
        es.reset();

        verifyProperties(cp);
    }
"
"    @Test
    public void testBasicConfiguration() throws Exception {

        EventBus eventBus = new EventBus(""test-event-bus"");

        MaterializedConfiguration materializedConfiguration = new
                SimpleMaterializedConfiguration();

        SourceRunner sourceRunner = mockLifeCycle(SourceRunner.class);
        materializedConfiguration.addSourceRunner(""test"", sourceRunner);

        SinkRunner sinkRunner = mockLifeCycle(SinkRunner.class);
        materializedConfiguration.addSinkRunner(""test"", sinkRunner);

        Channel channel = mockLifeCycle(Channel.class);
        materializedConfiguration.addChannel(""test"", channel);


        ConfigurationProvider configurationProvider = mock(ConfigurationProvider.class);
        when(configurationProvider.getConfiguration()).thenReturn(materializedConfiguration);

        Application application = new Application();
        eventBus.register(application);
        eventBus.post(materializedConfiguration);
        application.start();

        Thread.sleep(1000L);

        verify(sourceRunner).start();
        verify(sinkRunner).start();
        verify(channel).start();

        application.stop();

        Thread.sleep(1000L);

        verify(sourceRunner).stop();
        verify(sinkRunner).stop();
        verify(channel).stop();
    }
"
"    @Test
    public void testFLUME1854() throws Exception {
        File configFile = new File(baseDir, ""flume-conf.properties"");
        Files.copy(new File(getClass().getClassLoader()
                .getResource(""flume-conf.properties"").getFile()), configFile);
        Random random = new Random();
        for (int i = 0; i < 3; i++) {
            EventBus eventBus = new EventBus(""test-event-bus"");
            PollingPropertiesFileConfigurationProvider configurationProvider =
                    new PollingPropertiesFileConfigurationProvider(""host1"",
                            configFile, eventBus, 1);
            List<LifecycleAware> components = Lists.newArrayList();
            components.add(configurationProvider);
            Application application = new Application(components);
            eventBus.register(application);
            application.start();
            Thread.sleep(random.nextInt(10000));
            application.stop();
        }
    }
"
"    @Test(timeOut = 10000L)
    public void testFLUME2786() throws Exception {
        final String agentName = ""test"";
        final int interval = 1;
        final long intervalMs = 1000L;

        File configFile = new File(baseDir, ""flume-conf.properties"");
        Files.copy(new File(getClass().getClassLoader()
                .getResource(""flume-conf.properties.2786"").getFile()), configFile);
        File mockConfigFile = spy(configFile);
        when(mockConfigFile.lastModified()).then(new Answer<Long>() {
            @Override
            public Long answer(InvocationOnMock invocation) throws Throwable {
                Thread.sleep(intervalMs);
                return System.currentTimeMillis();
            }
"
"    @Test
    public void resolveEnvVar() {
        environmentVariables.set(""VARNAME"", ""varvalue"");
        String resolved = EnvVarResolverProperties.resolveEnvVars(""padding ${VARNAME} padding"");
        Assert.assertEquals(""padding varvalue padding"", resolved);
    }
"
"    @Test
    public void resolveEnvVars() {
        environmentVariables.set(""VARNAME1"", ""varvalue1"");
        environmentVariables.set(""VARNAME2"", ""varvalue2"");
        String resolved = EnvVarResolverProperties
                .resolveEnvVars(""padding ${VARNAME1} ${VARNAME2} padding"");
        Assert.assertEquals(""padding varvalue1 varvalue2 padding"", resolved);
    }
"
"    @Test
    public void getProperty() {
        String NC_PORT = ""6667"";
        environmentVariables.set(""NC_PORT"", NC_PORT);
        System.setProperty(""propertiesImplementation"",
                ""org.apache.pulsar.io.flume.node.EnvVarResolverProperties"");

        Assert.assertEquals(NC_PORT, provider.getFlumeConfiguration()
                .getConfigurationFor(""a1"")
                .getSourceContext().get(""r1"").getParameters().get(""port""));
    }
"
"    @Test
    public void testPropertyRead() throws Exception {
        verifyProperties(configurationProvider);
    }
"
"    @Test
    public void testPropertyRead() {

        FlumeConfiguration configuration = provider.getFlumeConfiguration();
        assertNotNull(configuration);

    /*
     * Test the known errors in the file
     */
        List<String> expected = Lists.newArrayList();
        expected.add(""host5 CONFIG_ERROR"");
        expected.add(""host5 INVALID_PROPERTY"");
        expected.add(""host4 CONFIG_ERROR"");
        expected.add(""host4 CONFIG_ERROR"");
        expected.add(""host4 PROPERTY_VALUE_NULL"");
        expected.add(""host4 PROPERTY_VALUE_NULL"");
        expected.add(""host4 PROPERTY_VALUE_NULL"");
        expected.add(""host4 AGENT_CONFIGURATION_INVALID"");
        expected.add(""ch2 ATTRS_MISSING"");
        expected.add(""host3 CONFIG_ERROR"");
        expected.add(""host3 PROPERTY_VALUE_NULL"");
        expected.add(""host3 AGENT_CONFIGURATION_INVALID"");
        expected.add(""host2 PROPERTY_VALUE_NULL"");
        expected.add(""host2 AGENT_CONFIGURATION_INVALID"");
        List<String> actual = Lists.newArrayList();
        for (FlumeConfigurationError error : configuration.getConfigurationErrors()) {
            actual.add(error.getComponentName() + "" "" + error.getErrorType().toString());
        }
        Collections.sort(expected);
        Collections.sort(actual);
        assertEquals(actual, expected);

        AgentConfiguration agentConfiguration =
                configuration.getConfigurationFor(""host1"");
        assertNotNull(agentConfiguration);

        LOGGER.info(agentConfiguration.getPrevalidationConfig());
        LOGGER.info(agentConfiguration.getPostvalidationConfig());

        Set<String> sources = Sets.newHashSet(""source1"");
        Set<String> sinks = Sets.newHashSet(""sink1"");
        Set<String> channels = Sets.newHashSet(""channel1"");

        assertEquals(agentConfiguration.getSourceSet(), sources);
        assertEquals(agentConfiguration.getSinkSet(), sinks);
        assertEquals(agentConfiguration.getChannelSet(), channels);
    }
"
"    @Test
    public void TestOpenAndWriteSink() throws Exception {
        Map<String, Object> conf = Maps.newHashMap();
        StringSink stringSink = new StringSink();
        conf.put(""name"", ""a1"");
        conf.put(""confFile"", ""./src/test/resources/flume/source.conf"");
        conf.put(""noReloadConf"", false);
        conf.put(""zkConnString"", """");
        conf.put(""zkBasePath"", """");
        stringSink.open(conf, mockSinkContext);
        send(stringSink, 100);

        Thread.sleep(3 * 1000);
        Transaction transaction = channel.getTransaction();
        transaction.begin();
        Event event = channel.take();

        Assert.assertNotNull(event);
        Assert.assertNotNull(mockRecord);

        verify(mockRecord, times(100)).ack();
        transaction.commit();
        transaction.close();
    }
"
"    @Test
    public void TestOpenAndReadSource() throws Exception {
        Map<String, Object> conf = Maps.newHashMap();
        StringSource stringSource = new StringSource();
        conf.put(""name"", ""a1"");
        conf.put(""confFile"", ""./src/test/resources/flume/sink.conf"");
        conf.put(""noReloadConf"", false);
        conf.put(""zkConnString"", """");
        conf.put(""zkBasePath"", """");
        Event event = EventBuilder.withBody(""test event 1"", Charsets.UTF_8);
        stringSource.open(conf, mockSourceContext);
        Thread.sleep(3 * 1000);
        sink.start();
        Transaction transaction = channel.getTransaction();

        transaction.begin();
        for (int i = 0; i < 10; i++) {
            channel.put(event);
        }
        transaction.commit();
        transaction.close();

        for (int i = 0; i < 5; i++) {
            Sink.Status status = sink.process();
            assertEquals(status, Sink.Status.READY);
        }

        assertEquals(sink.process(), Sink.Status.BACKOFF);
        stringSource.close();
    }
"
"    @Test
    public void testJsonSerialization() throws Exception {

        final String[] keyNames = { ""key1"", ""key2"" };
        final String key1Value = ""test1"";
        final String key2Value = ""test2"";
        final byte[][] keyValues = { key1Value.getBytes(), key2Value.getBytes() };
        final String param = ""param"";
        final String algo = ""algo"";
        int batchSize = 10;
        int compressionMsgSize = 10;

        // serialize to json
        byte[] data = ""payload"".getBytes();
        Map<String, String> properties = Maps.newHashMap();
        properties.put(""prop1"", ""value"");
        Map<String, String> metadata1 = Maps.newHashMap();
        metadata1.put(""version"", ""v1"");
        metadata1.put(""ckms"", ""cmks-1"");
        Map<String, String> metadata2 = Maps.newHashMap();
        metadata2.put(""version"", ""v2"");
        metadata2.put(""ckms"", ""cmks-2"");
        Record<byte[]> recordCtx = createRecord(data, algo, keyNames, keyValues, param.getBytes(), metadata1, metadata2,
                batchSize, compressionMsgSize, properties, true);
        String json = Utils.serializeRecordToJson(recordCtx);

        // deserialize from json and assert
        KinesisMessageResponse kinesisJsonResponse = deSerializeRecordFromJson(json);
        assertEquals(data, getDecoder().decode(kinesisJsonResponse.getPayloadBase64()));
        EncryptionCtx encryptionCtxDeser = kinesisJsonResponse.getEncryptionCtx();
        assertEquals(key1Value.getBytes(),
                getDecoder().decode(encryptionCtxDeser.getKeysMapBase64().get(keyNames[0])));
        assertEquals(key2Value.getBytes(),
                getDecoder().decode(encryptionCtxDeser.getKeysMapBase64().get(keyNames[1])));
        assertEquals(param.getBytes(), getDecoder().decode(encryptionCtxDeser.getEncParamBase64()));
        assertEquals(algo, encryptionCtxDeser.getAlgorithm());
        assertEquals(metadata1, encryptionCtxDeser.getKeysMetadataMap().get(keyNames[0]));
        assertEquals(metadata2, encryptionCtxDeser.getKeysMetadataMap().get(keyNames[1]));
        assertEquals(properties, kinesisJsonResponse.getProperties());

    }
"
"    @Test(dataProvider=""encryption"")
    public void testFbSerialization(boolean isEncryption) throws Exception {

        final String[] keyNames = { ""key1"", ""key2"" };
        final String param = ""param"";
        final String algo = ""algo"";
        int batchSize = 10;
        int compressionMsgSize = 10;

        for (int k = 0; k < 5; k++) {
            String payloadString = RandomStringUtils.random(142342 * k, String.valueOf(System.currentTimeMillis()));
            final String key1Value = payloadString + ""test1"";
            final String key2Value = payloadString + ""test2"";
            final byte[][] keyValues = { key1Value.getBytes(), key2Value.getBytes() };
            byte[] data = payloadString.getBytes();
            Map<String, String> properties = Maps.newHashMap();
            properties.put(""prop1"", payloadString);
            Map<String, String> metadata1 = Maps.newHashMap();
            metadata1.put(""version"", ""v1"");
            metadata1.put(""ckms"", ""cmks-1"");
            Map<String, String> metadata2 = Maps.newHashMap();
            metadata2.put(""version"", ""v2"");
            metadata2.put(""ckms"", ""cmks-2"");
            Record<byte[]> record = createRecord(data, algo, keyNames, keyValues, param.getBytes(), metadata1,
                    metadata2, batchSize, compressionMsgSize, properties, isEncryption);
            ByteBuffer flatBuffer = Utils.serializeRecordToFlatBuffer(record);

            Message kinesisJsonResponse = Message.getRootAsMessage(flatBuffer);
            byte[] fbPayloadBytes = new byte[kinesisJsonResponse.payloadLength()];
            kinesisJsonResponse.payloadAsByteBuffer().get(fbPayloadBytes);
            assertEquals(data, fbPayloadBytes);

            if(isEncryption) {
                org.apache.pulsar.io.kinesis.fbs.EncryptionCtx encryptionCtxDeser = kinesisJsonResponse.encryptionCtx();
                byte compressionType = encryptionCtxDeser.compressionType();
                int fbBatchSize = encryptionCtxDeser.batchSize();
                boolean isBathcMessage = encryptionCtxDeser.isBatchMessage();
                int fbCompressionMsgSize = encryptionCtxDeser.uncompressedMessageSize();
                int totalKeys = encryptionCtxDeser.keysLength();
                Map<String, Map<String, String>> fbKeyMetadataResult = Maps.newHashMap();
                Map<String, byte[]> fbKeyValueResult = Maps.newHashMap();
                for (int i = 0; i < encryptionCtxDeser.keysLength(); i++) {
                    org.apache.pulsar.io.kinesis.fbs.EncryptionKey encryptionKey = encryptionCtxDeser.keys(i);
                    String keyName = encryptionKey.key();
                    byte[] keyValueBytes = new byte[encryptionKey.valueLength()];
                    encryptionKey.valueAsByteBuffer().get(keyValueBytes);
                    fbKeyValueResult.put(keyName, keyValueBytes);
                    Map<String, String> fbMetadata = Maps.newHashMap();
                    for (int j = 0; j < encryptionKey.metadataLength(); j++) {
                        KeyValue encMtdata = encryptionKey.metadata(j);
                        fbMetadata.put(encMtdata.key(), encMtdata.value());
                    }
                    fbKeyMetadataResult.put(keyName, fbMetadata);
                }
                byte[] paramBytes = new byte[encryptionCtxDeser.paramLength()];
                encryptionCtxDeser.paramAsByteBuffer().get(paramBytes);

                assertEquals(totalKeys, 2);
                assertEquals(batchSize, fbBatchSize);
                assertTrue(isBathcMessage);
                assertEquals(compressionMsgSize, fbCompressionMsgSize);
                assertEquals(keyValues[0], fbKeyValueResult.get(keyNames[0]));
                assertEquals(keyValues[1], fbKeyValueResult.get(keyNames[1]));
                assertEquals(metadata1, fbKeyMetadataResult.get(keyNames[0]));
                assertEquals(metadata2, fbKeyMetadataResult.get(keyNames[1]));
                assertEquals(compressionType, org.apache.pulsar.io.kinesis.fbs.CompressionType.LZ4);
                assertEquals(param.getBytes(), paramBytes);
                assertEquals(algo, encryptionCtxDeser.algo());
            }

            Map<String, String> fbproperties = Maps.newHashMap();
            for (int i = 0; i < kinesisJsonResponse.propertiesLength(); i++) {
                KeyValue property = kinesisJsonResponse.properties(i);
                fbproperties.put(property.key(), property.value());
            }
            assertEquals(properties, fbproperties);

        }
    }
"
"    @Test
    public void testDefaultCredentialProvider() throws Exception {
        KinesisSink sink = new KinesisSink();
        Map<String, String> credentialParam = Maps.newHashMap();
        String awsCredentialPluginParam = new Gson().toJson(credentialParam);
        try {
            sink.defaultCredentialProvider(awsCredentialPluginParam);
            Assert.fail(""accessKey and SecretKey validation not applied"");
        } catch (IllegalArgumentException ie) {
            // Ok..
        }

        final String accesKey = ""ak"";
        final String secretKey = ""sk"";
        credentialParam.put(KinesisSink.ACCESS_KEY_NAME, accesKey);
        credentialParam.put(KinesisSink.SECRET_KEY_NAME, secretKey);
        awsCredentialPluginParam = new Gson().toJson(credentialParam);
        AWSCredentialsProvider credentialProvider = sink.defaultCredentialProvider(awsCredentialPluginParam)
                .getCredentialProvider();
        Assert.assertNotNull(credentialProvider);
        Assert.assertEquals(credentialProvider.getCredentials().getAWSAccessKeyId(), accesKey);
        Assert.assertEquals(credentialProvider.getCredentials().getAWSSecretKey(), secretKey);

        sink.close();
    }
"
"    @Test
    public void testCredentialProvider() throws Exception {
        KinesisSink sink = new KinesisSink();

        final String accesKey = ""ak"";
        final String secretKey = ""sk"";
        Map<String, String> credentialParam = Maps.newHashMap();
        credentialParam.put(KinesisSink.ACCESS_KEY_NAME, accesKey);
        credentialParam.put(KinesisSink.SECRET_KEY_NAME, secretKey);
        String awsCredentialPluginParam = new Gson().toJson(credentialParam);
        AWSCredentialsProvider credentialProvider = sink.createCredentialProvider(null, awsCredentialPluginParam)
                .getCredentialProvider();
        Assert.assertEquals(credentialProvider.getCredentials().getAWSAccessKeyId(), accesKey);
        Assert.assertEquals(credentialProvider.getCredentials().getAWSSecretKey(), secretKey);

        credentialProvider = sink.createCredentialProvider(AwsCredentialProviderPluginImpl.class.getName(), ""{}"")
                .getCredentialProvider();
        Assert.assertNotNull(credentialProvider);
        Assert.assertEquals(credentialProvider.getCredentials().getAWSAccessKeyId(),
                AwsCredentialProviderPluginImpl.accessKey);
        Assert.assertEquals(credentialProvider.getCredentials().getAWSSecretKey(),
                AwsCredentialProviderPluginImpl.secretKey);
        Assert.assertEquals(((BasicSessionCredentials) credentialProvider.getCredentials()).getSessionToken(),
                AwsCredentialProviderPluginImpl.sessionToken);

        sink.close();
    }
"
"    @Test
    public void testCredentialProviderPlugin() throws Exception {
        KinesisSink sink = new KinesisSink();

        AWSCredentialsProvider credentialProvider = sink
                .createCredentialProviderWithPlugin(AwsCredentialProviderPluginImpl.class.getName(), ""{}"")
                .getCredentialProvider();
        Assert.assertNotNull(credentialProvider);
        Assert.assertEquals(credentialProvider.getCredentials().getAWSAccessKeyId(),
                AwsCredentialProviderPluginImpl.accessKey);
        Assert.assertEquals(credentialProvider.getCredentials().getAWSSecretKey(),
                AwsCredentialProviderPluginImpl.secretKey);
        Assert.assertEquals(((BasicSessionCredentials) credentialProvider.getCredentials()).getSessionToken(),
                AwsCredentialProviderPluginImpl.sessionToken);

        sink.close();
    }
"
"    @Test
    public void TestOpenAndWriteSink() throws Exception {
        message = mock(MessageImpl.class);
        Map<String, Object> configs = new HashMap<>();
        configs.put(""solrUrl"", ""http://localhost:8983/solr"");
        configs.put(""solrMode"", ""Standalone"");
        configs.put(""solrCollection"", ""techproducts"");
        configs.put(""solrCommitWithinMs"", ""100"");
        configs.put(""username"", """");
        configs.put(""password"", """");
        GenericSchema<GenericRecord> genericAvroSchema;

        SolrGenericRecordSink sink = new SolrGenericRecordSink();

        // prepare a foo Record
        Foo obj = new Foo();
        obj.setField1(""FakeFiled1"");
        obj.setField2(""FakeFiled1"");
        AvroSchema<Foo> schema = AvroSchema.of(Foo.class);

        byte[] bytes = schema.encode(obj);
        AutoConsumeSchema autoConsumeSchema = new AutoConsumeSchema();
        autoConsumeSchema.setSchema(GenericSchemaImpl.of(schema.getSchemaInfo()));

        Record<GenericRecord> record = PulsarRecord.<GenericRecord>builder()
            .message(message)
            .topicName(""fake_topic_name"")
            .build();

        genericAvroSchema = new GenericAvroSchema(schema.getSchemaInfo());

        when(message.getValue())
                .thenReturn(genericAvroSchema.decode(bytes));

        log.info(""foo:{}, Message.getValue: {}, record.getValue: {}"",
            obj.toString(),
            message.getValue().toString(),
            record.getValue().toString());

        // open should success
        sink.open(configs, null);
    }
"
"    @Test
    public void testSinkContext() throws Exception {
        SinkContext sinkContext = mock(SinkContext.class);

        Sink testSink = spy(TestSink.class);
        testSink.open(new HashMap<>(), sinkContext);

        verify(sinkContext, times(1)).recordMetric(""foo"", 1);
    }
"
"  @Test(expectedExceptions = RuntimeException.class, expectedExceptionsMessageRegExp = ""test exception"")
  public void testNotifyErrors() throws Exception {
    testBatchSource.notifyError(new RuntimeException(""test exception""));
    testBatchSource.readNext();
  }
"
"    @Test
    public void testSinkContext() throws Exception {
        SourceContext sourceContext = mock(SourceContext.class);

        Source testSource = spy(TestSource.class);
        testSource.open(new HashMap<>(), sourceContext);

        verify(sourceContext, times(1)).recordMetric(""foo"", 1);
    }
"
"    @Test
    public void testGetBytesNoCopy() throws Exception {
        byte[] originalArray = {1, 2, 3};
        ByteBuffer wrapped = ByteBuffer.wrap(originalArray);
        assertEquals(0, wrapped.arrayOffset());
        assertEquals(3, wrapped.remaining());
        assertSame(ByteBufferSchemaWrapper.getBytes(wrapped), originalArray);
    }
"
"    @Test
    public void testGetBytesOffsetZeroDifferentLen() throws Exception {
        byte[] originalArray = {1, 2, 3};
        ByteBuffer wrapped = ByteBuffer.wrap(originalArray, 1, 2);
        assertEquals(0, wrapped.arrayOffset());
        assertEquals(2, wrapped.remaining());
        byte[] result = ByteBufferSchemaWrapper.getBytes(wrapped);
        assertNotSame(result, originalArray);
        assertArrayEquals(result, new byte[] {2,3});
    }
"
"    @Test
    public void testGetBytesOffsetNonZero() throws Exception {
        byte[] originalArray = {1, 2, 3};
        ByteBuffer wrapped = ByteBuffer.wrap(originalArray);
        wrapped.position(1);
        assertEquals(1, wrapped.position());
        wrapped = wrapped.slice();
        assertEquals(1, wrapped.arrayOffset());
        assertEquals(2, wrapped.remaining());
        byte[] result = ByteBufferSchemaWrapper.getBytes(wrapped);
        assertNotSame(result, originalArray);
        assertArrayEquals(result, new byte[] {2,3});
    }
"
"    @Test
    public void testGetBytesOffsetZero() throws Exception {
        byte[] originalArray = {1, 2, 3};
        ByteBuffer wrapped = ByteBuffer.wrap(originalArray, 0, 2);
        assertEquals(0, wrapped.arrayOffset());
        assertEquals(2, wrapped.remaining());
        byte[] result = ByteBufferSchemaWrapper.getBytes(wrapped);
        assertNotSame(result, originalArray);
        assertArrayEquals(result, new byte[] {1,2});
    }
"
"    @Test
    public void testNoKeyValueSchema() throws Exception {

        validateSchemaNoKeyValue(StringDeserializer.class.getName(), Schema.STRING,
                StringDeserializer.class.getName(), Schema.STRING);

        validateSchemaNoKeyValue(StringDeserializer.class.getName(), Schema.STRING,
                ByteBufferDeserializer.class.getName(), Schema.BYTEBUFFER);

        validateSchemaNoKeyValue(StringDeserializer.class.getName(), Schema.STRING,
                BytesDeserializer.class.getName(), Schema.BYTEBUFFER);

        validateSchemaNoKeyValue(StringDeserializer.class.getName(), Schema.STRING,
                DoubleDeserializer.class.getName(), Schema.DOUBLE);

        validateSchemaNoKeyValue(StringDeserializer.class.getName(), Schema.STRING,
                FloatDeserializer.class.getName(), Schema.FLOAT);

        validateSchemaNoKeyValue(StringDeserializer.class.getName(), Schema.STRING,
                IntegerDeserializer.class.getName(), Schema.INT32);

        validateSchemaNoKeyValue(StringDeserializer.class.getName(), Schema.STRING,
                LongDeserializer.class.getName(), Schema.INT64);

        validateSchemaNoKeyValue(StringDeserializer.class.getName(), Schema.STRING,
                ShortDeserializer.class.getName(), Schema.INT16);

        validateSchemaNoKeyValue(StringDeserializer.class.getName(), Schema.STRING,
                KafkaAvroDeserializer.class.getName(), KafkaBytesSource.DeferredSchemaPlaceholder.INSTANCE);

    }
"
"    @Test
    public void testKeyValueSchema() throws Exception {
        validateSchemaKeyValue(IntegerDeserializer.class.getName(), Schema.INT32,
                StringDeserializer.class.getName(), Schema.STRING,
                ByteBuffer.wrap(new IntegerSerializer().serialize(""test"", 10)),
                ByteBuffer.wrap(new StringSerializer().serialize(""test"", ""test"")));
    }
"
"    @Test
    public void testInvalidConfigWillThrownException() throws Exception {
        KafkaAbstractSink sink = new DummySink();
        Map<String, Object> config = new HashMap<>();
        SinkContext sc = new SinkContext() {
            @Override
            public int getInstanceId() {
                return 0;
            }
"
"    @Test
    public void testInvalidConfigWillThrownException() throws Exception {
        KafkaAbstractSource source = new DummySource();
        SourceContext ctx = mock(SourceContext.class);
        Map<String, Object> config = new HashMap<>();
        Assert.ThrowingRunnable openAndClose = ()->{
            try {
                source.open(config, ctx);
                fail();
            } finally {
                source.close();
            }
        };
        expectThrows(NullPointerException.class, openAndClose);
        config.put(""topic"", ""topic_1"");
        expectThrows(NullPointerException.class, openAndClose);
        config.put(""bootstrapServers"", ""localhost:8080"");
        expectThrows(NullPointerException.class, openAndClose);
        config.put(""groupId"", ""test-group"");
        config.put(""fetchMinBytes"", -1);
        expectThrows(IllegalArgumentException.class, openAndClose);
        config.put(""fetchMinBytes"", 1000);
        config.put(""autoCommitEnabled"", true);
        config.put(""autoCommitIntervalMs"", -1);
        expectThrows(IllegalArgumentException.class, openAndClose);
        config.put(""autoCommitIntervalMs"", 100);
        config.put(""sessionTimeoutMs"", -1);
        expectThrows(IllegalArgumentException.class, openAndClose);
        config.put(""sessionTimeoutMs"", 10000);
        config.put(""heartbeatIntervalMs"", -100);
        expectThrows(IllegalArgumentException.class, openAndClose);
        config.put(""heartbeatIntervalMs"", 20000);
        expectThrows(IllegalArgumentException.class, openAndClose);
        config.put(""heartbeatIntervalMs"", 5000);
        config.put(""autoOffsetReset"", ""some-value"");
        expectThrows(IllegalArgumentException.class, openAndClose);
        config.put(""autoOffsetReset"", ""earliest"");
        source.open(config, ctx);
        source.close();
    }
"
"    @Test
    public void shouldStartWithEmptyTopicAndStoreDataAndRecoverAllState() throws Exception {
        // Create the empty topic ...
        testHistoryTopicContent(false, true);
    }
"
"    @Test
    public void shouldIgnoreUnparseableMessages() throws Exception {
        try (final Producer<String> producer = pulsarClient.newProducer(Schema.STRING)
            .topic(topicName)
            .create()
        ) {
            producer.send("""");
            producer.send(""{\""position\"":{\""filename\"":\""my-txn-file.log\"",\""position\"":39},\""databaseName\"":\""db1\"",\""ddl\"":\""DROP TABLE foo;\""}"");
            producer.send(""{\""source\"":{\""server\"":\""my-server\""},\""databaseName\"":\""db1\"",\""ddl\"":\""DROP TABLE foo;\""}"");
            producer.send(""{\""source\"":{\""server\"":\""my-server\""},\""position\"":{\""filename\"":\""my-txn-file.log\"",\""position\"":39},\""databaseName\"":\""db1\"",\""ddl\"":\""DROP TABLE foo;\"""");
            producer.send(""\""source\"":{\""server\"":\""my-server\""},\""position\"":{\""filename\"":\""my-txn-file.log\"",\""position\"":39},\""databaseName\"":\""db1\"",\""ddl\"":\""DROP TABLE foo;\""}"");
            producer.send(""{\""source\"":{\""server\"":\""my-server\""},\""position\"":{\""filename\"":\""my-txn-file.log\"",\""position\"":39},\""databaseName\"":\""db1\"",\""ddl\"":\""xxxDROP TABLE foo;\""}"");
        }

        testHistoryTopicContent(true, true);
    }
"
"    @Test(expectedExceptions = ParsingException.class)
    public void shouldStopOnUnparseableSQL() throws Exception {
        try (final Producer<String> producer = pulsarClient.newProducer(Schema.STRING).topic(topicName).create()) {
            producer.send(""{\""source\"":{\""server\"":\""my-server\""},\""position\"":{\""filename\"":\""my-txn-file.log\"",\""position\"":39},\""databaseName\"":\""db1\"",\""ddl\"":\""xxxDROP TABLE foo;\""}"");
        }

        testHistoryTopicContent(false, false);
    }
"
"    @Test
    public void testExists() throws Exception {
        // happy path
        testHistoryTopicContent(true, false);
        assertTrue(history.exists());

        // Set history to use dummy topic
        Configuration config = Configuration.create()
            .with(PulsarDatabaseHistory.SERVICE_URL, brokerUrl.toString())
            .with(PulsarDatabaseHistory.TOPIC, ""persistent://my-property/my-ns/dummytopic"")
            .with(DatabaseHistory.NAME, ""my-db-history"")
            .with(DatabaseHistory.SKIP_UNPARSEABLE_DDL_STATEMENTS, true)
            .build();

        history.configure(config, null, DatabaseHistoryListener.NOOP, true);
        history.start();

        // dummytopic should not exist yet
        assertFalse(history.exists());
    }
"
"    @Test
    public void openInfluxV1() throws Exception {
        Map<String, Object> map = new HashMap<>();
        map.put(""influxdbUrl"", ""http://localhost:8086"");
        map.put(""database"", ""test_db"");

        InfluxDBGenericRecordSink sink = new InfluxDBGenericRecordSink();
        try {
            sink.open(map, mock(SinkContext.class));
        } catch (InfluxDBIOException e) {
            // Do nothing
        }
        assertTrue(sink.sink instanceof org.apache.pulsar.io.influxdb.v1.InfluxDBGenericRecordSink);
    }
"
"    @Test
    public void openInfluxV2() throws Exception {
        Map<String, Object> map = new HashMap();
        map.put(""influxdbUrl"", ""http://localhost:9999"");
        map.put(""token"", ""xxxx"");
        map.put(""organization"", ""example-org"");
        map.put(""bucket"", ""example-bucket"");

        InfluxDBGenericRecordSink sink = new InfluxDBGenericRecordSink();
        try {
            sink.open(map, mock(SinkContext.class));
        } catch (InfluxDBIOException e) {
            // Do nothing
        }
        assertTrue(sink.sink instanceof InfluxDBSink);
    }
"
"    @Test(expectedExceptions = Exception.class,
    public void openInvalidInfluxConfig() throws Exception {
        InfluxDBGenericRecordSink sink = new InfluxDBGenericRecordSink();
        sink.open(new HashMap<>(), mock(SinkContext.class));
    }
"
"    @Test
    public void testJsonSchema() {
        JSONSchema<Cpu> schema = JSONSchema.of(Cpu.class);

        AutoConsumeSchema autoConsumeSchema = new AutoConsumeSchema();
        autoConsumeSchema.setSchema(GenericSchemaImpl.of(schema.getSchemaInfo()));
        GenericSchema<GenericRecord> genericSchema = GenericSchemaImpl.of(autoConsumeSchema.getSchemaInfo());

        assertFalse(genericSchema instanceof GenericAvroSchema);

        byte[] bytes = schema.encode(cpu);
        GenericRecord record = genericSchema.decode(bytes);

        assertEquals(record.getField(""measurement""), ""cpu"");

        // compare the String type
        assertEquals(record.getField(""timestamp"").toString(), timestamp + """");

        assertEquals(((GenericRecord)record.getField(""tags"")).getField(""host""), ""server-1"");
        assertEquals(((GenericRecord)record.getField(""fields"")).getField(""value""), 10);
    }
"
"    @Test
    public void testAvroSchema() {
        AvroSchema<Cpu> schema = AvroSchema.of(Cpu.class);

        AutoConsumeSchema autoConsumeSchema = new AutoConsumeSchema();
        autoConsumeSchema.setSchema(GenericSchemaImpl.of(schema.getSchemaInfo()));
        GenericSchema<GenericRecord> genericAvroSchema = GenericSchemaImpl.of(autoConsumeSchema.getSchemaInfo());

        assertTrue(genericAvroSchema instanceof GenericAvroSchema);

        byte[] bytes = schema.encode(cpu);
        GenericRecord record = genericAvroSchema.decode(bytes);

        assertEquals(record.getField(""measurement""), ""cpu"");
        assertEquals(record.getField(""timestamp""), timestamp);
        assertEquals(((Map)record.getField(""tags"")).get(new Utf8(""host"")).toString(), ""server-1"");
        assertEquals(((Map)record.getField(""fields"")).get(new Utf8(""value"")), 10);
    }
"
"    @Test
    public void testOpenWriteCloseAvro() throws Exception {
        AvroSchema<Cpu> avroSchema = AvroSchema.of(Cpu.class);
        openWriteClose(avroSchema);
    }
"
"    @Test
    public void testOpenWriteCloseJson() throws Exception {
        JSONSchema<Cpu> jsonSchema = JSONSchema.of(Cpu.class);
        openWriteClose(jsonSchema);
    }
"
"    @Test(expectedExceptions = NullPointerException.class,
    public void testRequiredConfigMissing() throws Exception {
        Map<String, Object> map = buildValidConfigMap();
        map.remove(""influxdbUrl"");
        InfluxDBSinkConfig config = InfluxDBSinkConfig.load(map);
        config.validate();
    }
"
"    @Test(expectedExceptions = IllegalArgumentException.class,
    public void testBatchConfig() throws Exception {
        Map<String, Object> map = buildValidConfigMap();
        map.put(""batchSize"", -1);
        InfluxDBSinkConfig config = InfluxDBSinkConfig.load(map);
        config.validate();
    }
"
"    @Test
    public void testOpenAndWrite() throws Exception {
        message = mock(MessageImpl.class);
        GenericSchema<GenericRecord> genericAvroSchema;
        // prepare a cpu Record
        Cpu cpu = new Cpu();
        cpu.setMeasurement(""cpu"");
        cpu.setModel(""lenovo"");
        cpu.setValue(10);

        Map<String, String> tags = Maps.newHashMap();
        tags.put(""host"", ""server-1"");
        tags.put(""region"", ""us-west"");

        cpu.setTags(tags);
        AvroSchema<Cpu> schema = AvroSchema.of(Cpu.class);

        byte[] bytes = schema.encode(cpu);
        AutoConsumeSchema autoConsumeSchema = new AutoConsumeSchema();
        autoConsumeSchema.setSchema(GenericSchemaImpl.of(schema.getSchemaInfo()));

        Record<GenericRecord> record = PulsarRecord.<GenericRecord>builder()
            .message(message)
            .topicName(""influx_cpu"")
            .build();

        genericAvroSchema = new GenericAvroSchema(schema.getSchemaInfo());

        when(message.getValue())
                .thenReturn(genericAvroSchema.decode(bytes));

        log.info(""cpu:{}, Message.getValue: {}, record.getValue: {}"",
            cpu.toString(),
            message.getValue().toString(),
            record.getValue().toString());

        influxSink.open(configMap, mockSinkContext);

        verify(this.influxDB, times(1)).describeDatabases();
        verify(this.influxDB, times(1)).createDatabase(""testDB"");

        doAnswer(invocationOnMock -> {
            BatchPoints batchPoints = invocationOnMock.getArgument(0, BatchPoints.class);
            Assert.assertNotNull(batchPoints, ""batchPoints should not be null."");
            return null;
        }).when(influxDB).write(any(BatchPoints.class));

        influxSink.write(record);

        Thread.sleep(1000);

        verify(influxDB, times(1)).write(any(BatchPoints.class));
    }
"
"    @Test
    public void TestOpenAndWriteSink() throws Exception {
        Map<String, Object> configs = new HashMap<>();
        configs.put(""host"", ""localhost"");
        configs.put(""port"", ""5673"");
        configs.put(""virtualHost"", ""default"");
        configs.put(""username"", ""guest"");
        configs.put(""password"", ""guest"");
        configs.put(""connectionName"", ""test-connection"");
        configs.put(""requestedChannelMax"", ""0"");
        configs.put(""requestedFrameMax"", ""0"");
        configs.put(""connectionTimeout"", ""60000"");
        configs.put(""handshakeTimeout"", ""10000"");
        configs.put(""requestedHeartbeat"", ""60"");
        configs.put(""exchangeName"", ""test-exchange"");
        configs.put(""exchangeType"", ""fanout"");

        RabbitMQSink sink = new RabbitMQSink();

        // open should success
        // rabbitmq service may need time to initialize
        Awaitility.await().ignoreExceptions().untilAsserted(() -> sink.open(configs, null));

        // write should success
        Record<byte[]> record = build(""test-topic"", ""fakeKey"", ""fakeValue"", ""fakeRoutingKey"");
        sink.write(record);

        sink.close();
    }
"
"    @Test
    public void TestOpenAndWriteSink() {
        Map<String, Object> configs = new HashMap<>();
        configs.put(""host"", ""localhost"");
        configs.put(""port"", ""5672"");
        configs.put(""virtualHost"", ""default"");
        configs.put(""username"", ""guest"");
        configs.put(""password"", ""guest"");
        configs.put(""queueName"", ""test-queue"");
        configs.put(""connectionName"", ""test-connection"");
        configs.put(""requestedChannelMax"", ""0"");
        configs.put(""requestedFrameMax"", ""0"");
        configs.put(""connectionTimeout"", ""60000"");
        configs.put(""handshakeTimeout"", ""10000"");
        configs.put(""requestedHeartbeat"", ""60"");
        configs.put(""prefetchCount"", ""0"");
        configs.put(""prefetchGlobal"", ""false"");
        configs.put(""passive"", ""false"");

        RabbitMQSource source = new RabbitMQSource();

        // open should success
        // rabbitmq service may need time to initialize
        Awaitility.await().ignoreExceptions().untilAsserted(() -> source.open(configs, null));
    }
"
"    @Test
    public void testAvroToJson() throws IOException {
        Schema schema = SchemaBuilder.record(""record"").fields()
                .name(""n"").type().longType().longDefault(10)
                .name(""l"").type().longType().longDefault(10)
                .name(""i"").type().intType().intDefault(10)
                .name(""b"").type().booleanType().booleanDefault(true)
                .name(""bb"").type().bytesType().bytesDefault(""10"")
                .name(""d"").type().doubleType().doubleDefault(10.0)
                .name(""f"").type().floatType().floatDefault(10.0f)
                .name(""s"").type().stringType().stringDefault(""titi"")
                .name(""array"").type().optional().array().items(SchemaBuilder.builder().stringType())
                .name(""map"").type().optional().map().values(SchemaBuilder.builder().intType())
                .endRecord();
        GenericRecord genericRecord = new GenericData.Record(schema);
        genericRecord.put(""n"", null);
        genericRecord.put(""l"", 1L);
        genericRecord.put(""i"", 1);
        genericRecord.put(""b"", true);
        genericRecord.put(""bb"", ""10"".getBytes(StandardCharsets.UTF_8));
        genericRecord.put(""d"", 10.0);
        genericRecord.put(""f"", 10.0f);
        genericRecord.put(""s"", ""toto"");
        genericRecord.put(""array"", new String[] {""toto""});
        genericRecord.put(""map"", ImmutableMap.of(""a"",10));
        JsonNode jsonNode = JsonConverter.toJson(genericRecord);
        assertEquals(jsonNode.get(""n""), NullNode.getInstance());
        assertEquals(jsonNode.get(""l"").asLong(), 1L);
        assertEquals(jsonNode.get(""i"").asInt(), 1);
        assertEquals(jsonNode.get(""b"").asBoolean(), true);
        assertEquals(jsonNode.get(""bb"").binaryValue(), ""10"".getBytes(StandardCharsets.UTF_8));
        assertEquals(jsonNode.get(""d"").asDouble(), 10.0);
        assertEquals(jsonNode.get(""f"").numberValue(), 10.0f);
        assertEquals(jsonNode.get(""s"").asText(), ""toto"");
        assertTrue(jsonNode.get(""array"").isArray());
        assertEquals(jsonNode.get(""array"").iterator().next().asText(), ""toto"");
        assertTrue(jsonNode.get(""map"").isObject());
        assertEquals(jsonNode.get(""map"").elements().next().asText(), ""10"");
        assertEquals(jsonNode.get(""map"").get(""a"").numberValue(), 10);
    }
"
"    @Test
    public void testLogicalTypesToJson() {
        Schema decimalType = LogicalTypes.decimal(3,3).addToSchema(Schema.create(Schema.Type.BYTES));
        Schema dateType = LogicalTypes.date().addToSchema(Schema.create(Schema.Type.INT));
        Schema timestampMillisType = LogicalTypes.timestampMillis().addToSchema(Schema.create(Schema.Type.LONG));
        Schema timestampMicrosType = LogicalTypes.timestampMicros().addToSchema(Schema.create(Schema.Type.LONG));
        Schema timeMillisType = LogicalTypes.timeMillis().addToSchema(Schema.create(Schema.Type.INT));
        Schema timeMicrosType = LogicalTypes.timeMicros().addToSchema(Schema.create(Schema.Type.LONG));
        Schema uuidType = LogicalTypes.uuid().addToSchema(Schema.create(Schema.Type.STRING));
        Schema schema = SchemaBuilder.record(""record"")
                .fields()
                .name(""amount"").type(decimalType).noDefault()
                .name(""mydate"").type(dateType).noDefault()
                .name(""tsmillis"").type(timestampMillisType).noDefault()
                .name(""tsmicros"").type(timestampMicrosType).noDefault()
                .name(""timemillis"").type(timeMillisType).noDefault()
                .name(""timemicros"").type(timeMicrosType).noDefault()
                .name(""myuuid"").type(uuidType).noDefault()
                .endRecord();

        final long MILLIS_PER_DAY = 24 * 60 * 60 * 1000;
        BigDecimal myDecimal = new BigDecimal(""10.34"");
        UUID myUuid = UUID.randomUUID();
        Calendar calendar = new GregorianCalendar(TimeZone.getTimeZone(""Europe/Copenhagen""));
        GenericRecord genericRecord = new GenericData.Record(schema);
        genericRecord.put(""amount"", myDecimal);
        genericRecord.put(""mydate"", (int)calendar.toInstant().getEpochSecond());
        genericRecord.put(""tsmillis"", calendar.getTimeInMillis());
        genericRecord.put(""tsmicros"", calendar.getTimeInMillis() * 1000);
        genericRecord.put(""timemillis"", (int)(calendar.getTimeInMillis() % MILLIS_PER_DAY));
        genericRecord.put(""timemicros"", (calendar.getTimeInMillis() %MILLIS_PER_DAY) * 1000);
        genericRecord.put(""myuuid"", myUuid.toString());
        JsonNode jsonNode = JsonConverter.toJson(genericRecord);
        assertEquals(new BigDecimal(jsonNode.get(""amount"").asText()), myDecimal);
        assertEquals(jsonNode.get(""mydate"").asInt(), calendar.toInstant().getEpochSecond());
        assertEquals(jsonNode.get(""tsmillis"").asInt(), (int)calendar.getTimeInMillis());
        assertEquals(jsonNode.get(""tsmicros"").asLong(), calendar.getTimeInMillis() * 1000);
        assertEquals(jsonNode.get(""timemillis"").asInt(), (int)(calendar.getTimeInMillis() % MILLIS_PER_DAY));
        assertEquals(jsonNode.get(""timemicros"").asLong(), (calendar.getTimeInMillis() %MILLIS_PER_DAY) * 1000);
        assertEquals(UUID.fromString(jsonNode.get(""myuuid"").asText()), myUuid);
    }
"
"    @Test
    public void testExponentialWait() {
        RandomExponentialRetry backoffRetry = new RandomExponentialRetry(5);
        assertEquals(backoffRetry.waitInMs(0, 100), 100L);
        assertEquals(backoffRetry.waitInMs(1, 100), 200L);
        assertEquals(backoffRetry.waitInMs(2, 100), 400L);
        assertEquals(backoffRetry.waitInMs(3, 100), 800L);
        assertEquals(backoffRetry.waitInMs(4, 100), 1600L);
        assertEquals(backoffRetry.waitInMs(5, 100), 3200L);
        assertEquals(backoffRetry.waitInMs(6, 100), 5000L);
    }
"
"    @Test
    public void callWithNoRetries() throws Exception {
        MockTime mockTime = new MockTime();
        RandomExponentialRetry backoffRetry = new RandomExponentialRetry();
        assertEquals(0, (int)backoffRetry.retry( () -> testFunction(0), 3, 100, ""NoRetries"", mockTime));
        assertEquals(0L, mockTime.totalMs.get());
        assertEquals(0L, mockTime.sleeps.size());
    }
"
"    @Test(expectedExceptions = { IOException.class })
    public void callWithExhaustedRetries() throws Exception {
        MockTime mockTime = new MockTime();
        RandomExponentialRetry backoffRetry = new RandomExponentialRetry();
        assertEquals(4, (int)backoffRetry.retry( () -> testFunction(4), 3, 100, ""ExhautstedRetries"", mockTime));
    }
"
"    @Test
    public void callWithSomeRetries() throws Exception {
        int N = 10;
        MockTime mockTime = new MockTime();
        RandomExponentialRetry backoffRetry = new RandomExponentialRetry();
        assertEquals(N, (int)backoffRetry.retry( () -> testFunction(N), N+1, 100, ""SomeRetries"", mockTime));
        assertEquals(N, mockTime.sleeps.size());
        for(int i = 0; i < N; i++) {
            assertTrue(mockTime.sleeps.get(i) <=  backoffRetry.waitInMs(i, 100));
        }
        System.out.println(""sleeps=""+mockTime.sleeps);
    }
"
"    @Test
    public void testGenericRecord() throws Exception {
        String json = ""{\""c\"":\""1\"",\""d\"":1,\""e\"":{\""a\"":\""a\"",\""b\"":true,\""d\"":1.0,\""f\"":1.0,\""i\"":1,\""l\"":10}}"";

        ElasticSearchSink elasticSearchSink = new ElasticSearchSink();
        elasticSearchSink.open(ImmutableMap.of(""elasticSearchUrl"", ""http://localhost:9200"", ""schemaEnable"", ""true""), null);
        Pair<String, String> pair = elasticSearchSink.extractIdAndDocument(new Record<GenericObject>() {
            @Override
            public GenericObject getValue() {
                return new GenericObject() {
                    @Override
                    public SchemaType getSchemaType() {
                        return SchemaType.BYTES;
                    }
"
"    @Test
    public void testIndexDelete() throws Exception {
        String index = ""myindex-"" + UUID.randomUUID();
        try (ElasticSearchClient client = new ElasticSearchClient(new ElasticSearchConfig()
                .setElasticSearchUrl(""http://"" + container.getHttpHostAddress())
                .setIndexName(index));) {
            assertTrue(client.createIndexIfNeeded(index));
            try {
                MockRecord<GenericObject> mockRecord = new MockRecord<>();
                client.indexDocument(mockRecord, Pair.of(""1"", ""{ \""a\"":1}""));
                assertEquals(mockRecord.acked, 1);
                assertEquals(mockRecord.failed, 0);
                assertEquals(client.totalHits(index), 1);

                client.deleteDocument(mockRecord, ""1"");
                assertEquals(mockRecord.acked, 2);
                assertEquals(mockRecord.failed, 0);
                assertEquals(client.totalHits(index), 0);
            } finally {
                client.delete(index);
            }
        }
    }
"
"    @Test
    public void testIndexExists() throws IOException {
        String index = ""mynewindex-"" + UUID.randomUUID();
        try (ElasticSearchClient client = new ElasticSearchClient(new ElasticSearchConfig()
                .setElasticSearchUrl(""http://"" + container.getHttpHostAddress())
                .setIndexName(index));) {
            assertFalse(client.indexExists(index));
            assertTrue(client.createIndexIfNeeded(index));
            try {
                assertTrue(client.indexExists(index));
                assertFalse(client.createIndexIfNeeded(index));
            } finally {
                client.delete(index);
            }
        }
    }
"
"    @Test
    public void testTopicToIndexName() throws IOException {
        try (ElasticSearchClient client = new ElasticSearchClient(new ElasticSearchConfig()
                .setElasticSearchUrl(""http://"" + container.getHttpHostAddress())); ) {
            assertEquals(client.topicToIndexName(""data-ks1.table1""), ""data-ks1.table1"");
            assertEquals(client.topicToIndexName(""persistent://public/default/testesjson""), ""testesjson"");
            assertEquals(client.topicToIndexName(""default/testesjson""), ""testesjson"");
            assertEquals(client.topicToIndexName("".testesjson""), "".testesjson"");
            assertEquals(client.topicToIndexName(""TEST""), ""test"");

            assertThrows(RuntimeException.class, () -> client.topicToIndexName(""toto\\titi""));
            assertThrows(RuntimeException.class, () -> client.topicToIndexName(""_abc""));
            assertThrows(RuntimeException.class, () -> client.topicToIndexName(""-abc""));
            assertThrows(RuntimeException.class, () -> client.topicToIndexName(""+abc""));
        }
    }
"
"    @Test
    public void testMalformedDocFails() throws Exception {
        String index = ""indexmalformed-"" + UUID.randomUUID();
        ElasticSearchConfig config = new ElasticSearchConfig()
                .setElasticSearchUrl(""http://""+container.getHttpHostAddress())
                .setIndexName(index)
                .setBulkEnabled(true)
                .setMalformedDocAction(ElasticSearchConfig.MalformedDocAction.FAIL);
        try (ElasticSearchClient client = new ElasticSearchClient(config);) {
            MockRecord<GenericObject> mockRecord = new MockRecord<>();
            client.bulkIndex(mockRecord, Pair.of(""1"", ""{\""a\"":1}""));
            client.bulkIndex(mockRecord, Pair.of(""2"", ""{\""a\"":\""toto\""}""));
            client.flush();
            assertNotNull(client.irrecoverableError.get());
            assertTrue(client.irrecoverableError.get().getMessage().contains(""mapper_parsing_exception""));
            assertEquals(mockRecord.acked, 1);
            assertEquals(mockRecord.failed, 1);
            assertThrows(Exception.class, () -> client.bulkIndex(mockRecord, Pair.of(""3"", ""{\""a\"":3}"")));
            assertEquals(mockRecord.acked, 1);
            assertEquals(mockRecord.failed, 2);
        }
    }
"
"    @Test
    public void testMalformedDocIgnore() throws Exception {
        String index = ""indexmalformed2-"" + UUID.randomUUID();
        ElasticSearchConfig config = new ElasticSearchConfig()
                .setElasticSearchUrl(""http://""+container.getHttpHostAddress())
                .setIndexName(index)
                .setBulkEnabled(true)
                .setMalformedDocAction(ElasticSearchConfig.MalformedDocAction.IGNORE);
        try (ElasticSearchClient client = new ElasticSearchClient(config);) {
            MockRecord<GenericObject> mockRecord = new MockRecord<>();
            client.bulkIndex(mockRecord, Pair.of(""1"", ""{\""a\"":1}""));
            client.bulkIndex(mockRecord, Pair.of(""2"", ""{\""a\"":\""toto\""}""));
            client.flush();
            assertNull(client.irrecoverableError.get());
            assertEquals(mockRecord.acked, 1);
            assertEquals(mockRecord.failed, 1);
        }
    }
"
"    @Test
    public void testBulkRetry() throws Exception {
        final String index = ""indexbulktest-"" + UUID.randomUUID();
        ElasticSearchConfig config = new ElasticSearchConfig()
                .setElasticSearchUrl(""http://""+container.getHttpHostAddress())
                .setIndexName(index)
                .setBulkEnabled(true)
                .setMaxRetries(1000)
                .setBulkActions(2)
                .setRetryBackoffInMs(100)
                // disabled, we want to have full control over flush() method
                .setBulkFlushIntervalInMs(-1);

        try (ElasticSearchClient client = new ElasticSearchClient(config);) {
            try {
                assertTrue(client.createIndexIfNeeded(index));
                MockRecord<GenericObject> mockRecord = new MockRecord<>();
                client.bulkIndex(mockRecord, Pair.of(""1"", ""{\""a\"":1}""));
                client.bulkIndex(mockRecord, Pair.of(""2"", ""{\""a\"":2}""));
                assertEquals(mockRecord.acked, 2);
                assertEquals(mockRecord.failed, 0);
                assertEquals(client.totalHits(index), 2);

                ChaosContainer<?> chaosContainer = new ChaosContainer<>(container.getContainerName(), ""15s"");
                chaosContainer.start();

                client.bulkIndex(mockRecord, Pair.of(""3"", ""{\""a\"":3}""));
                assertEquals(mockRecord.acked, 2);
                assertEquals(mockRecord.failed, 0);
                assertEquals(client.totalHits(index), 2);

                chaosContainer.stop();
                client.flush();
                assertEquals(mockRecord.acked, 3);
                assertEquals(mockRecord.failed, 0);
                assertEquals(client.totalHits(index), 3);
            } finally {
                client.delete(index);
            }
        }
    }
"
"    @Test
    public void testBulkBlocking() throws Exception {
        final String index = ""indexblocking-"" + UUID.randomUUID();
        ElasticSearchConfig config = new ElasticSearchConfig()
                .setElasticSearchUrl(""http://""+container.getHttpHostAddress())
                .setIndexName(index)
                .setBulkEnabled(true)
                .setMaxRetries(1000)
                .setBulkActions(2)
                .setBulkConcurrentRequests(2)
                .setRetryBackoffInMs(100)
                .setBulkFlushIntervalInMs(10000);
        try (ElasticSearchClient client = new ElasticSearchClient(config);) {
            assertTrue(client.createIndexIfNeeded(index));

            try {
                MockRecord<GenericObject> mockRecord = new MockRecord<>();
                for (int i = 1; i <= 5; i++) {
                    client.bulkIndex(mockRecord, Pair.of(Integer.toString(i), ""{\""a\"":"" + i + ""}""));
                }

                Awaitility.await().untilAsserted(() -> {
                    assertThat(""acked record"", mockRecord.acked, greaterThanOrEqualTo(4));
                    assertEquals(mockRecord.failed, 0);
                    assertThat(""totalHits"", client.totalHits(index), greaterThanOrEqualTo(4L));
                });
                client.flush();
                Awaitility.await().untilAsserted(() -> {
                    assertEquals(mockRecord.acked, 5);
                    assertEquals(mockRecord.failed, 0);
                    assertEquals(client.totalHits(index), 5);
                });

                ChaosContainer<?> chaosContainer = new ChaosContainer<>(container.getContainerName(), ""30s"");
                chaosContainer.start();
                Thread.sleep(1000L);

                // 11th bulkIndex is blocking because we have 2 pending requests, and the 3rd request is blocked.
                long start = System.currentTimeMillis();
                for (int i = 6; i <= 15; i++) {
                    client.bulkIndex(mockRecord, Pair.of(Integer.toString(i), ""{\""a\"":"" + i + ""}""));
                    log.info(""{} index {}"", System.currentTimeMillis(), i);
                }
                long elapsed = System.currentTimeMillis() - start;
                log.info(""elapsed = {}"", elapsed);
                assertTrue(elapsed > 29000); // bulkIndex was blocking while elasticsearch was down or busy

                Thread.sleep(1000L);
                assertEquals(mockRecord.acked, 15);
                assertEquals(mockRecord.failed, 0);
                assertEquals(client.records.size(), 0);

                chaosContainer.stop();
            } finally {
                client.delete(index);
            }
        }
    }
"
"    @Test
        public Schema getSchema() {
            return  kvSchema;
        }
"
"    @Test
    public void testStripNullNodes() throws Exception {
        map.put(""stripNulls"", true);
        sink.open(map, mockSinkContext);
        GenericRecord genericRecord = genericSchema.newRecordBuilder()
                .set(""name"", null)
                .set(""userName"", ""boby"")
                .set(""email"", null)
                .build();
        String json = sink.stringifyValue(valueSchema, genericRecord);
        assertEquals(json, ""{\""userName\"":\""boby\""}"");
    }
"
"    @Test
    public void testKeepNullNodes() throws Exception {
        map.put(""stripNulls"", false);
        sink.open(map, mockSinkContext);
        GenericRecord genericRecord = genericSchema.newRecordBuilder()
                .set(""name"", null)
                .set(""userName"", ""boby"")
                .set(""email"", null)
                .build();
        String json = sink.stringifyValue(valueSchema, genericRecord);
        assertEquals(json, ""{\""name\"":null,\""userName\"":\""boby\"",\""email\"":null}"");
    }
"
"    @Test(expectedExceptions = PulsarClientException.InvalidMessageException.class)
    public void testNullValueFailure() throws Exception {
        String index = ""testnullvaluefail"";
        map.put(""indexName"", index);
        map.put(""keyIgnore"", ""false"");
        map.put(""nullValueAction"", ""FAIL"");
        sink.open(map, mockSinkContext);
        MockRecordNullValue mockRecordNullValue = new MockRecordNullValue();
        sink.write(mockRecordNullValue);
    }
"
"    @Test
    public void testNullValueIgnore() throws Exception {
        testNullValue(ElasticSearchConfig.NullValueAction.IGNORE);
    }
"
"    @Test
    public void testNullValueDelete() throws Exception {
        testNullValue(ElasticSearchConfig.NullValueAction.DELETE);
    }
"
"    @Test
    public void testGenericRecord() throws Exception {
        RecordSchemaBuilder valueSchemaBuilder = org.apache.pulsar.client.api.schema.SchemaBuilder.record(""value"");
        valueSchemaBuilder.field(""c"").type(SchemaType.STRING).optional().defaultValue(null);
        valueSchemaBuilder.field(""d"").type(SchemaType.INT32).optional().defaultValue(null);
        RecordSchemaBuilder udtSchemaBuilder = SchemaBuilder.record(""type1"");
        udtSchemaBuilder.field(""a"").type(SchemaType.STRING).optional().defaultValue(null);
        udtSchemaBuilder.field(""b"").type(SchemaType.BOOLEAN).optional().defaultValue(null);
        udtSchemaBuilder.field(""d"").type(SchemaType.DOUBLE).optional().defaultValue(null);
        udtSchemaBuilder.field(""f"").type(SchemaType.FLOAT).optional().defaultValue(null);
        udtSchemaBuilder.field(""i"").type(SchemaType.INT32).optional().defaultValue(null);
        udtSchemaBuilder.field(""l"").type(SchemaType.INT64).optional().defaultValue(null);
        GenericSchema<GenericRecord> udtGenericSchema = Schema.generic(udtSchemaBuilder.build(schemaType));
        valueSchemaBuilder.field(""e"", udtGenericSchema).type(schemaType).optional().defaultValue(null);
        GenericSchema<GenericRecord> valueSchema = Schema.generic(valueSchemaBuilder.build(schemaType));

        GenericRecord valueGenericRecord = valueSchema.newRecordBuilder()
                .set(""c"", ""1"")
                .set(""d"", 1)
                .set(""e"", udtGenericSchema.newRecordBuilder()
                        .set(""a"", ""a"")
                        .set(""b"", true)
                        .set(""d"", 1.0)
                        .set(""f"", 1.0f)
                        .set(""i"", 1)
                        .set(""l"", 10L)
                        .build())
                .build();

        Record<GenericObject> genericObjectRecord = new Record<GenericObject>() {
            @Override
            public Optional<String> getTopicName() {
                return Optional.of(""data-ks1.table1"");
            }
"
"    @Test
    public void testKeyValueGenericRecord() throws Exception {
        RecordSchemaBuilder keySchemaBuilder = org.apache.pulsar.client.api.schema.SchemaBuilder.record(""key"");
        keySchemaBuilder.field(""a"").type(SchemaType.STRING).optional().defaultValue(null);
        keySchemaBuilder.field(""b"").type(SchemaType.INT32).optional().defaultValue(null);
        GenericSchema<GenericRecord> keySchema = Schema.generic(keySchemaBuilder.build(schemaType));
        GenericRecord keyGenericRecord = keySchema.newRecordBuilder()
                .set(""a"", ""1"")
                .set(""b"", 1)
                .build();

        RecordSchemaBuilder valueSchemaBuilder = org.apache.pulsar.client.api.schema.SchemaBuilder.record(""value"");
        valueSchemaBuilder.field(""c"").type(SchemaType.STRING).optional().defaultValue(null);
        valueSchemaBuilder.field(""d"").type(SchemaType.INT32).optional().defaultValue(null);
        RecordSchemaBuilder udtSchemaBuilder = SchemaBuilder.record(""type1"");
        udtSchemaBuilder.field(""a"").type(SchemaType.STRING).optional().defaultValue(null);
        udtSchemaBuilder.field(""b"").type(SchemaType.BOOLEAN).optional().defaultValue(null);
        udtSchemaBuilder.field(""d"").type(SchemaType.DOUBLE).optional().defaultValue(null);
        udtSchemaBuilder.field(""f"").type(SchemaType.FLOAT).optional().defaultValue(null);
        udtSchemaBuilder.field(""i"").type(SchemaType.INT32).optional().defaultValue(null);
        udtSchemaBuilder.field(""l"").type(SchemaType.INT64).optional().defaultValue(null);
        GenericSchema<GenericRecord> udtGenericSchema = Schema.generic(udtSchemaBuilder.build(schemaType));
        valueSchemaBuilder.field(""e"", udtGenericSchema).type(schemaType).optional().defaultValue(null);
        GenericSchema<GenericRecord> valueSchema = Schema.generic(valueSchemaBuilder.build(schemaType));

        GenericRecord valueGenericRecord = valueSchema.newRecordBuilder()
                .set(""c"", ""1"")
                .set(""d"", 1)
                .set(""e"", udtGenericSchema.newRecordBuilder()
                        .set(""a"", ""a"")
                        .set(""b"", true)
                        .set(""d"", 1.0)
                        .set(""f"", 1.0f)
                        .set(""i"", 1)
                        .set(""l"", 10L)
                        .build())
                .build();

        Schema<KeyValue<GenericRecord, GenericRecord>> keyValueSchema = Schema.KeyValue(keySchema, valueSchema, KeyValueEncodingType.INLINE);
        KeyValue<GenericRecord, GenericRecord> keyValue = new KeyValue<>(keyGenericRecord, valueGenericRecord);
        GenericObject genericObject = new GenericObject() {
            @Override
            public SchemaType getSchemaType() {
                return SchemaType.KEY_VALUE;
            }
"
"    @Test
    public void testSslBasic() throws IOException {
        try(ElasticsearchContainer container = new ElasticsearchContainer(ELASTICSEARCH_IMAGE)
                .withCreateContainerCmdModifier(c -> c.withName(""elasticsearch""))
                .withFileSystemBind(sslResourceDir, configDir + ""/ssl"")
                .withEnv(""ELASTIC_PASSWORD"",""elastic"")  // boostrap password
                .withEnv(""xpack.license.self_generated.type"", ""trial"")
                .withEnv(""xpack.security.enabled"", ""true"")
                .withEnv(""xpack.security.http.ssl.enabled"", ""true"")
                .withEnv(""xpack.security.http.ssl.client_authentication"", ""optional"")
                .withEnv(""xpack.security.http.ssl.key"", configDir + ""/ssl/elasticsearch.key"")
                .withEnv(""xpack.security.http.ssl.certificate"", configDir + ""/ssl/elasticsearch.crt"")
                .withEnv(""xpack.security.http.ssl.certificate_authorities"", configDir + ""/ssl/cacert.crt"")
                .withEnv(""xpack.security.transport.ssl.enabled"", ""true"")
                .withEnv(""xpack.security.transport.ssl.verification_mode"", ""certificate"")
                .withEnv(""xpack.security.transport.ssl.key"", configDir + ""/ssl/elasticsearch.key"")
                .withEnv(""xpack.security.transport.ssl.certificate"", configDir + ""/ssl/elasticsearch.crt"")
                .withEnv(""xpack.security.transport.ssl.certificate_authorities"", configDir + ""/ssl/cacert.crt"")
                .waitingFor(Wait.forLogMessage("".*(Security is enabled|Active license).*"", 1)
                        .withStartupTimeout(Duration.ofMinutes(2)))) {
            container.start();

            ElasticSearchConfig config = new ElasticSearchConfig()
                    .setElasticSearchUrl(""https://"" + container.getHttpHostAddress())
                    .setIndexName(INDEX)
                    .setUsername(""elastic"")
                    .setPassword(""elastic"")
                    .setSsl(new ElasticSearchSslConfig()
                            .setEnabled(true)
                            .setTruststorePath(sslResourceDir + ""/truststore.jks"")
                            .setTruststorePassword(""changeit""));
            ElasticSearchClient client = new ElasticSearchClient(config);
            testIndexExists(client);
        }
    }
"
"    @Test
    public void testSslWithHostnameVerification() throws IOException {
        try(ElasticsearchContainer container = new ElasticsearchContainer(ELASTICSEARCH_IMAGE)
                .withCreateContainerCmdModifier(c -> c.withName(""elasticsearch""))
                .withFileSystemBind(sslResourceDir, configDir + ""/ssl"")
                .withEnv(""ELASTIC_PASSWORD"",""elastic"")  // boostrap password
                .withEnv(""xpack.license.self_generated.type"", ""trial"")
                .withEnv(""xpack.security.enabled"", ""true"")
                .withEnv(""xpack.security.http.ssl.enabled"", ""true"")
                .withEnv(""xpack.security.http.ssl.supported_protocols"", ""TLSv1.2,TLSv1.1"")
                .withEnv(""xpack.security.http.ssl.client_authentication"", ""optional"")
                .withEnv(""xpack.security.http.ssl.key"", configDir + ""/ssl/elasticsearch.key"")
                .withEnv(""xpack.security.http.ssl.certificate"", configDir + ""/ssl/elasticsearch.crt"")
                .withEnv(""xpack.security.http.ssl.certificate_authorities"", configDir + ""/ssl/cacert.crt"")
                .withEnv(""xpack.security.transport.ssl.enabled"", ""true"")
                .withEnv(""xpack.security.transport.ssl.verification_mode"", ""full"")
                .withEnv(""xpack.security.transport.ssl.key"", configDir + ""/ssl/elasticsearch.key"")
                .withEnv(""xpack.security.transport.ssl.certificate"", configDir + ""/ssl/elasticsearch.crt"")
                .withEnv(""xpack.security.transport.ssl.certificate_authorities"", configDir + ""/ssl/cacert.crt"")
                .waitingFor(Wait.forLogMessage("".*(Security is enabled|Active license).*"", 1)
                        .withStartupTimeout(Duration.ofMinutes(2)))) {
            container.start();

            ElasticSearchConfig config = new ElasticSearchConfig()
                    .setElasticSearchUrl(""https://"" + container.getHttpHostAddress())
                    .setIndexName(INDEX)
                    .setUsername(""elastic"")
                    .setPassword(""elastic"")
                    .setSsl(new ElasticSearchSslConfig()
                            .setEnabled(true)
                            .setProtocols(""TLSv1.2"")
                            .setHostnameVerification(true)
                            .setTruststorePath(sslResourceDir + ""/truststore.jks"")
                            .setTruststorePassword(""changeit""));
            ElasticSearchClient client = new ElasticSearchClient(config);
            testIndexExists(client);
        }
    }
"
"    @Test
    public void testSslWithClientAuth() throws IOException {
        try(ElasticsearchContainer container = new ElasticsearchContainer(ELASTICSEARCH_IMAGE)
                .withCreateContainerCmdModifier(c -> c.withName(""elasticsearch""))
                .withFileSystemBind(sslResourceDir, configDir + ""/ssl"")
                .withEnv(""ELASTIC_PASSWORD"",""elastic"")  // boostrap password
                .withEnv(""xpack.license.self_generated.type"", ""trial"")
                .withEnv(""xpack.security.enabled"", ""true"")
                .withEnv(""xpack.security.http.ssl.enabled"", ""true"")
                .withEnv(""xpack.security.http.ssl.client_authentication"", ""required"")
                .withEnv(""xpack.security.http.ssl.key"", configDir + ""/ssl/elasticsearch.key"")
                .withEnv(""xpack.security.http.ssl.certificate"", configDir + ""/ssl/elasticsearch.crt"")
                .withEnv(""xpack.security.http.ssl.certificate_authorities"", configDir + ""/ssl/cacert.crt"")
                .withEnv(""xpack.security.transport.ssl.enabled"", ""true"")
                .withEnv(""xpack.security.transport.ssl.verification_mode"", ""full"")
                .withEnv(""xpack.security.transport.ssl.key"", configDir + ""/ssl/elasticsearch.key"")
                .withEnv(""xpack.security.transport.ssl.certificate"", configDir + ""/ssl/elasticsearch.crt"")
                .withEnv(""xpack.security.transport.ssl.certificate_authorities"", configDir + ""/ssl/cacert.crt"")
                .waitingFor(Wait.forLogMessage("".*(Security is enabled|Active license).*"", 1)
                        .withStartupTimeout(Duration.ofMinutes(3)))) {
            container.start();

            ElasticSearchConfig config = new ElasticSearchConfig()
                    .setElasticSearchUrl(""https://"" + container.getHttpHostAddress())
                    .setIndexName(INDEX)
                    .setUsername(""elastic"")
                    .setPassword(""elastic"")
                    .setSsl(new ElasticSearchSslConfig()
                            .setEnabled(true)
                            .setHostnameVerification(true)
                            .setTruststorePath(sslResourceDir + ""/truststore.jks"")
                            .setTruststorePassword(""changeit"")
                            .setKeystorePath(sslResourceDir + ""/keystore.jks"")
                            .setKeystorePassword(""changeit""));
            ElasticSearchClient client = new ElasticSearchClient(config);
            testIndexExists(client);
        }
    }
"
"    @Test
    public void testOpenAndRead() throws Exception {
        kafkaConnectSource = new KafkaConnectSource();
        kafkaConnectSource.open(config, context);

        // use FileStreamSourceConnector, each line is a record, need ""\n"" and end of each record.
        OutputStream os = Files.newOutputStream(tempFile.toPath());

        String line1 = ""This is the first line\n"";
        os.write(line1.getBytes());
        os.flush();
        log.info(""write 2 lines."");

        String line2 = ""This is the second line\n"";
        os.write(line2.getBytes());
        os.flush();

        log.info(""finish write, will read 2 lines"");

        // Note: FileStreamSourceTask read the whole line as Value, and set Key as null.
        Record<KeyValue<byte[], byte[]>> record = kafkaConnectSource.read();
        String readBack1 = new String(record.getValue().getValue());
        assertTrue(line1.contains(readBack1));
        assertNull(record.getValue().getKey());
        log.info(""read line1: {}"", readBack1);
        record.ack();

        record = kafkaConnectSource.read();
        String readBack2 = new String(record.getValue().getValue());
        assertTrue(line2.contains(readBack2));
        assertNull(record.getValue().getKey());
        assertTrue(record.getPartitionId().isPresent());
        assertFalse(record.getPartitionIndex().isPresent());
        log.info(""read line2: {}"", readBack2);
        record.ack();

        String line3 = ""This is the 3rd line\n"";
        os.write(line3.getBytes());
        os.flush();

        try {
            kafkaConnectSource.read();
            fail(""expected exception"");
        } catch (Exception e) {
            log.info(""got exception"", e);
            assertTrue(e.getCause().getCause() instanceof org.apache.kafka.connect.errors.ConnectException);
        }
    }
"
"    @Test
    public void smokeTest() throws Exception {
        KafkaConnectSink sink = new KafkaConnectSink();
        sink.open(props, context);

        final GenericRecord rec = getGenericRecord(""value"", Schema.STRING);
        Message msg = mock(MessageImpl.class);
        when(msg.getValue()).thenReturn(rec);
        when(msg.getMessageId()).thenReturn(new MessageIdImpl(1, 0, 0));

        final AtomicInteger status = new AtomicInteger(0);
        Record<GenericObject> record = PulsarRecord.<String>builder()
                .topicName(""fake-topic"")
                .message(msg)
                .ackFunction(status::incrementAndGet)
                .failFunction(status::decrementAndGet)
                .schema(Schema.STRING)
                .build();

        sink.write(record);
        sink.flush();

        assertEquals(status.get(), 1);

        sink.close();

        List<String> lines = Files.readAllLines(file, StandardCharsets.US_ASCII);
        assertEquals(lines.get(0), ""value"");
    }
"
"    @Test
    public void seekPauseResumeTest() throws Exception {
        KafkaConnectSink sink = new KafkaConnectSink();
        sink.open(props, context);

        final GenericRecord rec = getGenericRecord(""value"", Schema.STRING);
        Message msg = mock(MessageImpl.class);
        when(msg.getValue()).thenReturn(rec);
        final MessageId msgId = new MessageIdImpl(10, 10, 0);
        when(msg.getMessageId()).thenReturn(msgId);

        final AtomicInteger status = new AtomicInteger(0);
        Record<GenericObject> record = PulsarRecord.<String>builder()
                .topicName(""fake-topic"")
                .message(msg)
                .ackFunction(status::incrementAndGet)
                .failFunction(status::decrementAndGet)
                .schema(Schema.STRING)
                .build();

        sink.write(record);
        sink.flush();

        assertEquals(status.get(), 1);

        final TopicPartition tp = new TopicPartition(""fake-topic"", 0);
        assertNotEquals(MessageIdUtils.getOffset(msgId), 0);
        assertEquals(sink.currentOffset(tp.topic(), tp.partition()), MessageIdUtils.getOffset(msgId));

        sink.taskContext.offset(tp, 0);
        verify(context, times(1)).seek(Mockito.anyString(), Mockito.anyInt(), any());
        assertEquals(sink.currentOffset(tp.topic(), tp.partition()), 0);

        sink.taskContext.pause(tp);
        verify(context, times(1)).pause(tp.topic(), tp.partition());
        sink.taskContext.resume(tp);
        verify(context, times(1)).resume(tp.topic(), tp.partition());

        sink.close();
    }
"
"    @Test
    public void subscriptionTypeTest() throws Exception {
        try (KafkaConnectSink sink = new KafkaConnectSink()) {
            log.info(""Failover is allowed"");
            sink.open(props, context);
        }

        when(context.getSubscriptionType()).thenReturn(SubscriptionType.Exclusive);
        try (KafkaConnectSink sink = new KafkaConnectSink()) {
            log.info(""Exclusive is allowed"");
            sink.open(props, context);
        }

        when(context.getSubscriptionType()).thenReturn(SubscriptionType.Key_Shared);
        try (KafkaConnectSink sink = new KafkaConnectSink()) {
            log.info(""Key_Shared is not allowed"");
            sink.open(props, context);
            fail(""expected exception"");
        } catch (IllegalArgumentException iae) {
            // pass
        }

        when(context.getSubscriptionType()).thenReturn(SubscriptionType.Shared);
        try (KafkaConnectSink sink = new KafkaConnectSink()) {
            log.info(""Shared is not allowed"");
            sink.open(props, context);
            fail(""expected exception"");
        } catch (IllegalArgumentException iae) {
            // pass
        }

        when(context.getSubscriptionType()).thenReturn(null);
        try (KafkaConnectSink sink = new KafkaConnectSink()) {
            log.info(""Type is required"");
            sink.open(props, context);
            fail(""expected exception"");
        } catch (IllegalArgumentException iae) {
            // pass
        }

    }
"
"    @Test
    public void testDefaults()
    {
        assertRecordedDefaults(recordDefaults(DevelopmentLoaderConfig.class)
                .setPlugins("""")
                .setMavenLocalRepository(ArtifactResolver.USER_LOCAL_REPO)
                .setMavenRemoteRepository(ArtifactResolver.MAVEN_CENTRAL_URI));
    }
"
"    @Test
    public void testExplicitPropertyMappings()
    {
        Map<String, String> properties = new ImmutableMap.Builder<String, String>()
                .put(""plugin.bundles"", ""a,b,c"")
                .put(""maven.repo.local"", ""local-repo"")
                .put(""maven.repo.remote"", ""remote-a,remote-b"")
                .buildOrThrow();

        DevelopmentLoaderConfig expected = new DevelopmentLoaderConfig()
                .setPlugins(ImmutableList.of(""a"", ""b"", ""c""))
                .setMavenLocalRepository(""local-repo"")
                .setMavenRemoteRepository(ImmutableList.of(""remote-a"", ""remote-b""));

        assertFullMapping(properties, expected);
    }
"
"    @Test(timeOut = 30_000)
    public void testJoinWithEmptyBuildSide()
    {
        @Language(""SQL"") String selectQuery = ""SELECT * FROM partitioned_lineitem JOIN supplier ON partitioned_lineitem.suppkey = supplier.suppkey AND supplier.name = 'abc'"";
        ResultWithQueryId<MaterializedResult> result = getDistributedQueryRunner().executeWithQueryId(
                getSession(),
                selectQuery);
        MaterializedResult expected = computeActual(withDynamicFilteringDisabled(), selectQuery);
        assertEqualsIgnoreOrder(result.getResult(), expected);

        OperatorStats probeStats = searchScanFilterAndProjectOperatorStats(result.getQueryId(), getQualifiedTableName(PARTITIONED_LINEITEM));
        assertEquals(probeStats.getInputPositions(), 0L);

        DynamicFiltersStats dynamicFiltersStats = getDynamicFilteringStats(result.getQueryId());
        assertEquals(dynamicFiltersStats.getTotalDynamicFilters(), 1L);
        assertEquals(dynamicFiltersStats.getLazyDynamicFilters(), 1L);
        assertEquals(dynamicFiltersStats.getReplicatedDynamicFilters(), 0L);
        assertEquals(dynamicFiltersStats.getDynamicFiltersCompleted(), 1L);

        DynamicFilterDomainStats domainStats = getOnlyElement(dynamicFiltersStats.getDynamicFilterDomainStats());
        assertEquals(domainStats.getSimplifiedDomain(), none(BIGINT).toString(getSession().toConnectorSession()));
        assertTrue(domainStats.getCollectionDuration().isPresent());
    }
"
"    @Test(timeOut = 30_000)
    public void testJoinWithSelectiveBuildSide()
    {
        @Language(""SQL"") String selectQuery = ""SELECT * FROM partitioned_lineitem JOIN supplier ON partitioned_lineitem.suppkey = supplier.suppkey "" +
                ""AND supplier.name = 'Supplier#000000001'"";
        ResultWithQueryId<MaterializedResult> result = getDistributedQueryRunner().executeWithQueryId(
                getSession(),
                selectQuery);
        MaterializedResult expected = computeActual(withDynamicFilteringDisabled(), selectQuery);
        assertEqualsIgnoreOrder(result.getResult(), expected);

        OperatorStats probeStats = searchScanFilterAndProjectOperatorStats(result.getQueryId(), getQualifiedTableName(PARTITIONED_LINEITEM));
        // Probe-side is partially scanned
        assertEquals(probeStats.getInputPositions(), 615L);

        DynamicFiltersStats dynamicFiltersStats = getDynamicFilteringStats(result.getQueryId());
        assertEquals(dynamicFiltersStats.getTotalDynamicFilters(), 1L);
        assertEquals(dynamicFiltersStats.getLazyDynamicFilters(), 1L);
        assertEquals(dynamicFiltersStats.getReplicatedDynamicFilters(), 0L);
        assertEquals(dynamicFiltersStats.getDynamicFiltersCompleted(), 1L);

        DynamicFilterDomainStats domainStats = getOnlyElement(dynamicFiltersStats.getDynamicFilterDomainStats());
        assertEquals(domainStats.getSimplifiedDomain(), singleValue(BIGINT, 1L).toString(getSession().toConnectorSession()));
    }
"
"    @Test(timeOut = 30_000)
    public void testJoinWithNonSelectiveBuildSide()
    {
        @Language(""SQL"") String selectQuery = ""SELECT * FROM partitioned_lineitem JOIN supplier ON partitioned_lineitem.suppkey = supplier.suppkey"";
        ResultWithQueryId<MaterializedResult> result = getDistributedQueryRunner().executeWithQueryId(
                getSession(),
                selectQuery);
        MaterializedResult expected = computeActual(withDynamicFilteringDisabled(), selectQuery);
        assertEqualsIgnoreOrder(result.getResult(), expected);

        OperatorStats probeStats = searchScanFilterAndProjectOperatorStats(result.getQueryId(), getQualifiedTableName(PARTITIONED_LINEITEM));
        // Probe-side is fully scanned
        assertEquals(probeStats.getInputPositions(), LINEITEM_COUNT);

        DynamicFiltersStats dynamicFiltersStats = getDynamicFilteringStats(result.getQueryId());
        assertEquals(dynamicFiltersStats.getTotalDynamicFilters(), 1L);
        assertEquals(dynamicFiltersStats.getLazyDynamicFilters(), 1L);
        assertEquals(dynamicFiltersStats.getReplicatedDynamicFilters(), 0L);
        assertEquals(dynamicFiltersStats.getDynamicFiltersCompleted(), 1L);

        DynamicFilterDomainStats domainStats = getOnlyElement(dynamicFiltersStats.getDynamicFilterDomainStats());
        assertThat(domainStats.getSimplifiedDomain())
                .isEqualTo(getSimplifiedDomainString(1L, 100L, 100, BIGINT));
    }
"
"    @Test(timeOut = 30_000)
    public void testJoinLargeBuildSideRangeDynamicFiltering()
    {
        @Language(""SQL"") String selectQuery = ""SELECT * FROM partitioned_lineitem JOIN orders ON partitioned_lineitem.orderkey = orders.orderkey"";
        ResultWithQueryId<MaterializedResult> result = getDistributedQueryRunner().executeWithQueryId(
                getSession(),
                selectQuery);
        MaterializedResult expected = computeActual(withDynamicFilteringDisabled(), selectQuery);
        assertEqualsIgnoreOrder(result.getResult(), expected);

        OperatorStats probeStats = searchScanFilterAndProjectOperatorStats(result.getQueryId(), getQualifiedTableName(PARTITIONED_LINEITEM));
        // Probe-side is fully scanned because the build-side is too large for dynamic filtering
        assertEquals(probeStats.getInputPositions(), LINEITEM_COUNT);

        DynamicFiltersStats dynamicFiltersStats = getDynamicFilteringStats(result.getQueryId());
        assertEquals(dynamicFiltersStats.getTotalDynamicFilters(), 1L);
        assertEquals(dynamicFiltersStats.getLazyDynamicFilters(), 1L);
        assertEquals(dynamicFiltersStats.getReplicatedDynamicFilters(), 0L);
        assertEquals(dynamicFiltersStats.getDynamicFiltersCompleted(), 1L);

        DynamicFilterDomainStats domainStats = getOnlyElement(dynamicFiltersStats.getDynamicFilterDomainStats());
        assertEquals(
                domainStats.getSimplifiedDomain(),
                Domain.create(ValueSet.ofRanges(range(BIGINT, 1L, true, 60000L, true)), false)
                        .toString(getSession().toConnectorSession()));
    }
"
"    @Test(timeOut = 30_000)
    public void testJoinWithMultipleDynamicFiltersOnProbe()
    {
        // supplier names Supplier#000000001 and Supplier#000000002 match suppkey 1 and 2
        @Language(""SQL"") String selectQuery = ""SELECT * FROM ("" +
                ""SELECT supplier.suppkey FROM "" +
                ""partitioned_lineitem JOIN tpch.tiny.supplier ON partitioned_lineitem.suppkey = supplier.suppkey AND supplier.name IN ('Supplier#000000001', 'Supplier#000000002')"" +
                "") t JOIN supplier ON t.suppkey = supplier.suppkey AND supplier.suppkey IN (2, 3)"";
        ResultWithQueryId<MaterializedResult> result = getDistributedQueryRunner().executeWithQueryId(
                getSession(),
                selectQuery);
        MaterializedResult expected = computeActual(withDynamicFilteringDisabled(), selectQuery);
        assertEqualsIgnoreOrder(result.getResult(), expected);

        OperatorStats probeStats = searchScanFilterAndProjectOperatorStats(result.getQueryId(), getQualifiedTableName(PARTITIONED_LINEITEM));
        // Probe-side is partially scanned
        assertEquals(probeStats.getInputPositions(), 558L);

        DynamicFiltersStats dynamicFiltersStats = getDynamicFilteringStats(result.getQueryId());
        assertEquals(dynamicFiltersStats.getTotalDynamicFilters(), 2L);
        assertEquals(dynamicFiltersStats.getLazyDynamicFilters(), 2L);
        assertEquals(dynamicFiltersStats.getReplicatedDynamicFilters(), 0L);
        assertEquals(dynamicFiltersStats.getDynamicFiltersCompleted(), 2);

        List<DynamicFilterDomainStats> domainStats = dynamicFiltersStats.getDynamicFilterDomainStats();
        assertThat(domainStats).map(DynamicFilterDomainStats::getSimplifiedDomain)
                .containsExactlyInAnyOrder(
                        getSimplifiedDomainString(2L, 3L, 2, BIGINT),
                        getSimplifiedDomainString(2L, 2L, 1, BIGINT));
    }
"
"    @Test(timeOut = 30_000)
    public void testJoinWithImplicitCoercion()
    {
        // setup partitioned fact table with integer suppkey
        createLineitemTable(""partitioned_lineitem_int"", ImmutableList.of(""orderkey"", ""CAST(suppkey as int) suppkey_int""), ImmutableList.of(""suppkey_int""));
        assertQuery(
                ""SELECT count(*) FROM partitioned_lineitem_int"",
                ""VALUES "" + LINEITEM_COUNT);

        @Language(""SQL"") String selectQuery = ""SELECT * FROM partitioned_lineitem_int l JOIN supplier s ON l.suppkey_int = s.suppkey AND s.name = 'Supplier#000000001'"";
        ResultWithQueryId<MaterializedResult> result = getDistributedQueryRunner().executeWithQueryId(
                getSession(),
                selectQuery);
        MaterializedResult expected = computeActual(withDynamicFilteringDisabled(), selectQuery);
        assertEqualsIgnoreOrder(result.getResult(), expected);

        OperatorStats probeStats = searchScanFilterAndProjectOperatorStats(result.getQueryId(), getQualifiedTableName(""partitioned_lineitem_int""));
        // Probe-side is partially scanned
        assertEquals(probeStats.getInputPositions(), 615L);

        DynamicFiltersStats dynamicFiltersStats = getDynamicFilteringStats(result.getQueryId());
        assertEquals(dynamicFiltersStats.getTotalDynamicFilters(), 1L);
        assertEquals(dynamicFiltersStats.getLazyDynamicFilters(), 1L);
        assertEquals(dynamicFiltersStats.getReplicatedDynamicFilters(), 0L);
        assertEquals(dynamicFiltersStats.getDynamicFiltersCompleted(), 1L);
        DynamicFilterDomainStats domainStats = getOnlyElement(dynamicFiltersStats.getDynamicFilterDomainStats());
        assertEquals(domainStats.getSimplifiedDomain(), singleValue(BIGINT, 1L).toString(getSession().toConnectorSession()));
    }
"
"    @Test(timeOut = 30_000)
    public void testSemiJoinWithEmptyBuildSide()
    {
        @Language(""SQL"") String selectQuery = ""SELECT * FROM partitioned_lineitem WHERE suppkey IN (SELECT suppkey FROM supplier WHERE name = 'abc')"";
        ResultWithQueryId<MaterializedResult> result = getDistributedQueryRunner().executeWithQueryId(
                getSession(),
                selectQuery);
        MaterializedResult expected = computeActual(withDynamicFilteringDisabled(), selectQuery);
        assertEqualsIgnoreOrder(result.getResult(), expected);

        OperatorStats probeStats = searchScanFilterAndProjectOperatorStats(result.getQueryId(), getQualifiedTableName(PARTITIONED_LINEITEM));
        assertEquals(probeStats.getInputPositions(), 0L);

        DynamicFiltersStats dynamicFiltersStats = getDynamicFilteringStats(result.getQueryId());
        assertEquals(dynamicFiltersStats.getTotalDynamicFilters(), 1L);
        assertEquals(dynamicFiltersStats.getLazyDynamicFilters(), 1L);
        assertEquals(dynamicFiltersStats.getReplicatedDynamicFilters(), 0L);
        assertEquals(dynamicFiltersStats.getDynamicFiltersCompleted(), 1L);

        DynamicFilterDomainStats domainStats = getOnlyElement(dynamicFiltersStats.getDynamicFilterDomainStats());
        assertEquals(domainStats.getSimplifiedDomain(), none(BIGINT).toString(getSession().toConnectorSession()));
    }
"
"    @Test(timeOut = 30_000)
    public void testSemiJoinWithSelectiveBuildSide()
    {
        @Language(""SQL"") String selectQuery = ""SELECT * FROM partitioned_lineitem WHERE suppkey IN (SELECT suppkey FROM supplier WHERE name = 'Supplier#000000001')"";
        ResultWithQueryId<MaterializedResult> result = getDistributedQueryRunner().executeWithQueryId(
                getSession(),
                selectQuery);
        MaterializedResult expected = computeActual(withDynamicFilteringDisabled(), selectQuery);
        assertEqualsIgnoreOrder(result.getResult(), expected);

        OperatorStats probeStats = searchScanFilterAndProjectOperatorStats(result.getQueryId(), getQualifiedTableName(PARTITIONED_LINEITEM));
        // Probe-side is partially scanned
        assertEquals(probeStats.getInputPositions(), 615L);

        DynamicFiltersStats dynamicFiltersStats = getDynamicFilteringStats(result.getQueryId());
        assertEquals(dynamicFiltersStats.getTotalDynamicFilters(), 1L);
        assertEquals(dynamicFiltersStats.getLazyDynamicFilters(), 1L);
        assertEquals(dynamicFiltersStats.getReplicatedDynamicFilters(), 0L);
        assertEquals(dynamicFiltersStats.getDynamicFiltersCompleted(), 1L);

        DynamicFilterDomainStats domainStats = getOnlyElement(dynamicFiltersStats.getDynamicFilterDomainStats());
        assertEquals(domainStats.getSimplifiedDomain(), singleValue(BIGINT, 1L).toString(getSession().toConnectorSession()));
    }
"
"    @Test(timeOut = 30_000)
    public void testSemiJoinWithNonSelectiveBuildSide()
    {
        @Language(""SQL"") String selectQuery = ""SELECT * FROM partitioned_lineitem WHERE suppkey IN (SELECT suppkey FROM supplier)"";
        ResultWithQueryId<MaterializedResult> result = getDistributedQueryRunner().executeWithQueryId(
                getSession(),
                selectQuery);
        MaterializedResult expected = computeActual(withDynamicFilteringDisabled(), selectQuery);
        assertEqualsIgnoreOrder(result.getResult(), expected);

        OperatorStats probeStats = searchScanFilterAndProjectOperatorStats(result.getQueryId(), getQualifiedTableName(PARTITIONED_LINEITEM));
        // Probe-side is fully scanned
        assertEquals(probeStats.getInputPositions(), LINEITEM_COUNT);

        DynamicFiltersStats dynamicFiltersStats = getDynamicFilteringStats(result.getQueryId());
        assertEquals(dynamicFiltersStats.getTotalDynamicFilters(), 1L);
        assertEquals(dynamicFiltersStats.getLazyDynamicFilters(), 1L);
        assertEquals(dynamicFiltersStats.getReplicatedDynamicFilters(), 0L);
        assertEquals(dynamicFiltersStats.getDynamicFiltersCompleted(), 1L);

        DynamicFilterDomainStats domainStats = getOnlyElement(dynamicFiltersStats.getDynamicFilterDomainStats());
        assertThat(domainStats.getSimplifiedDomain())
                .isEqualTo(getSimplifiedDomainString(1L, 100L, 100, BIGINT));
    }
"
"    @Test(timeOut = 30_000)
    public void testSemiJoinLargeBuildSideRangeDynamicFiltering()
    {
        @Language(""SQL"") String selectQuery = ""SELECT * FROM partitioned_lineitem WHERE orderkey IN (SELECT orderkey FROM orders)"";
        ResultWithQueryId<MaterializedResult> result = getDistributedQueryRunner().executeWithQueryId(
                getSession(),
                selectQuery);
        MaterializedResult expected = computeActual(withDynamicFilteringDisabled(), selectQuery);
        assertEqualsIgnoreOrder(result.getResult(), expected);

        OperatorStats probeStats = searchScanFilterAndProjectOperatorStats(result.getQueryId(), getQualifiedTableName(PARTITIONED_LINEITEM));
        // Probe-side is fully scanned because the build-side is too large for dynamic filtering
        assertEquals(probeStats.getInputPositions(), LINEITEM_COUNT);

        DynamicFiltersStats dynamicFiltersStats = getDynamicFilteringStats(result.getQueryId());
        assertEquals(dynamicFiltersStats.getTotalDynamicFilters(), 1L);
        assertEquals(dynamicFiltersStats.getLazyDynamicFilters(), 1L);
        assertEquals(dynamicFiltersStats.getReplicatedDynamicFilters(), 0L);
        assertEquals(dynamicFiltersStats.getDynamicFiltersCompleted(), 1L);

        DynamicFilterDomainStats domainStats = getOnlyElement(dynamicFiltersStats.getDynamicFilterDomainStats());
        assertEquals(
                domainStats.getSimplifiedDomain(),
                Domain.create(ValueSet.ofRanges(range(BIGINT, 1L, true, 60000L, true)), false)
                        .toString(getSession().toConnectorSession()));
    }
"
"    @Test(timeOut = 30_000)
    public void testRightJoinWithEmptyBuildSide()
    {
        @Language(""SQL"") String selectQuery = ""SELECT * FROM partitioned_lineitem l RIGHT JOIN supplier s ON l.suppkey = s.suppkey WHERE name = 'abc'"";
        ResultWithQueryId<MaterializedResult> result = getDistributedQueryRunner().executeWithQueryId(
                getSession(),
                selectQuery);
        MaterializedResult expected = computeActual(withDynamicFilteringDisabled(), selectQuery);
        assertEqualsIgnoreOrder(result.getResult(), expected);

        OperatorStats probeStats = searchScanFilterAndProjectOperatorStats(result.getQueryId(), getQualifiedTableName(PARTITIONED_LINEITEM));
        assertEquals(probeStats.getInputPositions(), 0L);

        DynamicFiltersStats dynamicFiltersStats = getDynamicFilteringStats(result.getQueryId());
        assertEquals(dynamicFiltersStats.getTotalDynamicFilters(), 1L);
        assertEquals(dynamicFiltersStats.getLazyDynamicFilters(), 1L);
        assertEquals(dynamicFiltersStats.getReplicatedDynamicFilters(), 0L);
        assertEquals(dynamicFiltersStats.getDynamicFiltersCompleted(), 1L);

        DynamicFilterDomainStats domainStats = getOnlyElement(dynamicFiltersStats.getDynamicFilterDomainStats());
        assertEquals(domainStats.getSimplifiedDomain(), none(BIGINT).toString(getSession().toConnectorSession()));
    }
"
"    @Test(timeOut = 30_000)
    public void testRightJoinWithSelectiveBuildSide()
    {
        @Language(""SQL"") String selectQuery = ""SELECT * FROM partitioned_lineitem l RIGHT JOIN supplier s ON l.suppkey = s.suppkey WHERE name = 'Supplier#000000001'"";
        ResultWithQueryId<MaterializedResult> result = getDistributedQueryRunner().executeWithQueryId(
                getSession(),
                selectQuery);
        MaterializedResult expected = computeActual(withDynamicFilteringDisabled(), selectQuery);
        assertEqualsIgnoreOrder(result.getResult(), expected);

        OperatorStats probeStats = searchScanFilterAndProjectOperatorStats(result.getQueryId(), getQualifiedTableName(PARTITIONED_LINEITEM));
        // Probe-side is partially scanned
        assertEquals(probeStats.getInputPositions(), 615L);

        DynamicFiltersStats dynamicFiltersStats = getDynamicFilteringStats(result.getQueryId());
        assertEquals(dynamicFiltersStats.getTotalDynamicFilters(), 1L);
        assertEquals(dynamicFiltersStats.getLazyDynamicFilters(), 1L);
        assertEquals(dynamicFiltersStats.getReplicatedDynamicFilters(), 0L);
        assertEquals(dynamicFiltersStats.getDynamicFiltersCompleted(), 1L);

        DynamicFilterDomainStats domainStats = getOnlyElement(dynamicFiltersStats.getDynamicFilterDomainStats());
        assertEquals(domainStats.getSimplifiedDomain(), singleValue(BIGINT, 1L).toString(getSession().toConnectorSession()));
    }
"
"    @Test(timeOut = 30_000)
    public void testRightJoinWithNonSelectiveBuildSide()
    {
        @Language(""SQL"") String selectQuery = ""SELECT * FROM partitioned_lineitem l RIGHT JOIN supplier s ON l.suppkey = s.suppkey"";
        ResultWithQueryId<MaterializedResult> result = getDistributedQueryRunner().executeWithQueryId(
                getSession(),
                selectQuery);
        MaterializedResult expected = computeActual(withDynamicFilteringDisabled(), selectQuery);
        assertEqualsIgnoreOrder(result.getResult(), expected);

        OperatorStats probeStats = searchScanFilterAndProjectOperatorStats(result.getQueryId(), getQualifiedTableName(PARTITIONED_LINEITEM));
        // Probe-side is fully scanned
        assertEquals(probeStats.getInputPositions(), LINEITEM_COUNT);

        DynamicFiltersStats dynamicFiltersStats = getDynamicFilteringStats(result.getQueryId());
        assertEquals(dynamicFiltersStats.getTotalDynamicFilters(), 1L);
        assertEquals(dynamicFiltersStats.getLazyDynamicFilters(), 1L);
        assertEquals(dynamicFiltersStats.getReplicatedDynamicFilters(), 0L);
        assertEquals(dynamicFiltersStats.getDynamicFiltersCompleted(), 1L);

        DynamicFilterDomainStats domainStats = getOnlyElement(dynamicFiltersStats.getDynamicFilterDomainStats());
        assertThat(domainStats.getSimplifiedDomain())
                .isEqualTo(getSimplifiedDomainString(1L, 100L, 100, BIGINT));
    }
"
"    @Test
    public void ensureTestNamingConvention()
    {
        // Enforce a naming convention to make code navigation easier.
        assertThat(getClass().getName())
                .endsWith(""ConnectorTest"");
    }
"
"    @Test
    public void testColumnsInReverseOrder()
    {
        assertQuery(""SELECT shippriority, clerk, totalprice FROM orders"");
    }
"
"    @Test
    public void testCharVarcharComparison()
    {
        skipTestUnless(hasBehavior(SUPPORTS_CREATE_TABLE));

        try (TestTable table = new TestTable(
                getQueryRunner()::execute,
                ""test_char_varchar"",
                ""(k, v) AS VALUES"" +
                        ""   (-1, CAST(NULL AS char(3))), "" +
                        ""   (3, CAST('   ' AS char(3))),"" +
                        ""   (6, CAST('x  ' AS char(3)))"")) {
            // varchar of length shorter than column's length
            assertQuery(
                    ""SELECT k, v FROM "" + table.getName() + "" WHERE v = CAST('  ' AS varchar(2))"",
                    // The value is included because both sides of the comparison are coerced to char(3)
                    ""VALUES (3, '   ')"");

            // varchar of length longer than column's length
            assertQuery(
                    ""SELECT k, v FROM "" + table.getName() + "" WHERE v = CAST('  ' AS varchar(4))"",
                    // The value is included because both sides of the comparison are coerced to char(4)
                    ""VALUES (3, '   ')"");

            // value that's not all-spaces
            assertQuery(
                    ""SELECT k, v FROM "" + table.getName() + "" WHERE v = CAST('x ' AS varchar(2))"",
                    // The value is included because both sides of the comparison are coerced to char(3)
                    ""VALUES (6, 'x  ')"");
        }
    }
"
"    @Test
    public void testVarcharCharComparison()
    {
        skipTestUnless(hasBehavior(SUPPORTS_CREATE_TABLE));

        try (TestTable table = new TestTable(
                getQueryRunner()::execute,
                ""test_varchar_char"",
                ""(k, v) AS VALUES"" +
                        ""   (-1, CAST(NULL AS varchar(3))), "" +
                        ""   (0, CAST('' AS varchar(3))),"" +
                        ""   (1, CAST(' ' AS varchar(3))), "" +
                        ""   (2, CAST('  ' AS varchar(3))), "" +
                        ""   (3, CAST('   ' AS varchar(3))),"" +
                        ""   (4, CAST('x' AS varchar(3))),"" +
                        ""   (5, CAST('x ' AS varchar(3))),"" +
                        ""   (6, CAST('x  ' AS varchar(3)))"")) {
            assertQuery(
                    ""SELECT k, v FROM "" + table.getName() + "" WHERE v = CAST('  ' AS char(2))"",
                    // The 3-spaces value is included because both sides of the comparison are coerced to char(3)
                    ""VALUES (0, ''), (1, ' '), (2, '  '), (3, '   ')"");

            // value that's not all-spaces
            assertQuery(
                    ""SELECT k, v FROM "" + table.getName() + "" WHERE v = CAST('x ' AS char(2))"",
                    // The 3-spaces value is included because both sides of the comparison are coerced to char(3)
                    ""VALUES (4, 'x'), (5, 'x '), (6, 'x  ')"");
        }
    }
"
"    @Test
    public void testAggregation()
    {
        assertQuery(""SELECT sum(orderkey) FROM orders"");
        assertQuery(""SELECT sum(totalprice) FROM orders"");
        assertQuery(""SELECT max(comment) FROM nation"");

        assertQuery(""SELECT count(*) FROM orders"");
        assertQuery(""SELECT count(*) FROM orders WHERE orderkey > 10"");
        assertQuery(""SELECT count(*) FROM (SELECT * FROM orders LIMIT 10)"");
        assertQuery(""SELECT count(*) FROM (SELECT * FROM orders WHERE orderkey > 10 LIMIT 10)"");

        assertQuery(""SELECT DISTINCT regionkey FROM nation"");
        assertQuery(""SELECT regionkey FROM nation GROUP BY regionkey"");

        // TODO support aggregation pushdown with GROUPING SETS
        assertQuery(
                ""SELECT regionkey, nationkey FROM nation GROUP BY GROUPING SETS ((regionkey), (nationkey))"",
                ""SELECT NULL, nationkey FROM nation "" +
                        ""UNION ALL SELECT DISTINCT regionkey, NULL FROM nation"");
        assertQuery(
                ""SELECT regionkey, nationkey, count(*) FROM nation GROUP BY GROUPING SETS ((), (regionkey), (nationkey), (regionkey, nationkey))"",
                ""SELECT NULL, NULL, count(*) FROM nation "" +
                        ""UNION ALL SELECT NULL, nationkey, 1 FROM nation "" +
                        ""UNION ALL SELECT regionkey, NULL, count(*) FROM nation GROUP BY regionkey "" +
                        ""UNION ALL SELECT regionkey, nationkey, 1 FROM nation"");

        assertQuery(""SELECT count(regionkey) FROM nation"");
        assertQuery(""SELECT count(DISTINCT regionkey) FROM nation"");
        assertQuery(""SELECT regionkey, count(*) FROM nation GROUP BY regionkey"");

        assertQuery(""SELECT min(regionkey), max(regionkey) FROM nation"");
        assertQuery(""SELECT min(DISTINCT regionkey), max(DISTINCT regionkey) FROM nation"");
        assertQuery(""SELECT regionkey, min(regionkey), min(name), max(regionkey), max(name) FROM nation GROUP BY regionkey"");

        assertQuery(""SELECT sum(regionkey) FROM nation"");
        assertQuery(""SELECT sum(DISTINCT regionkey) FROM nation"");
        assertQuery(""SELECT regionkey, sum(regionkey) FROM nation GROUP BY regionkey"");

        assertQuery(
                ""SELECT avg(nationkey) FROM nation"",
                ""SELECT avg(CAST(nationkey AS double)) FROM nation"");
        assertQuery(
                ""SELECT avg(DISTINCT nationkey) FROM nation"",
                ""SELECT avg(DISTINCT CAST(nationkey AS double)) FROM nation"");
        assertQuery(
                ""SELECT regionkey, avg(nationkey) FROM nation GROUP BY regionkey"",
                ""SELECT regionkey, avg(CAST(nationkey AS double)) FROM nation GROUP BY regionkey"");
    }
"
"    @Test
    public void testExactPredicate()
    {
        assertQueryReturnsEmptyResult(""SELECT * FROM orders WHERE orderkey = 10"");

        // filtered column is selected
        assertQuery(""SELECT custkey, orderkey FROM orders WHERE orderkey = 32"", ""VALUES (1301, 32)"");

        // filtered column is not selected
        assertQuery(""SELECT custkey FROM orders WHERE orderkey = 32"", ""VALUES (1301)"");
    }
"
"    @Test
    public void testInListPredicate()
    {
        assertQueryReturnsEmptyResult(""SELECT * FROM orders WHERE orderkey IN (10, 11, 20, 21)"");

        // filtered column is selected
        assertQuery(""SELECT custkey, orderkey FROM orders WHERE orderkey IN (7, 10, 32, 33)"", ""VALUES (392, 7), (1301, 32), (670, 33)"");

        // filtered column is not selected
        assertQuery(""SELECT custkey FROM orders WHERE orderkey IN (7, 10, 32, 33)"", ""VALUES (392), (1301), (670)"");
    }
"
"    @Test
    public void testIsNullPredicate()
    {
        assertQueryReturnsEmptyResult(""SELECT * FROM orders WHERE orderkey IS NULL"");
        assertQueryReturnsEmptyResult(""SELECT * FROM orders WHERE orderkey = 10 OR orderkey IS NULL"");

        // filtered column is selected
        assertQuery(""SELECT custkey, orderkey FROM orders WHERE orderkey = 32 OR orderkey IS NULL"", ""VALUES (1301, 32)"");

        // filtered column is not selected
        assertQuery(""SELECT custkey FROM orders WHERE orderkey = 32 OR orderkey IS NULL"", ""VALUES (1301)"");
    }
"
"    @Test
    public void testLikePredicate()
    {
        // filtered column is not selected
        assertQuery(""SELECT orderkey FROM orders WHERE orderpriority LIKE '5-L%'"");

        // filtered column is selected
        assertQuery(""SELECT orderkey, orderpriority FROM orders WHERE orderpriority LIKE '5-L%'"");

        // filtered column is not selected
        assertQuery(""SELECT orderkey FROM orders WHERE orderpriority LIKE '5-L__'"");

        // filtered column is selected
        assertQuery(""SELECT orderkey, orderpriority FROM orders WHERE orderpriority LIKE '5-L__'"");
    }
"
"    @Test
    public void testMultipleRangesPredicate()
    {
        // List columns explicitly. Some connectors do not maintain column ordering.
        assertQuery("""" +
                ""SELECT orderkey, custkey, orderstatus, totalprice, orderdate, orderpriority, clerk, shippriority, comment "" +
                ""FROM orders "" +
                ""WHERE orderkey BETWEEN 10 AND 50"");
    }
"
"    @Test
    public void testRangePredicate()
    {
        // List columns explicitly. Some connectors do not maintain column ordering.
        assertQuery("""" +
                ""SELECT orderkey, custkey, orderstatus, totalprice, orderdate, orderpriority, clerk, shippriority, comment "" +
                ""FROM orders "" +
                ""WHERE orderkey BETWEEN 10 AND 50"");
    }
"
"    @Test
    public void testPredicateReflectedInExplain()
    {
        // Even if the predicate is pushed down into the table scan, it should still be reflected in EXPLAIN (via ConnectorTableHandle.toString)
        assertExplain(
                ""EXPLAIN SELECT name FROM nation WHERE nationkey = 42"",
                ""(predicate|filterPredicate|constraint).{0,10}(nationkey|NATIONKEY)"");
    }
"
"    @Test
    public void testSortItemsReflectedInExplain()
    {
        // Even if the sort items are pushed down into the table scan, it should still be reflected in EXPLAIN (via ConnectorTableHandle.toString)
        @Language(""RegExp"") String expectedPattern = hasBehavior(SUPPORTS_TOPN_PUSHDOWN)
                ? ""sortOrder=\\[(?i:nationkey):.* DESC NULLS LAST] limit=5""
                : ""\\[5 by \\((?i:nationkey) DESC NULLS LAST\\)]"";

        assertExplain(
                ""EXPLAIN SELECT name FROM nation ORDER BY nationkey DESC NULLS LAST LIMIT 5"",
                expectedPattern);
    }
"
"    @Test
    public void testConcurrentScans()
    {
        String unionMultipleTimes = join("" UNION ALL "", nCopies(25, ""SELECT * FROM orders""));
        assertQuery(""SELECT sum(if(rand() >= 0, orderkey)) FROM ("" + unionMultipleTimes + "")"", ""VALUES 11246812500"");
    }
"
"    @Test
    public void testSelectAll()
    {
        assertQuery(""SELECT * FROM orders"");
    }
"
"    @Test(timeOut = 300_000, dataProvider = ""joinDistributionTypes"")
    public void testJoinWithEmptySides(JoinDistributionType joinDistributionType)
    {
        Session session = noJoinReordering(joinDistributionType);
        // empty build side
        assertQuery(session, ""SELECT count(*) FROM nation JOIN region ON nation.regionkey = region.regionkey AND region.name = ''"", ""VALUES 0"");
        assertQuery(session, ""SELECT count(*) FROM nation JOIN region ON nation.regionkey = region.regionkey AND region.regionkey < 0"", ""VALUES 0"");
        // empty probe side
        assertQuery(session, ""SELECT count(*) FROM region JOIN nation ON nation.regionkey = region.regionkey AND region.name = ''"", ""VALUES 0"");
        assertQuery(session, ""SELECT count(*) FROM nation JOIN region ON nation.regionkey = region.regionkey AND region.regionkey < 0"", ""VALUES 0"");
    }
"
"    @Test
    public void testJoin()
    {
        Session session = Session.builder(getSession())
                .setSystemProperty(IGNORE_STATS_CALCULATOR_FAILURES, ""false"")
                .build();

        // 2 inner joins, eligible for join reodering
        assertQuery(
                session,
                ""SELECT c.name, n.name, r.name "" +
                        ""FROM nation n "" +
                        ""JOIN customer c ON c.nationkey = n.nationkey "" +
                        ""JOIN region r ON n.regionkey = r.regionkey"");

        // 2 inner joins, eligible for join reodering, where one table has a filter
        assertQuery(
                session,
                ""SELECT c.name, n.name, r.name "" +
                        ""FROM nation n "" +
                        ""JOIN customer c ON c.nationkey = n.nationkey "" +
                        ""JOIN region r ON n.regionkey = r.regionkey "" +
                        ""WHERE n.name = 'ARGENTINA'"");

        // 2 inner joins, eligible for join reodering, on top of aggregation
        assertQuery(
                session,
                ""SELECT c.name, n.name, n.count, r.name "" +
                        ""FROM (SELECT name, regionkey, nationkey, count(*) count FROM nation GROUP BY name, regionkey, nationkey) n "" +
                        ""JOIN customer c ON c.nationkey = n.nationkey "" +
                        ""JOIN region r ON n.regionkey = r.regionkey"");
    }
"
"    @Test
    public void testDescribeTable()
    {
        MaterializedResult expectedColumns = MaterializedResult.resultBuilder(getSession(), VARCHAR, VARCHAR, VARCHAR, VARCHAR)
                .row(""orderkey"", ""bigint"", """", """")
                .row(""custkey"", ""bigint"", """", """")
                .row(""orderstatus"", ""varchar(1)"", """", """")
                .row(""totalprice"", ""double"", """", """")
                .row(""orderdate"", ""date"", """", """")
                .row(""orderpriority"", ""varchar(15)"", """", """")
                .row(""clerk"", ""varchar(15)"", """", """")
                .row(""shippriority"", ""integer"", """", """")
                .row(""comment"", ""varchar(79)"", """", """")
                .build();
        MaterializedResult actualColumns = computeActual(""DESCRIBE orders"");
        assertEquals(actualColumns, expectedColumns);
    }
"
"    @Test
    public void testMaterializedView()
    {
        if (!hasBehavior(SUPPORTS_CREATE_MATERIALIZED_VIEW)) {
            assertQueryFails(""CREATE MATERIALIZED VIEW nation_mv AS SELECT * FROM nation"", ""This connector does not support creating materialized views"");
            return;
        }

        QualifiedObjectName view = new QualifiedObjectName(
                getSession().getCatalog().orElseThrow(),
                getSession().getSchema().orElseThrow(),
                ""test_materialized_view_"" + randomTableSuffix());
        QualifiedObjectName otherView = new QualifiedObjectName(
                getSession().getCatalog().orElseThrow(),
                ""other_schema"",
                ""test_materialized_view_"" + randomTableSuffix());
        QualifiedObjectName viewWithComment = new QualifiedObjectName(
                getSession().getCatalog().orElseThrow(),
                getSession().getSchema().orElseThrow(),
                ""test_materialized_view_with_comment_"" + randomTableSuffix());

        createTestingMaterializedView(view, Optional.empty());
        createTestingMaterializedView(otherView, Optional.of(""sarcastic comment""));
        createTestingMaterializedView(viewWithComment, Optional.of(""mv_comment""));

        // verify comment
        MaterializedResult materializedRows = computeActual(""SHOW CREATE MATERIALIZED VIEW "" + viewWithComment);
        assertThat((String) materializedRows.getOnlyValue()).contains(""COMMENT 'mv_comment'"");
        assertThat(query(
                ""SELECT table_name, comment FROM system.metadata.table_comments "" +
                        ""WHERE catalog_name = '"" + view.getCatalogName() + ""' AND "" +
                        ""schema_name = '"" + view.getSchemaName() + ""'""))
                .skippingTypesCheck()
                .containsAll(""VALUES ('"" + view.getObjectName() + ""', null), ('"" + viewWithComment.getObjectName() + ""', 'mv_comment')"");

        // reading
        assertThat(query(""SELECT * FROM "" + view))
                .skippingTypesCheck()
                .matches(""SELECT * FROM nation"");
        assertThat(query(""SELECT * FROM "" + viewWithComment))
                .skippingTypesCheck()
                .matches(""SELECT * FROM nation"");

        // table listing
        assertThat(query(""SHOW TABLES""))
                .skippingTypesCheck()
                .containsAll(""VALUES '"" + view.getObjectName() + ""'"");
        // information_schema.tables without table_name filter
        assertThat(query(
                ""SELECT table_name, table_type FROM information_schema.tables "" +
                        ""WHERE table_schema = '"" + view.getSchemaName() + ""'""))
                .skippingTypesCheck()
                .containsAll(""VALUES ('"" + view.getObjectName() + ""', 'BASE TABLE')""); // TODO table_type should probably be ""* VIEW""
        // information_schema.tables with table_name filter
        assertQuery(
                ""SELECT table_name, table_type FROM information_schema.tables "" +
                        ""WHERE table_schema = '"" + view.getSchemaName() + ""' and table_name = '"" + view.getObjectName() + ""'"",
                ""VALUES ('"" + view.getObjectName() + ""', 'BASE TABLE')"");

        // system.jdbc.tables without filter
        assertThat(query(""SELECT table_schem, table_name, table_type FROM system.jdbc.tables""))
                .skippingTypesCheck()
                .containsAll(""VALUES ('"" + view.getSchemaName() + ""', '"" + view.getObjectName() + ""', 'TABLE')"");

        // system.jdbc.tables with table prefix filter
        assertQuery(
                ""SELECT table_schem, table_name, table_type "" +
                        ""FROM system.jdbc.tables "" +
                        ""WHERE table_cat = '"" + view.getCatalogName() + ""' AND "" +
                        ""table_schem = '"" + view.getSchemaName() + ""' AND "" +
                        ""table_name = '"" + view.getObjectName() + ""'"",
                ""VALUES ('"" + view.getSchemaName() + ""', '"" + view.getObjectName() + ""', 'TABLE')"");

        // column listing
        assertThat(query(""SHOW COLUMNS FROM "" + view.getObjectName()))
                .projected(0) // column types can very between connectors
                .skippingTypesCheck()
                .matches(""VALUES 'nationkey', 'name', 'regionkey', 'comment'"");

        assertThat(query(""DESCRIBE "" + view.getObjectName()))
                .projected(0) // column types can very between connectors
                .skippingTypesCheck()
                .matches(""VALUES 'nationkey', 'name', 'regionkey', 'comment'"");

        // information_schema.columns without table_name filter
        assertThat(query(
                ""SELECT table_name, column_name "" +
                        ""FROM information_schema.columns "" +
                        ""WHERE table_schema = '"" + view.getSchemaName() + ""'""))
                .skippingTypesCheck()
                .containsAll(
                        ""SELECT * FROM (VALUES '"" + view.getObjectName() + ""') "" +
                                ""CROSS JOIN UNNEST(ARRAY['nationkey', 'name', 'regionkey', 'comment'])"");

        // information_schema.columns with table_name filter
        assertThat(query(
                ""SELECT table_name, column_name "" +
                        ""FROM information_schema.columns "" +
                        ""WHERE table_schema = '"" + view.getSchemaName() + ""' and table_name = '"" + view.getObjectName() + ""'""))
                .skippingTypesCheck()
                .containsAll(
                        ""SELECT * FROM (VALUES '"" + view.getObjectName() + ""') "" +
                                ""CROSS JOIN UNNEST(ARRAY['nationkey', 'name', 'regionkey', 'comment'])"");

        // view-specific listings
        checkInformationSchemaViewsForMaterializedView(view.getSchemaName(), view.getObjectName());

        // system.jdbc.columns without filter
        @Language(""SQL"") String expectedValues = ""VALUES ('"" + view.getSchemaName() + ""', '"" + view.getObjectName() + ""', 'nationkey'), "" +
                ""('"" + view.getSchemaName() + ""', '"" + view.getObjectName() + ""', 'name'), "" +
                ""('"" + view.getSchemaName() + ""', '"" + view.getObjectName() + ""', 'regionkey'), "" +
                ""('"" + view.getSchemaName() + ""', '"" + view.getObjectName() + ""', 'comment')"";
        assertThat(query(
                ""SELECT table_schem, table_name, column_name FROM system.jdbc.columns""))
                .skippingTypesCheck()
                .containsAll(expectedValues);

        // system.jdbc.columns with schema filter
        assertThat(query(
                ""SELECT table_schem, table_name, column_name "" +
                        ""FROM system.jdbc.columns "" +
                        ""WHERE table_schem LIKE '%"" + view.getSchemaName() + ""%'""))
                .skippingTypesCheck()
                .containsAll(expectedValues);

        // system.jdbc.columns with table filter
        assertQuery(
                ""SELECT table_schem, table_name, column_name "" +
                        ""FROM system.jdbc.columns "" +
                        ""WHERE table_name LIKE '%"" + view.getObjectName() + ""%'"",
                expectedValues);

        // details
        assertThat(((String) computeScalar(""SHOW CREATE MATERIALIZED VIEW "" + view.getObjectName())))
                .matches(""(?s)"" +
                        ""CREATE MATERIALIZED VIEW \\Q"" + view + ""\\E"" +
                        "".* AS\n"" +
                        ""SELECT \\*\n"" +
                        ""FROM\n"" +
                        ""  nation"");

        // we only want to test filtering materialized views in different schemas,
        // `viewWithComment` is in the same schema as `view` so it is not needed
        assertUpdate(""DROP MATERIALIZED VIEW "" + viewWithComment);

        // test filtering materialized views in system metadata table
        assertThat(query(listMaterializedViewsSql(""catalog_name = '"" + view.getCatalogName() + ""'"")))
                .skippingTypesCheck()
                .containsAll(getTestingMaterializedViewsResultRows(view, otherView));

        assertThat(query(
                listMaterializedViewsSql(
                        ""catalog_name = '"" + otherView.getCatalogName() + ""'"",
                        ""schema_name = '"" + otherView.getSchemaName() + ""'"")))
                .skippingTypesCheck()
                .containsAll(getTestingMaterializedViewsResultRow(otherView, ""sarcastic comment""));

        assertThat(query(
                listMaterializedViewsSql(
                        ""catalog_name = '"" + view.getCatalogName() + ""'"",
                        ""schema_name = '"" + view.getSchemaName() + ""'"",
                        ""name = '"" + view.getObjectName() + ""'"")))
                .skippingTypesCheck()
                .containsAll(getTestingMaterializedViewsResultRow(view, """"));

        assertThat(query(
                listMaterializedViewsSql(""schema_name LIKE '%"" + view.getSchemaName() + ""%'"")))
                .skippingTypesCheck()
                .containsAll(getTestingMaterializedViewsResultRow(view, """"));

        assertThat(query(
                listMaterializedViewsSql(""name LIKE '%"" + view.getObjectName() + ""%'"")))
                .skippingTypesCheck()
                .containsAll(getTestingMaterializedViewsResultRow(view, """"));

        // verify write in transaction
        if (!hasBehavior(SUPPORTS_MULTI_STATEMENT_WRITES)) {
            assertThatThrownBy(() -> inTransaction(session -> computeActual(session, ""REFRESH MATERIALIZED VIEW "" + view)))
                    .hasMessageMatching(""Catalog only supports writes using autocommit: \\w+"");
        }

        assertUpdate(""DROP MATERIALIZED VIEW "" + view);
        assertUpdate(""DROP MATERIALIZED VIEW "" + otherView);

        assertQueryReturnsEmptyResult(listMaterializedViewsSql(""name = '"" + view.getObjectName() + ""'""));
        assertQueryReturnsEmptyResult(listMaterializedViewsSql(""name = '"" + otherView.getObjectName() + ""'""));
        assertQueryReturnsEmptyResult(listMaterializedViewsSql(""name = '"" + viewWithComment.getObjectName() + ""'""));
    }
"
"    @Test
    public void testRenameMaterializedView()
    {
        skipTestUnless(hasBehavior(SUPPORTS_CREATE_MATERIALIZED_VIEW));

        String schema = ""rename_mv_test"";
        Session session = Session.builder(getSession())
                .setSchema(schema)
                .build();

        QualifiedObjectName originalMaterializedView = new QualifiedObjectName(
                session.getCatalog().orElseThrow(),
                session.getSchema().orElseThrow(),
                ""test_materialized_view_rename_"" + randomTableSuffix());

        createTestingMaterializedView(originalMaterializedView, Optional.empty());

        String renamedMaterializedView = ""test_materialized_view_rename_new_"" + randomTableSuffix();
        if (!hasBehavior(SUPPORTS_RENAME_MATERIALIZED_VIEW)) {
            assertQueryFails(session, ""ALTER MATERIALIZED VIEW "" + originalMaterializedView + "" RENAME TO "" + renamedMaterializedView, ""This connector does not support renaming materialized views"");
            assertUpdate(session, ""DROP MATERIALIZED VIEW "" + originalMaterializedView);
            return;
        }

        // simple rename
        assertUpdate(session, ""ALTER MATERIALIZED VIEW "" + originalMaterializedView + "" RENAME TO "" + renamedMaterializedView);
        assertTestingMaterializedViewQuery(schema, renamedMaterializedView);
        // verify new name in the system.metadata.materialized_views
        assertQuery(session, ""SELECT catalog_name, schema_name FROM system.metadata.materialized_views WHERE name = '"" + renamedMaterializedView + ""'"",
                format(""VALUES ('%s', '%s')"", originalMaterializedView.getCatalogName(), originalMaterializedView.getSchemaName()));
        assertQueryReturnsEmptyResult(session, listMaterializedViewsSql(""name = '"" + originalMaterializedView.getObjectName() + ""'""));

        // rename with IF EXISTS on existing materialized view
        String testExistsMaterializedViewName = ""test_materialized_view_rename_exists_"" + randomTableSuffix();
        assertUpdate(session, ""ALTER MATERIALIZED VIEW IF EXISTS "" + renamedMaterializedView + "" RENAME TO "" + testExistsMaterializedViewName);
        assertTestingMaterializedViewQuery(schema, testExistsMaterializedViewName);

        // rename with upper-case, not delimited identifier
        String uppercaseName = ""TEST_MATERIALIZED_VIEW_RENAME_UPPERCASE_"" + randomTableSuffix();
        assertUpdate(session, ""ALTER MATERIALIZED VIEW "" + testExistsMaterializedViewName + "" RENAME TO "" + uppercaseName);
        assertTestingMaterializedViewQuery(schema, uppercaseName.toLowerCase(ENGLISH)); // Ensure select allows for lower-case, not delimited identifier

        String otherSchema = ""rename_mv_other_schema"";
        assertUpdate(format(""CREATE SCHEMA IF NOT EXISTS %s"", otherSchema));
        if (hasBehavior(SUPPORTS_RENAME_MATERIALIZED_VIEW_ACROSS_SCHEMAS)) {
            assertUpdate(session, ""ALTER MATERIALIZED VIEW "" + uppercaseName + "" RENAME TO "" + otherSchema + ""."" + originalMaterializedView.getObjectName());
            assertTestingMaterializedViewQuery(otherSchema, originalMaterializedView.getObjectName());

            assertUpdate(session, ""DROP MATERIALIZED VIEW "" + otherSchema + ""."" + originalMaterializedView.getObjectName());
        }
        else {
            assertQueryFails(
                    session,
                    ""ALTER MATERIALIZED VIEW "" + uppercaseName + "" RENAME TO "" + otherSchema + ""."" + originalMaterializedView.getObjectName(),
                    ""Materialized View rename across schemas is not supported"");
            assertUpdate(session, ""DROP MATERIALIZED VIEW "" + uppercaseName);
        }

        assertFalse(getQueryRunner().tableExists(session, originalMaterializedView.toString()));
        assertFalse(getQueryRunner().tableExists(session, renamedMaterializedView));
        assertFalse(getQueryRunner().tableExists(session, testExistsMaterializedViewName));

        // rename with IF EXISTS on NOT existing materialized view
        assertUpdate(session, ""ALTER TABLE IF EXISTS "" + originalMaterializedView + "" RENAME TO "" + renamedMaterializedView);
        assertQueryReturnsEmptyResult(session, listMaterializedViewsSql(""name = '"" + originalMaterializedView.getObjectName() + ""'""));
        assertQueryReturnsEmptyResult(session, listMaterializedViewsSql(""name = '"" + renamedMaterializedView + ""'""));
    }
"
"    @Test
    public void testViewAndMaterializedViewTogether()
    {
        if (!hasBehavior(SUPPORTS_CREATE_MATERIALIZED_VIEW) || !hasBehavior(SUPPORTS_CREATE_VIEW)) {
            return;
        }
        // Validate that it is possible to have views and materialized views defined at the same time and both are operational

        String schemaName = getSession().getSchema().orElseThrow();

        String regularViewName = ""test_views_together_normal_"" + randomTableSuffix();
        assertUpdate(""CREATE VIEW "" + regularViewName + "" AS SELECT * FROM region"");

        String materializedViewName = ""test_views_together_materialized_"" + randomTableSuffix();
        assertUpdate(""CREATE MATERIALIZED VIEW "" + materializedViewName + "" AS SELECT * FROM nation"");

        // both should be accessible via information_schema.views
        // TODO: actually it is not the cased now hence overridable `checkInformationSchemaViewsForMaterializedView`
        assertThat(query(""SELECT table_name FROM information_schema.views WHERE table_schema = '"" + schemaName + ""'""))
                .skippingTypesCheck()
                .containsAll(""VALUES '"" + regularViewName + ""'"");
        checkInformationSchemaViewsForMaterializedView(schemaName, materializedViewName);

        // check we can query from both
        assertThat(query(""SELECT * FROM "" + regularViewName)).containsAll(""SELECT * FROM region"");
        assertThat(query(""SELECT * FROM "" + materializedViewName)).containsAll(""SELECT * FROM nation"");

        assertUpdate(""DROP VIEW "" + regularViewName);
        assertUpdate(""DROP MATERIALIZED VIEW "" + materializedViewName);
    }
"
"    @Test
    public void testExplainAnalyze()
    {
        assertExplainAnalyze(""EXPLAIN ANALYZE SELECT * FROM orders"");
        assertExplainAnalyze(""EXPLAIN ANALYZE SELECT count(*), clerk FROM orders GROUP BY clerk"");
        assertExplainAnalyze(
                ""EXPLAIN ANALYZE SELECT x + y FROM ("" +
                        ""   SELECT orderdate, COUNT(*) x FROM orders GROUP BY orderdate) a JOIN ("" +
                        ""   SELECT orderdate, COUNT(*) y FROM orders GROUP BY orderdate) b ON a.orderdate = b.orderdate"");
        assertExplainAnalyze(""EXPLAIN ANALYZE SELECT count(*), clerk FROM orders GROUP BY clerk UNION ALL SELECT sum(orderkey), clerk FROM orders GROUP BY clerk"");

        assertExplainAnalyze(""EXPLAIN ANALYZE SHOW COLUMNS FROM orders"");
        assertExplainAnalyze(""EXPLAIN ANALYZE EXPLAIN SELECT count(*) FROM orders"");
        assertExplainAnalyze(""EXPLAIN ANALYZE EXPLAIN ANALYZE SELECT count(*) FROM orders"");
        assertExplainAnalyze(""EXPLAIN ANALYZE SHOW FUNCTIONS"");
        assertExplainAnalyze(""EXPLAIN ANALYZE SHOW TABLES"");
        assertExplainAnalyze(""EXPLAIN ANALYZE SHOW SCHEMAS"");
        assertExplainAnalyze(""EXPLAIN ANALYZE SHOW CATALOGS"");
        assertExplainAnalyze(""EXPLAIN ANALYZE SHOW SESSION"");
    }
"
"    @Test
    public void testExplainAnalyzeVerbose()
    {
        assertExplainAnalyze(""EXPLAIN ANALYZE VERBOSE SELECT * FROM orders"");
        assertExplainAnalyze(""EXPLAIN ANALYZE VERBOSE SELECT rank() OVER (PARTITION BY orderkey ORDER BY clerk DESC) FROM orders"");
        assertExplainAnalyze(""EXPLAIN ANALYZE VERBOSE SELECT rank() OVER (PARTITION BY orderkey ORDER BY clerk DESC) FROM orders WHERE orderkey < 0"");
    }
"
"    @Test
    public void testTableSampleSystem()
    {
        MaterializedResult fullSample = computeActual(""SELECT orderkey FROM orders TABLESAMPLE SYSTEM (100)"");
        MaterializedResult emptySample = computeActual(""SELECT orderkey FROM orders TABLESAMPLE SYSTEM (0)"");
        MaterializedResult randomSample = computeActual(""SELECT orderkey FROM orders TABLESAMPLE SYSTEM (50)"");
        MaterializedResult all = computeActual(""SELECT orderkey FROM orders"");

        assertContains(all, fullSample);
        assertEquals(emptySample.getMaterializedRows().size(), 0);
        assertTrue(all.getMaterializedRows().size() >= randomSample.getMaterializedRows().size());
    }
"
"    @Test
    public void testTableSampleWithFiltering()
    {
        MaterializedResult emptySample = computeActual(""SELECT DISTINCT orderkey, orderdate FROM orders TABLESAMPLE SYSTEM (99) WHERE orderkey BETWEEN 0 AND 0"");
        MaterializedResult halfSample = computeActual(""SELECT DISTINCT orderkey, orderdate FROM orders TABLESAMPLE SYSTEM (50) WHERE orderkey BETWEEN 0 AND 9999999999"");
        MaterializedResult all = computeActual(""SELECT orderkey, orderdate FROM orders"");

        assertEquals(emptySample.getMaterializedRows().size(), 0);
        // Assertions need to be loose here because SYSTEM sampling random selects data on split boundaries. In this case either all the data will be selected, or
        // none of it. Sampling with a 100% ratio is ignored, so that also cannot be used to guarantee results.
        assertTrue(all.getMaterializedRows().size() >= halfSample.getMaterializedRows().size());
    }
"
"    @Test
    public void testShowCreateTable()
    {
        assertThat((String) computeActual(""SHOW CREATE TABLE orders"").getOnlyValue())
                // If the connector reports additional column properties, the expected value needs to be adjusted in the test subclass
                .matches(""CREATE TABLE \\w+\\.\\w+\\.orders \\Q(\n"" +
                        ""   orderkey bigint,\n"" +
                        ""   custkey bigint,\n"" +
                        ""   orderstatus varchar(1),\n"" +
                        ""   totalprice double,\n"" +
                        ""   orderdate date,\n"" +
                        ""   orderpriority varchar(15),\n"" +
                        ""   clerk varchar(15),\n"" +
                        ""   shippriority integer,\n"" +
                        ""   comment varchar(79)\n"" +
                        "")"");
    }
"
"    @Test
    public void testSelectInformationSchemaTables()
    {
        String catalog = getSession().getCatalog().get();
        String schema = getSession().getSchema().get();
        String schemaPattern = schema.replaceAll(""^."", ""_"");

        assertQuery(""SELECT table_name FROM information_schema.tables WHERE table_schema = '"" + schema + ""' AND table_name = 'orders'"", ""VALUES 'orders'"");
        assertQuery(""SELECT table_name FROM information_schema.tables WHERE table_schema LIKE '"" + schema + ""' AND table_name LIKE '%rders'"", ""VALUES 'orders'"");
        assertQuery(""SELECT table_name FROM information_schema.tables WHERE table_schema LIKE '"" + schemaPattern + ""' AND table_name LIKE '%rders'"", ""VALUES 'orders'"");
        assertQuery(
                ""SELECT table_name FROM information_schema.tables "" +
                        ""WHERE table_catalog = '"" + catalog + ""' AND table_schema LIKE '"" + schema + ""' AND table_name LIKE '%orders'"",
                ""VALUES 'orders'"");
        assertQuery(""SELECT table_name FROM information_schema.tables WHERE table_catalog = 'something_else'"", ""SELECT '' WHERE false"");

        assertQuery(
                ""SELECT DISTINCT table_name FROM information_schema.tables WHERE table_schema = 'information_schema' OR rand() = 42 ORDER BY 1"",
                ""VALUES "" +
                        ""('applicable_roles'), "" +
                        ""('columns'), "" +
                        ""('enabled_roles'), "" +
                        ""('role_authorization_descriptors'), "" +
                        ""('roles'), "" +
                        ""('schemata'), "" +
                        ""('table_privileges'), "" +
                        ""('tables'), "" +
                        ""('views')"");
    }
"
"    @Test
    public void testSelectInformationSchemaColumns()
    {
        String catalog = getSession().getCatalog().get();
        String schema = getSession().getSchema().get();
        String schemaPattern = schema.replaceAll("".$"", ""_"");

        @Language(""SQL"") String ordersTableWithColumns = ""VALUES "" +
                ""('orders', 'orderkey'), "" +
                ""('orders', 'custkey'), "" +
                ""('orders', 'orderstatus'), "" +
                ""('orders', 'totalprice'), "" +
                ""('orders', 'orderdate'), "" +
                ""('orders', 'orderpriority'), "" +
                ""('orders', 'clerk'), "" +
                ""('orders', 'shippriority'), "" +
                ""('orders', 'comment')"";

        assertQuery(""SELECT table_schema FROM information_schema.columns WHERE table_schema = '"" + schema + ""' GROUP BY table_schema"", ""VALUES '"" + schema + ""'"");
        assertQuery(""SELECT table_name FROM information_schema.columns WHERE table_name = 'orders' GROUP BY table_name"", ""VALUES 'orders'"");
        assertQuery(""SELECT table_name, column_name FROM information_schema.columns WHERE table_schema = '"" + schema + ""' AND table_name = 'orders'"", ordersTableWithColumns);
        assertQuery(""SELECT table_name, column_name FROM information_schema.columns WHERE table_schema = '"" + schema + ""' AND table_name LIKE '%rders'"", ordersTableWithColumns);
        assertQuery(""SELECT table_name, column_name FROM information_schema.columns WHERE table_schema LIKE '"" + schemaPattern + ""' AND table_name LIKE '_rder_'"", ordersTableWithColumns);
        assertQuery(
                ""SELECT table_name, column_name FROM information_schema.columns "" +
                        ""WHERE table_catalog = '"" + catalog + ""' AND table_schema = '"" + schema + ""' AND table_name LIKE '%orders%'"",
                ordersTableWithColumns);

        assertQuerySucceeds(""SELECT * FROM information_schema.columns"");
        assertQuery(""SELECT DISTINCT table_name, column_name FROM information_schema.columns WHERE table_name LIKE '_rders'"", ordersTableWithColumns);
        assertQuerySucceeds(""SELECT * FROM information_schema.columns WHERE table_catalog = '"" + catalog + ""'"");
        assertQuerySucceeds(""SELECT * FROM information_schema.columns WHERE table_catalog = '"" + catalog + ""' AND table_schema = '"" + schema + ""'"");
        assertQuery(""SELECT table_name, column_name FROM information_schema.columns WHERE table_catalog = '"" + catalog + ""' AND table_schema = '"" + schema + ""' AND table_name LIKE '_rders'"", ordersTableWithColumns);
        assertQuerySucceeds(""SELECT * FROM information_schema.columns WHERE table_catalog = '"" + catalog + ""' AND table_name LIKE '%'"");
        assertQuery(""SELECT column_name FROM information_schema.columns WHERE table_catalog = 'something_else'"", ""SELECT '' WHERE false"");

        assertQuery(
                ""SELECT DISTINCT table_name FROM information_schema.columns WHERE table_schema = 'information_schema' OR rand() = 42 ORDER BY 1"",
                ""VALUES "" +
                        ""('applicable_roles'), "" +
                        ""('columns'), "" +
                        ""('enabled_roles'), "" +
                        ""('role_authorization_descriptors'), "" +
                        ""('roles'), "" +
                        ""('schemata'), "" +
                        ""('table_privileges'), "" +
                        ""('tables'), "" +
                        ""('views')"");
    }
"
"    @Test
    public void testShowCreateInformationSchema()
    {
        assertThat(query(""SHOW CREATE SCHEMA information_schema""))
                .skippingTypesCheck()
                .matches(format(""VALUES 'CREATE SCHEMA %s.information_schema'"", getSession().getCatalog().orElseThrow()));
    }
"
"    @Test
    public void testShowCreateInformationSchemaTable()
    {
        assertQueryFails(""SHOW CREATE VIEW information_schema.schemata"", ""line 1:1: Relation '\\w+.information_schema.schemata' is a table, not a view"");
        assertQueryFails(""SHOW CREATE MATERIALIZED VIEW information_schema.schemata"", ""line 1:1: Relation '\\w+.information_schema.schemata' is a table, not a materialized view"");

        assertThat((String) computeScalar(""SHOW CREATE TABLE information_schema.schemata""))
                .isEqualTo(""CREATE TABLE "" + getSession().getCatalog().orElseThrow() + "".information_schema.schemata (\n"" +
                        ""   catalog_name varchar,\n"" +
                        ""   schema_name varchar\n"" +
                        "")"");
    }
"
"    @Test
    public void testRollback()
    {
        skipTestUnless(hasBehavior(SUPPORTS_MULTI_STATEMENT_WRITES));

        String table = ""test_rollback_"" + randomTableSuffix();
        computeActual(format(""CREATE TABLE %s (x int)"", table));

        assertThatThrownBy(() ->
                inTransaction(session -> {
                    assertUpdate(session, format(""INSERT INTO %s VALUES (42)"", table), 1);
                    throw new RollbackException();
                }))
                .isInstanceOf(RollbackException.class);

        assertQuery(format(""SELECT count(*) FROM %s"", table), ""SELECT 0"");
    }
"
"    @Test
    public void testWriteNotAllowedInTransaction()
    {
        skipTestUnless(!hasBehavior(SUPPORTS_MULTI_STATEMENT_WRITES));

        assertWriteNotAllowedInTransaction(SUPPORTS_CREATE_SCHEMA, ""CREATE SCHEMA write_not_allowed"");
        assertWriteNotAllowedInTransaction(SUPPORTS_CREATE_TABLE, ""CREATE TABLE write_not_allowed (x int)"");
        assertWriteNotAllowedInTransaction(SUPPORTS_CREATE_TABLE, ""DROP TABLE region"");
        assertWriteNotAllowedInTransaction(SUPPORTS_CREATE_TABLE_WITH_DATA, ""CREATE TABLE write_not_allowed AS SELECT * FROM region"");
        assertWriteNotAllowedInTransaction(SUPPORTS_CREATE_VIEW, ""CREATE VIEW write_not_allowed AS SELECT * FROM region"");
        assertWriteNotAllowedInTransaction(SUPPORTS_CREATE_MATERIALIZED_VIEW, ""CREATE MATERIALIZED VIEW write_not_allowed AS SELECT * FROM region"");
        assertWriteNotAllowedInTransaction(SUPPORTS_RENAME_TABLE, ""ALTER TABLE region RENAME TO region_name"");
        assertWriteNotAllowedInTransaction(SUPPORTS_INSERT, ""INSERT INTO region (regionkey) VALUES (123)"");
        assertWriteNotAllowedInTransaction(SUPPORTS_DELETE, ""DELETE FROM region WHERE regionkey = 123"");

        // REFRESH MATERIALIZED VIEW is tested in testMaterializedView
    }
"
"    @Test
    public void testRenameSchema()
    {
        if (!hasBehavior(SUPPORTS_RENAME_SCHEMA)) {
            String schemaName = getSession().getSchema().orElseThrow();
            assertQueryFails(
                    format(""ALTER SCHEMA %s RENAME TO %s"", schemaName, schemaName + randomTableSuffix()),
                    ""This connector does not support renaming schemas"");
            return;
        }

        if (!hasBehavior(SUPPORTS_CREATE_SCHEMA)) {
            throw new SkipException(""Skipping as connector does not support CREATE SCHEMA"");
        }

        String schemaName = ""test_rename_schema_"" + randomTableSuffix();
        try {
            assertUpdate(""CREATE SCHEMA "" + schemaName);
            assertUpdate(""ALTER SCHEMA "" + schemaName + "" RENAME TO "" + schemaName + ""_renamed"");
        }
        finally {
            assertUpdate(""DROP SCHEMA IF EXISTS "" + schemaName);
            assertUpdate(""DROP SCHEMA IF EXISTS "" + schemaName + ""_renamed"");
        }
    }
"
"    @Test
    public void testRenameTableAcrossSchema()
    {
        if (!hasBehavior(SUPPORTS_RENAME_TABLE_ACROSS_SCHEMAS)) {
            if (!hasBehavior(SUPPORTS_RENAME_TABLE)) {
                throw new SkipException(""Skipping since rename table is not supported at all"");
            }
            assertQueryFails(""ALTER TABLE nation RENAME TO other_schema.yyyy"", ""This connector does not support renaming tables across schemas"");
            return;
        }

        if (!hasBehavior(SUPPORTS_CREATE_SCHEMA)) {
            throw new AssertionError(""Cannot test ALTER TABLE RENAME across schemas without CREATE SCHEMA, the test needs to be implemented in a connector-specific way"");
        }

        if (!hasBehavior(SUPPORTS_CREATE_TABLE)) {
            throw new AssertionError(""Cannot test ALTER TABLE RENAME across schemas without CREATE TABLE, the test needs to be implemented in a connector-specific way"");
        }

        String tableName = ""test_rename_old_"" + randomTableSuffix();
        assertUpdate(""CREATE TABLE "" + tableName + "" AS SELECT 123 x"", 1);

        String schemaName = ""test_schema_"" + randomTableSuffix();
        assertUpdate(""CREATE SCHEMA "" + schemaName);

        String renamedTable = schemaName + "".test_rename_new_"" + randomTableSuffix();
        assertUpdate(""ALTER TABLE "" + tableName + "" RENAME TO "" + renamedTable);

        assertFalse(getQueryRunner().tableExists(getSession(), tableName));
        assertQuery(""SELECT x FROM "" + renamedTable, ""VALUES 123"");

        assertUpdate(""DROP TABLE "" + renamedTable);
        assertUpdate(""DROP SCHEMA "" + schemaName);

        assertFalse(getQueryRunner().tableExists(getSession(), tableName));
        assertFalse(getQueryRunner().tableExists(getSession(), renamedTable));
    }
"
"    @Test
    public void testInsertIntoNotNullColumn()
    {
        skipTestUnless(hasBehavior(SUPPORTS_CREATE_TABLE));

        if (!hasBehavior(SUPPORTS_NOT_NULL_CONSTRAINT)) {
            assertQueryFails(
                    ""CREATE TABLE not_null_constraint (not_null_col INTEGER NOT NULL)"",
                    format(""line 1:35: Catalog '%s' does not support non-null column for column name 'not_null_col'"", getSession().getCatalog().orElseThrow()));
            return;
        }

        try (TestTable table = new TestTable(getQueryRunner()::execute, ""insert_not_null"", ""(nullable_col INTEGER, not_null_col INTEGER NOT NULL)"")) {
            assertUpdate(format(""INSERT INTO %s (not_null_col) VALUES (2)"", table.getName()), 1);
            assertQuery(""SELECT * FROM "" + table.getName(), ""VALUES (NULL, 2)"");
            // The error message comes from remote databases when ConnectorMetadata.supportsMissingColumnsOnInsert is true
            assertQueryFails(format(""INSERT INTO %s (nullable_col) VALUES (1)"", table.getName()), errorMessageForInsertIntoNotNullColumn(""not_null_col""));
        }

        try (TestTable table = new TestTable(getQueryRunner()::execute, ""commuted_not_null"", ""(nullable_col BIGINT, not_null_col BIGINT NOT NULL)"")) {
            assertUpdate(format(""INSERT INTO %s (not_null_col) VALUES (2)"", table.getName()), 1);
            assertQuery(""SELECT * FROM "" + table.getName(), ""VALUES (NULL, 2)"");
            // This is enforced by the engine and not the connector
            assertQueryFails(format(""INSERT INTO %s (not_null_col, nullable_col) VALUES (NULL, 3)"", table.getName()), ""NULL value not allowed for NOT NULL column: not_null_col"");
        }
    }
"
"    @Test
    public void verifySupportsDeleteDeclaration()
    {
        if (hasBehavior(SUPPORTS_DELETE)) {
            // Covered by testDeleteAllDataFromTable
            return;
        }

        skipTestUnless(hasBehavior(SUPPORTS_CREATE_TABLE));
        try (TestTable table = new TestTable(getQueryRunner()::execute, ""test_supports_delete"", ""AS SELECT * FROM region"")) {
            assertQueryFails(""DELETE FROM "" + table.getName(), ""This connector does not support deletes"");
        }
    }
"
"    @Test
    public void verifySupportsRowLevelDeleteDeclaration()
    {
        if (hasBehavior(SUPPORTS_ROW_LEVEL_DELETE)) {
            // Covered by testRowLevelDelete
            return;
        }

        skipTestUnless(hasBehavior(SUPPORTS_CREATE_TABLE));
        try (TestTable table = new TestTable(getQueryRunner()::execute, ""test_supports_row_level_delete"", ""AS SELECT * FROM region"")) {
            assertQueryFails(""DELETE FROM "" + table.getName() + "" WHERE regionkey = 2"", ""This connector does not support deletes"");
        }
    }
"
"    @Test
    public void testDeleteAllDataFromTable()
    {
        skipTestUnless(hasBehavior(SUPPORTS_CREATE_TABLE) && hasBehavior(SUPPORTS_DELETE));
        try (TestTable table = new TestTable(getQueryRunner()::execute, ""test_delete_all_data"", ""AS SELECT * FROM region"")) {
            // not using assertUpdate as some connectors provide update count and some not
            getQueryRunner().execute(""DELETE FROM "" + table.getName());
            assertQuery(""SELECT count(*) FROM "" + table.getName(), ""VALUES 0"");
        }
    }
"
"    @Test
    public void testRowLevelDelete()
    {
        skipTestUnless(hasBehavior(SUPPORTS_CREATE_TABLE) && hasBehavior(SUPPORTS_ROW_LEVEL_DELETE));
        // TODO (https://github.com/trinodb/trino/issues/5901) Use longer table name once Oracle version is updated
        try (TestTable table = new TestTable(getQueryRunner()::execute, ""test_row_delete"", ""AS SELECT * FROM region"")) {
            assertUpdate(""DELETE FROM "" + table.getName() + "" WHERE regionkey = 2"", 1);
            assertQuery(""SELECT count(*) FROM "" + table.getName(), ""VALUES 4"");
        }
    }
"
"    @Test
    public void testUpdate()
    {
        if (!hasBehavior(SUPPORTS_UPDATE)) {
            // Note this change is a no-op, if actually run
            assertQueryFails(""UPDATE nation SET nationkey = nationkey + regionkey WHERE regionkey < 1"", ""This connector does not support updates"");
            return;
        }

        try (TestTable table = new TestTable(getQueryRunner()::execute, ""test_update"", ""AS TABLE tpch.tiny.nation"")) {
            String tableName = table.getName();
            assertUpdate(""UPDATE "" + tableName + "" SET nationkey = 100 + nationkey WHERE regionkey = 2"", 5);
            assertThat(query(""SELECT * FROM "" + tableName))
                    .skippingTypesCheck()
                    .matches(""SELECT IF(regionkey=2, nationkey + 100, nationkey) nationkey, name, regionkey, comment FROM tpch.tiny.nation"");

            // UPDATE after UPDATE
            assertUpdate(""UPDATE "" + tableName + "" SET nationkey = nationkey * 2 WHERE regionkey IN (2,3)"", 10);
            assertThat(query(""SELECT * FROM "" + tableName))
                    .skippingTypesCheck()
                    .matches(""SELECT CASE regionkey WHEN 2 THEN 2*(nationkey+100) WHEN 3 THEN 2*nationkey ELSE nationkey END nationkey, name, regionkey, comment FROM tpch.tiny.nation"");
        }
    }
"
"    @Test(timeOut = 60_000, invocationCount = 4)
    public void testUpdateRowConcurrently()
            throws Exception
"
"    @Test
    public void testTruncateTable()
    {
        if (!hasBehavior(SUPPORTS_TRUNCATE)) {
            assertQueryFails(""TRUNCATE TABLE nation"", ""This connector does not support truncating tables"");
            return;
        }

        skipTestUnless(hasBehavior(SUPPORTS_CREATE_TABLE));

        try (TestTable table = new TestTable(getQueryRunner()::execute, ""test_truncate"", ""AS SELECT * FROM region"")) {
            assertUpdate(""TRUNCATE TABLE "" + table.getName());
            assertQuery(""SELECT count(*) FROM "" + table.getName(), ""VALUES 0"");
        }
    }
"
"    @Test(dataProvider = ""testColumnNameDataProvider"")
    public void testMaterializedViewColumnName(String columnName)
    {
        skipTestUnless(hasBehavior(SUPPORTS_CREATE_MATERIALIZED_VIEW));

        if (!requiresDelimiting(columnName)) {
            testMaterializedViewColumnName(columnName, false);
        }
        testMaterializedViewColumnName(columnName, true);
    }
"
"    @Test
    public void ensureDistributedQueryRunner()
    {
        assertThat(getQueryRunner().getNodeCount()).as(""query runner node count"")
                .isGreaterThanOrEqualTo(3);
    }
"
"    @Test
    public void testColumnsInReverseOrder()
    {
        assertQuery(""SELECT shippriority, clerk, totalprice FROM orders"");
    }
"
"    @Test
    public void testAggregation()
    {
        assertQuery(""SELECT sum(orderkey) FROM orders"");
        assertQuery(""SELECT sum(totalprice) FROM orders"");
        assertQuery(""SELECT max(comment) FROM nation"");

        assertQuery(""SELECT count(*) FROM orders"");
        assertQuery(""SELECT count(*) FROM orders WHERE orderkey > 10"");
        assertQuery(""SELECT count(*) FROM (SELECT * FROM orders LIMIT 10)"");
        assertQuery(""SELECT count(*) FROM (SELECT * FROM orders WHERE orderkey > 10 LIMIT 10)"");

        assertQuery(""SELECT DISTINCT regionkey FROM nation"");
        assertQuery(""SELECT regionkey FROM nation GROUP BY regionkey"");

        // TODO support aggregation pushdown with GROUPING SETS
        assertQuery(
                ""SELECT regionkey, nationkey FROM nation GROUP BY GROUPING SETS ((regionkey), (nationkey))"",
                ""SELECT NULL, nationkey FROM nation "" +
                        ""UNION ALL SELECT DISTINCT regionkey, NULL FROM nation"");
        assertQuery(
                ""SELECT regionkey, nationkey, count(*) FROM nation GROUP BY GROUPING SETS ((), (regionkey), (nationkey), (regionkey, nationkey))"",
                ""SELECT NULL, NULL, count(*) FROM nation "" +
                        ""UNION ALL SELECT NULL, nationkey, 1 FROM nation "" +
                        ""UNION ALL SELECT regionkey, NULL, count(*) FROM nation GROUP BY regionkey "" +
                        ""UNION ALL SELECT regionkey, nationkey, 1 FROM nation"");

        assertQuery(""SELECT count(regionkey) FROM nation"");
        assertQuery(""SELECT count(DISTINCT regionkey) FROM nation"");
        assertQuery(""SELECT regionkey, count(*) FROM nation GROUP BY regionkey"");

        assertQuery(""SELECT min(regionkey), max(regionkey) FROM nation"");
        assertQuery(""SELECT min(DISTINCT regionkey), max(DISTINCT regionkey) FROM nation"");
        assertQuery(""SELECT regionkey, min(regionkey), min(name), max(regionkey), max(name) FROM nation GROUP BY regionkey"");

        assertQuery(""SELECT sum(regionkey) FROM nation"");
        assertQuery(""SELECT sum(DISTINCT regionkey) FROM nation"");
        assertQuery(""SELECT regionkey, sum(regionkey) FROM nation GROUP BY regionkey"");

        assertQuery(
                ""SELECT avg(nationkey) FROM nation"",
                ""SELECT avg(CAST(nationkey AS double)) FROM nation"");
        assertQuery(
                ""SELECT avg(DISTINCT nationkey) FROM nation"",
                ""SELECT avg(DISTINCT CAST(nationkey AS double)) FROM nation"");
        assertQuery(
                ""SELECT regionkey, avg(nationkey) FROM nation GROUP BY regionkey"",
                ""SELECT regionkey, avg(CAST(nationkey AS double)) FROM nation GROUP BY regionkey"");
    }
"
"    @Test
    public void testExactPredicate()
    {
        assertQueryReturnsEmptyResult(""SELECT * FROM orders WHERE orderkey = 10"");

        // filtered column is selected
        assertQuery(""SELECT custkey, orderkey FROM orders WHERE orderkey = 32"", ""VALUES (1301, 32)"");

        // filtered column is not selected
        assertQuery(""SELECT custkey FROM orders WHERE orderkey = 32"", ""VALUES (1301)"");
    }
"
"    @Test
    public void testInListPredicate()
    {
        assertQueryReturnsEmptyResult(""SELECT * FROM orders WHERE orderkey IN (10, 11, 20, 21)"");

        // filtered column is selected
        assertQuery(""SELECT custkey, orderkey FROM orders WHERE orderkey IN (7, 10, 32, 33)"", ""VALUES (392, 7), (1301, 32), (670, 33)"");

        // filtered column is not selected
        assertQuery(""SELECT custkey FROM orders WHERE orderkey IN (7, 10, 32, 33)"", ""VALUES (392), (1301), (670)"");
    }
"
"    @Test
    public void testIsNullPredicate()
    {
        assertQueryReturnsEmptyResult(""SELECT * FROM orders WHERE orderkey IS NULL"");
        assertQueryReturnsEmptyResult(""SELECT * FROM orders WHERE orderkey = 10 OR orderkey IS NULL"");

        // filtered column is selected
        assertQuery(""SELECT custkey, orderkey FROM orders WHERE orderkey = 32 OR orderkey IS NULL"", ""VALUES (1301, 32)"");

        // filtered column is not selected
        assertQuery(""SELECT custkey FROM orders WHERE orderkey = 32 OR orderkey IS NULL"", ""VALUES (1301)"");
    }
"
"    @Test
    public void testLikePredicate()
    {
        // filtered column is not selected
        assertQuery(""SELECT orderkey FROM orders WHERE orderpriority LIKE '5-L%'"");

        // filtered column is selected
        assertQuery(""SELECT orderkey, orderpriority FROM orders WHERE orderpriority LIKE '5-L%'"");

        // filtered column is not selected
        assertQuery(""SELECT orderkey FROM orders WHERE orderpriority LIKE '5-L__'"");

        // filtered column is selected
        assertQuery(""SELECT orderkey, orderpriority FROM orders WHERE orderpriority LIKE '5-L__'"");
    }
"
"    @Test
    public void testLimit()
    {
        assertEquals(computeActual(""SELECT * FROM orders LIMIT 10"").getRowCount(), 10);
    }
"
"    @Test
    public void testMultipleRangesPredicate()
    {
        // List columns explicitly. Some connectors do not maintain column ordering.
        assertQuery("""" +
                ""SELECT orderkey, custkey, orderstatus, totalprice, orderdate, orderpriority, clerk, shippriority, comment "" +
                ""FROM orders "" +
                ""WHERE orderkey BETWEEN 10 AND 50"");
    }
"
"    @Test
    public void testRangePredicate()
    {
        // List columns explicitly. Some connectors do not maintain column ordering.
        assertQuery("""" +
                ""SELECT orderkey, custkey, orderstatus, totalprice, orderdate, orderpriority, clerk, shippriority, comment "" +
                ""FROM orders "" +
                ""WHERE orderkey BETWEEN 10 AND 50"");
    }
"
"    @Test
    public void testConcurrentScans()
    {
        String unionMultipleTimes = join("" UNION ALL "", nCopies(25, ""SELECT * FROM orders""));
        assertQuery(""SELECT sum(if(rand() >= 0, orderkey)) FROM ("" + unionMultipleTimes + "")"", ""VALUES 11246812500"");
    }
"
"    @Test
    public void testSelectAll()
    {
        assertQuery(""SELECT * FROM orders"");
    }
"
"    @Test(timeOut = 300_000, dataProvider = ""joinDistributionTypes"")
    public void testJoinWithEmptySides(JoinDistributionType joinDistributionType)
    {
        Session session = noJoinReordering(joinDistributionType);
        // empty build side
        assertQuery(session, ""SELECT count(*) FROM nation JOIN region ON nation.regionkey = region.regionkey AND region.name = ''"", ""VALUES 0"");
        assertQuery(session, ""SELECT count(*) FROM nation JOIN region ON nation.regionkey = region.regionkey AND region.regionkey < 0"", ""VALUES 0"");
        // empty probe side
        assertQuery(session, ""SELECT count(*) FROM region JOIN nation ON nation.regionkey = region.regionkey AND region.name = ''"", ""VALUES 0"");
        assertQuery(session, ""SELECT count(*) FROM nation JOIN region ON nation.regionkey = region.regionkey AND region.regionkey < 0"", ""VALUES 0"");
    }
"
"    @Test
    public void testJoin()
    {
        Session session = Session.builder(getSession())
                .setSystemProperty(IGNORE_STATS_CALCULATOR_FAILURES, ""false"")
                .build();

        // 2 inner joins, eligible for join reodering
        assertQuery(
                session,
                ""SELECT c.name, n.name, r.name "" +
                        ""FROM nation n "" +
                        ""JOIN customer c ON c.nationkey = n.nationkey "" +
                        ""JOIN region r ON n.regionkey = r.regionkey"");

        // 2 inner joins, eligible for join reodering, where one table has a filter
        assertQuery(
                session,
                ""SELECT c.name, n.name, r.name "" +
                        ""FROM nation n "" +
                        ""JOIN customer c ON c.nationkey = n.nationkey "" +
                        ""JOIN region r ON n.regionkey = r.regionkey "" +
                        ""WHERE n.name = 'ARGENTINA'"");

        // 2 inner joins, eligible for join reodering, on top of aggregation
        assertQuery(
                session,
                ""SELECT c.name, n.name, n.count, r.name "" +
                        ""FROM (SELECT name, regionkey, nationkey, count(*) count FROM nation GROUP BY name, regionkey, nationkey) n "" +
                        ""JOIN customer c ON c.nationkey = n.nationkey "" +
                        ""JOIN region r ON n.regionkey = r.regionkey"");
    }
"
"    @Test
    public void testShowSchemas()
    {
        MaterializedResult actualSchemas = computeActual(""SHOW SCHEMAS"").toTestTypes();

        MaterializedResult.Builder resultBuilder = MaterializedResult.resultBuilder(getSession(), VARCHAR)
                .row(getSession().getSchema().orElse(""tpch""));

        assertContains(actualSchemas, resultBuilder.build());
    }
"
"    @Test
    public void testShowTables()
    {
        MaterializedResult actualTables = computeActual(""SHOW TABLES"").toTestTypes();
        MaterializedResult expectedTables = MaterializedResult.resultBuilder(getSession(), VARCHAR)
                .row(""orders"")
                .build();
        assertContains(actualTables, expectedTables);
    }
"
"    @Test
    public void testDescribeTable()
    {
        MaterializedResult expectedColumns = MaterializedResult.resultBuilder(getSession(), VARCHAR, VARCHAR, VARCHAR, VARCHAR)
                .row(""orderkey"", ""bigint"", """", """")
                .row(""custkey"", ""bigint"", """", """")
                .row(""orderstatus"", ""varchar(1)"", """", """")
                .row(""totalprice"", ""double"", """", """")
                .row(""orderdate"", ""date"", """", """")
                .row(""orderpriority"", ""varchar(15)"", """", """")
                .row(""clerk"", ""varchar(15)"", """", """")
                .row(""shippriority"", ""integer"", """", """")
                .row(""comment"", ""varchar(79)"", """", """")
                .build();
        MaterializedResult actualColumns = computeActual(""DESCRIBE orders"");
        assertEquals(actualColumns, expectedColumns);
    }
"
"    @Test
    public void testExplainAnalyze()
    {
        assertExplainAnalyze(""EXPLAIN ANALYZE SELECT * FROM orders"");
        assertExplainAnalyze(""EXPLAIN ANALYZE SELECT count(*), clerk FROM orders GROUP BY clerk"");
        assertExplainAnalyze(
                ""EXPLAIN ANALYZE SELECT x + y FROM ("" +
                        ""   SELECT orderdate, COUNT(*) x FROM orders GROUP BY orderdate) a JOIN ("" +
                        ""   SELECT orderdate, COUNT(*) y FROM orders GROUP BY orderdate) b ON a.orderdate = b.orderdate"");
        assertExplainAnalyze(""EXPLAIN ANALYZE SELECT count(*), clerk FROM orders GROUP BY clerk UNION ALL SELECT sum(orderkey), clerk FROM orders GROUP BY clerk"");

        assertExplainAnalyze(""EXPLAIN ANALYZE SHOW COLUMNS FROM orders"");
        assertExplainAnalyze(""EXPLAIN ANALYZE EXPLAIN SELECT count(*) FROM orders"");
        assertExplainAnalyze(""EXPLAIN ANALYZE EXPLAIN ANALYZE SELECT count(*) FROM orders"");
        assertExplainAnalyze(""EXPLAIN ANALYZE SHOW FUNCTIONS"");
        assertExplainAnalyze(""EXPLAIN ANALYZE SHOW TABLES"");
        assertExplainAnalyze(""EXPLAIN ANALYZE SHOW SCHEMAS"");
        assertExplainAnalyze(""EXPLAIN ANALYZE SHOW CATALOGS"");
        assertExplainAnalyze(""EXPLAIN ANALYZE SHOW SESSION"");
    }
"
"    @Test
    public void testExplainAnalyzeVerbose()
    {
        assertExplainAnalyze(""EXPLAIN ANALYZE VERBOSE SELECT * FROM orders"");
        assertExplainAnalyze(""EXPLAIN ANALYZE VERBOSE SELECT rank() OVER (PARTITION BY orderkey ORDER BY clerk DESC) FROM orders"");
        assertExplainAnalyze(""EXPLAIN ANALYZE VERBOSE SELECT rank() OVER (PARTITION BY orderkey ORDER BY clerk DESC) FROM orders WHERE orderkey < 0"");
    }
"
"    @Test
    public void testTableSampleSystem()
    {
        MaterializedResult fullSample = computeActual(""SELECT orderkey FROM orders TABLESAMPLE SYSTEM (100)"");
        MaterializedResult emptySample = computeActual(""SELECT orderkey FROM orders TABLESAMPLE SYSTEM (0)"");
        MaterializedResult randomSample = computeActual(""SELECT orderkey FROM orders TABLESAMPLE SYSTEM (50)"");
        MaterializedResult all = computeActual(""SELECT orderkey FROM orders"");

        assertContains(all, fullSample);
        assertEquals(emptySample.getMaterializedRows().size(), 0);
        assertTrue(all.getMaterializedRows().size() >= randomSample.getMaterializedRows().size());
    }
"
"    @Test
    public void testTableSampleWithFiltering()
    {
        MaterializedResult emptySample = computeActual(""SELECT DISTINCT orderkey, orderdate FROM orders TABLESAMPLE SYSTEM (99) WHERE orderkey BETWEEN 0 AND 0"");
        MaterializedResult halfSample = computeActual(""SELECT DISTINCT orderkey, orderdate FROM orders TABLESAMPLE SYSTEM (50) WHERE orderkey BETWEEN 0 AND 9999999999"");
        MaterializedResult all = computeActual(""SELECT orderkey, orderdate FROM orders"");

        assertEquals(emptySample.getMaterializedRows().size(), 0);
        // Assertions need to be loose here because SYSTEM sampling random selects data on split boundaries. In this case either all the data will be selected, or
        // none of it. Sampling with a 100% ratio is ignored, so that also cannot be used to guarantee results.
        assertTrue(all.getMaterializedRows().size() >= halfSample.getMaterializedRows().size());
    }
"
"    @Test
    public void testShowCreateTable()
    {
        assertThat((String) computeScalar(""SHOW CREATE TABLE orders""))
                // If the connector reports additional column properties, the expected value needs to be adjusted in the test subclass
                .matches(""CREATE TABLE \\w+\\.\\w+\\.orders \\Q(\n"" +
                        ""   orderkey bigint,\n"" +
                        ""   custkey bigint,\n"" +
                        ""   orderstatus varchar(1),\n"" +
                        ""   totalprice double,\n"" +
                        ""   orderdate date,\n"" +
                        ""   orderpriority varchar(15),\n"" +
                        ""   clerk varchar(15),\n"" +
                        ""   shippriority integer,\n"" +
                        ""   comment varchar(79)\n"" +
                        "")"");
    }
"
"    @Test
    public void testSelectInformationSchemaTables()
    {
        String catalog = getSession().getCatalog().get();
        String schema = getSession().getSchema().get();
        String schemaPattern = schema.replaceAll(""^."", ""_"");

        assertQuery(""SELECT table_name FROM information_schema.tables WHERE table_schema = '"" + schema + ""' AND table_name = 'orders'"", ""VALUES 'orders'"");
        assertQuery(""SELECT table_name FROM information_schema.tables WHERE table_schema LIKE '"" + schema + ""' AND table_name LIKE '%rders'"", ""VALUES 'orders'"");
        assertQuery(""SELECT table_name FROM information_schema.tables WHERE table_schema LIKE '"" + schemaPattern + ""' AND table_name LIKE '%rders'"", ""VALUES 'orders'"");
        assertQuery(
                ""SELECT table_name FROM information_schema.tables "" +
                        ""WHERE table_catalog = '"" + catalog + ""' AND table_schema LIKE '"" + schema + ""' AND table_name LIKE '%orders'"",
                ""VALUES 'orders'"");
        assertQuery(""SELECT table_name FROM information_schema.tables WHERE table_catalog = 'something_else'"", ""SELECT '' WHERE false"");

        assertQuery(
                ""SELECT DISTINCT table_name FROM information_schema.tables WHERE table_schema = 'information_schema' OR rand() = 42 ORDER BY 1"",
                ""VALUES "" +
                        ""('applicable_roles'), "" +
                        ""('columns'), "" +
                        ""('enabled_roles'), "" +
                        ""('role_authorization_descriptors'), "" +
                        ""('roles'), "" +
                        ""('schemata'), "" +
                        ""('table_privileges'), "" +
                        ""('tables'), "" +
                        ""('views')"");
    }
"
"    @Test
    public void testSelectInformationSchemaColumns()
    {
        String catalog = getSession().getCatalog().get();
        String schema = getSession().getSchema().get();
        String schemaPattern = schema.replaceAll("".$"", ""_"");

        @Language(""SQL"") String ordersTableWithColumns = ""VALUES "" +
                ""('orders', 'orderkey'), "" +
                ""('orders', 'custkey'), "" +
                ""('orders', 'orderstatus'), "" +
                ""('orders', 'totalprice'), "" +
                ""('orders', 'orderdate'), "" +
                ""('orders', 'orderpriority'), "" +
                ""('orders', 'clerk'), "" +
                ""('orders', 'shippriority'), "" +
                ""('orders', 'comment')"";

        assertQuery(""SELECT table_schema FROM information_schema.columns WHERE table_schema = '"" + schema + ""' GROUP BY table_schema"", ""VALUES '"" + schema + ""'"");
        assertQuery(""SELECT table_name FROM information_schema.columns WHERE table_name = 'orders' GROUP BY table_name"", ""VALUES 'orders'"");
        assertQuery(""SELECT table_name, column_name FROM information_schema.columns WHERE table_schema = '"" + schema + ""' AND table_name = 'orders'"", ordersTableWithColumns);
        assertQuery(""SELECT table_name, column_name FROM information_schema.columns WHERE table_schema = '"" + schema + ""' AND table_name LIKE '%rders'"", ordersTableWithColumns);
        assertQuery(""SELECT table_name, column_name FROM information_schema.columns WHERE table_schema LIKE '"" + schemaPattern + ""' AND table_name LIKE '_rder_'"", ordersTableWithColumns);
        assertQuery(
                ""SELECT table_name, column_name FROM information_schema.columns "" +
                        ""WHERE table_catalog = '"" + catalog + ""' AND table_schema = '"" + schema + ""' AND table_name LIKE '%orders%'"",
                ordersTableWithColumns);

        assertQuerySucceeds(""SELECT * FROM information_schema.columns"");
        assertQuery(""SELECT DISTINCT table_name, column_name FROM information_schema.columns WHERE table_name LIKE '_rders'"", ordersTableWithColumns);
        assertQuerySucceeds(""SELECT * FROM information_schema.columns WHERE table_catalog = '"" + catalog + ""'"");
        assertQuerySucceeds(""SELECT * FROM information_schema.columns WHERE table_catalog = '"" + catalog + ""' AND table_schema = '"" + schema + ""'"");
        assertQuery(""SELECT table_name, column_name FROM information_schema.columns WHERE table_catalog = '"" + catalog + ""' AND table_schema = '"" + schema + ""' AND table_name LIKE '_rders'"", ordersTableWithColumns);
        assertQuerySucceeds(""SELECT * FROM information_schema.columns WHERE table_catalog = '"" + catalog + ""' AND table_name LIKE '%'"");
        assertQuery(""SELECT column_name FROM information_schema.columns WHERE table_catalog = 'something_else'"", ""SELECT '' WHERE false"");

        assertQuery(
                ""SELECT DISTINCT table_name FROM information_schema.columns WHERE table_schema = 'information_schema' OR rand() = 42 ORDER BY 1"",
                ""VALUES "" +
                        ""('applicable_roles'), "" +
                        ""('columns'), "" +
                        ""('enabled_roles'), "" +
                        ""('role_authorization_descriptors'), "" +
                        ""('roles'), "" +
                        ""('schemata'), "" +
                        ""('table_privileges'), "" +
                        ""('tables'), "" +
                        ""('views')"");
    }
"
"    @Test
    public void ensureDistributedQueryRunner()
    {
        assertThat(getQueryRunner().getNodeCount()).as(""query runner node count"")
                .isGreaterThanOrEqualTo(3);
    }
"
"    @Test
    public void ensureTestNamingConvention()
    {
        // Enforce a naming convention to make code navigation easier.
        assertThat(getClass().getName())
                .endsWith(""ConnectorSmokeTest"");
    }
"
"    @Test
    public void testSelect()
    {
        assertQuery(""SELECT name FROM region"");
    }
"
"    @Test
    public void testPredicate()
    {
        assertQuery(""SELECT name, regionkey FROM nation WHERE nationkey = 10"");
        assertQuery(""SELECT name, regionkey FROM nation WHERE nationkey BETWEEN 5 AND 15"");
        assertQuery(""SELECT name, regionkey FROM nation WHERE name = 'EGYPT'"");
    }
"
"    @Test
    public void testLimit()
    {
        assertQuery(""SELECT name FROM region LIMIT 5"");
    }
"
"    @Test
    public void testTopN()
    {
        assertQuery(""SELECT regionkey FROM nation ORDER BY name LIMIT 3"");
    }
"
"    @Test
    public void testAggregation()
    {
        assertQuery(""SELECT sum(regionkey) FROM nation"");
        assertQuery(""SELECT sum(nationkey) FROM nation GROUP BY regionkey"");
    }
"
"    @Test
    public void testHaving()
    {
        assertQuery(""SELECT regionkey, sum(nationkey) FROM nation GROUP BY regionkey HAVING sum(nationkey) = 58"", ""VALUES (4, 58)"");
    }
"
"    @Test
    public void testJoin()
    {
        assertQuery(""SELECT n.name, r.name FROM nation n JOIN region r on n.regionkey = r.regionkey"");
    }
"
"    @Test
    public void testCreateTable()
    {
        if (!hasBehavior(SUPPORTS_CREATE_TABLE)) {
            assertQueryFails(""CREATE TABLE xxxx (a bigint, b double)"", ""This connector does not support creating tables"");
            return;
        }

        String tableName = ""test_create_"" + randomTableSuffix();
        assertUpdate(""CREATE TABLE "" + tableName + "" (a bigint, b double)"");
        assertThat(query(""SELECT a, b FROM "" + tableName))
                .returnsEmptyResult();
        assertUpdate(""DROP TABLE "" + tableName);
    }
"
"    @Test
    public void testCreateTableAsSelect()
    {
        if (!hasBehavior(SUPPORTS_CREATE_TABLE_WITH_DATA)) {
            assertQueryFails(""CREATE TABLE xxxx AS SELECT BIGINT '42' a, DOUBLE '-38.5' b"", ""This connector does not support creating tables with data"");
            return;
        }

        String tableName = ""test_create_"" + randomTableSuffix();
        assertUpdate(""CREATE TABLE "" + tableName + "" AS SELECT BIGINT '42' a, DOUBLE '-38.5' b"", 1);
        assertThat(query(""SELECT CAST(a AS bigint), b FROM "" + tableName))
                .matches(""VALUES (BIGINT '42', -385e-1)"");
        assertUpdate(""DROP TABLE "" + tableName);
    }
"
"    @Test
    public void testInsert()
    {
        if (!hasBehavior(SUPPORTS_INSERT)) {
            assertQueryFails(""INSERT INTO region (regionkey) VALUES (42)"", ""This connector does not support inserts"");
            return;
        }

        if (!hasBehavior(SUPPORTS_CREATE_TABLE)) {
            throw new AssertionError(""Cannot test INSERT without CREATE TABLE, the test needs to be implemented in a connector-specific way"");
        }

        try (TestTable table = new TestTable(getQueryRunner()::execute, ""test_insert_"", ""(a bigint, b double)"")) {
            assertUpdate(""INSERT INTO "" + table.getName() + "" (a, b) VALUES (42, -38.5)"", 1);
            assertThat(query(""SELECT CAST(a AS bigint), b FROM "" + table.getName()))
                    .matches(""VALUES (BIGINT '42', -385e-1)"");
        }
    }
"
"    @Test
    public void verifySupportsDeleteDeclaration()
    {
        if (hasBehavior(SUPPORTS_DELETE)) {
            // Covered by testDeleteAllDataFromTable
            return;
        }

        skipTestUnless(hasBehavior(SUPPORTS_CREATE_TABLE));
        try (TestTable table = new TestTable(getQueryRunner()::execute, ""test_supports_delete"", ""AS SELECT * FROM region"")) {
            assertQueryFails(""DELETE FROM "" + table.getName(), ""This connector does not support deletes"");
        }
    }
"
"    @Test
    public void verifySupportsRowLevelDeleteDeclaration()
    {
        if (hasBehavior(SUPPORTS_ROW_LEVEL_DELETE)) {
            // Covered by testRowLevelDelete
            return;
        }

        skipTestUnless(hasBehavior(SUPPORTS_CREATE_TABLE));
        try (TestTable table = new TestTable(getQueryRunner()::execute, ""test_supports_row_level_delete"", ""AS SELECT * FROM region"")) {
            assertQueryFails(""DELETE FROM "" + table.getName() + "" WHERE regionkey = 2"", ""This connector does not support deletes"");
        }
    }
"
"    @Test
    public void testDeleteAllDataFromTable()
    {
        skipTestUnless(hasBehavior(SUPPORTS_CREATE_TABLE) && hasBehavior(SUPPORTS_DELETE));
        try (TestTable table = new TestTable(getQueryRunner()::execute, ""test_delete_all_data"", ""AS SELECT * FROM region"")) {
            // not using assertUpdate as some connectors provide update count and some do not
            getQueryRunner().execute(""DELETE FROM "" + table.getName());
            assertQuery(""SELECT count(*) FROM "" + table.getName(), ""VALUES 0"");
        }
    }
"
"    @Test
    public void testRowLevelDelete()
    {
        skipTestUnless(hasBehavior(SUPPORTS_CREATE_TABLE) && hasBehavior(SUPPORTS_ROW_LEVEL_DELETE));
        // TODO (https://github.com/trinodb/trino/issues/5901) Use longer table name once Oracle version is updated
        try (TestTable table = new TestTable(getQueryRunner()::execute, ""test_row_delete"", ""AS SELECT * FROM region"")) {
            assertUpdate(""DELETE FROM "" + table.getName() + "" WHERE regionkey = 2"", 1);
            assertThat(query(""SELECT * FROM "" + table.getName() + "" WHERE regionkey = 2""))
                    .returnsEmptyResult();
            assertThat(query(""SELECT cast(regionkey AS integer) FROM "" + table.getName()))
                    .skippingTypesCheck()
                    .matches(""VALUES 0, 1, 3, 4"");
        }
    }
"
"    @Test
    public void testUpdate()
    {
        if (!hasBehavior(SUPPORTS_UPDATE)) {
            // Note this change is a no-op, if actually run
            assertQueryFails(""UPDATE nation SET nationkey = nationkey + regionkey WHERE regionkey < 1"", ""This connector does not support updates"");
            return;
        }

        try (TestTable table = new TestTable(getQueryRunner()::execute, ""test_update"", ""AS TABLE tpch.tiny.nation"")) {
            String tableName = table.getName();
            assertUpdate(""UPDATE "" + tableName + "" SET nationkey = 100 + nationkey WHERE regionkey = 2"", 5);
            assertThat(query(""SELECT * FROM "" + tableName))
                    .skippingTypesCheck()
                    .matches(""SELECT IF(regionkey=2, nationkey + 100, nationkey) nationkey, name, regionkey, comment FROM tpch.tiny.nation"");
        }
    }
"
"    @Test
    public void testCreateSchema()
    {
        String schemaName = ""test_schema_create_"" + randomTableSuffix();
        if (!hasBehavior(SUPPORTS_CREATE_SCHEMA)) {
            assertQueryFails(""CREATE SCHEMA "" + schemaName, ""This connector does not support creating schemas"");
            return;
        }

        assertUpdate(""CREATE SCHEMA "" + schemaName);
        assertThat(query(""SHOW SCHEMAS""))
                .skippingTypesCheck()
                .containsAll(format(""VALUES '%s', '%s'"", getSession().getSchema().orElseThrow(), schemaName));
        assertUpdate(""DROP SCHEMA "" + schemaName);
    }
"
"  @Test
  public void testTopNWithDistinctCountAgg() throws Exception
  {
    TopNQueryEngine engine = new TopNQueryEngine(pool);

    IncrementalIndex index = new OnheapIncrementalIndex.Builder()
        .setIndexSchema(
            new IncrementalIndexSchema.Builder()
                .withQueryGranularity(Granularities.SECOND)
                .withMetrics(new CountAggregatorFactory(""cnt""))
                .build()
        )
        .setMaxRowCount(1000)
        .build();

    String visitor_id = ""visitor_id"";
    String client_type = ""client_type"";
    DateTime time = DateTimes.of(""2016-03-04T00:00:00.000Z"");
    long timestamp = time.getMillis();
    index.add(
        new MapBasedInputRow(
            timestamp,
            Lists.newArrayList(visitor_id, client_type),
            ImmutableMap.of(visitor_id, ""0"", client_type, ""iphone"")
        )
    );
    index.add(
        new MapBasedInputRow(
            timestamp,
            Lists.newArrayList(visitor_id, client_type),
            ImmutableMap.of(visitor_id, ""1"", client_type, ""iphone"")
        )
    );
    index.add(
        new MapBasedInputRow(
            timestamp,
            Lists.newArrayList(visitor_id, client_type),
            ImmutableMap.of(visitor_id, ""2"", client_type, ""android"")
        )
    );

    TopNQuery query = new TopNQueryBuilder().dataSource(QueryRunnerTestHelper.DATA_SOURCE)
                          .granularity(QueryRunnerTestHelper.ALL_GRAN)
                          .intervals(QueryRunnerTestHelper.FULL_ON_INTERVAL_SPEC)
                          .dimension(client_type)
                          .metric(""UV"")
                          .threshold(10)
                          .aggregators(
                              QueryRunnerTestHelper.ROWS_COUNT,
                              new DistinctCountAggregatorFactory(""UV"", visitor_id, null)
                          )
                          .build();

    final Iterable<Result<TopNResultValue>> results =
        engine.query(query, new IncrementalIndexStorageAdapter(index), null).toList();

    List<Result<TopNResultValue>> expectedResults = Collections.singletonList(
        new Result<>(
            time,
            new TopNResultValue(
                Arrays.<Map<String, Object>>asList(
                    ImmutableMap.of(
                        client_type, ""iphone"",
                        ""UV"", 2L,
                        ""rows"", 2L
                    ),
                    ImmutableMap.of(
                        client_type, ""android"",
                        ""UV"", 1L,
                        ""rows"", 1L
                    )
                )
            )
        )
    );
    TestHelper.assertExpectedResults(expectedResults, results);
  }
"
"  @Test
  public void testTimeseriesWithDistinctCountAgg() throws Exception
  {
    TimeseriesQueryEngine engine = new TimeseriesQueryEngine();

    IncrementalIndex index = new OnheapIncrementalIndex.Builder()
        .setIndexSchema(
            new IncrementalIndexSchema.Builder()
                .withQueryGranularity(Granularities.SECOND)
                .withMetrics(new CountAggregatorFactory(""cnt""))
                .build()
        )
        .setMaxRowCount(1000)
        .build();

    String visitor_id = ""visitor_id"";
    String client_type = ""client_type"";
    DateTime time = DateTimes.of(""2016-03-04T00:00:00.000Z"");
    long timestamp = time.getMillis();
    index.add(
        new MapBasedInputRow(
            timestamp,
            Lists.newArrayList(visitor_id, client_type),
            ImmutableMap.of(visitor_id, ""0"", client_type, ""iphone"")
        )
    );
    index.add(
        new MapBasedInputRow(
            timestamp,
            Lists.newArrayList(visitor_id, client_type),
            ImmutableMap.of(visitor_id, ""1"", client_type, ""iphone"")
        )
    );
    index.add(
        new MapBasedInputRow(
            timestamp,
            Lists.newArrayList(visitor_id, client_type),
            ImmutableMap.of(visitor_id, ""2"", client_type, ""android"")
        )
    );

    TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()
                                  .dataSource(QueryRunnerTestHelper.DATA_SOURCE)
                                  .granularity(QueryRunnerTestHelper.ALL_GRAN)
                                  .intervals(QueryRunnerTestHelper.FULL_ON_INTERVAL_SPEC)
                                  .aggregators(
                                      Lists.newArrayList(
                                          QueryRunnerTestHelper.ROWS_COUNT,
                                          new DistinctCountAggregatorFactory(""UV"", visitor_id, null)
                                      )
                                  )
                                  .build();

    final Iterable<Result<TimeseriesResultValue>> results =
        engine.process(query, new IncrementalIndexStorageAdapter(index)).toList();

    List<Result<TimeseriesResultValue>> expectedResults = Collections.singletonList(
        new Result<>(
            time,
            new TimeseriesResultValue(
                ImmutableMap.of(""UV"", 3, ""rows"", 3L)
            )
        )
    );
    TestHelper.assertExpectedResults(expectedResults, results);
  }
"
"  @Test
  public void testGroupByWithDistinctCountAgg() throws Exception
  {
    IncrementalIndex index = new OnheapIncrementalIndex.Builder()
        .setIndexSchema(
            new IncrementalIndexSchema.Builder()
                .withQueryGranularity(Granularities.SECOND)
                .withMetrics(new CountAggregatorFactory(""cnt""))
                .build()
        )
        .setConcurrentEventAdd(true)
        .setMaxRowCount(1000)
        .build();

    String visitor_id = ""visitor_id"";
    String client_type = ""client_type"";
    long timestamp = DateTimes.of(""2010-01-01"").getMillis();
    index.add(
        new MapBasedInputRow(
            timestamp,
            Lists.newArrayList(visitor_id, client_type),
            ImmutableMap.of(visitor_id, ""0"", client_type, ""iphone"")
        )
    );
    index.add(
        new MapBasedInputRow(
            timestamp + 1,
            Lists.newArrayList(visitor_id, client_type),
            ImmutableMap.of(visitor_id, ""1"", client_type, ""iphone"")
        )
    );
    index.add(
        new MapBasedInputRow(
            timestamp + 2,
            Lists.newArrayList(visitor_id, client_type),
            ImmutableMap.of(visitor_id, ""2"", client_type, ""android"")
        )
    );

    GroupByQuery query = new GroupByQuery.Builder()
        .setDataSource(QueryRunnerTestHelper.DATA_SOURCE)
        .setGranularity(QueryRunnerTestHelper.ALL_GRAN)
        .setDimensions(new DefaultDimensionSpec(
            client_type,
            client_type
        ))
        .setInterval(QueryRunnerTestHelper.FULL_ON_INTERVAL_SPEC)
        .setLimitSpec(
            new DefaultLimitSpec(
                Collections.singletonList(new OrderByColumnSpec(client_type, OrderByColumnSpec.Direction.DESCENDING)),
                10
            )
        )
        .setAggregatorSpecs(QueryRunnerTestHelper.ROWS_COUNT, new DistinctCountAggregatorFactory(""UV"", visitor_id, null))
        .build();
    final Segment incrementalIndexSegment = new IncrementalIndexSegment(index, null);

    Iterable<ResultRow> results = GroupByQueryRunnerTestHelper.runQuery(
        factory,
        factory.createRunner(incrementalIndexSegment),
        query
    );

    List<ResultRow> expectedResults = Arrays.asList(
        GroupByQueryRunnerTestHelper.createExpectedRow(
            query,
            ""1970-01-01T00:00:00.000Z"",
            client_type, ""iphone"",
            ""UV"", 2L,
            ""rows"", 2L
        ),
        GroupByQueryRunnerTestHelper.createExpectedRow(
            query,
            ""1970-01-01T00:00:00.000Z"",
            client_type, ""android"",
            ""UV"", 1L,
            ""rows"", 1L
        )
    );
    TestHelper.assertExpectedObjects(expectedResults, results, ""distinct-count"");
  }
"
"  @Test
  public void testClusterPriority() throws IOException
  {
    ObjectMapper mapper = new ObjectMapper();
    RedisCacheConfig fromJson = mapper.readValue(""{\""expiration\"": 1000,""
                                                 + ""\""cluster\"": {""
                                                 + ""\""nodes\"": \""127.0.0.1:6379\""""
                                                 + ""},""
                                                 + ""\""host\"": \""127.0.0.1\"",""
                                                 + ""\""port\"": 6379""
                                                 + ""}"", RedisCacheConfig.class);

    try (Cache cache = RedisCacheFactory.create(fromJson)) {
      Assert.assertTrue(cache instanceof RedisClusterCache);
    }
  }
"
"  @Test
  public void testClusterInvalidNode() throws IOException
  {
    ObjectMapper mapper = new ObjectMapper();
    RedisCacheConfig fromJson = mapper.readValue(
        ""{\""expiration\"": 1000,""
        + ""\""cluster\"": {""
        + ""\""nodes\"": \""127.0.0.1\"""" //<===Invalid Node
        + ""}""
        + ""}"",
        RedisCacheConfig.class
    );

    expectedException.expect(new ExceptionMatcher(
        IAE.class,
        new StartWithMatcher(""Invalid redis cluster"")
    ));
    RedisCacheFactory.create(fromJson);
  }
"
"  @Test
  public void testClusterLackOfPort() throws IOException
  {
    ObjectMapper mapper = new ObjectMapper();
    RedisCacheConfig fromJson = mapper.readValue(
        ""{\""expiration\"":1000,""
        + ""\""cluster\"": {""
        + ""\""nodes\"": \""127.0.0.1:\""""
        + ""}""
        + ""}"",
        RedisCacheConfig.class
    );

    expectedException.expect(new ExceptionMatcher(
        IAE.class,
        new StartWithMatcher(""Invalid port"")
    ));
    RedisCacheFactory.create(fromJson);
  }
"
"  @Test
  public void testInvalidClusterNodePort0() throws IOException
  {
    ObjectMapper mapper = new ObjectMapper();
    RedisCacheConfig fromJson = mapper.readValue(
        ""{\""expiration\"": 1000,""
        + ""\""cluster\"": {""
        + ""\""nodes\"": \""127.0.0.1:0\"""" //<===Invalid Port
        + ""}""
        + ""}"",
        RedisCacheConfig.class
    );

    expectedException.expect(new ExceptionMatcher(
        IAE.class,
        new ContainsMatcher(""Invalid port"")
    ));
    RedisCacheFactory.create(fromJson);
  }
"
"  @Test
  public void testInvalidClusterNodePort65536() throws IOException
  {
    ObjectMapper mapper = new ObjectMapper();
    RedisCacheConfig fromJson = mapper.readValue(
        ""{\""expiration\"": 1000,""
        + ""\""cluster\"": {""
        + ""\""nodes\"": \""127.0.0.1:65536\"""" //<===Invalid Port
        + ""}""
        + ""}"",
        RedisCacheConfig.class
    );

    expectedException.expect(new ExceptionMatcher(
        IAE.class,
        new ContainsMatcher(""Invalid port"")
    ));
    RedisCacheFactory.create(fromJson);
  }
"
"  @Test
  public void testNoClusterAndHost() throws IOException
  {
    ObjectMapper mapper = new ObjectMapper();
    RedisCacheConfig fromJson = mapper.readValue(
        ""{\""expiration\"": 1000""
        + ""}"",
        RedisCacheConfig.class
    );

    expectedException.expect(new ExceptionMatcher(
        IAE.class,
        new ContainsMatcher(""no redis server"")
    ));
    RedisCacheFactory.create(fromJson);
  }
"
"  @Test
  public void testConfig() throws JsonProcessingException
  {
    ObjectMapper mapper = new ObjectMapper();
    RedisCacheConfig fromJson = mapper.readValue(""{\""expiration\"": 1000}"", RedisCacheConfig.class);
    Assert.assertEquals(1, fromJson.getExpiration().getSeconds());

    fromJson = mapper.readValue(""{\""expiration\"": \""PT1H\""}"", RedisCacheConfig.class);
    Assert.assertEquals(3600, fromJson.getExpiration().getSeconds());
  }
"
"  @Test
  public void testCache()
  {
    Assert.assertNull(cache.get(new Cache.NamedKey(""the"", HI)));

    Cache.NamedKey key1 = new Cache.NamedKey(""the"", HI);
    Cache.NamedKey key2 = new Cache.NamedKey(""the"", HO);
    Cache.NamedKey key3 = new Cache.NamedKey(""a"", HI);
    Cache.NamedKey notExist = new Cache.NamedKey(""notExist"", HI);

    //test put and get
    cache.put(key1, new byte[]{1, 2, 3, 4});
    cache.put(key2, new byte[]{2, 3, 4, 5});
    cache.put(key3, new byte[]{3, 4, 5, 6});
    Assert.assertEquals(0x01020304, Ints.fromByteArray(cache.get(key1)));
    Assert.assertEquals(0x02030405, Ints.fromByteArray(cache.get(key2)));
    Assert.assertEquals(0x03040506, Ints.fromByteArray(cache.get(key3)));
    Assert.assertEquals(0x03040506, Ints.fromByteArray(cache.get(key3)));
    Assert.assertNull(cache.get(notExist));

    this.mgetCount.set(0);

    //test multi get
    Map<Cache.NamedKey, byte[]> result = cache.getBulk(
        Lists.newArrayList(
            key1,
            key2,
            key3,
            notExist
        )
    );

    // these 4 keys are distributed among different nodes, so there should be 4 times call of MGET
    Assert.assertEquals(mgetCount.get(), 4);
    Assert.assertEquals(result.size(), 3);
    Assert.assertEquals(0x01020304, Ints.fromByteArray(result.get(key1)));
    Assert.assertEquals(0x02030405, Ints.fromByteArray(result.get(key2)));
    Assert.assertEquals(0x03040506, Ints.fromByteArray(result.get(key3)));
  }
"
"  @Test
  public void testBasicInjection() throws Exception
  {
    String json = ""{ \""host\"": \""localhost\"", \""port\"": 6379, \""expiration\"": 3600}"";
    final RedisCacheConfig config = new ObjectMapper().readValue(json, RedisCacheConfig.class);

    Injector injector = Initialization.makeInjectorWithModules(
        GuiceInjectors.makeStartupInjector(), ImmutableList.of(
            binder -> {
              binder.bindConstant().annotatedWith(Names.named(""serviceName"")).to(""druid/test/redis"");
              binder.bindConstant().annotatedWith(Names.named(""servicePort"")).to(0);
              binder.bindConstant().annotatedWith(Names.named(""tlsServicePort"")).to(-1);

              binder.bindConstant().annotatedWith(Names.named(""host"")).to(""localhost"");
              binder.bindConstant().annotatedWith(Names.named(""port"")).to(6379);

              binder.bind(RedisCacheConfig.class).toInstance(config);
              binder.bind(Cache.class).toProvider(RedisCacheProviderWithConfig.class).in(ManageLifecycle.class);
            }
        )
    );
    Lifecycle lifecycle = injector.getInstance(Lifecycle.class);
    lifecycle.start();
    try {
      Cache cache = injector.getInstance(Cache.class);
      Assert.assertEquals(RedisStandaloneCache.class, cache.getClass());
    }
    finally {
      lifecycle.stop();
    }
  }
"
"  @Test
  public void testSimpleInjection()
  {
    final String uuid = UUID.randomUUID().toString();
    System.setProperty(uuid + "".type"", ""redis"");
    final Injector injector = Initialization.makeInjectorWithModules(
        GuiceInjectors.makeStartupInjector(), ImmutableList.of(
            binder -> {
              binder.bindConstant().annotatedWith(Names.named(""serviceName"")).to(""druid/test/redis"");
              binder.bindConstant().annotatedWith(Names.named(""servicePort"")).to(0);
              binder.bindConstant().annotatedWith(Names.named(""tlsServicePort"")).to(-1);

              binder.bind(Cache.class).toProvider(CacheProvider.class);
              JsonConfigProvider.bind(binder, uuid, CacheProvider.class);
            }
        )
    );
    final CacheProvider cacheProvider = injector.getInstance(CacheProvider.class);
    Assert.assertNotNull(cacheProvider);
    Assert.assertEquals(RedisCacheProvider.class, cacheProvider.getClass());
  }
"
"  @Test
  public void testSanity()
  {
    Assert.assertNull(cache.get(new Cache.NamedKey(""a"", HI)));
    put(cache, ""a"", HI, 0);
    Assert.assertEquals(0, get(cache, ""a"", HI));
    Assert.assertNull(cache.get(new Cache.NamedKey(""the"", HI)));

    put(cache, ""the"", HI, 1);
    Assert.assertEquals(0, get(cache, ""a"", HI));
    Assert.assertEquals(1, get(cache, ""the"", HI));

    put(cache, ""the"", HO, 10);
    Assert.assertEquals(0, get(cache, ""a"", HI));
    Assert.assertNull(cache.get(new Cache.NamedKey(""a"", HO)));
    Assert.assertEquals(1, get(cache, ""the"", HI));
    Assert.assertEquals(10, get(cache, ""the"", HO));

    cache.close(""the"");
    Assert.assertEquals(0, get(cache, ""a"", HI));
    Assert.assertNull(cache.get(new Cache.NamedKey(""a"", HO)));
  }
"
"  @Test
  public void testGetBulk()
  {
    Assert.assertNull(cache.get(new Cache.NamedKey(""the"", HI)));

    put(cache, ""the"", HI, 1);
    put(cache, ""the"", HO, 10);

    Cache.NamedKey key1 = new Cache.NamedKey(""the"", HI);
    Cache.NamedKey key2 = new Cache.NamedKey(""the"", HO);
    Cache.NamedKey key3 = new Cache.NamedKey(""a"", HI);

    Map<Cache.NamedKey, byte[]> result = cache.getBulk(
        Lists.newArrayList(
            key1,
            key2,
            key3
        )
    );

    Assert.assertEquals(1, Ints.fromByteArray(result.get(key1)));
    Assert.assertEquals(10, Ints.fromByteArray(result.get(key2)));
    Assert.assertEquals(null, result.get(key3));
  }
"
"  @Test
  public void testParse(String name, String input, Parsed expected)
  {
    Parser<String, Object> parser = new InfluxParser(null);
    Map<String, Object> parsed = parser.parseToMap(input);
    MatcherAssert.assertThat(
        ""correct measurement name"",
        parsed.get(""measurement""),
        Matchers.equalTo(expected.measurement)
    );
    MatcherAssert.assertThat(
        ""correct timestamp"",
        parsed.get(InfluxParser.TIMESTAMP_KEY),
        Matchers.equalTo(expected.timestamp)
    );
    expected.kv.forEach((k, v) -> MatcherAssert.assertThat(""correct field "" + k, parsed.get(k), Matchers.equalTo(v)));
    parsed.remove(""measurement"");
    parsed.remove(InfluxParser.TIMESTAMP_KEY);
    MatcherAssert.assertThat(""No extra keys in parsed data"", parsed.keySet(), Matchers.equalTo(expected.kv.keySet()));
  }
"
"  @Test
  public void testParseWhitelistPass()
  {
    Parser<String, Object> parser = new InfluxParser(Sets.newHashSet(""cpu""));
    String input = ""cpu,host=foo.bar.baz,region=us-east,application=echo pct_idle=99.3,pct_user=88.8,m1_load=2 1465839830100400200"";
    Map<String, Object> parsed = parser.parseToMap(input);
    MatcherAssert.assertThat(parsed.get(""measurement""), Matchers.equalTo(""cpu""));
  }
"
"  @Test
  public void testParseWhitelistFail()
  {
    Parser<String, Object> parser = new InfluxParser(Sets.newHashSet(""mem""));
    String input = ""cpu,host=foo.bar.baz,region=us-east,application=echo pct_idle=99.3,pct_user=88.8,m1_load=2 1465839830100400200"";
    try {
      parser.parseToMap(input);
    }
    catch (ParseException t) {
      MatcherAssert.assertThat(t, Matchers.isA(ParseException.class));
      return;
    }

    Assert.fail(""Exception not thrown"");
  }
"
"  @Test
  public void testParseFailures(Pair<String, String> testCase)
  {
    Parser<String, Object> parser = new InfluxParser(null);
    try {
      parser.parseToMap(testCase.rhs);
    }
    catch (ParseException t) {
      MatcherAssert.assertThat(t, Matchers.isA(ParseException.class));
      return;
    }

    Assert.fail(testCase.rhs + "": exception not thrown"");
  }
"
"  @Test
  public void testCheckSegments() throws IOException
  {
    Set<DataSegment> baseSegments = Sets.newHashSet(
        new DataSegment(
            ""base"",
            Intervals.of(""2015-01-01T00Z/2015-01-02T00Z""),
            ""2015-01-02"",
            ImmutableMap.of(),
            ImmutableList.of(""dim1"", ""dim2""),
            ImmutableList.of(""m1""),
            new HashBasedNumberedShardSpec(0, 1, 0, 1, null, null, null),
            9,
            1024
        ),
        new DataSegment(
            ""base"",
            Intervals.of(""2015-01-02T00Z/2015-01-03T00Z""),
            ""2015-01-03"",
            ImmutableMap.of(),
            ImmutableList.of(""dim1"", ""dim2""),
            ImmutableList.of(""m1""),
            new HashBasedNumberedShardSpec(0, 1, 0, 1, null, null, null),
            9,
            1024
        ),
        new DataSegment(
            ""base"",
            Intervals.of(""2015-01-03T00Z/2015-01-04T00Z""),
            ""2015-01-04"",
            ImmutableMap.of(),
            ImmutableList.of(""dim1"", ""dim2""),
            ImmutableList.of(""m1""),
            new HashBasedNumberedShardSpec(0, 1, 0, 1, null, null, null),
            9,
            1024
        )
    );
    Set<DataSegment> derivativeSegments = Sets.newHashSet(
        new DataSegment(
            derivativeDatasourceName,
            Intervals.of(""2015-01-01T00Z/2015-01-02T00Z""),
            ""2015-01-02"",
            ImmutableMap.of(),
            ImmutableList.of(""dim1"", ""dim2""),
            ImmutableList.of(""m1""),
            new HashBasedNumberedShardSpec(0, 1, 0, 1, null, null, null),
            9,
            1024
        ),
        new DataSegment(
            derivativeDatasourceName,
            Intervals.of(""2015-01-02T00Z/2015-01-03T00Z""),
            ""3015-01-01"",
            ImmutableMap.of(),
            ImmutableList.of(""dim1"", ""dim2""),
            ImmutableList.of(""m1""),
            new HashBasedNumberedShardSpec(0, 1, 0, 1, null, null, null),
            9,
            1024
        )
    );
    indexerMetadataStorageCoordinator.announceHistoricalSegments(baseSegments);
    indexerMetadataStorageCoordinator.announceHistoricalSegments(derivativeSegments);
    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();
    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.absent()).anyTimes();
    EasyMock.expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of()).anyTimes();
    Pair<SortedMap<Interval, String>, Map<Interval, List<DataSegment>>> toBuildInterval = supervisor.checkSegments();
    Set<Interval> expectedToBuildInterval = Sets.newHashSet(Intervals.of(""2015-01-01T00Z/2015-01-02T00Z""));
    Map<Interval, List<DataSegment>> expectedSegments = new HashMap<>();
    expectedSegments.put(
        Intervals.of(""2015-01-01T00Z/2015-01-02T00Z""),
        Collections.singletonList(
            new DataSegment(
                ""base"",
                Intervals.of(""2015-01-01T00Z/2015-01-02T00Z""),
                ""2015-01-02"",
                ImmutableMap.of(),
                ImmutableList.of(""dim1"", ""dim2""),
                ImmutableList.of(""m1""),
                new HashBasedNumberedShardSpec(0, 1, 0, 1, null, null, null),
                9,
                1024
            )
        )
    );
    expectedSegments.put(
        Intervals.of(""2015-01-02T00Z/2015-01-03T00Z""),
        Collections.singletonList(
            new DataSegment(
                ""base"",
                Intervals.of(""2015-01-02T00Z/2015-01-03T00Z""),
                ""2015-01-03"",
                ImmutableMap.of(),
                ImmutableList.of(""dim1"", ""dim2""),
                ImmutableList.of(""m1""),
                new HashBasedNumberedShardSpec(0, 1, 0, 1, null, null, null),
                9,
                1024
            )
        )
    );
    Assert.assertEquals(expectedToBuildInterval, toBuildInterval.lhs.keySet());
    Assert.assertEquals(expectedSegments, toBuildInterval.rhs);
  }
"
"  @Test
  public void testCheckSegmentsAndSubmitTasks() throws IOException
  {
    Set<DataSegment> baseSegments = Sets.newHashSet(
        new DataSegment(
            ""base"",
            Intervals.of(""2015-01-02T00Z/2015-01-03T00Z""),
            ""2015-01-03"",
            ImmutableMap.of(),
            ImmutableList.of(""dim1"", ""dim2""),
            ImmutableList.of(""m1""),
            new HashBasedNumberedShardSpec(0, 1, 0, 1, null, null, null),
            9,
            1024
        )
    );
    indexerMetadataStorageCoordinator.announceHistoricalSegments(baseSegments);
    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();
    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.absent()).anyTimes();
    EasyMock.expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of()).anyTimes();
    EasyMock.expect(taskStorage.getStatus(""test_task1""))
            .andReturn(Optional.of(TaskStatus.failure(""test_task1"", ""Dummy task status failure err message"")))
            .anyTimes();
    EasyMock.expect(taskStorage.getStatus(""test_task2""))
            .andReturn(Optional.of(TaskStatus.running(""test_task2"")))
            .anyTimes();
    EasyMock.replay(taskStorage);

    Pair<Map<Interval, HadoopIndexTask>, Map<Interval, String>> runningTasksPair = supervisor.getRunningTasks();
    Map<Interval, HadoopIndexTask> runningTasks = runningTasksPair.lhs;
    Map<Interval, String> runningVersion = runningTasksPair.rhs;

    DataSchema dataSchema = new DataSchema(
        ""test_datasource"",
        null,
        null,
        null,
        TransformSpec.NONE,
        objectMapper
    );
    HadoopIOConfig hadoopIOConfig = new HadoopIOConfig(new HashMap<>(), null, null);
    HadoopIngestionSpec spec = new HadoopIngestionSpec(dataSchema, hadoopIOConfig, null);
    HadoopIndexTask task1 = new HadoopIndexTask(
        ""test_task1"",
        spec,
        null,
        null,
        null,
        objectMapper,
        null,
        null,
        null
    );
    runningTasks.put(Intervals.of(""2015-01-01T00Z/2015-01-02T00Z""), task1);
    runningVersion.put(Intervals.of(""2015-01-01T00Z/2015-01-02T00Z""), ""test_version1"");

    HadoopIndexTask task2 = new HadoopIndexTask(
        ""test_task2"",
        spec,
        null,
        null,
        null,
        objectMapper,
        null,
        null,
        null
    );
    runningTasks.put(Intervals.of(""2015-01-02T00Z/2015-01-03T00Z""), task2);
    runningVersion.put(Intervals.of(""2015-01-02T00Z/2015-01-03T00Z""), ""test_version2"");

    supervisor.checkSegmentsAndSubmitTasks();

    Map<Interval, HadoopIndexTask> expectedRunningTasks = new HashMap<>();
    Map<Interval, String> expectedRunningVersion = new HashMap<>();
    expectedRunningTasks.put(Intervals.of(""2015-01-02T00Z/2015-01-03T00Z""), task2);
    expectedRunningVersion.put(Intervals.of(""2015-01-02T00Z/2015-01-03T00Z""), ""test_version2"");

    Assert.assertEquals(expectedRunningTasks, runningTasks);
    Assert.assertEquals(expectedRunningVersion, runningVersion);

  }
"
"  @Test
  public void testCreateTask()
  {
    List<DataSegment> baseSegments = Collections.singletonList(
        new DataSegment(
            ""base"",
            Intervals.of(""2015-01-02T00Z/2015-01-03T00Z""),
            ""2015-01-03"",
            ImmutableMap.of(),
            ImmutableList.of(""dim1"", ""dim2""),
            ImmutableList.of(""m1""),
            new HashBasedNumberedShardSpec(0, 1, 0, 1, null, null, null),
            9,
            1024
        )
    );

    HadoopIndexTask task = spec.createTask(
        Intervals.of(""2015-01-02T00Z/2015-01-03T00Z""),
        ""2015-01-03"",
        baseSegments
    );

    Assert.assertNotNull(task);
  }
"
"  @Test
  public void testSuspendedDoesntRun()
  {
    MaterializedViewSupervisorSpec suspended = new MaterializedViewSupervisorSpec(
        ""base"",
        new DimensionsSpec(Collections.singletonList(new StringDimensionSchema(""dim"")), null, null),
        new AggregatorFactory[]{new LongSumAggregatorFactory(""m1"", ""m1"")},
        HadoopTuningConfig.makeDefaultTuningConfig(),
        null,
        null,
        null,
        null,
        null,
        true,
        objectMapper,
        taskMaster,
        taskStorage,
        metadataSupervisorManager,
        sqlSegmentsMetadataManager,
        indexerMetadataStorageCoordinator,
        new MaterializedViewTaskConfig(),
        EasyMock.createMock(AuthorizerMapper.class),
        EasyMock.createMock(ChatHandlerProvider.class),
        new SupervisorStateManagerConfig()
    );
    MaterializedViewSupervisor supervisor = (MaterializedViewSupervisor) suspended.createSupervisor();

    // mock IndexerSQLMetadataStorageCoordinator to ensure that retrieveDataSourceMetadata is not called
    // which will be true if truly suspended, since this is the first operation of the 'run' method otherwise
    IndexerSQLMetadataStorageCoordinator mock = EasyMock.createMock(IndexerSQLMetadataStorageCoordinator.class);
    EasyMock.expect(mock.retrieveDataSourceMetadata(suspended.getDataSourceName()))
            .andAnswer(() -> {
              Assert.fail();
              return null;
            })
            .anyTimes();

    EasyMock.replay(mock);
    supervisor.run();
  }
"
"  @Test
  public void testEmptyBaseDataSource()
  {
    expectedException.expect(CoreMatchers.instanceOf(IllegalArgumentException.class));
    expectedException.expectMessage(
        ""baseDataSource cannot be null or empty. Please provide a baseDataSource.""
    );
    String baseDataSource = """";
    Set<String> dims = Sets.newHashSet(""dim1"", ""dim2"", ""dim3"");
    Set<String> metrics = Sets.newHashSet(""cost"");
    DerivativeDataSourceMetadata metadata = new DerivativeDataSourceMetadata(baseDataSource, dims, metrics);
  }
"
"  @Test
  public void testNullBaseDataSource()
  {
    expectedException.expect(CoreMatchers.instanceOf(IllegalArgumentException.class));
    expectedException.expectMessage(
        ""baseDataSource cannot be null or empty. Please provide a baseDataSource.""
    );
    String baseDataSource = null;
    Set<String> dims = Sets.newHashSet(""dim1"", ""dim2"", ""dim3"");
    Set<String> metrics = Sets.newHashSet(""cost"");
    DerivativeDataSourceMetadata metadata = new DerivativeDataSourceMetadata(baseDataSource, dims, metrics);
  }
"
"  @Test
  public void testSupervisorSerialization() throws IOException
  {
    String supervisorStr = ""{\n"" +
                           ""  \""type\"" : \""derivativeDataSource\"",\n"" +
                           ""  \""baseDataSource\"": \""wikiticker\"",\n"" +
                           ""  \""dimensionsSpec\"":{\n"" +
                           ""            \""dimensions\"" : [\n"" +
                           ""              \""isUnpatrolled\"",\n"" +
                           ""              \""metroCode\"",\n"" +
                           ""              \""namespace\"",\n"" +
                           ""              \""page\"",\n"" +
                           ""              \""regionIsoCode\"",\n"" +
                           ""              \""regionName\"",\n"" +
                           ""              \""user\""\n"" +
                           ""            ]\n"" +
                           ""          },\n"" +
                           ""    \""metricsSpec\"" : [\n"" +
                           ""        {\n"" +
                           ""          \""name\"" : \""count\"",\n"" +
                           ""          \""type\"" : \""count\""\n"" +
                           ""        },\n"" +
                           ""        {\n"" +
                           ""          \""name\"" : \""added\"",\n"" +
                           ""          \""type\"" : \""longSum\"",\n"" +
                           ""          \""fieldName\"" : \""added\""\n"" +
                           ""        }\n"" +
                           ""      ],\n"" +
                           ""  \""tuningConfig\"": {\n"" +
                           ""      \""type\"" : \""hadoop\""\n"" +
                           ""  }\n"" +
                           ""}"";
    MaterializedViewSupervisorSpec expected = new MaterializedViewSupervisorSpec(
        ""wikiticker"",
        new DimensionsSpec(
            Lists.newArrayList(
                new StringDimensionSchema(""isUnpatrolled""),
                new StringDimensionSchema(""metroCode""),
                new StringDimensionSchema(""namespace""),
                new StringDimensionSchema(""page""),
                new StringDimensionSchema(""regionIsoCode""),
                new StringDimensionSchema(""regionName""),
                new StringDimensionSchema(""user"")
            ),
            null,
            null
        ),
        new AggregatorFactory[]{
            new CountAggregatorFactory(""count""),
            new LongSumAggregatorFactory(""added"", ""added"")
        },
        HadoopTuningConfig.makeDefaultTuningConfig(),
        null,
        null,
        null,
        null,
        null,
        false,
        objectMapper,
        null,
        null,
        null,
        null,
        null,
        new MaterializedViewTaskConfig(),
        EasyMock.createMock(AuthorizerMapper.class),
        new NoopChatHandlerProvider(),
        new SupervisorStateManagerConfig()
    );
    MaterializedViewSupervisorSpec spec = objectMapper.readValue(supervisorStr, MaterializedViewSupervisorSpec.class);
    Assert.assertEquals(expected.getBaseDataSource(), spec.getBaseDataSource());
    Assert.assertEquals(expected.getId(), spec.getId());
    Assert.assertEquals(expected.getDataSourceName(), spec.getDataSourceName());
    Assert.assertEquals(expected.getDimensions(), spec.getDimensions());
    Assert.assertEquals(expected.getMetrics(), spec.getMetrics());
  }
"
"  @Test
  public void testMaterializedViewSupervisorSpecCreated()
  {
    Exception ex = null;

    try {
      MaterializedViewSupervisorSpec spec = new MaterializedViewSupervisorSpec(
              ""wikiticker"",
              new DimensionsSpec(
                      Lists.newArrayList(
                              new StringDimensionSchema(""isUnpatrolled""),
                              new StringDimensionSchema(""metroCode""),
                              new StringDimensionSchema(""namespace""),
                              new StringDimensionSchema(""page""),
                              new StringDimensionSchema(""regionIsoCode""),
                              new StringDimensionSchema(""regionName""),
                              new StringDimensionSchema(""user"")
                      ),
                      null,
                      null
              ),
              new AggregatorFactory[]{
                  new CountAggregatorFactory(""count""),
                  new LongSumAggregatorFactory(""added"", ""added"")
              },
              HadoopTuningConfig.makeDefaultTuningConfig(),
              null,
              null,
              null,
              null,
              null,
              false,
              objectMapper,
              null,
              null,
              null,
              null,
              null,
              new MaterializedViewTaskConfig(),
              EasyMock.createMock(AuthorizerMapper.class),
              new NoopChatHandlerProvider(),
              new SupervisorStateManagerConfig()
      );
      Supervisor supervisor = spec.createSupervisor();
      Assert.assertTrue(supervisor instanceof MaterializedViewSupervisor);

      SupervisorTaskAutoScaler autoscaler = spec.createAutoscaler(supervisor);
      Assert.assertNull(autoscaler);

      try {
        supervisor.computeLagStats();
      }
      catch (Exception e) {
        Assert.assertTrue(e instanceof UnsupportedOperationException);
      }

      try {
        int count = supervisor.getActiveTaskGroupsCount();
      }
      catch (Exception e) {
        Assert.assertTrue(e instanceof UnsupportedOperationException);
      }

      Callable<Integer> noop = new Callable<Integer>() {
        @Override
        public Integer call()
        {
          return -1;
        }
"
"  @Test
  public void testSuspendResuume() throws IOException
  {
    String supervisorStr = ""{\n"" +
                           ""  \""type\"" : \""derivativeDataSource\"",\n"" +
                           ""  \""baseDataSource\"": \""wikiticker\"",\n"" +
                           ""  \""dimensionsSpec\"":{\n"" +
                           ""            \""dimensions\"" : [\n"" +
                           ""              \""isUnpatrolled\"",\n"" +
                           ""              \""metroCode\"",\n"" +
                           ""              \""namespace\"",\n"" +
                           ""              \""page\"",\n"" +
                           ""              \""regionIsoCode\"",\n"" +
                           ""              \""regionName\"",\n"" +
                           ""              \""user\""\n"" +
                           ""            ]\n"" +
                           ""          },\n"" +
                           ""    \""metricsSpec\"" : [\n"" +
                           ""        {\n"" +
                           ""          \""name\"" : \""count\"",\n"" +
                           ""          \""type\"" : \""count\""\n"" +
                           ""        },\n"" +
                           ""        {\n"" +
                           ""          \""name\"" : \""added\"",\n"" +
                           ""          \""type\"" : \""longSum\"",\n"" +
                           ""          \""fieldName\"" : \""added\""\n"" +
                           ""        }\n"" +
                           ""      ],\n"" +
                           ""  \""tuningConfig\"": {\n"" +
                           ""      \""type\"" : \""hadoop\""\n"" +
                           ""  }\n"" +
                           ""}"";

    MaterializedViewSupervisorSpec spec = objectMapper.readValue(supervisorStr, MaterializedViewSupervisorSpec.class);
    Assert.assertFalse(spec.isSuspended());

    String suspendedSerialized = objectMapper.writeValueAsString(spec.createSuspendedSpec());
    MaterializedViewSupervisorSpec suspendedSpec = objectMapper.readValue(
        suspendedSerialized,
        MaterializedViewSupervisorSpec.class
    );
    Assert.assertTrue(suspendedSpec.isSuspended());

    String runningSerialized = objectMapper.writeValueAsString(spec.createRunningSpec());
    MaterializedViewSupervisorSpec runningSpec = objectMapper.readValue(
        runningSerialized,
        MaterializedViewSupervisorSpec.class
    );
    Assert.assertFalse(runningSpec.isSuspended());
  }
"
"  @Test
  public void testEmptyBaseDataSource()
  {
    expectedException.expect(CoreMatchers.instanceOf(IllegalArgumentException.class));
    expectedException.expectMessage(
        ""baseDataSource cannot be null or empty. Please provide a baseDataSource.""
    );
    //noinspection ResultOfObjectAllocationIgnored (this method call will trigger the expected exception)
    new MaterializedViewSupervisorSpec(
        """",
        new DimensionsSpec(
            Lists.newArrayList(
                new StringDimensionSchema(""isUnpatrolled""),
                new StringDimensionSchema(""metroCode""),
                new StringDimensionSchema(""namespace""),
                new StringDimensionSchema(""page""),
                new StringDimensionSchema(""regionIsoCode""),
                new StringDimensionSchema(""regionName""),
                new StringDimensionSchema(""user"")
            ),
            null,
            null
        ),
        new AggregatorFactory[]{
            new CountAggregatorFactory(""count""),
            new LongSumAggregatorFactory(""added"", ""added"")
        },
        HadoopTuningConfig.makeDefaultTuningConfig(),
        null,
        null,
        null,
        null,
        null,
        false,
        objectMapper,
        null,
        null,
        null,
        null,
        null,
        new MaterializedViewTaskConfig(),
        EasyMock.createMock(AuthorizerMapper.class),
        new NoopChatHandlerProvider(),
        new SupervisorStateManagerConfig()
    );
  }
"
"  @Test
  public void testNullBaseDataSource()
  {
    expectedException.expect(CoreMatchers.instanceOf(IllegalArgumentException.class));
    expectedException.expectMessage(
        ""baseDataSource cannot be null or empty. Please provide a baseDataSource.""
    );
    //noinspection ResultOfObjectAllocationIgnored (this method call will trigger the expected exception)
    new MaterializedViewSupervisorSpec(
        null,
        new DimensionsSpec(
            Lists.newArrayList(
                new StringDimensionSchema(""isUnpatrolled""),
                new StringDimensionSchema(""metroCode""),
                new StringDimensionSchema(""namespace""),
                new StringDimensionSchema(""page""),
                new StringDimensionSchema(""regionIsoCode""),
                new StringDimensionSchema(""regionName""),
                new StringDimensionSchema(""user"")
            ),
            null,
            null
        ),
        new AggregatorFactory[]{
            new CountAggregatorFactory(""count""),
            new LongSumAggregatorFactory(""added"", ""added"")
        },
        HadoopTuningConfig.makeDefaultTuningConfig(),
        null,
        null,
        null,
        null,
        null,
        false,
        objectMapper,
        null,
        null,
        null,
        null,
        null,
        new MaterializedViewTaskConfig(),
        EasyMock.createMock(AuthorizerMapper.class),
        new NoopChatHandlerProvider(),
        new SupervisorStateManagerConfig()
    );
  }
"
"  @Test(timeout = 15_000)
  public void testKafkaEmitter() throws InterruptedException
  {
    final List<ServiceMetricEvent> serviceMetricEvents = ImmutableList.of(
        ServiceMetricEvent.builder().build(""m1"", 1).build(""service"", ""host"")
    );

    final List<AlertEvent> alertEvents = ImmutableList.of(
        new AlertEvent(""service"", ""host"", ""description"")
    );

    final List<RequestLogEvent> requestLogEvents = ImmutableList.of(
        DefaultRequestLogEventBuilderFactory.instance().createRequestLogEventBuilder(""requests"",
            RequestLogLine.forSql("""", null, DateTimes.nowUtc(), null, new QueryStats(ImmutableMap.of()))
        ).build(""service"", ""host"")
    );

    int totalEvents = serviceMetricEvents.size() + alertEvents.size() + requestLogEvents.size();
    int totalEventsExcludingRequestLogEvents = totalEvents - requestLogEvents.size();

    final CountDownLatch countDownSentEvents = new CountDownLatch(
        requestTopic == null ? totalEventsExcludingRequestLogEvents : totalEvents);
    final KafkaProducer<String, String> producer = EasyMock.createStrictMock(KafkaProducer.class);
    final KafkaEmitter kafkaEmitter = new KafkaEmitter(
        new KafkaEmitterConfig("""", ""metrics"", ""alerts"", requestTopic, ""test-cluster"", null),
        new ObjectMapper()
    )
    {
      @Override
      protected Producer<String, String> setKafkaProducer()
      {
        return producer;
      }

      @Override
      protected void sendToKafka(final String topic, MemoryBoundLinkedBlockingQueue<String> recordQueue,
          Callback callback
      )
      {
        countDownSentEvents.countDown();
        super.sendToKafka(topic, recordQueue, callback);
      }
    };

    EasyMock.expect(producer.send(EasyMock.anyObject(), EasyMock.anyObject())).andReturn(null)
        .times(requestTopic == null ? totalEventsExcludingRequestLogEvents : totalEvents);
    EasyMock.replay(producer);
    kafkaEmitter.start();

    for (Event event : serviceMetricEvents) {
      kafkaEmitter.emit(event);
    }
    for (Event event : alertEvents) {
      kafkaEmitter.emit(event);
    }
    for (Event event : requestLogEvents) {
      kafkaEmitter.emit(event);
    }
    countDownSentEvents.await();

    Assert.assertEquals(0, kafkaEmitter.getMetricLostCount());
    Assert.assertEquals(0, kafkaEmitter.getAlertLostCount());
    Assert.assertEquals(requestTopic == null ? requestLogEvents.size() : 0, kafkaEmitter.getRequestLostCount());
    Assert.assertEquals(0, kafkaEmitter.getInvalidLostCount());

    while (true) {
      try {
        EasyMock.verify(producer);
        break;
      }
      catch (Throwable e) {
        // although the latch may have count down, producer.send may not have been called yet in KafkaEmitter
        // so wait for sometime before verifying the mock
        Thread.sleep(100);
        // just continue
      }
    }
  }
"
"  @Test
  public void testSerDeserKafkaEmitterConfig() throws IOException
  {
    KafkaEmitterConfig kafkaEmitterConfig = new KafkaEmitterConfig(""hostname"", ""metricTest"",
        ""alertTest"", ""requestTest"",
        ""clusterNameTest"", ImmutableMap.<String, String>builder()
        .put(""testKey"", ""testValue"").build()
    );
    String kafkaEmitterConfigString = mapper.writeValueAsString(kafkaEmitterConfig);
    KafkaEmitterConfig kafkaEmitterConfigExpected = mapper.readerFor(KafkaEmitterConfig.class)
        .readValue(kafkaEmitterConfigString);
    Assert.assertEquals(kafkaEmitterConfigExpected, kafkaEmitterConfig);
  }
"
"  @Test
  public void testSerDeserKafkaEmitterConfigNullRequestTopic() throws IOException
  {
    KafkaEmitterConfig kafkaEmitterConfig = new KafkaEmitterConfig(""hostname"", ""metricTest"",
        ""alertTest"", null,
        ""clusterNameTest"", ImmutableMap.<String, String>builder()
        .put(""testKey"", ""testValue"").build()
    );
    String kafkaEmitterConfigString = mapper.writeValueAsString(kafkaEmitterConfig);
    KafkaEmitterConfig kafkaEmitterConfigExpected = mapper.readerFor(KafkaEmitterConfig.class)
        .readValue(kafkaEmitterConfigString);
    Assert.assertEquals(kafkaEmitterConfigExpected, kafkaEmitterConfig);
  }
"
"  @Test
  public void testSerDeNotRequiredKafkaProducerConfig()
  {
    KafkaEmitterConfig kafkaEmitterConfig = new KafkaEmitterConfig(""localhost:9092"", ""metricTest"",
        ""alertTest"", null,
        ""clusterNameTest"", null
    );
    try {
      @SuppressWarnings(""unused"")
      KafkaEmitter emitter = new KafkaEmitter(kafkaEmitterConfig, mapper);
    }
    catch (NullPointerException e) {
      Assert.fail();
    }
  }
"
"  @Test
  public void testJacksonModules()
  {
    Assert.assertTrue(new KafkaEmitterModule().getJacksonModules().isEmpty());
  }
"
"  @Test
  public void testGetThriftClass() throws Exception
  {
    ThriftInputRowParser parser1 = new ThriftInputRowParser(
        parseSpec,
        ""example/book.jar"",
        ""org.apache.druid.data.input.thrift.Book""
    );
    Assert.assertEquals(""org.apache.druid.data.input.thrift.Book"", parser1.getThriftClass().getName());

    ThriftInputRowParser parser2 = new ThriftInputRowParser(parseSpec, null, ""org.apache.druid.data.input.thrift.Book"");
    Assert.assertEquals(""org.apache.druid.data.input.thrift.Book"", parser2.getThriftClass().getName());
  }
"
"  @Test
  public void testParse() throws Exception
  {
    ThriftInputRowParser parser = new ThriftInputRowParser(
        parseSpec,
        ""example/book.jar"",
        ""org.apache.druid.data.input.thrift.Book""
    );
    Book book = new Book().setDate(""2016-08-29"").setPrice(19.9).setTitle(""title"")
                          .setAuthor(new Author().setFirstName(""first"").setLastName(""last""));

    TSerializer serializer;
    byte[] bytes;

    // 1. compact
    serializer = new TSerializer(new TCompactProtocol.Factory());
    bytes = serializer.serialize(book);
    serializationAndTest(parser, bytes);

    // 2. binary + base64
    serializer = new TSerializer(new TBinaryProtocol.Factory());
    serializationAndTest(parser, StringUtils.encodeBase64(serializer.serialize(book)));

    // 3. json
    serializer = new TSerializer(new TJSONProtocol.Factory());
    bytes = serializer.serialize(book);
    serializationAndTest(parser, bytes);
  }
"
"  @Test
  public void testDisableJavaScript()
  {
    final JavaScriptParseSpec parseSpec = new JavaScriptParseSpec(
        new TimestampSpec(""timestamp"", ""auto"", null),
        new DimensionsSpec(
            DimensionsSpec.getDefaultSchemas(
                ImmutableList.of(
                    ""dim1"",
                    ""dim2""
                )
            ),
            null,
            null
        ),
        ""func"",
        new JavaScriptConfig(false)
    );
    ThriftInputRowParser parser = new ThriftInputRowParser(
        parseSpec,
        ""example/book.jar"",
        ""org.apache.druid.data.input.thrift.Book""
    );

    expectedException.expect(CoreMatchers.instanceOf(IllegalStateException.class));
    expectedException.expectMessage(""JavaScript is disabled"");

    //noinspection ResultOfMethodCallIgnored (this method call will trigger the expected exception)
    parser.parseBatch(ByteBuffer.allocate(1)).get(0);
  }
"
"  @Test
  public void testNext()
  {

    List<DimensionSpec> dims = Arrays.asList(
        new DefaultDimensionSpec(GENDER, GENDER),
        new DefaultDimensionSpec(AGE, AGE),
        new DefaultDimensionSpec(COUNTRY, COUNTRY)
    );

    Sequence<RowBucket> dayBuckets = Sequences.simple(Arrays.asList(
        new RowBucket(JAN_1, Arrays.asList(
            new MapBasedRow(JAN_1, DIMS1),
            new MapBasedRow(JAN_1, DIMS2)
        )),
        new RowBucket(JAN_2, Collections.singletonList(
            new MapBasedRow(JAN_2, DIMS1)
        )),
        new RowBucket(JAN_3, Collections.emptyList()),
        new RowBucket(JAN_4, Arrays.asList(
            new MapBasedRow(JAN_4, DIMS2),
            new MapBasedRow(JAN_4, DIMS3)
        ))
    ));

    Iterable<Row> iterable = new MovingAverageIterable(
        dayBuckets,
        dims,
        Collections.singletonList(new ConstantAveragerFactory(""noop"", 1, 1.1f)),
        Collections.emptyList(),
        Collections.emptyList()
    );

    Iterator<Row> iter = iterable.iterator();

    Assert.assertTrue(iter.hasNext());
    Row r = iter.next();
    Assert.assertEquals(JAN_1, r.getTimestamp());
    Assert.assertEquals(""m"", r.getRaw(GENDER));

    Assert.assertTrue(iter.hasNext());
    r = iter.next();
    Assert.assertEquals(JAN_1, r.getTimestamp());
    Assert.assertEquals(""f"", r.getRaw(GENDER));

    Assert.assertTrue(iter.hasNext());
    r = iter.next();
    Assert.assertEquals(JAN_2, r.getTimestamp());
    Assert.assertEquals(""m"", r.getRaw(GENDER));

    Assert.assertTrue(iter.hasNext());
    r = iter.next();
    Assert.assertEquals(JAN_2, r.getTimestamp());
    Assert.assertEquals(""f"", r.getRaw(GENDER));

    Assert.assertTrue(iter.hasNext());
    r = iter.next();
    Row r2 = r;
    Assert.assertEquals(JAN_3, r.getTimestamp());
    Assert.assertEquals(""US"", r.getRaw(COUNTRY));

    Assert.assertTrue(iter.hasNext());
    r = iter.next();
    Assert.assertEquals(JAN_3, r.getTimestamp());
    Assert.assertEquals(""US"", r.getRaw(COUNTRY));
    Assert.assertThat(r.getRaw(AGE), CoreMatchers.not(CoreMatchers.equalTo(r2.getRaw(AGE))));

    Assert.assertTrue(iter.hasNext());
    r = iter.next();
    Assert.assertEquals(JAN_4, r.getTimestamp());
    Assert.assertEquals(""f"", r.getRaw(GENDER));

    Assert.assertTrue(iter.hasNext());
    r = iter.next();
    Assert.assertEquals(JAN_4, r.getTimestamp());
    Assert.assertEquals(""u"", r.getRaw(GENDER));

    Assert.assertTrue(iter.hasNext());
    r = iter.next();
    Assert.assertEquals(JAN_4, r.getTimestamp());
    Assert.assertEquals(""m"", r.getRaw(GENDER));

    Assert.assertFalse(iter.hasNext());
  }
"
"  @Test
  public void testAveraging()
  {

    Map<String, Object> event1 = new HashMap<>();
    Map<String, Object> event2 = new HashMap<>();
    Map<String, Object> event3 = new HashMap<>();
    Map<String, Object> event4 = new HashMap<>();

    List<DimensionSpec> ds = new ArrayList<>();
    ds.add(new DefaultDimensionSpec(""gender"", ""gender""));

    event1.put(""gender"", ""m"");
    event1.put(""pageViews"", 10L);
    Row row1 = new MapBasedRow(JAN_1, event1);

    event2.put(""gender"", ""m"");
    event2.put(""pageViews"", 20L);
    Row row2 = new MapBasedRow(JAN_2, event2);

    event3.put(""gender"", ""m"");
    event3.put(""pageViews"", 30L);
    Row row3 = new MapBasedRow(JAN_3, event3);

    event4.put(""gender"", ""f"");
    event4.put(""pageViews"", 40L);
    Row row4 = new MapBasedRow(JAN_3, event4);

    float retval = 14.5f;

    Sequence<RowBucket> seq = Sequences.simple(Arrays.asList(
        new RowBucket(JAN_1, Collections.singletonList(row1)),
        new RowBucket(JAN_2, Collections.singletonList(row2)),
        new RowBucket(JAN_3, Arrays.asList(row3, row4))
    ));

    Iterator<Row> iter = new MovingAverageIterable(
        seq,
        ds,
        Arrays.asList(
            new ConstantAveragerFactory(""costPageViews"", 7, retval),
            new LongMeanAveragerFactory(""movingAvgPageViews"", 7, 1, ""pageViews"")
        ),
        Collections.emptyList(),
        Collections.singletonList(new LongSumAggregatorFactory(""pageViews"", ""pageViews""))
    ).iterator();

    Assert.assertTrue(iter.hasNext());
    Row caResult = iter.next();

    Assert.assertEquals(JAN_1, caResult.getTimestamp());
    Assert.assertEquals(""m"", (caResult.getDimension(""gender"")).get(0));
    Assert.assertEquals(retval, caResult.getMetric(""costPageViews"").floatValue(), 0.0f);
    Assert.assertEquals(1.4285715f, caResult.getMetric(""movingAvgPageViews"").floatValue(), 0.0f);

    Assert.assertTrue(iter.hasNext());
    caResult = iter.next();
    Assert.assertEquals(""m"", (caResult.getDimension(""gender"")).get(0));
    Assert.assertEquals(4.285714f, caResult.getMetric(""movingAvgPageViews"").floatValue(), 0.0f);

    Assert.assertTrue(iter.hasNext());
    caResult = iter.next();
    Assert.assertEquals(""m"", (caResult.getDimension(""gender"")).get(0));
    Assert.assertEquals(8.571428f, caResult.getMetric(""movingAvgPageViews"").floatValue(), 0.0f);

    Assert.assertTrue(iter.hasNext());
    caResult = iter.next();
    Assert.assertEquals(""f"", (caResult.getDimension(""gender"")).get(0));
    Assert.assertEquals(5.714285850f, caResult.getMetric(""movingAvgPageViews"").floatValue(), 0.0f);

    Assert.assertFalse(iter.hasNext());

  }
"
"  @Test
  public void testCompleteData()
  {

    Map<String, Object> event1 = new HashMap<>();
    Map<String, Object> event2 = new HashMap<>();
    Map<String, Object> event3 = new HashMap<>();

    event1.put(""gender"", ""m"");
    event1.put(""pageViews"", 10L);
    event2.put(""gender"", ""f"");
    event2.put(""pageViews"", 20L);
    event3.put(""gender"", ""u"");
    event3.put(""pageViews"", 30L);

    List<DimensionSpec> ds = new ArrayList<>();
    ds.add(new DefaultDimensionSpec(""gender"", ""gender""));

    Row jan1Row1 = new MapBasedRow(JAN_1, event1);
    Row jan1Row2 = new MapBasedRow(JAN_1, event2);
    Row jan1Row3 = new MapBasedRow(JAN_1, event3);

    Row jan2Row1 = new MapBasedRow(JAN_2, event1);
    Row jan2Row2 = new MapBasedRow(JAN_2, event2);
    Row jan2Row3 = new MapBasedRow(JAN_2, event3);

    Sequence<RowBucket> seq = Sequences.simple(Arrays.asList(
        new RowBucket(JAN_1, Arrays.asList(jan1Row1, jan1Row2, jan1Row3)),
        new RowBucket(JAN_2, Arrays.asList(jan2Row1, jan2Row2, jan2Row3))
    ));

    Iterator<Row> iter = new MovingAverageIterable(
        seq,
        ds,
        Collections.singletonList(
            new LongMeanAveragerFactory(""movingAvgPageViews"", 2, 1, ""pageViews"")
        ),
        Collections.emptyList(),
        Collections.singletonList(new LongSumAggregatorFactory(""pageViews"", ""pageViews""))
    ).iterator();

    Assert.assertTrue(iter.hasNext());
    Row result = iter.next();
    Assert.assertEquals(""m"", (result.getDimension(""gender"")).get(0));
    Assert.assertEquals(JAN_1, (result.getTimestamp()));

    Assert.assertTrue(iter.hasNext());
    result = iter.next();
    Assert.assertEquals(""f"", (result.getDimension(""gender"")).get(0));
    Assert.assertEquals(JAN_1, (result.getTimestamp()));

    Assert.assertTrue(iter.hasNext());
    result = iter.next();
    Assert.assertEquals(""u"", (result.getDimension(""gender"")).get(0));
    Assert.assertEquals(JAN_1, (result.getTimestamp()));

    Assert.assertTrue(iter.hasNext());
    result = iter.next();
    Assert.assertEquals(""m"", (result.getDimension(""gender"")).get(0));
    Assert.assertEquals(JAN_2, (result.getTimestamp()));

    Assert.assertTrue(iter.hasNext());
    result = iter.next();
    Assert.assertEquals(""f"", (result.getDimension(""gender"")).get(0));
    Assert.assertEquals(JAN_2, (result.getTimestamp()));

    Assert.assertTrue(iter.hasNext());
    result = iter.next();
    Assert.assertEquals(""u"", (result.getDimension(""gender"")).get(0));
    Assert.assertEquals(JAN_2, (result.getTimestamp()));

    Assert.assertFalse(iter.hasNext());

  }
"
"  @Test
  public void testMissingDataAtBeginning()
  {

    Map<String, Object> event1 = new HashMap<>();
    Map<String, Object> event2 = new HashMap<>();
    Map<String, Object> event3 = new HashMap<>();

    event1.put(""gender"", ""m"");
    event1.put(""pageViews"", 10L);
    event2.put(""gender"", ""f"");
    event2.put(""pageViews"", 20L);
    event3.put(""gender"", ""u"");
    event3.put(""pageViews"", 30L);

    List<DimensionSpec> ds = new ArrayList<>();
    ds.add(new DefaultDimensionSpec(""gender"", ""gender""));

    Row jan1Row1 = new MapBasedRow(JAN_1, event1);

    Row jan2Row1 = new MapBasedRow(JAN_2, event1);
    Row jan2Row2 = new MapBasedRow(JAN_2, event2);
    Row jan2Row3 = new MapBasedRow(JAN_2, event3);

    Sequence<RowBucket> seq = Sequences.simple(Arrays.asList(
        new RowBucket(JAN_1, Collections.singletonList(jan1Row1)),
        new RowBucket(JAN_2, Arrays.asList(jan2Row1, jan2Row2, jan2Row3))
    ));

    Iterator<Row> iter = new MovingAverageIterable(
        seq,
        ds,
        Collections.singletonList(
            new LongMeanAveragerFactory(""movingAvgPageViews"", 2, 1, ""pageViews"")
        ),
        Collections.emptyList(),
        Collections.singletonList(new LongSumAggregatorFactory(""pageViews"", ""pageViews""))
    ).iterator();

    Assert.assertTrue(iter.hasNext());
    Row result = iter.next();
    Assert.assertEquals(""m"", (result.getDimension(""gender"")).get(0));
    Assert.assertEquals(JAN_1, (result.getTimestamp()));

    Assert.assertTrue(iter.hasNext());
    result = iter.next();
    Assert.assertEquals(""m"", (result.getDimension(""gender"")).get(0));
    Assert.assertEquals(JAN_2, (result.getTimestamp()));

    Assert.assertTrue(iter.hasNext());
    result = iter.next();
    Assert.assertEquals(""f"", (result.getDimension(""gender"")).get(0));
    Assert.assertEquals(JAN_2, (result.getTimestamp()));

    Assert.assertTrue(iter.hasNext());
    result = iter.next();
    Assert.assertEquals(""u"", (result.getDimension(""gender"")).get(0));
    Assert.assertEquals(JAN_2, (result.getTimestamp()));

    Assert.assertFalse(iter.hasNext());
  }
"
"  @Test
  public void testMissingDataAtTheEnd()
  {

    Map<String, Object> event1 = new HashMap<>();
    Map<String, Object> event2 = new HashMap<>();
    Map<String, Object> event3 = new HashMap<>();

    event1.put(""gender"", ""m"");
    event1.put(""pageViews"", 10L);
    event2.put(""gender"", ""f"");
    event2.put(""pageViews"", 20L);
    event3.put(""gender"", ""u"");
    event3.put(""pageViews"", 30L);

    List<DimensionSpec> ds = new ArrayList<>();
    ds.add(new DefaultDimensionSpec(""gender"", ""gender""));

    Row jan1Row1 = new MapBasedRow(JAN_1, event1);
    Row jan1Row2 = new MapBasedRow(JAN_1, event2);
    Row jan1Row3 = new MapBasedRow(JAN_1, event3);
    Row jan2Row1 = new MapBasedRow(JAN_2, event1);

    Sequence<RowBucket> seq = Sequences.simple(Arrays.asList(
        new RowBucket(JAN_1, Arrays.asList(jan1Row1, jan1Row2, jan1Row3)),
        new RowBucket(JAN_2, Collections.singletonList(jan2Row1))
    ));

    Iterator<Row> iter = new MovingAverageIterable(
        seq,
        ds,
        Collections.singletonList(
            new LongMeanAveragerFactory(""movingAvgPageViews"", 2, 1, ""pageViews"")
        ),
        Collections.emptyList(),
        Collections.singletonList(new LongSumAggregatorFactory(""pageViews"", ""pageViews""))
    ).iterator();

    Assert.assertTrue(iter.hasNext());
    Row result = iter.next();
    Assert.assertEquals(""m"", (result.getDimension(""gender"")).get(0));
    Assert.assertEquals(JAN_1, (result.getTimestamp()));

    Assert.assertTrue(iter.hasNext());
    result = iter.next();
    Assert.assertEquals(""f"", (result.getDimension(""gender"")).get(0));
    Assert.assertEquals(JAN_1, (result.getTimestamp()));

    Assert.assertTrue(iter.hasNext());
    result = iter.next();
    Assert.assertEquals(""u"", (result.getDimension(""gender"")).get(0));
    Assert.assertEquals(JAN_1, (result.getTimestamp()));

    Assert.assertTrue(iter.hasNext());
    result = iter.next();
    Assert.assertEquals(""m"", (result.getDimension(""gender"")).get(0));
    Assert.assertEquals(JAN_2, (result.getTimestamp()));

    Assert.assertTrue(iter.hasNext());
    result = iter.next();
    Assert.assertEquals(""u"", (result.getDimension(""gender"")).get(0));
    Assert.assertEquals(JAN_2, (result.getTimestamp()));

    Assert.assertTrue(iter.hasNext());
    result = iter.next();
    Assert.assertEquals(""f"", (result.getDimension(""gender"")).get(0));
    Assert.assertEquals(JAN_2, (result.getTimestamp()));

    Assert.assertFalse(iter.hasNext());
  }
"
"  @Test
  public void testMissingDataAtMiddle()
  {

    Map<String, Object> eventM = new HashMap<>();
    Map<String, Object> eventF = new HashMap<>();
    Map<String, Object> eventU = new HashMap<>();

    eventM.put(""gender"", ""m"");
    eventM.put(""pageViews"", 10L);
    eventF.put(""gender"", ""f"");
    eventF.put(""pageViews"", 20L);
    eventU.put(""gender"", ""u"");
    eventU.put(""pageViews"", 30L);

    List<DimensionSpec> ds = new ArrayList<>();
    ds.add(new DefaultDimensionSpec(""gender"", ""gender""));

    Row jan1Row1M = new MapBasedRow(JAN_1, eventM);
    Row jan1Row2F = new MapBasedRow(JAN_1, eventF);
    Row jan1Row3U = new MapBasedRow(JAN_1, eventU);
    Row jan2Row1M = new MapBasedRow(JAN_2, eventM);
    Row jan3Row1M = new MapBasedRow(JAN_3, eventM);
    Row jan3Row2F = new MapBasedRow(JAN_3, eventF);
    Row jan3Row3U = new MapBasedRow(JAN_3, eventU);
    Row jan4Row1M = new MapBasedRow(JAN_4, eventM);

    Sequence<RowBucket> seq = Sequences.simple(Arrays.asList(
        new RowBucket(JAN_1, Arrays.asList(jan1Row1M, jan1Row2F, jan1Row3U)),
        new RowBucket(JAN_2, Collections.singletonList(jan2Row1M)),
        new RowBucket(JAN_3, Arrays.asList(jan3Row1M, jan3Row2F, jan3Row3U)),
        new RowBucket(JAN_4, Collections.singletonList(jan4Row1M))
    ));

    Iterator<Row> iter = new MovingAverageIterable(
        seq,
        ds,
        Collections.singletonList(
            new LongMeanAveragerFactory(""movingAvgPageViews"", 3, 1, ""pageViews"")
        ),
        Collections.emptyList(),
        Collections.singletonList(new LongSumAggregatorFactory(""pageViews"", ""pageViews""))
    ).iterator();

    // Jan 1
    Assert.assertTrue(iter.hasNext());
    Row result = iter.next();
    Assert.assertEquals(""m"", (result.getDimension(""gender"")).get(0));
    Assert.assertEquals(JAN_1, (result.getTimestamp()));

    Assert.assertTrue(iter.hasNext());
    result = iter.next();
    Assert.assertEquals(""f"", (result.getDimension(""gender"")).get(0));
    Assert.assertEquals(JAN_1, (result.getTimestamp()));

    Assert.assertTrue(iter.hasNext());
    result = iter.next();
    Assert.assertEquals(""u"", (result.getDimension(""gender"")).get(0));
    Assert.assertEquals(JAN_1, (result.getTimestamp()));

    // Jan 2
    Assert.assertTrue(iter.hasNext());
    result = iter.next();
    Assert.assertEquals(""m"", (result.getDimension(""gender"")).get(0));
    Assert.assertEquals(JAN_2, (result.getTimestamp()));

    Assert.assertTrue(iter.hasNext());
    result = iter.next();
    Assert.assertEquals(""u"", (result.getDimension(""gender"")).get(0));
    Assert.assertEquals(JAN_2, (result.getTimestamp()));

    Assert.assertTrue(iter.hasNext());
    result = iter.next();
    Assert.assertEquals(""f"", (result.getDimension(""gender"")).get(0));
    Assert.assertEquals(JAN_2, (result.getTimestamp()));

    // Jan 3
    Assert.assertTrue(iter.hasNext());
    result = iter.next();
    Assert.assertEquals(""m"", (result.getDimension(""gender"")).get(0));
    Assert.assertEquals(JAN_3, (result.getTimestamp()));

    Assert.assertTrue(iter.hasNext());
    result = iter.next();
    Assert.assertEquals(""f"", (result.getDimension(""gender"")).get(0));
    Assert.assertEquals(JAN_3, (result.getTimestamp()));

    Assert.assertTrue(iter.hasNext());
    result = iter.next();
    Assert.assertEquals(""u"", (result.getDimension(""gender"")).get(0));
    Assert.assertEquals(JAN_3, (result.getTimestamp()));

    // Jan 4
    Assert.assertTrue(iter.hasNext());
    result = iter.next();
    Assert.assertEquals(""m"", (result.getDimension(""gender"")).get(0));
    Assert.assertEquals(JAN_4, (result.getTimestamp()));

    Assert.assertTrue(iter.hasNext());
    result = iter.next();
    Assert.assertEquals(""u"", (result.getDimension(""gender"")).get(0));
    Assert.assertEquals(JAN_4, (result.getTimestamp()));

    Assert.assertTrue(iter.hasNext());
    result = iter.next();
    Assert.assertEquals(""f"", (result.getDimension(""gender"")).get(0));
    Assert.assertEquals(JAN_4, (result.getTimestamp()));

    Assert.assertFalse(iter.hasNext());
  }
"
"  @Test
  public void testMissingDaysAtBegining()
  {

    Map<String, Object> event1 = new HashMap<>();
    Map<String, Object> event2 = new HashMap<>();

    List<DimensionSpec> ds = new ArrayList<>();
    ds.add(new DefaultDimensionSpec(""gender"", ""gender""));

    event1.put(""gender"", ""m"");
    event1.put(""pageViews"", 10L);
    Row row1 = new MapBasedRow(JAN_3, event1);

    event2.put(""gender"", ""m"");
    event2.put(""pageViews"", 20L);
    Row row2 = new MapBasedRow(JAN_4, event2);

    Sequence<RowBucket> seq = Sequences.simple(Arrays.asList(
        new RowBucket(JAN_1, Collections.emptyList()),
        new RowBucket(JAN_2, Collections.emptyList()),
        new RowBucket(JAN_3, Collections.singletonList(row1)),
        new RowBucket(JAN_4, Collections.singletonList(row2))
    ));

    Iterator<Row> iter = new MovingAverageIterable(
        seq,
        ds,
        Collections.singletonList(
            new LongMeanAveragerFactory(""movingAvgPageViews"", 4, 1, ""pageViews"")
        ),
        Collections.emptyList(),
        Collections.singletonList(new LongSumAggregatorFactory(""pageViews"", ""pageViews""))
    ).iterator();

    Assert.assertTrue(iter.hasNext());
    Row result = iter.next();
    Assert.assertEquals(""m"", (result.getDimension(""gender"")).get(0));
    Assert.assertEquals(2.5f, result.getMetric(""movingAvgPageViews"").floatValue(), 0.0f);

    Assert.assertTrue(iter.hasNext());
    result = iter.next();
    Assert.assertEquals(""m"", (result.getDimension(""gender"")).get(0));
    Assert.assertEquals(7.5f, result.getMetric(""movingAvgPageViews"").floatValue(), 0.0f);

    Assert.assertFalse(iter.hasNext());
  }
"
"  @Test
  public void testMissingDaysInMiddle()
  {
    System.setProperty(""druid.generic.useDefaultValueForNull"", ""true"");
    NullHandling.initializeForTests();
    Map<String, Object> event1 = new HashMap<>();
    Map<String, Object> event2 = new HashMap<>();

    List<DimensionSpec> ds = new ArrayList<>();
    ds.add(new DefaultDimensionSpec(""gender"", ""gender""));

    event1.put(""gender"", ""m"");
    event1.put(""pageViews"", 10L);
    Row row1 = new MapBasedRow(JAN_1, event1);

    event2.put(""gender"", ""m"");
    event2.put(""pageViews"", 20L);
    Row row2 = new MapBasedRow(JAN_4, event2);

    Sequence<RowBucket> seq = Sequences.simple(Arrays.asList(
        new RowBucket(JAN_1, Collections.singletonList(row1)),
        new RowBucket(JAN_2, Collections.emptyList()),
        new RowBucket(JAN_3, Collections.emptyList()),
        new RowBucket(JAN_4, Collections.singletonList(row2))
    ));

    Iterator<Row> iter = new MovingAverageIterable(
        seq,
        ds,
        Collections.singletonList(
            new LongMeanAveragerFactory(""movingAvgPageViews"", 4, 1, ""pageViews"")
        ),
        Collections.emptyList(),
        Collections.singletonList(new LongSumAggregatorFactory(""pageViews"", ""pageViews""))
    ).iterator();

    Assert.assertTrue(iter.hasNext());
    Row result = iter.next();
    Assert.assertEquals(""m"", (result.getDimension(""gender"")).get(0));
    Assert.assertEquals(2.5f, result.getMetric(""movingAvgPageViews"").floatValue(), 0.0f);

    Assert.assertTrue(iter.hasNext());
    result = iter.next();
    Assert.assertEquals(""m"", (result.getDimension(""gender"")).get(0));
    Assert.assertEquals(2.5f, result.getMetric(""movingAvgPageViews"").floatValue(), 0.0f);

    Assert.assertTrue(iter.hasNext());
    result = iter.next();
    Assert.assertEquals(""m"", (result.getDimension(""gender"")).get(0));
    Assert.assertEquals(2.5f, result.getMetric(""movingAvgPageViews"").floatValue(), 0.0f);

    Assert.assertTrue(iter.hasNext());
    result = iter.next();
    Assert.assertEquals(""m"", (result.getDimension(""gender"")).get(0));
    Assert.assertEquals(7.5f, result.getMetric(""movingAvgPageViews"").floatValue(), 0.0f);

    Assert.assertFalse(iter.hasNext());
  }
"
"  @Test
  public void testWithFilteredAggregation()
  {

    Map<String, Object> event1 = new HashMap<>();
    Map<String, Object> event2 = new HashMap<>();

    List<DimensionSpec> ds = new ArrayList<>();
    ds.add(new DefaultDimensionSpec(""gender"", ""gender""));

    event1.put(""gender"", ""m"");
    event1.put(""pageViews"", 10L);
    Row row1 = new MapBasedRow(JAN_1, event1);

    event2.put(""gender"", ""m"");
    event2.put(""pageViews"", 20L);
    Row row2 = new MapBasedRow(JAN_4, event2);

    Sequence<RowBucket> seq = Sequences.simple(Arrays.asList(
        new RowBucket(JAN_1, Collections.singletonList(row1)),
        new RowBucket(JAN_2, Collections.emptyList()),
        new RowBucket(JAN_3, Collections.emptyList()),
        new RowBucket(JAN_4, Collections.singletonList(row2))
    ));

    AveragerFactory averagerfactory = new LongMeanAveragerFactory(""movingAvgPageViews"", 4, 1, ""pageViews"");
    AggregatorFactory aggregatorFactory = new LongSumAggregatorFactory(""pageViews"", ""pageViews"");
    DimFilter filter = new SelectorDimFilter(""gender"", ""m"", null);
    FilteredAggregatorFactory filteredAggregatorFactory = new FilteredAggregatorFactory(aggregatorFactory, filter);

    Iterator<Row> iter = new MovingAverageIterable(
        seq,
        ds,
        Collections.singletonList(averagerfactory),
        Collections.emptyList(),
        Collections.singletonList(filteredAggregatorFactory)
    ).iterator();

    Assert.assertTrue(iter.hasNext());
    Row result = iter.next();
    Assert.assertEquals(""m"", (result.getDimension(""gender"")).get(0));
    Assert.assertEquals(2.5f, result.getMetric(""movingAvgPageViews"").floatValue(), 0.0f);

    Assert.assertTrue(iter.hasNext());
    result = iter.next();
    Assert.assertEquals(""m"", (result.getDimension(""gender"")).get(0));
    Assert.assertEquals(2.5f, result.getMetric(""movingAvgPageViews"").floatValue(), 0.0f);

    Assert.assertTrue(iter.hasNext());
    result = iter.next();
    Assert.assertEquals(""m"", (result.getDimension(""gender"")).get(0));
    Assert.assertEquals(2.5f, result.getMetric(""movingAvgPageViews"").floatValue(), 0.0f);

    Assert.assertTrue(iter.hasNext());
    result = iter.next();
    Assert.assertEquals(""m"", (result.getDimension(""gender"")).get(0));
    Assert.assertEquals(7.5f, result.getMetric(""movingAvgPageViews"").floatValue(), 0.0f);

    Assert.assertFalse(iter.hasNext());
  }
"
"  @Test
  public void testMissingDaysAtEnd()
  {
    System.setProperty(""druid.generic.useDefaultValueForNull"", ""true"");
    NullHandling.initializeForTests();
    Map<String, Object> event1 = new HashMap<>();
    Map<String, Object> event2 = new HashMap<>();

    List<DimensionSpec> ds = new ArrayList<>();
    ds.add(new DefaultDimensionSpec(""gender"", ""gender""));

    event1.put(""gender"", ""m"");
    event1.put(""pageViews"", 10L);
    Row row1 = new MapBasedRow(JAN_1, event1);

    event2.put(""gender"", ""m"");
    event2.put(""pageViews"", 20L);
    Row row2 = new MapBasedRow(JAN_2, event2);

    Sequence<RowBucket> seq = Sequences.simple(Arrays.asList(
        new RowBucket(JAN_1, Collections.singletonList(row1)),
        new RowBucket(JAN_2, Collections.singletonList(row2)),
        new RowBucket(JAN_3, Collections.emptyList()),
        new RowBucket(JAN_4, Collections.emptyList()),
        new RowBucket(JAN_5, Collections.emptyList()),
        new RowBucket(JAN_6, Collections.emptyList())
    ));

    Iterator<Row> iter = new MovingAverageIterable(
        seq,
        ds,
        Collections.singletonList(
            new LongMeanAveragerFactory(""movingAvgPageViews"", 4, 1, ""pageViews"")
        ),
        Collections.emptyList(),
        Collections.singletonList(new LongSumAggregatorFactory(""pageViews"", ""pageViews""))
    ).iterator();

    Assert.assertTrue(iter.hasNext());
    Row result = iter.next();

    Assert.assertEquals(JAN_1, result.getTimestamp());
    Assert.assertEquals(""m"", (result.getDimension(""gender"")).get(0));
    Assert.assertEquals(2.5f, result.getMetric(""movingAvgPageViews"").floatValue(), 0.0f);

    Assert.assertTrue(iter.hasNext());
    result = iter.next();
    Assert.assertEquals(JAN_2, result.getTimestamp());
    Assert.assertEquals(""m"", (result.getDimension(""gender"")).get(0));
    Assert.assertEquals(7.5f, result.getMetric(""movingAvgPageViews"").floatValue(), 0.0f);

    Assert.assertTrue(iter.hasNext());
    result = iter.next();
    Assert.assertEquals(JAN_3, result.getTimestamp());
    Assert.assertEquals(""m"", (result.getDimension(""gender"")).get(0));
    Assert.assertEquals(7.5f, result.getMetric(""movingAvgPageViews"").floatValue(), 0.0f);

    Assert.assertTrue(iter.hasNext());
    result = iter.next();
    Assert.assertEquals(JAN_4, result.getTimestamp());
    Assert.assertEquals(""m"", (result.getDimension(""gender"")).get(0));
    Assert.assertEquals(7.5f, result.getMetric(""movingAvgPageViews"").floatValue(), 0.0f);

    Assert.assertTrue(iter.hasNext());
    result = iter.next();
    Assert.assertEquals(JAN_5, result.getTimestamp());
    Assert.assertEquals(""m"", (result.getDimension(""gender"")).get(0));
    Assert.assertEquals(5.0f, result.getMetric(""movingAvgPageViews"").floatValue(), 0.0f);

    Assert.assertTrue(iter.hasNext());
    result = iter.next();
    Assert.assertEquals(JAN_6, result.getTimestamp());
    Assert.assertEquals(""m"", (result.getDimension(""gender"")).get(0));
    Assert.assertEquals(0.0f, result.getMetric(""movingAvgPageViews"").floatValue(), 0.0f);

    Assert.assertFalse(iter.hasNext());
  }
"
"  @Test
  public void testApply()
  {
    event.put(""count"", 10.0);
    event.put(""avgCount"", 12.0);

    Row result = pac.apply(row);

    Assert.assertEquals(10.0f / 12.0f, result.getMetric(""avgCountRatio"").floatValue(), 0.0);
  }
"
"  @Test
  public void testApplyMissingColumn()
  {
    event.put(""count"", 10.0);

    Row result = pac.apply(row);

    Assert.assertEquals(0.0, result.getMetric(""avgCountRatio"").floatValue(), 0.0);
    Assert.assertNull(result.getRaw(""avgCountRatio""));
  }
"
"  @Test
  public void testCreateAverager()
  {
    AveragerFactory<?, ?> fac = new DoubleMaxAveragerFactory(""test"", 5, 1, ""field"");
    Assert.assertThat(fac.createAverager(), CoreMatchers.instanceOf(DoubleMaxAverager.class));
  }
"
"  @Test
  public void testCreateAverager()
  {
    AveragerFactory<?, ?> fac = new DoubleMinAveragerFactory(""test"", 5, 1, ""field"");
    Assert.assertThat(fac.createAverager(), IsInstanceOf.instanceOf(DoubleMinAverager.class));
  }
"
"  @Test
  public void testCreateAverager()
  {
    AveragerFactory<?, ?> fac = new DoubleSumAveragerFactory(""test"", 5, 1, ""field"");
    Assert.assertThat(fac.createAverager(), IsInstanceOf.instanceOf(DoubleSumAverager.class));
  }
"
"  @Test
  public void testCreateAverager()
  {
    AveragerFactory<?, ?> fac = new LongMaxAveragerFactory(""test"", 5, 1, ""field"");
    Assert.assertThat(fac.createAverager(), IsInstanceOf.instanceOf(LongMaxAverager.class));
  }
"
"  @Test
  public void testCreateAverager()
  {
    AveragerFactory<?, ?> fac = new LongMeanNoNullAveragerFactory(""test"", 5, 1, ""field"");
    Assert.assertThat(fac.createAverager(), IsInstanceOf.instanceOf(LongMeanNoNullAverager.class));
  }
"
"  @Test
  public void testCreateAverager()
  {
    AveragerFactory<?, ?> fac = new LongMinAveragerFactory(""test"", 5, 1, ""field"");
    Assert.assertThat(fac.createAverager(), IsInstanceOf.instanceOf(LongMinAverager.class));
  }
"
"  @Test
  public void testComputeResult()
  {
    BaseAverager<Number, Double> avg = new DoubleSumAverager(3, ""test"", ""field"", 1);

    Assert.assertEquals(0.0, avg.computeResult(), 0.0);

    avg.addElement(Collections.singletonMap(""field"", 3.0), new HashMap<>());
    Assert.assertEquals(3.0, avg.computeResult(), 0.0);

    avg.addElement(Collections.singletonMap(""field"", 3.0), new HashMap<>());
    Assert.assertEquals(6.0, avg.computeResult(), 0.0);

    avg.addElement(Collections.singletonMap(""field"", new Integer(0)), new HashMap<>());
    Assert.assertEquals(6.0, avg.computeResult(), 0.0);

    avg.addElement(Collections.singletonMap(""field"", 2.5), new HashMap<>());
    avg.addElement(Collections.singletonMap(""field"", 2.0), new HashMap<>());
    avg.addElement(Collections.singletonMap(""field"", 2.0), new HashMap<>());
    Assert.assertEquals(6.5, avg.computeResult(), 0.0);

    avg.skip();
    Assert.assertEquals(4.0, avg.computeResult(), 0.0);

  }
"
"  @Test
  public void testComputeResult()
  {
    BaseAverager<Number, Double> avg = new LongMeanNoNullAverager(3, ""test"", ""field"", 1);

    Assert.assertEquals(Double.NaN, avg.computeResult(), 0.0);

    avg.addElement(Collections.singletonMap(""field"", 3L), new HashMap<>());
    Assert.assertEquals(3.0, avg.computeResult(), 0.0);

    avg.addElement(Collections.singletonMap(""field"", 3L), new HashMap<>());
    Assert.assertEquals(3.0, avg.computeResult(), 0.0);

    avg.addElement(Collections.singletonMap(""field"", 0), new HashMap<>());
    Assert.assertEquals(2.0, avg.computeResult(), 0.0);

    avg.addElement(Collections.singletonMap(""field"", 2L), new HashMap<>());
    avg.addElement(Collections.singletonMap(""field"", 2L), new HashMap<>());
    avg.addElement(Collections.singletonMap(""field"", 2L), new HashMap<>());
    Assert.assertEquals(2.0, avg.computeResult(), 0.0);

    avg.skip();
    Assert.assertEquals(2.0, avg.computeResult(), 0.0);
  }
"
"  @Test
  public void testComputeResult()
  {
    BaseAverager<Number, Double> avg = new DoubleMaxAverager(3, ""test"", ""field"", 1);

    Assert.assertEquals(Double.NEGATIVE_INFINITY, avg.computeResult(), 0.0);

    avg.addElement(Collections.singletonMap(""field"", -1.1e100), new HashMap<>());
    Assert.assertEquals(-1.1e100, avg.computeResult(), 0.0);

    avg.addElement(Collections.singletonMap(""field"", 1.0), new HashMap<>());
    Assert.assertEquals(1.0, avg.computeResult(), 0.0);

    avg.addElement(Collections.singletonMap(""field"", 1), new HashMap<>());
    Assert.assertEquals(1.0, avg.computeResult(), 0.0);

    avg.addElement(Collections.singletonMap(""field"", 5.0), new HashMap<>());
    avg.addElement(Collections.singletonMap(""field"", 3.0), new HashMap<>());
    avg.addElement(Collections.singletonMap(""field"", 2.0), new HashMap<>());
    Assert.assertEquals(5.0, avg.computeResult(), 0.0);

    avg.skip();
    Assert.assertEquals(3.0, avg.computeResult(), 0.0);
  }
"
"  @Test
  public void testComputeResult()
  {
    BaseAverager<Number, Double> avg = new DoubleMeanAverager(3, ""test"", ""field"", 1);

    Assert.assertEquals(0.0, avg.computeResult(), 0.0);

    avg.addElement(Collections.singletonMap(""field"", 3.0), new HashMap<>());
    Assert.assertEquals(1.0, avg.computeResult(), 0.0);

    avg.addElement(Collections.singletonMap(""field"", 3.0), new HashMap<>());
    Assert.assertEquals(2.0, avg.computeResult(), 0.0);

    avg.addElement(Collections.singletonMap(""field"", 0), new HashMap<>());
    Assert.assertEquals(2.0, avg.computeResult(), 0.0);

    avg.addElement(Collections.singletonMap(""field"", 2.0), new HashMap<>());
    avg.addElement(Collections.singletonMap(""field"", 2.0), new HashMap<>());
    avg.addElement(Collections.singletonMap(""field"", 2.0), new HashMap<>());
    Assert.assertEquals(2.0, avg.computeResult(), 0.0);

    avg.skip();
    Assert.assertEquals(4.0 / 3, avg.computeResult(), 0.0);
  }
"
"  @Test
  public void testCreateAverager()
  {
    AveragerFactory<?, ?> fac = new DoubleMeanNoNullAveragerFactory(""test"", 5, 1, ""field"");
    Assert.assertThat(fac.createAverager(), IsInstanceOf.instanceOf(DoubleMeanNoNullAverager.class));
  }
"
"  @Test
  public void testGetDependentFields()
  {
    List<String> dependentFields = fac.getDependentFields();
    Assert.assertEquals(1, dependentFields.size());
    Assert.assertEquals(""field"", dependentFields.get(0));
  }
"
"  @Test
  public void testFinalization()
  {
    Long input = 5L;
    Assert.assertEquals(input, fac.finalizeComputation(input));
  }
"
"  @Test
  public void testComputeResult()
  {
    BaseAverager<Number, Double> avg = new DoubleMeanNoNullAverager(3, ""test"", ""field"", 1);

    Assert.assertEquals(Double.NaN, avg.computeResult(), 0.0);

    avg.addElement(Collections.singletonMap(""field"", 3.0), new HashMap<>());
    Assert.assertEquals(3.0, avg.computeResult(), 0.0);

    avg.addElement(Collections.singletonMap(""field"", 3.0), new HashMap<>());
    Assert.assertEquals(3.0, avg.computeResult(), 0.0);

    avg.addElement(Collections.singletonMap(""field"", 0), new HashMap<>());
    Assert.assertEquals(2.0, avg.computeResult(), 0.0);

    avg.addElement(Collections.singletonMap(""field"", 2.0), new HashMap<>());
    avg.addElement(Collections.singletonMap(""field"", 2.0), new HashMap<>());
    avg.addElement(Collections.singletonMap(""field"", 2.0), new HashMap<>());
    Assert.assertEquals(2.0, avg.computeResult(), 0.0);

    avg.skip();
    Assert.assertEquals(2.0, avg.computeResult(), 0.0);

    // testing cycleSize functionality
    BaseAverager<Number, Double> averager = new DoubleMeanNoNullAverager(14, ""test"", ""field"", 7);

    averager.addElement(Collections.singletonMap(""field"", 2.0), new HashMap<>());
    Assert.assertEquals(2.0, averager.computeResult(), 0.0);

    averager.addElement(Collections.singletonMap(""field"", 4.0), new HashMap<>());
    averager.addElement(Collections.singletonMap(""field"", 5.0), new HashMap<>());
    averager.addElement(Collections.singletonMap(""field"", 6.0), new HashMap<>());
    averager.addElement(Collections.singletonMap(""field"", 7.0), new HashMap<>());
    averager.addElement(Collections.singletonMap(""field"", 8.0), new HashMap<>());
    averager.addElement(Collections.singletonMap(""field"", 9.0), new HashMap<>());
    averager.addElement(Collections.singletonMap(""field"", null), new HashMap<>());
    averager.addElement(Collections.singletonMap(""field"", 11.0), new HashMap<>());
    averager.addElement(Collections.singletonMap(""field"", 12.0), new HashMap<>());
    averager.addElement(Collections.singletonMap(""field"", 13.0), new HashMap<>());
    averager.addElement(Collections.singletonMap(""field"", 14.0), new HashMap<>());
    averager.addElement(Collections.singletonMap(""field"", 15.0), new HashMap<>());
    averager.addElement(Collections.singletonMap(""field"", 16.0), new HashMap<>());

    Assert.assertEquals(7.5, averager.computeResult(), 0.0);

    averager.addElement(Collections.singletonMap(""field"", 3.0), new HashMap<>());
    Assert.assertEquals(8.5, averager.computeResult(), 0.0);
  }
"
"  @Test
  public void testComputeResult()
  {
    BaseAverager<Number, Long> avg = new LongSumAverager(3, ""test"", ""field"", 1);

    Assert.assertEquals(0.0, avg.computeResult(), 0.0);

    avg.addElement(Collections.singletonMap(""field"", 3L), new HashMap<>());
    Assert.assertEquals(3.0, avg.computeResult(), 0.0);

    avg.addElement(Collections.singletonMap(""field"", 3L), new HashMap<>());
    Assert.assertEquals(6.0, avg.computeResult(), 0.0);

    avg.addElement(Collections.singletonMap(""field"", 3), new HashMap<>());
    Assert.assertEquals(9.0, avg.computeResult(), 0.0);

    avg.addElement(Collections.singletonMap(""field"", 2L), new HashMap<>());
    avg.addElement(Collections.singletonMap(""field"", 2L), new HashMap<>());
    avg.addElement(Collections.singletonMap(""field"", 2L), new HashMap<>());
    Assert.assertEquals(6.0, avg.computeResult(), 0.0);

    avg.skip();
    Assert.assertEquals(4.0, avg.computeResult(), 0.0);
  }
"
"  @Test
  public void testCreateAverager()
  {
    AveragerFactory<?, ?> fac = new LongMeanAveragerFactory(""test"", 5, 1, ""field"");
    Assert.assertThat(fac.createAverager(), IsInstanceOf.instanceOf(LongMeanAverager.class));
  }
"
"  @Test
  public void testCreateAverager()
  {
    AveragerFactory<?, ?> fac = new LongSumAveragerFactory(""test"", 5, 1, ""field"");
    Assert.assertThat(fac.createAverager(), IsInstanceOf.instanceOf(LongSumAverager.class));
  }
"
"  @Test
  public void testComputeResult()
  {
    BaseAverager<Number, Double> averager = new DoubleMeanAverager(14, ""test"", ""field"", 7);

    averager.addElement(Collections.singletonMap(""field"", 7.0), new HashMap<>());
    averager.addElement(Collections.singletonMap(""field"", 1.0), new HashMap<>());
    averager.addElement(Collections.singletonMap(""field"", 2.0), new HashMap<>());
    averager.addElement(Collections.singletonMap(""field"", 3.0), new HashMap<>());
    averager.addElement(Collections.singletonMap(""field"", 4.0), new HashMap<>());
    averager.addElement(Collections.singletonMap(""field"", 5.0), new HashMap<>());
    averager.addElement(Collections.singletonMap(""field"", 6.0), new HashMap<>());
    averager.addElement(Collections.singletonMap(""field"", 7.0), new HashMap<>());
    averager.addElement(Collections.singletonMap(""field"", 1.0), new HashMap<>());
    averager.addElement(Collections.singletonMap(""field"", 2.0), new HashMap<>());
    averager.addElement(Collections.singletonMap(""field"", 3.0), new HashMap<>());
    averager.addElement(Collections.singletonMap(""field"", 4.0), new HashMap<>());
    averager.addElement(Collections.singletonMap(""field"", 5.0), new HashMap<>());
    averager.addElement(Collections.singletonMap(""field"", 6.0), new HashMap<>());

    Assert.assertEquals(7, averager.computeResult(), 0.0); // (7+7)/2

    averager.addElement(Collections.singletonMap(""field"", 3.0), new HashMap<>());
    Assert.assertEquals(1, averager.computeResult(), 0.0); // (1+1)/2

    BaseAverager<Number, Double> averager1 = new DoubleMeanAverager(14, ""test"", ""field"", 3);

    averager1.addElement(Collections.singletonMap(""field"", 1.0), new HashMap<>());
    averager1.addElement(Collections.singletonMap(""field"", 2.0), new HashMap<>());
    averager1.addElement(Collections.singletonMap(""field"", 3.0), new HashMap<>());
    averager1.addElement(Collections.singletonMap(""field"", 1.0), new HashMap<>());
    averager1.addElement(Collections.singletonMap(""field"", 2.0), new HashMap<>());
    averager1.addElement(Collections.singletonMap(""field"", 3.0), new HashMap<>());
    averager1.addElement(Collections.singletonMap(""field"", 1.0), new HashMap<>());
    averager1.addElement(Collections.singletonMap(""field"", 2.0), new HashMap<>());
    averager1.addElement(Collections.singletonMap(""field"", 3.0), new HashMap<>());
    averager1.addElement(Collections.singletonMap(""field"", 1.0), new HashMap<>());
    averager1.addElement(Collections.singletonMap(""field"", 2.0), new HashMap<>());
    averager1.addElement(Collections.singletonMap(""field"", 3.0), new HashMap<>());
    averager1.addElement(Collections.singletonMap(""field"", 1.0), new HashMap<>());
    averager1.addElement(Collections.singletonMap(""field"", 2.0), new HashMap<>());

    Assert.assertEquals(1, averager1.computeResult(), 0.0); // (1+1+1+1+1)/5

    Assert.assertEquals(2, averager1.computeResult(), 0.0); // (2+2+2+2+2)/5

    Assert.assertEquals(13.0 / 5, averager1.computeResult(), 0.0); // (3+3+3+3+1)/5
  }
"
"  @Test
  public void testBaseAverager()
  {
    BaseAverager<Integer, Integer> avg = new TestAverager(Integer.class, 5, ""test"", ""field"", 1);

    Assert.assertEquals(""test"", avg.getName());
    Assert.assertEquals(5, avg.getNumBuckets());
    Assert.assertEquals(5, avg.getBuckets().length);
    Assert.assertTrue(avg.getBuckets().getClass().isArray());
  }
"
"  @Test
  public void testAddElement()
  {
    BaseAverager<Integer, Integer> avg = new TestAverager(Integer.class, 3, ""test"", ""field"", 1);
    Object[] buckets = avg.getBuckets();

    avg.addElement(Collections.singletonMap(""field"", 1), Collections.emptyMap());
    Assert.assertEquals(1, buckets[0]);
    Assert.assertNull(buckets[1]);
    Assert.assertNull(buckets[2]);

    avg.addElement(Collections.singletonMap(""field"", 2), Collections.emptyMap());
    Assert.assertEquals(1, buckets[0]);
    Assert.assertEquals(2, buckets[1]);
    Assert.assertNull(buckets[2]);

    avg.addElement(Collections.singletonMap(""field"", 3), Collections.emptyMap());
    Assert.assertEquals(1, buckets[0]);
    Assert.assertEquals(2, buckets[1]);
    Assert.assertEquals(3, buckets[2]);

    avg.addElement(Collections.singletonMap(""field"", 4), Collections.emptyMap());
    Assert.assertEquals(4, buckets[0]);
    Assert.assertEquals(2, buckets[1]);
    Assert.assertEquals(3, buckets[2]);
  }
"
"  @Test
  public void testSkip()
  {
    BaseAverager<Integer, Integer> avg = new TestAverager(Integer.class, 3, ""test"", ""field"", 1);
    Object[] buckets = avg.getBuckets();

    avg.addElement(Collections.singletonMap(""field"", 1), Collections.emptyMap());
    avg.addElement(Collections.singletonMap(""field"", 1), Collections.emptyMap());
    avg.addElement(Collections.singletonMap(""field"", 1), Collections.emptyMap());

    Assert.assertEquals(1, buckets[0]);
    Assert.assertEquals(1, buckets[1]);
    Assert.assertEquals(1, buckets[2]);

    avg.skip();
    Assert.assertNull(buckets[0]);
    Assert.assertNotNull(buckets[1]);
    Assert.assertNotNull(buckets[2]);

    avg.skip();
    Assert.assertNull(buckets[0]);
    Assert.assertNull(buckets[1]);
    Assert.assertNotNull(buckets[2]);

    avg.skip();
    Assert.assertNull(buckets[0]);
    Assert.assertNull(buckets[1]);
    Assert.assertNull(buckets[2]);

    // poke some test data into the array
    buckets[0] = 1;

    avg.skip();
    Assert.assertNull(buckets[0]);
    Assert.assertNull(buckets[1]);
    Assert.assertNull(buckets[2]);
  }
"
"  @Test
  public void testHasData()
  {
    BaseAverager<Integer, Integer> avg = new TestAverager(Integer.class, 3, ""test"", ""field"", 1);

    Assert.assertFalse(avg.hasData());

    avg.addElement(Collections.singletonMap(""field"", 1), Collections.emptyMap());
    Assert.assertTrue(avg.hasData());

    avg.skip();
    avg.skip();
    avg.skip();

    Assert.assertFalse(avg.hasData());
  }
"
"  @Test
  public void testGetResult()
  {
    BaseAverager<Integer, Integer> avg = new TestAverager(Integer.class, 3, ""test"", ""field"", 1);

    Assert.assertNull(avg.getResult());

    avg.addElement(Collections.singletonMap(""field"", 1), Collections.emptyMap());
    Assert.assertEquals(Integer.valueOf(1), avg.getResult());
  }
"
"  @Test
  public void testCreateAverager()
  {
    AveragerFactory<?, ?> fac = new DoubleMeanAveragerFactory(""test"", 5, 1, ""field"");
    Assert.assertThat(fac.createAverager(), IsInstanceOf.instanceOf(DoubleMeanAverager.class));
  }
"
"  @Test
  public void testComputeResult()
  {
    BaseAverager<Number, Double> avg = new DoubleMinAverager(3, ""test"", ""field"", 1);

    Assert.assertEquals(Double.POSITIVE_INFINITY, avg.computeResult(), 0.0);

    avg.addElement(Collections.singletonMap(""field"", -1.1e100), new HashMap<>());
    Assert.assertEquals(-1.1e100, avg.computeResult(), 0.0);

    avg.addElement(Collections.singletonMap(""field"", 1.0), new HashMap<>());
    Assert.assertEquals(-1.1e100, avg.computeResult(), 0.0);

    avg.addElement(Collections.singletonMap(""field"", new Integer(1)), new HashMap<>());
    Assert.assertEquals(-1.1e100, avg.computeResult(), 0.0);

    avg.addElement(Collections.singletonMap(""field"", 5.0), new HashMap<>());
    avg.addElement(Collections.singletonMap(""field"", 2.0), new HashMap<>());
    avg.addElement(Collections.singletonMap(""field"", 3.0), new HashMap<>());
    Assert.assertEquals(2.0, avg.computeResult(), 0.0);

    avg.skip();
    avg.skip();
    Assert.assertEquals(3.0, avg.computeResult(), 0.0);
  }
"
"  @Test
  public void testComputeResult()
  {
    BaseAverager<Number, Double> avg = new LongMeanAverager(3, ""test"", ""field"", 1);

    Assert.assertEquals(0.0, avg.computeResult(), 0.0);

    avg.addElement(Collections.singletonMap(""field"", 3L), new HashMap<>());
    Assert.assertEquals(1.0, avg.computeResult(), 0.0);

    avg.addElement(Collections.singletonMap(""field"", 3L), new HashMap<>());
    Assert.assertEquals(2.0, avg.computeResult(), 0.0);

    avg.addElement(Collections.singletonMap(""field"", 3), new HashMap<>());
    Assert.assertEquals(3.0, avg.computeResult(), 0.0);

    avg.addElement(Collections.singletonMap(""field"", 2L), new HashMap<>());
    avg.addElement(Collections.singletonMap(""field"", 2L), new HashMap<>());
    avg.addElement(Collections.singletonMap(""field"", 2L), new HashMap<>());
    Assert.assertEquals(2.0, avg.computeResult(), 0.0);

    avg.skip();
    Assert.assertEquals(4.0 / 3, avg.computeResult(), 0.0);
  }
"
"  @Test
  public void testComputeResult()
  {
    BaseAverager<Number, Long> avg = new LongMaxAverager(3, ""test"", ""field"", 1);

    Assert.assertEquals(Long.MIN_VALUE, (long) avg.computeResult());

    avg.addElement(Collections.singletonMap(""field"", -1000000L), new HashMap<>());
    Assert.assertEquals(-1000000, (long) avg.computeResult());

    avg.addElement(Collections.singletonMap(""field"", 1L), new HashMap<>());
    Assert.assertEquals(1, (long) avg.computeResult());

    avg.addElement(Collections.singletonMap(""field"", 1), new HashMap<>());
    Assert.assertEquals(1, (long) avg.computeResult());

    avg.addElement(Collections.singletonMap(""field"", 5L), new HashMap<>());
    avg.addElement(Collections.singletonMap(""field"", 3L), new HashMap<>());
    avg.addElement(Collections.singletonMap(""field"", 2L), new HashMap<>());
    Assert.assertEquals(5, (long) avg.computeResult());

    avg.skip();
    Assert.assertEquals(3, (long) avg.computeResult());
  }
"
"  @Test
  public void testComputeResult()
  {
    BaseAverager<Number, Long> avg = new LongMinAverager(3, ""test"", ""field"", 1);

    Assert.assertEquals(Long.MAX_VALUE, (long) avg.computeResult());

    avg.addElement(Collections.singletonMap(""field"", -10000L), new HashMap<>());
    Assert.assertEquals(-10000, (long) avg.computeResult());

    avg.addElement(Collections.singletonMap(""field"", 1L), new HashMap<>());
    Assert.assertEquals(-10000, (long) avg.computeResult());

    avg.addElement(Collections.singletonMap(""field"", 1000), new HashMap<>());
    Assert.assertEquals(-10000, (long) avg.computeResult());

    avg.addElement(Collections.singletonMap(""field"", 5L), new HashMap<>());
    avg.addElement(Collections.singletonMap(""field"", 2L), new HashMap<>());
    avg.addElement(Collections.singletonMap(""field"", 3L), new HashMap<>());
    Assert.assertEquals(2, (long) avg.computeResult());

    avg.skip();
    avg.skip();
    Assert.assertEquals(3, (long) avg.computeResult());
  }
"
"  @Test
  public void testQuery() throws IOException
  {
    Query<?> query = jsonMapper.readValue(getQueryString(), Query.class);
    Assert.assertThat(query, IsInstanceOf.instanceOf(getExpectedQueryType()));

    List<MapBasedRow> expectedResults = jsonMapper.readValue(getExpectedResultString(), getExpectedResultType());
    Assert.assertNotNull(expectedResults);
    Assert.assertThat(expectedResults, IsInstanceOf.instanceOf(List.class));

    CachingClusteredClient baseClient = new CachingClusteredClient(
        warehouse,
        new TimelineServerView()
        {
          @Override
          public Optional<? extends TimelineLookup<String, ServerSelector>> getTimeline(DataSourceAnalysis analysis)
          {
            return Optional.empty();
          }

          @Override
          public List<ImmutableDruidServer> getDruidServers()
          {
            return null;
          }
"
"  @Test
  public void testCompleteData()
  {
    intervals = new ArrayList<>();
    intervals.add(INTERVAL_JAN_1_4);

    rows = new ArrayList<>();
    rows.add(JAN_1_M_10);
    rows.add(JAN_2_M_10);
    rows.add(JAN_3_M_10);
    rows.add(JAN_4_M_10);

    List<Row> expectedDay1 = Collections.singletonList(JAN_1_M_10);
    List<Row> expectedDay2 = Collections.singletonList(JAN_2_M_10);
    List<Row> expectedDay3 = Collections.singletonList(JAN_3_M_10);
    List<Row> expectedDay4 = Collections.singletonList(JAN_4_M_10);

    Sequence<Row> seq = Sequences.simple(rows);
    RowBucketIterable rbi = new RowBucketIterable(seq, intervals, ONE_DAY);
    Iterator<RowBucket> iter = rbi.iterator();

    RowBucket actual = iter.next();
    Assert.assertEquals(JAN_1, actual.getDateTime());
    Assert.assertEquals(expectedDay1, actual.getRows());

    actual = iter.next();
    Assert.assertEquals(JAN_2, actual.getDateTime());
    Assert.assertEquals(expectedDay2, actual.getRows());

    actual = iter.next();
    Assert.assertEquals(JAN_3, actual.getDateTime());
    Assert.assertEquals(expectedDay3, actual.getRows());

    actual = iter.next();
    Assert.assertEquals(JAN_4, actual.getDateTime());
    Assert.assertEquals(expectedDay4, actual.getRows());
  }
"
"  @Test
  public void testApplyLastDaySingleRow()
  {
    intervals = new ArrayList<>();
    intervals.add(INTERVAL_JAN_1_4);

    List<Row> expectedDay1 = Arrays.asList(JAN_1_M_10, JAN_1_F_20);
    List<Row> expectedDay2 = Collections.singletonList(JAN_2_M_10);
    List<Row> expectedDay3 = Collections.singletonList(JAN_3_F_20);
    List<Row> expectedDay4 = Collections.singletonList(JAN_4_M_10);

    rows = new ArrayList<>();
    rows.add(JAN_1_M_10);
    rows.add(JAN_1_F_20);
    rows.add(JAN_2_M_10);
    rows.add(JAN_3_F_20);
    rows.add(JAN_4_M_10);

    Sequence<Row> seq = Sequences.simple(rows);
    RowBucketIterable rbi = new RowBucketIterable(seq, intervals, ONE_DAY);
    Iterator<RowBucket> iter = rbi.iterator();

    RowBucket actual = iter.next();
    Assert.assertEquals(expectedDay1, actual.getRows());

    actual = iter.next();
    Assert.assertEquals(expectedDay2, actual.getRows());

    actual = iter.next();
    Assert.assertEquals(expectedDay3, actual.getRows());

    actual = iter.next();
    Assert.assertEquals(expectedDay4, actual.getRows());
  }
"
"  @Test
  public void testApplyLastDayMultipleRows()
  {
    intervals = new ArrayList<>();
    intervals.add(INTERVAL_JAN_1_4);

    List<Row> expectedDay1 = Arrays.asList(JAN_1_M_10, JAN_1_F_20);
    List<Row> expectedDay2 = Collections.singletonList(JAN_2_M_10);
    List<Row> expectedDay3 = Collections.singletonList(JAN_3_F_20);
    List<Row> expectedDay4 = Arrays.asList(JAN_4_M_10, JAN_4_F_20, JAN_4_U_30);

    rows = new ArrayList<>();
    rows.add(JAN_1_M_10);
    rows.add(JAN_1_F_20);
    rows.add(JAN_2_M_10);
    rows.add(JAN_3_F_20);
    rows.add(JAN_4_M_10);
    rows.add(JAN_4_F_20);
    rows.add(JAN_4_U_30);

    Sequence<Row> seq = Sequences.simple(rows);
    RowBucketIterable rbi = new RowBucketIterable(seq, intervals, ONE_DAY);
    Iterator<RowBucket> iter = rbi.iterator();

    RowBucket actual = iter.next();
    Assert.assertEquals(expectedDay1, actual.getRows());

    actual = iter.next();
    Assert.assertEquals(expectedDay2, actual.getRows());

    actual = iter.next();
    Assert.assertEquals(expectedDay3, actual.getRows());

    actual = iter.next();
    Assert.assertEquals(expectedDay4, actual.getRows());
  }
"
"  @Test
  public void testSingleDaySingleRow()
  {
    intervals = new ArrayList<>();
    intervals.add(INTERVAL_JAN_1_1);

    rows = new ArrayList<>();
    rows.add(JAN_1_M_10);

    List<Row> expectedDay1 = Collections.singletonList(JAN_1_M_10);

    Sequence<Row> seq = Sequences.simple(rows);
    RowBucketIterable rbi = new RowBucketIterable(seq, intervals, ONE_DAY);
    Iterator<RowBucket> iter = rbi.iterator();

    RowBucket actual = iter.next();
    Assert.assertEquals(expectedDay1, actual.getRows());
    Assert.assertEquals(JAN_1, actual.getDateTime());
  }
"
"  @Test
  public void testSingleDayMultipleRow()
  {
    intervals = new ArrayList<>();
    intervals.add(INTERVAL_JAN_1_1);

    rows = new ArrayList<>();
    rows.add(JAN_1_M_10);
    rows.add(JAN_1_F_20);
    rows.add(JAN_1_U_30);

    List<Row> expectedDay1 = Arrays.asList(JAN_1_M_10, JAN_1_F_20, JAN_1_U_30);

    Sequence<Row> seq = Sequences.simple(rows);
    RowBucketIterable rbi = new RowBucketIterable(seq, intervals, ONE_DAY);
    Iterator<RowBucket> iter = rbi.iterator();

    RowBucket actual = iter.next();
    Assert.assertEquals(JAN_1, actual.getDateTime());
    Assert.assertEquals(expectedDay1, actual.getRows());
  }
"
"  @Test
  public void testMissingDaysAtBegining()
  {
    List<Row> expectedDay1 = Collections.emptyList();
    List<Row> expectedDay2 = Collections.singletonList(JAN_2_M_10);

    intervals = new ArrayList<>();
    intervals.add(INTERVAL_JAN_1_2);

    rows = new ArrayList<>();
    rows.add(JAN_2_M_10);

    Sequence<Row> seq = Sequences.simple(rows);
    RowBucketIterable rbi = new RowBucketIterable(seq, intervals, ONE_DAY);
    Iterator<RowBucket> iter = rbi.iterator();

    RowBucket actual = iter.next();
    Assert.assertEquals(JAN_1, actual.getDateTime());
    Assert.assertEquals(expectedDay1, actual.getRows());

    actual = iter.next();
    Assert.assertEquals(JAN_2, actual.getDateTime());
    Assert.assertEquals(expectedDay2, actual.getRows());
  }
"
"  @Test
  public void testMissingDaysAtBeginingFollowedByMultipleRow()
  {
    List<Row> expectedDay1 = Collections.emptyList();
    List<Row> expectedDay2 = Collections.singletonList(JAN_2_M_10);
    List<Row> expectedDay3 = Collections.singletonList(JAN_3_M_10);
    List<Row> expectedDay4 = Collections.singletonList(JAN_4_M_10);

    intervals = new ArrayList<>();
    intervals.add(INTERVAL_JAN_1_4);

    rows = new ArrayList<>();
    rows.add(JAN_2_M_10);
    rows.add(JAN_3_M_10);
    rows.add(JAN_4_M_10);

    Sequence<Row> seq = Sequences.simple(rows);
    RowBucketIterable rbi = new RowBucketIterable(seq, intervals, ONE_DAY);
    Iterator<RowBucket> iter = rbi.iterator();

    RowBucket actual = iter.next();
    Assert.assertEquals(JAN_1, actual.getDateTime());
    Assert.assertEquals(expectedDay1, actual.getRows());

    actual = iter.next();
    Assert.assertEquals(JAN_2, actual.getDateTime());
    Assert.assertEquals(expectedDay2, actual.getRows());

    actual = iter.next();
    Assert.assertEquals(JAN_3, actual.getDateTime());
    Assert.assertEquals(expectedDay3, actual.getRows());

    actual = iter.next();
    Assert.assertEquals(JAN_4, actual.getDateTime());
    Assert.assertEquals(expectedDay4, actual.getRows());
  }
"
"  @Test
  public void testMissingDaysAtBeginingAndAtTheEnd()
  {
    List<Row> expectedDay1 = Collections.emptyList();
    List<Row> expectedDay2 = Collections.singletonList(JAN_2_M_10);
    List<Row> expectedDay3 = Collections.singletonList(JAN_3_M_10);
    List<Row> expectedDay4 = Collections.emptyList();

    intervals = new ArrayList<>();
    intervals.add(INTERVAL_JAN_1_4);

    rows = new ArrayList<>();
    rows.add(JAN_2_M_10);
    rows.add(JAN_3_M_10);

    Sequence<Row> seq = Sequences.simple(rows);
    RowBucketIterable rbi = new RowBucketIterable(seq, intervals, ONE_DAY);
    Iterator<RowBucket> iter = rbi.iterator();

    RowBucket actual = iter.next();
    Assert.assertEquals(JAN_1, actual.getDateTime());
    Assert.assertEquals(expectedDay1, actual.getRows());

    actual = iter.next();
    Assert.assertEquals(JAN_2, actual.getDateTime());
    Assert.assertEquals(expectedDay2, actual.getRows());

    actual = iter.next();
    Assert.assertEquals(JAN_3, actual.getDateTime());
    Assert.assertEquals(expectedDay3, actual.getRows());

    actual = iter.next();
    Assert.assertEquals(JAN_4, actual.getDateTime());
    Assert.assertEquals(expectedDay4, actual.getRows());
  }
"
"  @Test
  public void testMultipleMissingDays()
  {
    List<Row> expectedDay1 = Collections.emptyList();
    List<Row> expectedDay2 = Collections.singletonList(JAN_2_M_10);
    List<Row> expectedDay3 = Collections.emptyList();
    List<Row> expectedDay4 = Collections.singletonList(JAN_4_M_10);

    intervals = new ArrayList<>();
    intervals.add(INTERVAL_JAN_1_4);

    rows = new ArrayList<>();
    rows.add(JAN_2_M_10);
    rows.add(JAN_4_M_10);

    Sequence<Row> seq = Sequences.simple(rows);
    RowBucketIterable rbi = new RowBucketIterable(seq, intervals, ONE_DAY);
    Iterator<RowBucket> iter = rbi.iterator();

    RowBucket actual = iter.next();
    Assert.assertEquals(JAN_1, actual.getDateTime());
    Assert.assertEquals(expectedDay1, actual.getRows());

    actual = iter.next();
    Assert.assertEquals(JAN_2, actual.getDateTime());
    Assert.assertEquals(expectedDay2, actual.getRows());

    actual = iter.next();
    Assert.assertEquals(JAN_3, actual.getDateTime());
    Assert.assertEquals(expectedDay3, actual.getRows());

    actual = iter.next();
    Assert.assertEquals(JAN_4, actual.getDateTime());
    Assert.assertEquals(expectedDay4, actual.getRows());
  }
"
"  @Test
  public void testMultipleMissingDaysMultipleRowAtTheEnd()
  {
    List<Row> expectedDay1 = Collections.emptyList();
    List<Row> expectedDay2 = Collections.singletonList(JAN_2_M_10);
    List<Row> expectedDay3 = Collections.emptyList();
    List<Row> expectedDay4 = Collections.singletonList(JAN_4_M_10);
    List<Row> expectedDay5 = Collections.singletonList(JAN_5_M_10);

    intervals = new ArrayList<>();
    intervals.add(INTERVAL_JAN_1_5);

    rows = new ArrayList<>();
    rows.add(JAN_2_M_10);
    rows.add(JAN_4_M_10);
    rows.add(JAN_5_M_10);

    Sequence<Row> seq = Sequences.simple(rows);
    RowBucketIterable rbi = new RowBucketIterable(seq, intervals, ONE_DAY);
    Iterator<RowBucket> iter = rbi.iterator();

    RowBucket actual = iter.next();
    Assert.assertEquals(JAN_1, actual.getDateTime());
    Assert.assertEquals(expectedDay1, actual.getRows());

    actual = iter.next();
    Assert.assertEquals(JAN_2, actual.getDateTime());
    Assert.assertEquals(expectedDay2, actual.getRows());

    actual = iter.next();
    Assert.assertEquals(JAN_3, actual.getDateTime());
    Assert.assertEquals(expectedDay3, actual.getRows());

    actual = iter.next();
    Assert.assertEquals(JAN_4, actual.getDateTime());
    Assert.assertEquals(expectedDay4, actual.getRows());

    actual = iter.next();
    Assert.assertEquals(JAN_5, actual.getDateTime());
    Assert.assertEquals(expectedDay5, actual.getRows());
  }
"
"  @Test
  public void testMissingDaysInMiddleOneRow()
  {
    List<Row> expectedDay1 = Collections.singletonList(JAN_1_M_10);
    List<Row> expectedDay2 = Collections.singletonList(JAN_2_M_10);
    List<Row> expectedDay3 = Collections.emptyList();
    List<Row> expectedDay4 = Collections.singletonList(JAN_4_M_10);

    rows = new ArrayList<>();
    rows.add(JAN_1_M_10);
    rows.add(JAN_2_M_10);
    rows.add(JAN_4_M_10);

    intervals = new ArrayList<>();
    intervals.add(INTERVAL_JAN_1_4);

    Sequence<Row> seq = Sequences.simple(rows);
    RowBucketIterable rbi = new RowBucketIterable(seq, intervals, ONE_DAY);
    Iterator<RowBucket> iter = rbi.iterator();

    RowBucket actual = iter.next();
    Assert.assertEquals(expectedDay1, actual.getRows());

    actual = iter.next();
    Assert.assertEquals(expectedDay2, actual.getRows());

    actual = iter.next();
    Assert.assertEquals(JAN_3, actual.getDateTime());
    Assert.assertEquals(expectedDay3, actual.getRows());

    actual = iter.next();
    Assert.assertEquals(expectedDay4, actual.getRows());
  }
"
"  @Test
  public void testMissingDaysInMiddleMultipleRow()
  {
    List<Row> expectedDay1 = Collections.singletonList(JAN_1_M_10);
    List<Row> expectedDay2 = Collections.emptyList();
    List<Row> expectedDay3 = Collections.singletonList(JAN_3_M_10);
    List<Row> expectedDay4 = Collections.singletonList(JAN_4_M_10);

    intervals = new ArrayList<>();
    intervals.add(INTERVAL_JAN_1_4);

    rows = new ArrayList<>();
    rows.add(JAN_1_M_10);
    rows.add(JAN_3_M_10);
    rows.add(JAN_4_M_10);

    Sequence<Row> seq = Sequences.simple(rows);
    RowBucketIterable rbi = new RowBucketIterable(seq, intervals, ONE_DAY);
    Iterator<RowBucket> iter = rbi.iterator();

    RowBucket actual = iter.next();
    Assert.assertEquals(JAN_1, actual.getDateTime());
    Assert.assertEquals(expectedDay1, actual.getRows());

    actual = iter.next();
    Assert.assertEquals(JAN_2, actual.getDateTime());
    Assert.assertEquals(expectedDay2, actual.getRows());

    actual = iter.next();
    Assert.assertEquals(JAN_3, actual.getDateTime());
    Assert.assertEquals(expectedDay3, actual.getRows());

    actual = iter.next();
    Assert.assertEquals(JAN_4, actual.getDateTime());
    Assert.assertEquals(expectedDay4, actual.getRows());
  }
"
"  @Test
  public void testApplyLastDayNoRows()
  {
    intervals = new ArrayList<>();
    intervals.add(INTERVAL_JAN_1_4);

    List<Row> expectedDay1 = Arrays.asList(JAN_1_M_10, JAN_1_F_20);
    List<Row> expectedDay2 = Collections.singletonList(JAN_2_M_10);
    List<Row> expectedDay3 = Collections.singletonList(JAN_3_F_20);
    List<Row> expectedDay4 = Collections.emptyList();

    rows = new ArrayList<>();
    rows.add(JAN_1_M_10);
    rows.add(JAN_1_F_20);
    rows.add(JAN_2_M_10);
    rows.add(JAN_3_F_20);

    Sequence<Row> seq = Sequences.simple(rows);
    RowBucketIterable rbi = new RowBucketIterable(seq, intervals, ONE_DAY);
    Iterator<RowBucket> iter = rbi.iterator();

    RowBucket actual = iter.next();
    Assert.assertEquals(expectedDay1, actual.getRows());

    actual = iter.next();
    Assert.assertEquals(expectedDay2, actual.getRows());

    actual = iter.next();
    Assert.assertEquals(expectedDay3, actual.getRows());

    actual = iter.next();
    Assert.assertEquals(JAN_4, actual.getDateTime());
    Assert.assertEquals(expectedDay4, actual.getRows());
  }
"
"  @Test
  public void testApplyLastTwoDayNoRows()
  {
    List<Row> expectedDay1 = Arrays.asList(JAN_1_M_10, JAN_1_F_20);
    List<Row> expectedDay2 = Collections.singletonList(JAN_2_M_10);
    List<Row> expectedDay3 = Collections.emptyList();
    List<Row> expectedDay4 = Collections.emptyList();

    rows = new ArrayList<>();
    rows.add(JAN_1_M_10);
    rows.add(JAN_1_F_20);
    rows.add(JAN_2_M_10);

    intervals = new ArrayList<>();
    intervals.add(INTERVAL_JAN_1_4);

    Sequence<Row> seq = Sequences.simple(rows);
    RowBucketIterable rbi = new RowBucketIterable(seq, intervals, ONE_DAY);
    Iterator<RowBucket> iter = rbi.iterator();

    RowBucket actual = iter.next();
    Assert.assertEquals(expectedDay1, actual.getRows());

    actual = iter.next();
    Assert.assertEquals(expectedDay2, actual.getRows());

    actual = iter.next();
    Assert.assertEquals(JAN_3, actual.getDateTime());
    Assert.assertEquals(expectedDay3, actual.getRows());

    actual = iter.next();
    Assert.assertEquals(JAN_4, actual.getDateTime());
    Assert.assertEquals(expectedDay4, actual.getRows());
  }
"
"  @Test
  public void testApplyMultipleInterval()
  {
    intervals = new ArrayList<>();
    intervals.add(INTERVAL_JAN_1_4);
    intervals.add(INTERVAL_JAN_6_8);

    List<Row> expectedDay1 = Arrays.asList(JAN_1_M_10, JAN_1_F_20);
    List<Row> expectedDay2 = Collections.singletonList(JAN_2_M_10);
    List<Row> expectedDay3 = Collections.singletonList(JAN_3_F_20);
    List<Row> expectedDay4 = Arrays.asList(JAN_4_M_10, JAN_4_F_20, JAN_4_U_30);
    List<Row> expectedDay6 = Collections.singletonList(JAN_6_M_10);
    List<Row> expectedDay7 = Collections.singletonList(JAN_7_F_20);
    List<Row> expectedDay8 = Collections.singletonList(JAN_8_U_30);

    rows = new ArrayList<>();
    rows.add(JAN_1_M_10);
    rows.add(JAN_1_F_20);
    rows.add(JAN_2_M_10);
    rows.add(JAN_3_F_20);
    rows.add(JAN_4_M_10);
    rows.add(JAN_4_F_20);
    rows.add(JAN_4_U_30);
    rows.add(JAN_6_M_10);
    rows.add(JAN_7_F_20);
    rows.add(JAN_8_U_30);

    Sequence<Row> seq = Sequences.simple(rows);
    RowBucketIterable rbi = new RowBucketIterable(seq, intervals, ONE_DAY);
    Iterator<RowBucket> iter = rbi.iterator();

    RowBucket actual = iter.next();
    Assert.assertEquals(expectedDay1, actual.getRows());

    actual = iter.next();
    Assert.assertEquals(expectedDay2, actual.getRows());

    actual = iter.next();
    Assert.assertEquals(expectedDay3, actual.getRows());

    actual = iter.next();
    Assert.assertEquals(expectedDay4, actual.getRows());

    actual = iter.next();
    Assert.assertEquals(expectedDay6, actual.getRows());

    actual = iter.next();
    Assert.assertEquals(expectedDay7, actual.getRows());

    actual = iter.next();
    Assert.assertEquals(expectedDay8, actual.getRows());
  }
"
"  @Test
  public void testNodata()
  {
    intervals = new ArrayList<>();
    intervals.add(INTERVAL_JAN_1_4);
    intervals.add(INTERVAL_JAN_6_8);

    rows = new ArrayList<>();

    Sequence<Row> seq = Sequences.simple(rows);
    RowBucketIterable rbi = new RowBucketIterable(seq, intervals, ONE_DAY);
    Iterator<RowBucket> iter = rbi.iterator();

    Assert.assertTrue(iter.hasNext());
    RowBucket actual = iter.next();
    Assert.assertEquals(Collections.emptyList(), actual.getRows());
  }
"
"  @Test
  public void testResultArraySignature()
  {
    final TimeseriesQuery query =
        Druids.newTimeseriesQueryBuilder()
              .dataSource(""dummy"")
              .intervals(""2000/3000"")
              .granularity(Granularities.HOUR)
              .aggregators(
                  new CountAggregatorFactory(""count""),
                  new TDigestSketchAggregatorFactory(""tdigest"", ""col"", null)
              )
              .postAggregators(
                  new FieldAccessPostAggregator(""tdigest-access"", ""tdigest""),
                  new FinalizingFieldAccessPostAggregator(""tdigest-finalize"", ""tdigest"")
              )
              .build();

    Assert.assertEquals(
        RowSignature.builder()
                    .addTimeColumn()
                    .add(""count"", ColumnType.LONG)
                    .add(""tdigest"", TDigestSketchAggregatorFactory.TYPE)
                    .add(""tdigest-access"", TDigestSketchAggregatorFactory.TYPE)
                    .add(""tdigest-finalize"", TDigestSketchAggregatorFactory.TYPE)
                    .build(),
        new TimeseriesQueryQueryToolChest().resultArraySignature(query)
    );
  }
"
"  @Test
  public void testSerde() throws Exception
  {
    TDigestSketchToQuantilePostAggregator there =
        new TDigestSketchToQuantilePostAggregator(""post"", new ConstantPostAggregator("""", 100), 0.5);

    DefaultObjectMapper mapper = new DefaultObjectMapper();
    TDigestSketchToQuantilePostAggregator andBackAgain = mapper.readValue(
        mapper.writeValueAsString(there),
        TDigestSketchToQuantilePostAggregator.class
    );

    Assert.assertEquals(there, andBackAgain);
    Assert.assertArrayEquals(there.getCacheKey(), andBackAgain.getCacheKey());
    Assert.assertEquals(there.getDependentFields(), andBackAgain.getDependentFields());
  }
"
"  @Test
  public void testToString()
  {
    PostAggregator postAgg =
        new TDigestSketchToQuantilePostAggregator(""post"", new ConstantPostAggregator("""", 100), 0.5);

    Assert.assertEquals(
        ""TDigestSketchToQuantilePostAggregator{name='post', field=ConstantPostAggregator{name='', constantValue=100}, fraction=0.5}"",
        postAgg.toString()
    );
  }
"
"  @Test
  public void testEquals()
  {
    EqualsVerifier.forClass(TDigestSketchToQuantilePostAggregator.class)
                  .withNonnullFields(""name"", ""field"", ""fraction"")
                  .usingGetClass()
                  .verify();
  }
"
"  @Test
  public void testSerde() throws Exception
  {
    TDigestSketchToQuantilesPostAggregator there =
        new TDigestSketchToQuantilesPostAggregator(""post"", new ConstantPostAggregator("""", 100), new double[]{0.25, 0.75});

    DefaultObjectMapper mapper = new DefaultObjectMapper();
    TDigestSketchToQuantilesPostAggregator andBackAgain = mapper.readValue(
        mapper.writeValueAsString(there),
        TDigestSketchToQuantilesPostAggregator.class
    );

    Assert.assertEquals(there, andBackAgain);
    Assert.assertArrayEquals(there.getCacheKey(), andBackAgain.getCacheKey());
    Assert.assertEquals(there.getDependentFields(), andBackAgain.getDependentFields());
  }
"
"    @Test
    public void shouldRecordStarts() {
        assertEquals(0, counter.starts());
        counter.recordStart();
        assertEquals(1, counter.starts());
        counter.recordStart();
        assertEquals(2, counter.starts());
        assertEquals(2, counter.starts());
    }
"
"    @Test
    public void shouldRecordStops() {
        assertEquals(0, counter.stops());
        counter.recordStop();
        assertEquals(1, counter.stops());
        counter.recordStop();
        assertEquals(2, counter.stops());
        assertEquals(2, counter.stops());
    }
"
"    @Test
    public void shouldExpectRestarts() throws Exception {
        waiters = Executors.newSingleThreadExecutor();

        latch = counter.expectedRestarts(1);
        Future<Boolean> future = asyncAwait(100, TimeUnit.MILLISECONDS);

        clock.sleep(1000);
        counter.recordStop();
        counter.recordStart();
        assertTrue(future.get(200, TimeUnit.MILLISECONDS));
        assertTrue(future.isDone());
    }
"
"    @Test
    public void shouldFailToWaitForRestartThatNeverHappens() throws Exception {
        waiters = Executors.newSingleThreadExecutor();

        latch = counter.expectedRestarts(1);
        Future<Boolean> future = asyncAwait(100, TimeUnit.MILLISECONDS);

        clock.sleep(1000);
        // Record a stop but NOT a start
        counter.recordStop();
        assertFalse(future.get(200, TimeUnit.MILLISECONDS));
        assertTrue(future.isDone());
    }
"
"    @Test
    public void testAddAndRemoveWorker() throws Exception {
        connect = connectBuilder.build();
        // start the clusters
        connect.start();

        int numTasks = 4;
        // create test topic
        connect.kafka().createTopic(""test-topic"", NUM_TOPIC_PARTITIONS);

        // setup up props for the sink connector
        Map<String, String> props = new HashMap<>();
        props.put(CONNECTOR_CLASS_CONFIG, MonitorableSourceConnector.class.getSimpleName());
        props.put(TASKS_MAX_CONFIG, String.valueOf(numTasks));
        props.put(""throughput"", String.valueOf(1));
        props.put(""messages.per.poll"", String.valueOf(10));
        props.put(KEY_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());
        props.put(VALUE_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());

        waitForCondition(() -> assertWorkersUp(NUM_WORKERS).orElse(false),
                WORKER_SETUP_DURATION_MS, ""Initial group of workers did not start in time."");

        // start a source connector
        connect.configureConnector(CONNECTOR_NAME, props);

        waitForCondition(() -> assertConnectorAndTasksRunning(CONNECTOR_NAME, numTasks).orElse(false),
                CONNECTOR_SETUP_DURATION_MS, ""Connector tasks did not start in time."");

        WorkerHandle extraWorker = connect.addWorker();

        waitForCondition(() -> assertWorkersUp(NUM_WORKERS + 1).orElse(false),
                WORKER_SETUP_DURATION_MS, ""Expanded group of workers did not start in time."");

        waitForCondition(() -> assertConnectorAndTasksRunning(CONNECTOR_NAME, numTasks).orElse(false),
                CONNECTOR_SETUP_DURATION_MS, ""Connector tasks are not all in running state."");

        Set<WorkerHandle> workers = connect.activeWorkers();
        assertTrue(workers.contains(extraWorker));

        connect.removeWorker(extraWorker);

        waitForCondition(() -> assertWorkersUp(NUM_WORKERS).orElse(false) && !assertWorkersUp(NUM_WORKERS + 1).orElse(false),
                WORKER_SETUP_DURATION_MS, ""Group of workers did not shrink in time."");

        workers = connect.activeWorkers();
        assertFalse(workers.contains(extraWorker));
    }
"
"    @Test
    public void testRestartFailedTask() throws Exception {
        connect = connectBuilder.build();
        // start the clusters
        connect.start();

        int numTasks = 1;

        // Properties for the source connector. The task should fail at startup due to the bad broker address.
        Map<String, String> connectorProps = new HashMap<>();
        connectorProps.put(CONNECTOR_CLASS_CONFIG, MonitorableSourceConnector.class.getName());
        connectorProps.put(TASKS_MAX_CONFIG, Objects.toString(numTasks));
        connectorProps.put(CONNECTOR_CLIENT_PRODUCER_OVERRIDES_PREFIX + BOOTSTRAP_SERVERS_CONFIG, ""nobrokerrunningatthisaddress"");

        waitForCondition(() -> assertWorkersUp(NUM_WORKERS).orElse(false),
                WORKER_SETUP_DURATION_MS, ""Initial group of workers did not start in time."");

        // Try to start the connector and its single task.
        connect.configureConnector(CONNECTOR_NAME, connectorProps);

        waitForCondition(() -> assertConnectorTasksFailed(CONNECTOR_NAME, numTasks).orElse(false),
                CONNECTOR_SETUP_DURATION_MS, ""Connector tasks did not fail in time"");

        // Reconfigure the connector without the bad broker address.
        connectorProps.remove(CONNECTOR_CLIENT_PRODUCER_OVERRIDES_PREFIX + BOOTSTRAP_SERVERS_CONFIG);
        connect.configureConnector(CONNECTOR_NAME, connectorProps);

        // Restart the failed task
        String taskRestartEndpoint = connect.endpointForResource(
            String.format(""connectors/%s/tasks/0/restart"", CONNECTOR_NAME));
        connect.executePost(taskRestartEndpoint, """", Collections.emptyMap());

        // Ensure the task started successfully this time
        waitForCondition(() -> assertConnectorAndTasksRunning(CONNECTOR_NAME, numTasks).orElse(false),
            CONNECTOR_SETUP_DURATION_MS, ""Connector tasks are not all in running state."");
    }
"
"    @Test
    public void testBrokerCoordinator() throws Exception {
        workerProps.put(DistributedConfig.SCHEDULED_REBALANCE_MAX_DELAY_MS_CONFIG, String.valueOf(5000));
        connect = connectBuilder.workerProps(workerProps).build();
        // start the clusters
        connect.start();
        int numTasks = 4;
        // create test topic
        connect.kafka().createTopic(""test-topic"", NUM_TOPIC_PARTITIONS);

        // setup up props for the sink connector
        Map<String, String> props = new HashMap<>();
        props.put(CONNECTOR_CLASS_CONFIG, MonitorableSourceConnector.class.getSimpleName());
        props.put(TASKS_MAX_CONFIG, String.valueOf(numTasks));
        props.put(""topic"", ""test-topic"");
        props.put(""throughput"", String.valueOf(1));
        props.put(""messages.per.poll"", String.valueOf(10));
        props.put(KEY_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());
        props.put(VALUE_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());

        waitForCondition(() -> assertWorkersUp(NUM_WORKERS).orElse(false),
                WORKER_SETUP_DURATION_MS, ""Initial group of workers did not start in time."");

        // start a source connector
        connect.configureConnector(CONNECTOR_NAME, props);

        waitForCondition(() -> assertConnectorAndTasksRunning(CONNECTOR_NAME, numTasks).orElse(false),
                CONNECTOR_SETUP_DURATION_MS, ""Connector tasks did not start in time."");

        connect.kafka().stopOnlyKafka();

        waitForCondition(() -> assertWorkersUp(NUM_WORKERS).orElse(false),
                WORKER_SETUP_DURATION_MS, ""Group of workers did not remain the same after broker shutdown"");

        // Allow for the workers to discover that the coordinator is unavailable, wait is
        // heartbeat timeout * 2 + 4sec
        Thread.sleep(TimeUnit.SECONDS.toMillis(10));

        connect.kafka().startOnlyKafkaOnSamePorts();

        // Allow for the kafka brokers to come back online
        Thread.sleep(TimeUnit.SECONDS.toMillis(10));

        waitForCondition(() -> assertWorkersUp(NUM_WORKERS).orElse(false),
                WORKER_SETUP_DURATION_MS, ""Group of workers did not remain the same within the ""
                        + ""designated time."");

        // Allow for the workers to rebalance and reach a steady state
        Thread.sleep(TimeUnit.SECONDS.toMillis(10));

        waitForCondition(() -> assertConnectorAndTasksRunning(CONNECTOR_NAME, numTasks).orElse(false),
                CONNECTOR_SETUP_DURATION_MS, ""Connector tasks did not start in time."");
    }
"
"    @Test
    public void testStartTwoConnectors() throws Exception {
        // create test topic
        connect.kafka().createTopic(TOPIC_NAME, NUM_TOPIC_PARTITIONS);

        // setup up props for the source connector
        Map<String, String> props = new HashMap<>();
        props.put(CONNECTOR_CLASS_CONFIG, MonitorableSourceConnector.class.getSimpleName());
        props.put(TASKS_MAX_CONFIG, String.valueOf(NUM_TASKS));
        props.put(""throughput"", String.valueOf(1));
        props.put(""messages.per.poll"", String.valueOf(10));
        props.put(TOPIC_CONFIG, TOPIC_NAME);
        props.put(KEY_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());
        props.put(VALUE_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());

        // start a source connector
        connect.configureConnector(CONNECTOR_NAME, props);

        waitForCondition(() -> this.assertConnectorAndTasksRunning(CONNECTOR_NAME, NUM_TASKS).orElse(false),
                CONNECTOR_SETUP_DURATION_MS, ""Connector tasks did not start in time."");

        // start a source connector
        connect.configureConnector(""another-source"", props);

        waitForCondition(() -> this.assertConnectorAndTasksRunning(CONNECTOR_NAME, NUM_TASKS).orElse(false),
                CONNECTOR_SETUP_DURATION_MS, ""Connector tasks did not start in time."");

        waitForCondition(() -> this.assertConnectorAndTasksRunning(""another-source"", 4).orElse(false),
                CONNECTOR_SETUP_DURATION_MS, ""Connector tasks did not start in time."");
    }
"
"    @Test
    public void testReconfigConnector() throws Exception {
        ConnectorHandle connectorHandle = RuntimeHandles.get().connectorHandle(CONNECTOR_NAME);

        // create test topic
        String anotherTopic = ""another-topic"";
        connect.kafka().createTopic(TOPIC_NAME, NUM_TOPIC_PARTITIONS);
        connect.kafka().createTopic(anotherTopic, NUM_TOPIC_PARTITIONS);

        // setup up props for the source connector
        Map<String, String> props = new HashMap<>();
        props.put(CONNECTOR_CLASS_CONFIG, MonitorableSourceConnector.class.getSimpleName());
        props.put(TASKS_MAX_CONFIG, String.valueOf(NUM_TASKS));
        props.put(""throughput"", String.valueOf(1));
        props.put(""messages.per.poll"", String.valueOf(10));
        props.put(TOPIC_CONFIG, TOPIC_NAME);
        props.put(KEY_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());
        props.put(VALUE_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());

        // start a source connector
        connect.configureConnector(CONNECTOR_NAME, props);

        waitForCondition(() -> this.assertConnectorAndTasksRunning(CONNECTOR_NAME, NUM_TASKS).orElse(false),
                CONNECTOR_SETUP_DURATION_MS, ""Connector tasks did not start in time."");

        int numRecordsProduced = 100;
        long recordTransferDurationMs = TimeUnit.SECONDS.toMillis(30);

        // consume all records from the source topic or fail, to ensure that they were correctly produced
        int recordNum = connect.kafka().consume(numRecordsProduced, recordTransferDurationMs, TOPIC_NAME).count();
        assertTrue(""Not enough records produced by source connector. Expected at least: "" + numRecordsProduced + "" + but got "" + recordNum,
                recordNum >= numRecordsProduced);

        // expect that we're going to restart the connector and its tasks
        StartAndStopLatch restartLatch = connectorHandle.expectedStarts(1);

        // Reconfigure the source connector by changing the Kafka topic used as output
        props.put(TOPIC_CONFIG, anotherTopic);
        connect.configureConnector(CONNECTOR_NAME, props);

        // Wait for the connector *and tasks* to be restarted
        assertTrue(""Failed to alter connector configuration and see connector and tasks restart ""
                   + ""within "" + CONNECTOR_SETUP_DURATION_MS + ""ms"",
                restartLatch.await(CONNECTOR_SETUP_DURATION_MS, TimeUnit.MILLISECONDS));

        // And wait for the Connect to show the connectors and tasks are running
        waitForCondition(() -> this.assertConnectorAndTasksRunning(CONNECTOR_NAME, NUM_TASKS).orElse(false),
                CONNECTOR_SETUP_DURATION_MS, ""Connector tasks did not start in time."");

        // consume all records from the source topic or fail, to ensure that they were correctly produced
        recordNum = connect.kafka().consume(numRecordsProduced, recordTransferDurationMs, anotherTopic).count();
        assertTrue(""Not enough records produced by source connector. Expected at least: "" + numRecordsProduced + "" + but got "" + recordNum,
                recordNum >= numRecordsProduced);
    }
"
"    @Test
    public void testDeleteConnector() throws Exception {
        // create test topic
        connect.kafka().createTopic(TOPIC_NAME, NUM_TOPIC_PARTITIONS);

        // setup up props for the source connector
        Map<String, String> props = new HashMap<>();
        props.put(CONNECTOR_CLASS_CONFIG, MonitorableSourceConnector.class.getSimpleName());
        props.put(TASKS_MAX_CONFIG, String.valueOf(NUM_TASKS));
        props.put(""throughput"", String.valueOf(1));
        props.put(""messages.per.poll"", String.valueOf(10));
        props.put(TOPIC_CONFIG, TOPIC_NAME);
        props.put(KEY_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());
        props.put(VALUE_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());

        waitForCondition(() -> this.assertWorkersUp(3),
                WORKER_SETUP_DURATION_MS, ""Connect workers did not start in time."");

        // start a source connector
        IntStream.range(0, 4).forEachOrdered(
            i -> {
                try {
                    connect.configureConnector(CONNECTOR_NAME + i, props);
                } catch (IOException e) {
                    throw new ConnectException(e);
                }
            });

        waitForCondition(() -> this.assertConnectorAndTasksRunning(CONNECTOR_NAME + 3, NUM_TASKS).orElse(true),
                CONNECTOR_SETUP_DURATION_MS, ""Connector tasks did not start in time."");

        // delete connector
        connect.deleteConnector(CONNECTOR_NAME + 3);

        waitForCondition(() -> !this.assertConnectorAndTasksRunning(CONNECTOR_NAME + 3, NUM_TASKS).orElse(true),
                CONNECTOR_SETUP_DURATION_MS, ""Connector tasks did not stop in time."");

        waitForCondition(this::assertConnectorAndTasksAreUnique,
                WORKER_SETUP_DURATION_MS, ""Connect and tasks are imbalanced between the workers."");
    }
"
"    @Test
    public void testAddingWorker() throws Exception {
        // create test topic
        connect.kafka().createTopic(TOPIC_NAME, NUM_TOPIC_PARTITIONS);

        // setup up props for the source connector
        Map<String, String> props = new HashMap<>();
        props.put(CONNECTOR_CLASS_CONFIG, MonitorableSourceConnector.class.getSimpleName());
        props.put(TASKS_MAX_CONFIG, String.valueOf(NUM_TASKS));
        props.put(""throughput"", String.valueOf(1));
        props.put(""messages.per.poll"", String.valueOf(10));
        props.put(TOPIC_CONFIG, TOPIC_NAME);
        props.put(KEY_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());
        props.put(VALUE_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());

        waitForCondition(() -> this.assertWorkersUp(3),
                WORKER_SETUP_DURATION_MS, ""Connect workers did not start in time."");

        // start a source connector
        IntStream.range(0, 4).forEachOrdered(
            i -> {
                try {
                    connect.configureConnector(CONNECTOR_NAME + i, props);
                } catch (IOException e) {
                    throw new ConnectException(e);
                }
            });

        waitForCondition(() -> this.assertConnectorAndTasksRunning(CONNECTOR_NAME + 3, NUM_TASKS).orElse(false),
                CONNECTOR_SETUP_DURATION_MS, ""Connector tasks did not start in time."");

        connect.addWorker();

        waitForCondition(() -> this.assertWorkersUp(4),
                WORKER_SETUP_DURATION_MS, ""Connect workers did not start in time."");

        waitForCondition(() -> this.assertConnectorAndTasksRunning(CONNECTOR_NAME + 3, NUM_TASKS).orElse(false),
                CONNECTOR_SETUP_DURATION_MS, ""Connector tasks did not start in time."");

        waitForCondition(this::assertConnectorAndTasksAreUnique,
                WORKER_SETUP_DURATION_MS, ""Connect and tasks are imbalanced between the workers."");
    }
"
"    @Test
    public void testRemovingWorker() throws Exception {
        // create test topic
        connect.kafka().createTopic(TOPIC_NAME, NUM_TOPIC_PARTITIONS);

        // setup up props for the source connector
        Map<String, String> props = new HashMap<>();
        props.put(CONNECTOR_CLASS_CONFIG, MonitorableSourceConnector.class.getSimpleName());
        props.put(TASKS_MAX_CONFIG, String.valueOf(NUM_TASKS));
        props.put(""throughput"", String.valueOf(1));
        props.put(""messages.per.poll"", String.valueOf(10));
        props.put(TOPIC_CONFIG, TOPIC_NAME);
        props.put(KEY_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());
        props.put(VALUE_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());

        waitForCondition(() -> this.assertWorkersUp(3),
                WORKER_SETUP_DURATION_MS, ""Connect workers did not start in time."");

        // start a source connector
        IntStream.range(0, 4).forEachOrdered(
            i -> {
                try {
                    connect.configureConnector(CONNECTOR_NAME + i, props);
                } catch (IOException e) {
                    throw new ConnectException(e);
                }
            });

        waitForCondition(() -> this.assertConnectorAndTasksRunning(CONNECTOR_NAME + 3, NUM_TASKS).orElse(false),
                CONNECTOR_SETUP_DURATION_MS, ""Connector tasks did not start in time."");

        connect.removeWorker();

        waitForCondition(() -> this.assertWorkersUp(2),
                WORKER_SETUP_DURATION_MS, ""Connect workers did not start in time."");

        waitForCondition(this::assertConnectorAndTasksAreUnique,
                WORKER_SETUP_DURATION_MS, ""Connect and tasks are imbalanced between the workers."");
    }
"
"    @Test
    public void ensureInternalEndpointIsSecured() throws Throwable {
        final String connectorTasksEndpoint = connect.endpointForResource(String.format(
            ""connectors/%s/tasks"",
            CONNECTOR_NAME
        ));
        final Map<String, String> emptyHeaders = new HashMap<>();
        final Map<String, String> invalidSignatureHeaders = new HashMap<>();
        invalidSignatureHeaders.put(SIGNATURE_HEADER, ""S2Fma2Flc3F1ZQ=="");
        invalidSignatureHeaders.put(SIGNATURE_ALGORITHM_HEADER, ""HmacSHA256"");

        // We haven't created the connector yet, but this should still return a 400 instead of a 404
        // if the endpoint is secured
        log.info(
            ""Making a POST request to the {} endpoint with no connector started and no signature header; "" 
                + ""expecting 400 error response"",
            connectorTasksEndpoint
        );
        assertEquals(
            BAD_REQUEST.getStatusCode(),
            connect.executePost(connectorTasksEndpoint, ""[]"", emptyHeaders)
        );

        // Try again, but with an invalid signature
        log.info(
            ""Making a POST request to the {} endpoint with no connector started and an invalid signature header; ""
                + ""expecting 403 error response"",
            connectorTasksEndpoint
        );
        assertEquals(
            FORBIDDEN.getStatusCode(),
            connect.executePost(connectorTasksEndpoint, ""[]"", invalidSignatureHeaders)
        );

        // Create the connector now
        // setup up props for the sink connector
        Map<String, String> connectorProps = new HashMap<>();
        connectorProps.put(CONNECTOR_CLASS_CONFIG, MonitorableSinkConnector.class.getSimpleName());
        connectorProps.put(TASKS_MAX_CONFIG, String.valueOf(1));
        connectorProps.put(TOPICS_CONFIG, ""test-topic"");
        connectorProps.put(KEY_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());
        connectorProps.put(VALUE_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());

        // start a sink connector
        log.info(""Starting the {} connector"", CONNECTOR_NAME);
        StartAndStopLatch startLatch = connectorHandle.expectedStarts(1);
        connect.configureConnector(CONNECTOR_NAME, connectorProps);
        startLatch.await(CONNECTOR_SETUP_DURATION_MS, TimeUnit.MILLISECONDS);


        // Verify the exact same behavior, after starting the connector

        // We haven't created the connector yet, but this should still return a 400 instead of a 404
        // if the endpoint is secured
        log.info(
            ""Making a POST request to the {} endpoint with the connector started and no signature header; ""
                + ""expecting 400 error response"",
            connectorTasksEndpoint
        );
        assertEquals(
            BAD_REQUEST.getStatusCode(),
            connect.executePost(connectorTasksEndpoint, ""[]"", emptyHeaders)
        );

        // Try again, but with an invalid signature
        log.info(
            ""Making a POST request to the {} endpoint with the connector started and an invalid signature header; ""
                + ""expecting 403 error response"",
            connectorTasksEndpoint
        );
        assertEquals(
            FORBIDDEN.getStatusCode(),
            connect.executePost(connectorTasksEndpoint, ""[]"", invalidSignatureHeaders)
        );
    }
"
"    @Test
    public void testSkipRetryAndDLQWithHeaders() throws Exception {
        // create test topic
        connect.kafka().createTopic(""test-topic"");

        // setup connector config
        Map<String, String> props = new HashMap<>();
        props.put(CONNECTOR_CLASS_CONFIG, MonitorableSinkConnector.class.getSimpleName());
        props.put(TASKS_MAX_CONFIG, String.valueOf(NUM_TASKS));
        props.put(TOPICS_CONFIG, ""test-topic"");
        props.put(KEY_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());
        props.put(VALUE_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());
        props.put(TRANSFORMS_CONFIG, ""failing_transform"");
        props.put(""transforms.failing_transform.type"", FaultyPassthrough.class.getName());

        // log all errors, along with message metadata
        props.put(ERRORS_LOG_ENABLE_CONFIG, ""true"");
        props.put(ERRORS_LOG_INCLUDE_MESSAGES_CONFIG, ""true"");

        // produce bad messages into dead letter queue
        props.put(DLQ_TOPIC_NAME_CONFIG, DLQ_TOPIC);
        props.put(DLQ_CONTEXT_HEADERS_ENABLE_CONFIG, ""true"");
        props.put(DLQ_TOPIC_REPLICATION_FACTOR_CONFIG, ""1"");

        // tolerate all erros
        props.put(ERRORS_TOLERANCE_CONFIG, ""all"");

        // retry for up to one second
        props.put(ERRORS_RETRY_TIMEOUT_CONFIG, ""1000"");

        // set expected records to successfully reach the task
        connectorHandle.taskHandle(TASK_ID).expectedRecords(EXPECTED_CORRECT_RECORDS);

        connect.configureConnector(CONNECTOR_NAME, props);

        waitForCondition(this::checkForPartitionAssignment,
                CONNECTOR_SETUP_DURATION_MS,
                ""Connector task was not assigned a partition."");

        // produce some strings into test topic
        for (int i = 0; i < NUM_RECORDS_PRODUCED; i++) {
            connect.kafka().produce(""test-topic"", ""key-"" + i, ""value-"" + i);
        }

        // consume all records from test topic
        log.info(""Consuming records from test topic"");
        int i = 0;
        for (ConsumerRecord<byte[], byte[]> rec : connect.kafka().consume(NUM_RECORDS_PRODUCED, CONSUME_MAX_DURATION_MS, ""test-topic"")) {
            String k = new String(rec.key());
            String v = new String(rec.value());
            log.debug(""Consumed record (key='{}', value='{}') from topic {}"", k, v, rec.topic());
            assertEquals(""Unexpected key"", k, ""key-"" + i);
            assertEquals(""Unexpected value"", v, ""value-"" + i);
            i++;
        }

        // wait for records to reach the task
        connectorHandle.taskHandle(TASK_ID).awaitRecords(CONSUME_MAX_DURATION_MS);

        // consume failed records from dead letter queue topic
        log.info(""Consuming records from test topic"");
        ConsumerRecords<byte[], byte[]> messages = connect.kafka().consume(EXPECTED_INCORRECT_RECORDS, CONSUME_MAX_DURATION_MS, DLQ_TOPIC);
        for (ConsumerRecord<byte[], byte[]> recs : messages) {
            log.debug(""Consumed record (key={}, value={}) from dead letter queue topic {}"",
                    new String(recs.key()), new String(recs.value()), DLQ_TOPIC);
            assertTrue(recs.headers().toArray().length > 0);
            assertValue(""test-topic"", recs.headers(), ERROR_HEADER_ORIG_TOPIC);
            assertValue(RetriableException.class.getName(), recs.headers(), ERROR_HEADER_EXCEPTION);
            assertValue(""Error when value='value-7'"", recs.headers(), ERROR_HEADER_EXCEPTION_MESSAGE);
        }

        connect.deleteConnector(CONNECTOR_NAME);
    }
"
"    @Test
    public void testCreateWithOverridesForNonePolicy() throws Exception {
        Map<String, String> props = basicConnectorConfig();
        props.put(ConnectorConfig.CONNECTOR_CLIENT_CONSUMER_OVERRIDES_PREFIX + SaslConfigs.SASL_JAAS_CONFIG, ""sasl"");
        assertFailCreateConnector(""None"", props);
    }
"
"    @Test
    public void testCreateWithNotAllowedOverridesForPrincipalPolicy() throws Exception {
        Map<String, String> props = basicConnectorConfig();
        props.put(ConnectorConfig.CONNECTOR_CLIENT_CONSUMER_OVERRIDES_PREFIX + SaslConfigs.SASL_JAAS_CONFIG, ""sasl"");
        props.put(ConnectorConfig.CONNECTOR_CLIENT_CONSUMER_OVERRIDES_PREFIX + ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, ""latest"");
        assertFailCreateConnector(""Principal"", props);
    }
"
"    @Test
    public void testCreateWithAllowedOverridesForPrincipalPolicy() throws Exception {
        Map<String, String> props = basicConnectorConfig();
        props.put(ConnectorConfig.CONNECTOR_CLIENT_CONSUMER_OVERRIDES_PREFIX + CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, ""PLAIN"");
        assertPassCreateConnector(""Principal"", props);
    }
"
"    @Test
    public void testCreateWithAllowedOverridesForAllPolicy() throws Exception {
        // setup up props for the sink connector
        Map<String, String> props = basicConnectorConfig();
        props.put(ConnectorConfig.CONNECTOR_CLIENT_CONSUMER_OVERRIDES_PREFIX + CommonClientConfigs.CLIENT_ID_CONFIG, ""test"");
        assertPassCreateConnector(""All"", props);
    }
"
"    @Test
    public void shouldReturnFalseWhenAwaitingForStartToNeverComplete() throws Throwable {
        latch = new StartAndStopLatch(1, 1, this::complete, dependents, clock);
        future = asyncAwait(100);
        clock.sleep(10);
        assertFalse(future.get(200, TimeUnit.MILLISECONDS));
        assertTrue(future.isDone());
    }
"
"    @Test
    public void shouldReturnFalseWhenAwaitingForStopToNeverComplete() throws Throwable {
        latch = new StartAndStopLatch(1, 1, this::complete, dependents, clock);
        future = asyncAwait(100);
        latch.recordStart();
        clock.sleep(10);
        assertFalse(future.get(200, TimeUnit.MILLISECONDS));
        assertTrue(future.isDone());
    }
"
"    @Test
    public void shouldReturnTrueWhenAwaitingForStartAndStopToComplete() throws Throwable {
        latch = new StartAndStopLatch(1, 1, this::complete, dependents, clock);
        future = asyncAwait(100);
        latch.recordStart();
        latch.recordStop();
        clock.sleep(10);
        assertTrue(future.get(200, TimeUnit.MILLISECONDS));
        assertTrue(future.isDone());
    }
"
"    @Test
    public void shouldReturnFalseWhenAwaitingForDependentLatchToComplete() throws Throwable {
        StartAndStopLatch depLatch = new StartAndStopLatch(1, 1, this::complete, null, clock);
        dependents = Collections.singletonList(depLatch);
        latch = new StartAndStopLatch(1, 1, this::complete, dependents, clock);

        future = asyncAwait(100);
        latch.recordStart();
        latch.recordStop();
        clock.sleep(10);
        assertFalse(future.get(200, TimeUnit.MILLISECONDS));
        assertTrue(future.isDone());
    }
"
"    @Test
    public void shouldReturnTrueWhenAwaitingForStartAndStopAndDependentLatch() throws Throwable {
        StartAndStopLatch depLatch = new StartAndStopLatch(1, 1, this::complete, null, clock);
        dependents = Collections.singletonList(depLatch);
        latch = new StartAndStopLatch(1, 1, this::complete, dependents, clock);

        future = asyncAwait(100);
        latch.recordStart();
        latch.recordStop();
        depLatch.recordStart();
        depLatch.recordStop();
        clock.sleep(10);
        assertTrue(future.get(200, TimeUnit.MILLISECONDS));
        assertTrue(future.isDone());
    }
"
"    @Test
    public void testSinkConnector() throws Exception {
        // create test topic
        connect.kafka().createTopic(""test-topic"", NUM_TOPIC_PARTITIONS);

        // setup up props for the sink connector
        Map<String, String> props = new HashMap<>();
        props.put(CONNECTOR_CLASS_CONFIG, MonitorableSinkConnector.class.getSimpleName());
        props.put(TASKS_MAX_CONFIG, String.valueOf(NUM_TASKS));
        props.put(TOPICS_CONFIG, ""test-topic"");
        props.put(KEY_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());
        props.put(VALUE_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());

        // expect all records to be consumed by the connector
        connectorHandle.expectedRecords(NUM_RECORDS_PRODUCED);

        // expect all records to be consumed by the connector
        connectorHandle.expectedCommits(NUM_RECORDS_PRODUCED);

        // start a sink connector
        connect.configureConnector(CONNECTOR_NAME, props);

        waitForCondition(this::checkForPartitionAssignment,
                CONNECTOR_SETUP_DURATION_MS,
                ""Connector tasks were not assigned a partition each."");

        // produce some messages into source topic partitions
        for (int i = 0; i < NUM_RECORDS_PRODUCED; i++) {
            connect.kafka().produce(""test-topic"", i % NUM_TOPIC_PARTITIONS, ""key"", ""simple-message-value-"" + i);
        }

        // consume all records from the source topic or fail, to ensure that they were correctly produced.
        assertEquals(""Unexpected number of records consumed"", NUM_RECORDS_PRODUCED,
                connect.kafka().consume(NUM_RECORDS_PRODUCED, RECORD_TRANSFER_DURATION_MS, ""test-topic"").count());

        // wait for the connector tasks to consume all records.
        connectorHandle.awaitRecords(RECORD_TRANSFER_DURATION_MS);

        // wait for the connector tasks to commit all records.
        connectorHandle.awaitCommits(RECORD_TRANSFER_DURATION_MS);

        // delete connector
        connect.deleteConnector(CONNECTOR_NAME);
    }
"
"    @Test
    public void testSourceConnector() throws Exception {
        // create test topic
        connect.kafka().createTopic(""test-topic"", NUM_TOPIC_PARTITIONS);

        // setup up props for the sink connector
        Map<String, String> props = new HashMap<>();
        props.put(CONNECTOR_CLASS_CONFIG, MonitorableSourceConnector.class.getSimpleName());
        props.put(TASKS_MAX_CONFIG, String.valueOf(NUM_TASKS));
        props.put(""topic"", ""test-topic"");
        props.put(""throughput"", String.valueOf(500));
        props.put(KEY_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());
        props.put(VALUE_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());

        // expect all records to be produced by the connector
        connectorHandle.expectedRecords(NUM_RECORDS_PRODUCED);

        // expect all records to be produced by the connector
        connectorHandle.expectedCommits(NUM_RECORDS_PRODUCED);

        // start a source connector
        connect.configureConnector(CONNECTOR_NAME, props);

        // wait for the connector tasks to produce enough records
        connectorHandle.awaitRecords(RECORD_TRANSFER_DURATION_MS);

        // wait for the connector tasks to commit enough records
        connectorHandle.awaitCommits(RECORD_TRANSFER_DURATION_MS);

        // consume all records from the source topic or fail, to ensure that they were correctly produced
        int recordNum = connect.kafka().consume(NUM_RECORDS_PRODUCED, RECORD_TRANSFER_DURATION_MS, ""test-topic"").count();
        assertTrue(""Not enough records produced by source connector. Expected at least: "" + NUM_RECORDS_PRODUCED + "" + but got "" + recordNum,
                recordNum >= NUM_RECORDS_PRODUCED);

        // delete connector
        connect.deleteConnector(CONNECTOR_NAME);
    }
"
"    @Test
    public void testRestExtensionApi() throws IOException, InterruptedException {
        // setup Connect worker properties
        Map<String, String> workerProps = new HashMap<>();
        workerProps.put(REST_EXTENSION_CLASSES_CONFIG, IntegrationTestRestExtension.class.getName());

        // build a Connect cluster backed by Kafka and Zk
        connect = new EmbeddedConnectCluster.Builder()
            .name(""connect-cluster"")
            .numWorkers(1)
            .numBrokers(1)
            .workerProps(workerProps)
            .build();

        // start the clusters
        connect.start();

        WorkerHandle worker = connect.workers().stream()
            .findFirst()
            .orElseThrow(() -> new AssertionError(""At least one worker handle should be available""));

        waitForCondition(
            this::extensionIsRegistered,
            REST_EXTENSION_REGISTRATION_TIMEOUT_MS,
            ""REST extension was never registered""
        );

        ConnectorHandle connectorHandle = RuntimeHandles.get().connectorHandle(""test-conn"");
        try {
            // setup up props for the connector
            Map<String, String> connectorProps = new HashMap<>();
            connectorProps.put(CONNECTOR_CLASS_CONFIG, MonitorableSinkConnector.class.getSimpleName());
            connectorProps.put(TASKS_MAX_CONFIG, String.valueOf(1));
            connectorProps.put(TOPICS_CONFIG, ""test-topic"");

            // start a connector
            connectorHandle.taskHandle(connectorHandle.name() + ""-0"");
            StartAndStopLatch connectorStartLatch = connectorHandle.expectedStarts(1);
            connect.configureConnector(connectorHandle.name(), connectorProps);
            connectorStartLatch.await(CONNECTOR_HEALTH_AND_CONFIG_TIMEOUT_MS, TimeUnit.MILLISECONDS);

            String workerId = String.format(""%s:%d"", worker.url().getHost(), worker.url().getPort());
            ConnectorHealth expectedHealth = new ConnectorHealth(
                connectorHandle.name(),
                new ConnectorState(
                    ""RUNNING"",
                    workerId,
                    null
                ),
                Collections.singletonMap(
                    0,
                    new TaskState(0, ""RUNNING"", workerId, null)
                ),
                ConnectorType.SINK
            );

            connectorProps.put(NAME_CONFIG, connectorHandle.name());

            // Test the REST extension API; specifically, that the connector's health and configuration
            // are available to the REST extension we registered and that they contain expected values
            waitForCondition(
                () -> verifyConnectorHealthAndConfig(connectorHandle.name(), expectedHealth, connectorProps),
                CONNECTOR_HEALTH_AND_CONFIG_TIMEOUT_MS,
                ""Connector health and/or config was never accessible by the REST extension""
            );
        } finally {
            RuntimeHandles.get().deleteConnector(connectorHandle.name());
        }
    }
"
"    @Test
    public void testWhiteListedManifestResources() {
        assertTrue(
            DelegatingClassLoader.serviceLoaderManifestForPlugin(""META-INF/services/org.apache.kafka.connect.rest.ConnectRestExtension""));
        assertTrue(
            DelegatingClassLoader.serviceLoaderManifestForPlugin(""META-INF/services/org.apache.kafka.common.config.provider.ConfigProvider""));
    }
"
"    @Test
    public void testOtherResources() {
        assertFalse(
            DelegatingClassLoader.serviceLoaderManifestForPlugin(""META-INF/services/org.apache.kafka.connect.transforms.Transformation""));
        assertFalse(DelegatingClassLoader.serviceLoaderManifestForPlugin(""resource/version.properties""));
    }
"
"    @Test(expected = ClassNotFoundException.class)
    public void testLoadingUnloadedPluginClass() throws ClassNotFoundException {
        TestPlugins.assertAvailable();
        DelegatingClassLoader classLoader = new DelegatingClassLoader(Collections.emptyList());
        classLoader.initLoaders();
        for (String pluginClassName : TestPlugins.pluginClasses()) {
            classLoader.loadClass(pluginClassName);
        }
    }
"
"    @Test
    public void testLoadingPluginClass() throws ClassNotFoundException {
        TestPlugins.assertAvailable();
        DelegatingClassLoader classLoader = new DelegatingClassLoader(TestPlugins.pluginPath());
        classLoader.initLoaders();
        for (String pluginClassName : TestPlugins.pluginClasses()) {
            assertNotNull(classLoader.loadClass(pluginClassName));
            assertNotNull(classLoader.pluginClassLoader(pluginClassName));
        }
    }
"
"    @Test
    public void testJavaLibraryClasses() {
        assertFalse(PluginUtils.shouldLoadInIsolation(""java.""));
        assertFalse(PluginUtils.shouldLoadInIsolation(""java.lang.Object""));
        assertFalse(PluginUtils.shouldLoadInIsolation(""java.lang.String""));
        assertFalse(PluginUtils.shouldLoadInIsolation(""java.util.HashMap$Entry""));
        assertFalse(PluginUtils.shouldLoadInIsolation(""java.io.Serializable""));
        assertFalse(PluginUtils.shouldLoadInIsolation(""javax.rmi.""));
        assertFalse(PluginUtils.shouldLoadInIsolation(
                ""javax.management.loading.ClassLoaderRepository"")
        );
        assertFalse(PluginUtils.shouldLoadInIsolation(""org.omg.CORBA.""));
        assertFalse(PluginUtils.shouldLoadInIsolation(""org.omg.CORBA.Object""));
        assertFalse(PluginUtils.shouldLoadInIsolation(""org.w3c.dom.""));
        assertFalse(PluginUtils.shouldLoadInIsolation(""org.w3c.dom.traversal.TreeWalker""));
        assertFalse(PluginUtils.shouldLoadInIsolation(""org.xml.sax.""));
        assertFalse(PluginUtils.shouldLoadInIsolation(""org.xml.sax.EntityResolver""));
    }
"
"    @Test
    public void testThirdPartyClasses() {
        assertFalse(PluginUtils.shouldLoadInIsolation(""org.slf4j.""));
        assertFalse(PluginUtils.shouldLoadInIsolation(""org.slf4j.LoggerFactory""));
    }
"
"    @Test
    public void testConnectFrameworkClasses() {
        assertFalse(PluginUtils.shouldLoadInIsolation(""org.apache.kafka.common.""));
        assertFalse(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.common.config.AbstractConfig"")
        );
        assertFalse(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.common.config.ConfigDef$Type"")
        );
        assertFalse(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.common.serialization.Deserializer"")
        );
        assertFalse(PluginUtils.shouldLoadInIsolation(""org.apache.kafka.connect.""));
        assertFalse(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.connector.Connector"")
        );
        assertFalse(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.source.SourceConnector"")
        );
        assertFalse(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.sink.SinkConnector"")
        );
        assertFalse(PluginUtils.shouldLoadInIsolation(""org.apache.kafka.connect.connector.Task""));
        assertFalse(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.source.SourceTask"")
        );
        assertFalse(PluginUtils.shouldLoadInIsolation(""org.apache.kafka.connect.sink.SinkTask""));
        assertFalse(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.transforms.Transformation"")
        );
        assertFalse(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.storage.Converter"")
        );
        assertFalse(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.storage.OffsetBackingStore"")
        );
        assertFalse(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.clients.producer.ProducerConfig"")
        );
        assertFalse(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.clients.consumer.ConsumerConfig"")
        );
        assertFalse(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.clients.admin.KafkaAdminClient"")
        );
        assertFalse(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.rest.ConnectRestExtension"")
        );
    }
"
"    @Test
    public void testAllowedConnectFrameworkClasses() {
        assertTrue(PluginUtils.shouldLoadInIsolation(""org.apache.kafka.connect.transforms.""));
        assertTrue(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.transforms.ExtractField"")
        );
        assertTrue(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.transforms.ExtractField$Key"")
        );
        assertTrue(PluginUtils.shouldLoadInIsolation(""org.apache.kafka.connect.json.""));
        assertTrue(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.json.JsonConverter"")
        );
        assertTrue(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.json.JsonConverter$21"")
        );
        assertTrue(PluginUtils.shouldLoadInIsolation(""org.apache.kafka.connect.file.""));
        assertTrue(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.file.FileStreamSourceTask"")
        );
        assertTrue(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.file.FileStreamSinkConnector"")
        );
        assertTrue(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.mirror.MirrorSourceTask"")
        );
        assertTrue(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.mirror.MirrorSourceConnector"")
        );
        assertTrue(PluginUtils.shouldLoadInIsolation(""org.apache.kafka.connect.converters.""));
        assertTrue(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.converters.ByteArrayConverter"")
        );
        assertTrue(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.converters.DoubleConverter"")
        );
        assertTrue(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.converters.FloatConverter"")
        );
        assertTrue(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.converters.IntegerConverter"")
        );
        assertTrue(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.converters.LongConverter"")
        );
        assertTrue(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.converters.ShortConverter"")
        );
        assertTrue(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.storage.StringConverter"")
        );
        assertTrue(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.storage.SimpleHeaderConverter"")
        );
        assertTrue(PluginUtils.shouldLoadInIsolation(
            ""org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension""
        ));
    }
"
"    @Test
    public void testClientConfigProvider() {
        assertFalse(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.common.config.provider.ConfigProvider"")
        );
        assertTrue(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.common.config.provider.FileConfigProvider"")
        );
        assertTrue(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.common.config.provider.FutureConfigProvider"")
        );
    }
"
"    @Test
    public void testConnectorClientConfigOverridePolicy() {
        assertFalse(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.connector.policy.ConnectorClientConfigOverridePolicy"")
        );
        assertTrue(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.connector.policy.AbstractConnectorClientConfigOverridePolicy"")
        );
        assertTrue(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy"")
        );
        assertTrue(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy"")
        );
        assertTrue(PluginUtils.shouldLoadInIsolation(
                ""org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy"")
        );
    }
"
"    @Test
    public void testEmptyPluginUrls() throws Exception {
        assertEquals(Collections.<Path>emptyList(), PluginUtils.pluginUrls(pluginPath));
    }
"
"    @Test
    public void testEmptyStructurePluginUrls() throws Exception {
        createBasicDirectoryLayout();
        assertEquals(Collections.<Path>emptyList(), PluginUtils.pluginUrls(pluginPath));
    }
"
"    @Test
    public void testPluginUrlsWithJars() throws Exception {
        createBasicDirectoryLayout();

        List<Path> expectedUrls = createBasicExpectedUrls();

        assertUrls(expectedUrls, PluginUtils.pluginUrls(pluginPath));
    }
"
"    @Test
    public void testOrderOfPluginUrlsWithJars() throws Exception {
        createBasicDirectoryLayout();
        // Here this method is just used to create the files. The result is not used.
        createBasicExpectedUrls();

        List<Path> actual = PluginUtils.pluginUrls(pluginPath);
        // 'simple-transform.jar' is created first. In many cases, without sorting within the
        // PluginUtils, this jar will be placed before 'another-transform.jar'. However this is
        // not guaranteed because a DirectoryStream does not maintain a certain order in its
        // results. Besides this test case, sorted order in every call to assertUrls below.
        int i = Arrays.toString(actual.toArray()).indexOf(""another-transform.jar"");
        int j = Arrays.toString(actual.toArray()).indexOf(""simple-transform.jar"");
        assertTrue(i < j);
    }
"
"    @Test
    public void testPluginUrlsWithZips() throws Exception {
        createBasicDirectoryLayout();

        List<Path> expectedUrls = new ArrayList<>();
        expectedUrls.add(Files.createFile(pluginPath.resolve(""connectorA/my-sink.zip"")));
        expectedUrls.add(Files.createFile(pluginPath.resolve(""connectorB/a-source.zip"")));
        expectedUrls.add(Files.createFile(pluginPath.resolve(""transformC/simple-transform.zip"")));
        expectedUrls.add(Files.createFile(
                pluginPath.resolve(""transformC/deps/another-transform.zip""))
        );

        assertUrls(expectedUrls, PluginUtils.pluginUrls(pluginPath));
    }
"
"    @Test
    public void testPluginUrlsWithClasses() throws Exception {
        Files.createDirectories(pluginPath.resolve(""org/apache/kafka/converters""));
        Files.createDirectories(pluginPath.resolve(""com/mycompany/transforms""));
        Files.createDirectories(pluginPath.resolve(""edu/research/connectors""));
        Files.createFile(pluginPath.resolve(""org/apache/kafka/converters/README.txt""));
        Files.createFile(pluginPath.resolve(""org/apache/kafka/converters/AlienFormat.class""));
        Files.createDirectories(pluginPath.resolve(""com/mycompany/transforms/Blackhole.class""));
        Files.createDirectories(pluginPath.resolve(""edu/research/connectors/HalSink.class""));

        List<Path> expectedUrls = new ArrayList<>();
        expectedUrls.add(pluginPath);

        assertUrls(expectedUrls, PluginUtils.pluginUrls(pluginPath));
    }
"
"    @Test
    public void testPluginUrlsWithAbsoluteSymlink() throws Exception {
        createBasicDirectoryLayout();

        Path anotherPath = rootDir.newFolder(""moreplugins"").toPath().toRealPath();
        Files.createDirectories(anotherPath.resolve(""connectorB-deps""));
        Files.createSymbolicLink(
                pluginPath.resolve(""connectorB/deps/symlink""),
                anotherPath.resolve(""connectorB-deps"")
        );

        List<Path> expectedUrls = createBasicExpectedUrls();
        expectedUrls.add(Files.createFile(anotherPath.resolve(""connectorB-deps/converter.jar"")));

        assertUrls(expectedUrls, PluginUtils.pluginUrls(pluginPath));
    }
"
"    @Test
    public void testPluginUrlsWithRelativeSymlinkBackwards() throws Exception {
        createBasicDirectoryLayout();

        Path anotherPath = rootDir.newFolder(""moreplugins"").toPath().toRealPath();
        Files.createDirectories(anotherPath.resolve(""connectorB-deps""));
        Files.createSymbolicLink(
                pluginPath.resolve(""connectorB/deps/symlink""),
                Paths.get(""../../../moreplugins/connectorB-deps"")
        );

        List<Path> expectedUrls = createBasicExpectedUrls();
        expectedUrls.add(Files.createFile(anotherPath.resolve(""connectorB-deps/converter.jar"")));

        assertUrls(expectedUrls, PluginUtils.pluginUrls(pluginPath));
    }
"
"    @Test
    public void testPluginUrlsWithRelativeSymlinkForwards() throws Exception {
        // Since this test case defines a relative symlink within an already included path, the main
        // assertion of this test is absence of exceptions and correct resolution of paths.
        createBasicDirectoryLayout();
        Files.createDirectories(pluginPath.resolve(""connectorB/deps/more""));
        Files.createSymbolicLink(
                pluginPath.resolve(""connectorB/deps/symlink""),
                Paths.get(""more"")
        );

        List<Path> expectedUrls = createBasicExpectedUrls();
        expectedUrls.add(
                Files.createFile(pluginPath.resolve(""connectorB/deps/more/converter.jar""))
        );

        assertUrls(expectedUrls, PluginUtils.pluginUrls(pluginPath));
    }
"
"    @Test
    public void shouldInstantiateAndConfigureConverters() {
        instantiateAndConfigureConverter(WorkerConfig.KEY_CONVERTER_CLASS_CONFIG, ClassLoaderUsage.CURRENT_CLASSLOADER);
        // Validate extra configs got passed through to overridden converters
        assertEquals(""true"", converter.configs.get(JsonConverterConfig.SCHEMAS_ENABLE_CONFIG));
        assertEquals(""foo1"", converter.configs.get(""extra.config""));

        instantiateAndConfigureConverter(WorkerConfig.VALUE_CONVERTER_CLASS_CONFIG, ClassLoaderUsage.PLUGINS);
        // Validate extra configs got passed through to overridden converters
        assertEquals(""true"", converter.configs.get(JsonConverterConfig.SCHEMAS_ENABLE_CONFIG));
        assertEquals(""foo2"", converter.configs.get(""extra.config""));
    }
"
"    @Test
    public void shouldInstantiateAndConfigureInternalConverters() {
        instantiateAndConfigureInternalConverter(WorkerConfig.INTERNAL_KEY_CONVERTER_CLASS_CONFIG, ClassLoaderUsage.CURRENT_CLASSLOADER);
        // Validate schemas.enable is defaulted to false for internal converter
        assertEquals(false, internalConverter.configs.get(JsonConverterConfig.SCHEMAS_ENABLE_CONFIG));
        // Validate internal converter properties can still be set
        assertEquals(""bar1"", internalConverter.configs.get(""extra.config""));

        instantiateAndConfigureInternalConverter(WorkerConfig.INTERNAL_VALUE_CONVERTER_CLASS_CONFIG, ClassLoaderUsage.PLUGINS);
        // Validate schemas.enable is defaulted to false for internal converter
        assertEquals(false, internalConverter.configs.get(JsonConverterConfig.SCHEMAS_ENABLE_CONFIG));
        // Validate internal converter properties can still be set
        assertEquals(""bar2"", internalConverter.configs.get(""extra.config""));
    }
"
"    @Test
    public void shouldInstantiateAndConfigureExplicitlySetHeaderConverterWithCurrentClassLoader() {
        assertNotNull(props.get(WorkerConfig.HEADER_CONVERTER_CLASS_CONFIG));
        HeaderConverter headerConverter = plugins.newHeaderConverter(config,
                                                                     WorkerConfig.HEADER_CONVERTER_CLASS_CONFIG,
                                                                     ClassLoaderUsage.CURRENT_CLASSLOADER);
        assertNotNull(headerConverter);
        assertTrue(headerConverter instanceof TestHeaderConverter);
        this.headerConverter = (TestHeaderConverter) headerConverter;

        // Validate extra configs got passed through to overridden converters
        assertConverterType(ConverterType.HEADER, this.headerConverter.configs);
        assertEquals(""baz"", this.headerConverter.configs.get(""extra.config""));

        headerConverter = plugins.newHeaderConverter(config,
                                                     WorkerConfig.HEADER_CONVERTER_CLASS_CONFIG,
                                                     ClassLoaderUsage.PLUGINS);
        assertNotNull(headerConverter);
        assertTrue(headerConverter instanceof TestHeaderConverter);
        this.headerConverter = (TestHeaderConverter) headerConverter;

        // Validate extra configs got passed through to overridden converters
        assertConverterType(ConverterType.HEADER, this.headerConverter.configs);
        assertEquals(""baz"", this.headerConverter.configs.get(""extra.config""));
    }
"
"    @Test
    public void shouldInstantiateAndConfigureConnectRestExtension() {
        props.put(WorkerConfig.REST_EXTENSION_CLASSES_CONFIG,
                  TestConnectRestExtension.class.getName());
        createConfig();

        List<ConnectRestExtension> connectRestExtensions =
            plugins.newPlugins(config.getList(WorkerConfig.REST_EXTENSION_CLASSES_CONFIG),
                               config,
                               ConnectRestExtension.class);
        assertNotNull(connectRestExtensions);
        assertEquals(""One Rest Extension expected"", 1, connectRestExtensions.size());
        assertNotNull(connectRestExtensions.get(0));
        assertTrue(""Should be instance of TestConnectRestExtension"",
                   connectRestExtensions.get(0) instanceof TestConnectRestExtension);
        assertNotNull(((TestConnectRestExtension) connectRestExtensions.get(0)).configs);
        assertEquals(config.originals(),
                     ((TestConnectRestExtension) connectRestExtensions.get(0)).configs);
    }
"
"    @Test
    public void shouldInstantiateAndConfigureDefaultHeaderConverter() {
        props.remove(WorkerConfig.HEADER_CONVERTER_CLASS_CONFIG);
        createConfig();

        // Because it's not explicitly set on the supplied configuration, the logic to use the current classloader for the connector
        // will exit immediately, and so this method always returns null
        HeaderConverter headerConverter = plugins.newHeaderConverter(config,
                                                                     WorkerConfig.HEADER_CONVERTER_CLASS_CONFIG,
                                                                     ClassLoaderUsage.CURRENT_CLASSLOADER);
        assertNull(headerConverter);
        // But we should always find it (or the worker's default) when using the plugins classloader ...
        headerConverter = plugins.newHeaderConverter(config,
                                                     WorkerConfig.HEADER_CONVERTER_CLASS_CONFIG,
                                                     ClassLoaderUsage.PLUGINS);
        assertNotNull(headerConverter);
        assertTrue(headerConverter instanceof SimpleHeaderConverter);
    }
"
"    @Test(expected = ConnectException.class)
    public void shouldThrowIfPluginThrows() {
        TestPlugins.assertAvailable();

        plugins.newPlugin(
            TestPlugins.ALWAYS_THROW_EXCEPTION,
            new AbstractConfig(new ConfigDef(), Collections.emptyMap()),
            Converter.class
        );
    }
"
"    @Test
    public void shouldShareStaticValuesBetweenSamePlugin() {
        // Plugins are not isolated from other instances of their own class.
        TestPlugins.assertAvailable();
        Converter firstPlugin = plugins.newPlugin(
            TestPlugins.ALIASED_STATIC_FIELD,
            new AbstractConfig(new ConfigDef(), Collections.emptyMap()),
            Converter.class
        );

        assertInstanceOf(SamplingTestPlugin.class, firstPlugin, ""Cannot collect samples"");

        Converter secondPlugin = plugins.newPlugin(
            TestPlugins.ALIASED_STATIC_FIELD,
            new AbstractConfig(new ConfigDef(), Collections.emptyMap()),
            Converter.class
        );

        assertInstanceOf(SamplingTestPlugin.class, secondPlugin, ""Cannot collect samples"");
        assertSame(
            ((SamplingTestPlugin) firstPlugin).otherSamples(),
            ((SamplingTestPlugin) secondPlugin).otherSamples()
        );
    }
"
"    @Test
    public void newPluginShouldServiceLoadWithPluginClassLoader() {
        TestPlugins.assertAvailable();
        Converter plugin = plugins.newPlugin(
            TestPlugins.SERVICE_LOADER,
            new AbstractConfig(new ConfigDef(), Collections.emptyMap()),
            Converter.class
        );

        assertInstanceOf(SamplingTestPlugin.class, plugin, ""Cannot collect samples"");
        Map<String, SamplingTestPlugin> samples = ((SamplingTestPlugin) plugin).flatten();
        // Assert that the service loaded subclass is found in both environments
        assertTrue(samples.containsKey(""ServiceLoadedSubclass.static""));
        assertTrue(samples.containsKey(""ServiceLoadedSubclass.dynamic""));
        assertPluginClassLoaderAlwaysActive(samples);
    }
"
"    @Test
    public void newPluginShouldInstantiateWithPluginClassLoader() {
        TestPlugins.assertAvailable();
        Converter plugin = plugins.newPlugin(
            TestPlugins.ALIASED_STATIC_FIELD,
            new AbstractConfig(new ConfigDef(), Collections.emptyMap()),
            Converter.class
        );

        assertInstanceOf(SamplingTestPlugin.class, plugin, ""Cannot collect samples"");
        Map<String, SamplingTestPlugin> samples = ((SamplingTestPlugin) plugin).flatten();
        assertPluginClassLoaderAlwaysActive(samples);
    }
"
"    @Test(expected = ConfigException.class)
    public void shouldFailToFindConverterInCurrentClassloader() {
        TestPlugins.assertAvailable();
        props.put(WorkerConfig.KEY_CONVERTER_CLASS_CONFIG, TestPlugins.SAMPLING_CONVERTER);
        createConfig();
    }
"
"    @Test
    public void newConverterShouldConfigureWithPluginClassLoader() {
        TestPlugins.assertAvailable();
        props.put(WorkerConfig.KEY_CONVERTER_CLASS_CONFIG, TestPlugins.SAMPLING_CONVERTER);
        ClassLoader classLoader = plugins.delegatingLoader().pluginClassLoader(TestPlugins.SAMPLING_CONVERTER);
        ClassLoader savedLoader = Plugins.compareAndSwapLoaders(classLoader);
        createConfig();
        Plugins.compareAndSwapLoaders(savedLoader);

        Converter plugin = plugins.newConverter(
            config,
            WorkerConfig.KEY_CONVERTER_CLASS_CONFIG,
            ClassLoaderUsage.PLUGINS
        );

        assertInstanceOf(SamplingTestPlugin.class, plugin, ""Cannot collect samples"");
        Map<String, SamplingTestPlugin> samples = ((SamplingTestPlugin) plugin).flatten();
        assertTrue(samples.containsKey(""configure""));
        assertPluginClassLoaderAlwaysActive(samples);
    }
"
"    @Test
    public void newConfigProviderShouldConfigureWithPluginClassLoader() {
        TestPlugins.assertAvailable();
        String providerPrefix = ""some.provider"";
        props.put(providerPrefix + "".class"", TestPlugins.SAMPLING_CONFIG_PROVIDER);

        PluginClassLoader classLoader = plugins.delegatingLoader().pluginClassLoader(TestPlugins.SAMPLING_CONFIG_PROVIDER);
        assertNotNull(classLoader);
        ClassLoader savedLoader = Plugins.compareAndSwapLoaders(classLoader);
        createConfig();
        Plugins.compareAndSwapLoaders(savedLoader);

        ConfigProvider plugin = plugins.newConfigProvider(
            config,
            providerPrefix,
            ClassLoaderUsage.PLUGINS
        );

        assertInstanceOf(SamplingTestPlugin.class, plugin, ""Cannot collect samples"");
        Map<String, SamplingTestPlugin> samples = ((SamplingTestPlugin) plugin).flatten();
        assertTrue(samples.containsKey(""configure""));
        assertPluginClassLoaderAlwaysActive(samples);
    }
"
"    @Test
    public void newHeaderConverterShouldConfigureWithPluginClassLoader() {
        TestPlugins.assertAvailable();
        props.put(WorkerConfig.HEADER_CONVERTER_CLASS_CONFIG, TestPlugins.SAMPLING_HEADER_CONVERTER);
        ClassLoader classLoader = plugins.delegatingLoader().pluginClassLoader(TestPlugins.SAMPLING_HEADER_CONVERTER);
        ClassLoader savedLoader = Plugins.compareAndSwapLoaders(classLoader);
        createConfig();
        Plugins.compareAndSwapLoaders(savedLoader);

        HeaderConverter plugin = plugins.newHeaderConverter(
            config,
            WorkerConfig.HEADER_CONVERTER_CLASS_CONFIG,
            ClassLoaderUsage.PLUGINS
        );

        assertInstanceOf(SamplingTestPlugin.class, plugin, ""Cannot collect samples"");
        Map<String, SamplingTestPlugin> samples = ((SamplingTestPlugin) plugin).flatten();
        assertTrue(samples.containsKey(""configure"")); // HeaderConverter::configure was called
        assertPluginClassLoaderAlwaysActive(samples);
    }
"
"    @Test
    public void newPluginsShouldConfigureWithPluginClassLoader() {
        TestPlugins.assertAvailable();
        List<Configurable> configurables = plugins.newPlugins(
            Collections.singletonList(TestPlugins.SAMPLING_CONFIGURABLE),
            config,
            Configurable.class
        );
        assertEquals(1, configurables.size());
        Configurable plugin = configurables.get(0);

        assertInstanceOf(SamplingTestPlugin.class, plugin, ""Cannot collect samples"");
        Map<String, SamplingTestPlugin> samples = ((SamplingTestPlugin) plugin).flatten();
        assertTrue(samples.containsKey(""configure"")); // Configurable::configure was called
        assertPluginClassLoaderAlwaysActive(samples);
    }
"
"    @Test
    public void testRegularPluginDesc() {
        PluginDesc<Connector> connectorDesc = new PluginDesc<>(
                Connector.class,
                regularVersion,
                pluginLoader
        );

        assertPluginDesc(connectorDesc, Connector.class, regularVersion, pluginLoader.location());

        PluginDesc<Converter> converterDesc = new PluginDesc<>(
                Converter.class,
                snaphotVersion,
                pluginLoader
        );

        assertPluginDesc(converterDesc, Converter.class, snaphotVersion, pluginLoader.location());

        PluginDesc<Transformation> transformDesc = new PluginDesc<>(
                Transformation.class,
                noVersion,
                pluginLoader
        );

        assertPluginDesc(transformDesc, Transformation.class, noVersion, pluginLoader.location());
    }
"
"    @Test
    public void testPluginDescWithSystemClassLoader() {
        String location = ""classpath"";
        PluginDesc<SinkConnector> connectorDesc = new PluginDesc<>(
                SinkConnector.class,
                regularVersion,
                systemLoader
        );

        assertPluginDesc(connectorDesc, SinkConnector.class, regularVersion, location);

        PluginDesc<Converter> converterDesc = new PluginDesc<>(
                Converter.class,
                snaphotVersion,
                systemLoader
        );

        assertPluginDesc(converterDesc, Converter.class, snaphotVersion, location);

        PluginDesc<Transformation> transformDesc = new PluginDesc<>(
                Transformation.class,
                noVersion,
                systemLoader
        );

        assertPluginDesc(transformDesc, Transformation.class, noVersion, location);
    }
"
"    @Test
    public void testPluginDescWithNullVersion() {
        String nullVersion = ""null"";
        PluginDesc<SourceConnector> connectorDesc = new PluginDesc<>(
                SourceConnector.class,
                null,
                pluginLoader
        );

        assertPluginDesc(
                connectorDesc,
                SourceConnector.class,
                nullVersion,
                pluginLoader.location()
        );

        String location = ""classpath"";
        PluginDesc<Converter> converterDesc = new PluginDesc<>(
                Converter.class,
                null,
                systemLoader
        );

        assertPluginDesc(converterDesc, Converter.class, nullVersion, location);
    }
"
"    @Test
    public void testPluginDescEquality() {
        PluginDesc<Connector> connectorDescPluginPath = new PluginDesc<>(
                Connector.class,
                snaphotVersion,
                pluginLoader
        );

        PluginDesc<Connector> connectorDescClasspath = new PluginDesc<>(
                Connector.class,
                snaphotVersion,
                systemLoader
        );

        assertEquals(connectorDescPluginPath, connectorDescClasspath);
        assertEquals(connectorDescPluginPath.hashCode(), connectorDescClasspath.hashCode());

        PluginDesc<Converter> converterDescPluginPath = new PluginDesc<>(
                Converter.class,
                noVersion,
                pluginLoader
        );

        PluginDesc<Converter> converterDescClasspath = new PluginDesc<>(
                Converter.class,
                noVersion,
                systemLoader
        );

        assertEquals(converterDescPluginPath, converterDescClasspath);
        assertEquals(converterDescPluginPath.hashCode(), converterDescClasspath.hashCode());

        PluginDesc<Transformation> transformDescPluginPath = new PluginDesc<>(
                Transformation.class,
                null,
                pluginLoader
        );

        PluginDesc<Transformation> transformDescClasspath = new PluginDesc<>(
                Transformation.class,
                noVersion,
                pluginLoader
        );

        assertNotEquals(transformDescPluginPath, transformDescClasspath);
    }
"
"    @Test
    public void testPluginDescComparison() {
        PluginDesc<Connector> connectorDescPluginPath = new PluginDesc<>(
                Connector.class,
                regularVersion,
                pluginLoader
        );

        PluginDesc<Connector> connectorDescClasspath = new PluginDesc<>(
                Connector.class,
                newerVersion,
                systemLoader
        );

        assertNewer(connectorDescPluginPath, connectorDescClasspath);

        PluginDesc<Converter> converterDescPluginPath = new PluginDesc<>(
                Converter.class,
                noVersion,
                pluginLoader
        );

        PluginDesc<Converter> converterDescClasspath = new PluginDesc<>(
                Converter.class,
                snaphotVersion,
                systemLoader
        );

        assertNewer(converterDescPluginPath, converterDescClasspath);

        PluginDesc<Transformation> transformDescPluginPath = new PluginDesc<>(
                Transformation.class,
                null,
                pluginLoader
        );

        PluginDesc<Transformation> transformDescClasspath = new PluginDesc<>(
                Transformation.class,
                regularVersion,
                systemLoader
        );

        assertNewer(transformDescPluginPath, transformDescClasspath);
    }
"
"    @Test
    public void testAdminListenersConfigAllowedValues() {
        Map<String, String> props = baseProps();

        // no value set for ""admin.listeners""
        WorkerConfig config = new WorkerConfig(WorkerConfig.baseConfigDef(), props);
        assertNull(""Default value should be null."", config.getList(WorkerConfig.ADMIN_LISTENERS_CONFIG));

        props.put(WorkerConfig.ADMIN_LISTENERS_CONFIG, """");
        config = new WorkerConfig(WorkerConfig.baseConfigDef(), props);
        assertTrue(config.getList(WorkerConfig.ADMIN_LISTENERS_CONFIG).isEmpty());

        props.put(WorkerConfig.ADMIN_LISTENERS_CONFIG, ""http://a.b:9999, https://a.b:7812"");
        config = new WorkerConfig(WorkerConfig.baseConfigDef(), props);
        assertEquals(config.getList(WorkerConfig.ADMIN_LISTENERS_CONFIG), Arrays.asList(""http://a.b:9999"", ""https://a.b:7812""));

        new WorkerConfig(WorkerConfig.baseConfigDef(), props);
    }
"
"    @Test(expected = ConfigException.class)
    public void testAdminListenersNotAllowingEmptyStrings() {
        Map<String, String> props = baseProps();
        props.put(WorkerConfig.ADMIN_LISTENERS_CONFIG, ""http://a.b:9999,"");
        new WorkerConfig(WorkerConfig.baseConfigDef(), props);
    }
"
"    @Test(expected = ConfigException.class)
    public void testAdminListenersNotAllowingBlankStrings() {
        Map<String, String> props = baseProps();
        props.put(WorkerConfig.ADMIN_LISTENERS_CONFIG, ""http://a.b:9999, ,https://a.b:9999"");
        new WorkerConfig(WorkerConfig.baseConfigDef(), props);
    }
"
"    @Test
    public void testStartPaused() throws Exception {
        final CountDownLatch pauseLatch = new CountDownLatch(1);

        createWorkerTask(TargetState.PAUSED);

        statusListener.onPause(taskId);
        EasyMock.expectLastCall().andAnswer(new IAnswer<Void>() {
            @Override
            public Void answer() throws Throwable {
                pauseLatch.countDown();
                return null;
            }
"
"    @Test
    public void testPause() throws Exception {
        createWorkerTask();

        sourceTask.initialize(EasyMock.anyObject(SourceTaskContext.class));
        EasyMock.expectLastCall();
        sourceTask.start(TASK_PROPS);
        EasyMock.expectLastCall();
        statusListener.onStartup(taskId);
        EasyMock.expectLastCall();

        AtomicInteger count = new AtomicInteger(0);
        CountDownLatch pollLatch = expectPolls(10, count);
        // In this test, we don't flush, so nothing goes any further than the offset writer

        statusListener.onPause(taskId);
        EasyMock.expectLastCall();

        sourceTask.stop();
        EasyMock.expectLastCall();
        expectOffsetFlush(true);

        statusListener.onShutdown(taskId);
        EasyMock.expectLastCall();

        producer.close(EasyMock.anyObject(Duration.class));
        EasyMock.expectLastCall();

        transformationChain.close();
        EasyMock.expectLastCall();

        PowerMock.replayAll();

        workerTask.initialize(TASK_CONFIG);
        Future<?> taskFuture = executor.submit(workerTask);
        assertTrue(awaitLatch(pollLatch));

        workerTask.transitionTo(TargetState.PAUSED);

        int priorCount = count.get();
        Thread.sleep(100);

        // since the transition is observed asynchronously, the count could be off by one loop iteration
        assertTrue(count.get() - priorCount <= 1);

        workerTask.stop();
        assertTrue(workerTask.awaitStop(1000));

        taskFuture.get();

        PowerMock.verifyAll();
    }
"
"    @Test
    public void testPollsInBackground() throws Exception {
        createWorkerTask();

        sourceTask.initialize(EasyMock.anyObject(SourceTaskContext.class));
        EasyMock.expectLastCall();
        sourceTask.start(TASK_PROPS);
        EasyMock.expectLastCall();
        statusListener.onStartup(taskId);
        EasyMock.expectLastCall();

        final CountDownLatch pollLatch = expectPolls(10);
        // In this test, we don't flush, so nothing goes any further than the offset writer

        sourceTask.stop();
        EasyMock.expectLastCall();
        expectOffsetFlush(true);

        statusListener.onShutdown(taskId);
        EasyMock.expectLastCall();

        producer.close(EasyMock.anyObject(Duration.class));
        EasyMock.expectLastCall();

        transformationChain.close();
        EasyMock.expectLastCall();

        PowerMock.replayAll();

        workerTask.initialize(TASK_CONFIG);
        Future<?> taskFuture = executor.submit(workerTask);

        assertTrue(awaitLatch(pollLatch));
        workerTask.stop();
        assertTrue(workerTask.awaitStop(1000));

        taskFuture.get();
        assertPollMetrics(10);

        PowerMock.verifyAll();
    }
"
"    @Test
    public void testFailureInPoll() throws Exception {
        createWorkerTask();

        sourceTask.initialize(EasyMock.anyObject(SourceTaskContext.class));
        EasyMock.expectLastCall();
        sourceTask.start(TASK_PROPS);
        EasyMock.expectLastCall();
        statusListener.onStartup(taskId);
        EasyMock.expectLastCall();

        final CountDownLatch pollLatch = new CountDownLatch(1);
        final RuntimeException exception = new RuntimeException();
        EasyMock.expect(sourceTask.poll()).andAnswer(new IAnswer<List<SourceRecord>>() {
            @Override
            public List<SourceRecord> answer() throws Throwable {
                pollLatch.countDown();
                throw exception;
            }
"
"    @Test
    public void testPollReturnsNoRecords() throws Exception {
        // Test that the task handles an empty list of records
        createWorkerTask();

        sourceTask.initialize(EasyMock.anyObject(SourceTaskContext.class));
        EasyMock.expectLastCall();
        sourceTask.start(TASK_PROPS);
        EasyMock.expectLastCall();
        statusListener.onStartup(taskId);
        EasyMock.expectLastCall();

        // We'll wait for some data, then trigger a flush
        final CountDownLatch pollLatch = expectEmptyPolls(1, new AtomicInteger());
        expectOffsetFlush(true);

        sourceTask.stop();
        EasyMock.expectLastCall();
        expectOffsetFlush(true);

        statusListener.onShutdown(taskId);
        EasyMock.expectLastCall();

        producer.close(EasyMock.anyObject(Duration.class));
        EasyMock.expectLastCall();

        transformationChain.close();
        EasyMock.expectLastCall();

        PowerMock.replayAll();

        workerTask.initialize(TASK_CONFIG);
        Future<?> taskFuture = executor.submit(workerTask);

        assertTrue(awaitLatch(pollLatch));
        assertTrue(workerTask.commitOffsets());
        workerTask.stop();
        assertTrue(workerTask.awaitStop(1000));

        taskFuture.get();
        assertPollMetrics(0);

        PowerMock.verifyAll();
    }
"
"    @Test
    public void testCommit() throws Exception {
        // Test that the task commits properly when prompted
        createWorkerTask();

        sourceTask.initialize(EasyMock.anyObject(SourceTaskContext.class));
        EasyMock.expectLastCall();
        sourceTask.start(TASK_PROPS);
        EasyMock.expectLastCall();
        statusListener.onStartup(taskId);
        EasyMock.expectLastCall();

        // We'll wait for some data, then trigger a flush
        final CountDownLatch pollLatch = expectPolls(1);
        expectOffsetFlush(true);

        sourceTask.stop();
        EasyMock.expectLastCall();
        expectOffsetFlush(true);

        statusListener.onShutdown(taskId);
        EasyMock.expectLastCall();

        producer.close(EasyMock.anyObject(Duration.class));
        EasyMock.expectLastCall();

        transformationChain.close();
        EasyMock.expectLastCall();

        PowerMock.replayAll();

        workerTask.initialize(TASK_CONFIG);
        Future<?> taskFuture = executor.submit(workerTask);

        assertTrue(awaitLatch(pollLatch));
        assertTrue(workerTask.commitOffsets());
        workerTask.stop();
        assertTrue(workerTask.awaitStop(1000));

        taskFuture.get();
        assertPollMetrics(1);

        PowerMock.verifyAll();
    }
"
"    @Test
    public void testCommitFailure() throws Exception {
        // Test that the task commits properly when prompted
        createWorkerTask();

        sourceTask.initialize(EasyMock.anyObject(SourceTaskContext.class));
        EasyMock.expectLastCall();
        sourceTask.start(TASK_PROPS);
        EasyMock.expectLastCall();
        statusListener.onStartup(taskId);
        EasyMock.expectLastCall();

        // We'll wait for some data, then trigger a flush
        final CountDownLatch pollLatch = expectPolls(1);
        expectOffsetFlush(true);

        sourceTask.stop();
        EasyMock.expectLastCall();
        expectOffsetFlush(false);

        statusListener.onShutdown(taskId);
        EasyMock.expectLastCall();

        producer.close(EasyMock.anyObject(Duration.class));
        EasyMock.expectLastCall();

        transformationChain.close();
        EasyMock.expectLastCall();

        PowerMock.replayAll();

        workerTask.initialize(TASK_CONFIG);
        Future<?> taskFuture = executor.submit(workerTask);

        assertTrue(awaitLatch(pollLatch));
        assertTrue(workerTask.commitOffsets());
        workerTask.stop();
        assertTrue(workerTask.awaitStop(1000));

        taskFuture.get();
        assertPollMetrics(1);

        PowerMock.verifyAll();
    }
"
"    @Test
    public void testSendRecordsConvertsData() throws Exception {
        createWorkerTask();

        List<SourceRecord> records = new ArrayList<>();
        // Can just use the same record for key and value
        records.add(new SourceRecord(PARTITION, OFFSET, ""topic"", null, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD));

        Capture<ProducerRecord<byte[], byte[]>> sent = expectSendRecordAnyTimes();

        PowerMock.replayAll();

        Whitebox.setInternalState(workerTask, ""toSend"", records);
        Whitebox.invokeMethod(workerTask, ""sendRecords"");
        assertEquals(SERIALIZED_KEY, sent.getValue().key());
        assertEquals(SERIALIZED_RECORD, sent.getValue().value());

        PowerMock.verifyAll();
    }
"
"    @Test
    public void testSendRecordsPropagatesTimestamp() throws Exception {
        final Long timestamp = System.currentTimeMillis();

        createWorkerTask();

        List<SourceRecord> records = Collections.singletonList(
                new SourceRecord(PARTITION, OFFSET, ""topic"", null, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD, timestamp)
        );

        Capture<ProducerRecord<byte[], byte[]>> sent = expectSendRecordAnyTimes();

        PowerMock.replayAll();

        Whitebox.setInternalState(workerTask, ""toSend"", records);
        Whitebox.invokeMethod(workerTask, ""sendRecords"");
        assertEquals(timestamp, sent.getValue().timestamp());

        PowerMock.verifyAll();
    }
"
"    @Test(expected = InvalidRecordException.class)
    public void testSendRecordsCorruptTimestamp() throws Exception {
        final Long timestamp = -3L;
        createWorkerTask();

        List<SourceRecord> records = Collections.singletonList(
                new SourceRecord(PARTITION, OFFSET, ""topic"", null, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD, timestamp)
        );

        Capture<ProducerRecord<byte[], byte[]>> sent = expectSendRecordAnyTimes();

        PowerMock.replayAll();

        Whitebox.setInternalState(workerTask, ""toSend"", records);
        Whitebox.invokeMethod(workerTask, ""sendRecords"");
        assertEquals(null, sent.getValue().timestamp());

        PowerMock.verifyAll();
    }
"
"    @Test
    public void testSendRecordsNoTimestamp() throws Exception {
        final Long timestamp = -1L;
        createWorkerTask();

        List<SourceRecord> records = Collections.singletonList(
                new SourceRecord(PARTITION, OFFSET, ""topic"", null, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD, timestamp)
        );

        Capture<ProducerRecord<byte[], byte[]>> sent = expectSendRecordAnyTimes();

        PowerMock.replayAll();

        Whitebox.setInternalState(workerTask, ""toSend"", records);
        Whitebox.invokeMethod(workerTask, ""sendRecords"");
        assertEquals(null, sent.getValue().timestamp());

        PowerMock.verifyAll();
    }
"
"    @Test
    public void testSendRecordsRetries() throws Exception {
        createWorkerTask();

        // Differentiate only by Kafka partition so we can reuse conversion expectations
        SourceRecord record1 = new SourceRecord(PARTITION, OFFSET, ""topic"", 1, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD);
        SourceRecord record2 = new SourceRecord(PARTITION, OFFSET, ""topic"", 2, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD);
        SourceRecord record3 = new SourceRecord(PARTITION, OFFSET, ""topic"", 3, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD);

        // First round
        expectSendRecordOnce(false);
        // Any Producer retriable exception should work here
        expectSendRecordSyncFailure(new org.apache.kafka.common.errors.TimeoutException(""retriable sync failure""));

        // Second round
        expectSendRecordOnce(true);
        expectSendRecordOnce(false);

        PowerMock.replayAll();

        // Try to send 3, make first pass, second fail. Should save last two
        Whitebox.setInternalState(workerTask, ""toSend"", Arrays.asList(record1, record2, record3));
        Whitebox.invokeMethod(workerTask, ""sendRecords"");
        assertEquals(true, Whitebox.getInternalState(workerTask, ""lastSendFailed""));
        assertEquals(Arrays.asList(record2, record3), Whitebox.getInternalState(workerTask, ""toSend""));

        // Next they all succeed
        Whitebox.invokeMethod(workerTask, ""sendRecords"");
        assertEquals(false, Whitebox.getInternalState(workerTask, ""lastSendFailed""));
        assertNull(Whitebox.getInternalState(workerTask, ""toSend""));

        PowerMock.verifyAll();
    }
"
"    @Test(expected = ConnectException.class)
    public void testSendRecordsProducerCallbackFail() throws Exception {
        createWorkerTask();

        SourceRecord record1 = new SourceRecord(PARTITION, OFFSET, ""topic"", 1, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD);
        SourceRecord record2 = new SourceRecord(PARTITION, OFFSET, ""topic"", 2, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD);

        expectSendRecordProducerCallbackFail();

        PowerMock.replayAll();

        Whitebox.setInternalState(workerTask, ""toSend"", Arrays.asList(record1, record2));
        Whitebox.invokeMethod(workerTask, ""sendRecords"");
    }
"
"    @Test
    public void testSendRecordsTaskCommitRecordFail() throws Exception {
        createWorkerTask();

        // Differentiate only by Kafka partition so we can reuse conversion expectations
        SourceRecord record1 = new SourceRecord(PARTITION, OFFSET, ""topic"", 1, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD);
        SourceRecord record2 = new SourceRecord(PARTITION, OFFSET, ""topic"", 2, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD);
        SourceRecord record3 = new SourceRecord(PARTITION, OFFSET, ""topic"", 3, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD);

        // Source task commit record failure will not cause the task to abort
        expectSendRecordOnce(false);
        expectSendRecordTaskCommitRecordFail(false, false);
        expectSendRecordOnce(false);

        PowerMock.replayAll();

        Whitebox.setInternalState(workerTask, ""toSend"", Arrays.asList(record1, record2, record3));
        Whitebox.invokeMethod(workerTask, ""sendRecords"");
        assertEquals(false, Whitebox.getInternalState(workerTask, ""lastSendFailed""));
        assertNull(Whitebox.getInternalState(workerTask, ""toSend""));

        PowerMock.verifyAll();
    }
"
"    @Test
    public void testSlowTaskStart() throws Exception {
        final CountDownLatch startupLatch = new CountDownLatch(1);
        final CountDownLatch finishStartupLatch = new CountDownLatch(1);

        createWorkerTask();

        sourceTask.initialize(EasyMock.anyObject(SourceTaskContext.class));
        EasyMock.expectLastCall();
        sourceTask.start(TASK_PROPS);
        EasyMock.expectLastCall().andAnswer(new IAnswer<Object>() {
            @Override
            public Object answer() throws Throwable {
                startupLatch.countDown();
                assertTrue(awaitLatch(finishStartupLatch));
                return null;
            }
"
"    @Test
    public void testCancel() {
        createWorkerTask();

        offsetReader.close();
        PowerMock.expectLastCall();

        PowerMock.replayAll();

        workerTask.cancel();

        PowerMock.verifyAll();
    }
"
"    @Test
    public void testMetricsGroup() {
        SourceTaskMetricsGroup group = new SourceTaskMetricsGroup(taskId, metrics);
        SourceTaskMetricsGroup group1 = new SourceTaskMetricsGroup(taskId1, metrics);
        for (int i = 0; i != 10; ++i) {
            group.recordPoll(100, 1000 + i * 100);
            group.recordWrite(10);
        }
        for (int i = 0; i != 20; ++i) {
            group1.recordPoll(100, 1000 + i * 100);
            group1.recordWrite(10);
        }
        assertEquals(1900.0, metrics.currentMetricValueAsDouble(group.metricGroup(), ""poll-batch-max-time-ms""), 0.001d);
        assertEquals(1450.0, metrics.currentMetricValueAsDouble(group.metricGroup(), ""poll-batch-avg-time-ms""), 0.001d);
        assertEquals(33.333, metrics.currentMetricValueAsDouble(group.metricGroup(), ""source-record-poll-rate""), 0.001d);
        assertEquals(1000, metrics.currentMetricValueAsDouble(group.metricGroup(), ""source-record-poll-total""), 0.001d);
        assertEquals(3.3333, metrics.currentMetricValueAsDouble(group.metricGroup(), ""source-record-write-rate""), 0.001d);
        assertEquals(100, metrics.currentMetricValueAsDouble(group.metricGroup(), ""source-record-write-total""), 0.001d);
        assertEquals(900.0, metrics.currentMetricValueAsDouble(group.metricGroup(), ""source-record-active-count""), 0.001d);

        // Close the group
        group.close();

        for (MetricName metricName : group.metricGroup().metrics().metrics().keySet()) {
            // Metrics for this group should no longer exist
            assertFalse(group.metricGroup().groupId().includes(metricName));
        }
        // Sensors for this group should no longer exist
        assertNull(group.metricGroup().metrics().getSensor(""sink-record-read""));
        assertNull(group.metricGroup().metrics().getSensor(""sink-record-send""));
        assertNull(group.metricGroup().metrics().getSensor(""sink-record-active-count""));
        assertNull(group.metricGroup().metrics().getSensor(""partition-count""));
        assertNull(group.metricGroup().metrics().getSensor(""offset-seq-number""));
        assertNull(group.metricGroup().metrics().getSensor(""offset-commit-completion""));
        assertNull(group.metricGroup().metrics().getSensor(""offset-commit-completion-skip""));
        assertNull(group.metricGroup().metrics().getSensor(""put-batch-time""));

        assertEquals(2900.0, metrics.currentMetricValueAsDouble(group1.metricGroup(), ""poll-batch-max-time-ms""), 0.001d);
        assertEquals(1950.0, metrics.currentMetricValueAsDouble(group1.metricGroup(), ""poll-batch-avg-time-ms""), 0.001d);
        assertEquals(66.667, metrics.currentMetricValueAsDouble(group1.metricGroup(), ""source-record-poll-rate""), 0.001d);
        assertEquals(2000, metrics.currentMetricValueAsDouble(group1.metricGroup(), ""source-record-poll-total""), 0.001d);
        assertEquals(6.667, metrics.currentMetricValueAsDouble(group1.metricGroup(), ""source-record-write-rate""), 0.001d);
        assertEquals(200, metrics.currentMetricValueAsDouble(group1.metricGroup(), ""source-record-write-total""), 0.001d);
        assertEquals(1800.0, metrics.currentMetricValueAsDouble(group1.metricGroup(), ""source-record-active-count""), 0.001d);
    }
"
"    @Test
    public void testHeaders() throws Exception {
        Headers headers = new RecordHeaders();
        headers.add(""header_key"", ""header_value"".getBytes());

        org.apache.kafka.connect.header.Headers connectHeaders = new ConnectHeaders();
        connectHeaders.add(""header_key"", new SchemaAndValue(Schema.STRING_SCHEMA, ""header_value""));

        createWorkerTask();

        List<SourceRecord> records = new ArrayList<>();
        records.add(new SourceRecord(PARTITION, OFFSET, ""topic"", null, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD, null, connectHeaders));

        Capture<ProducerRecord<byte[], byte[]>> sent = expectSendRecord(true, false, true, true, true, headers);

        PowerMock.replayAll();

        Whitebox.setInternalState(workerTask, ""toSend"", records);
        Whitebox.invokeMethod(workerTask, ""sendRecords"");
        assertEquals(SERIALIZED_KEY, sent.getValue().key());
        assertEquals(SERIALIZED_RECORD, sent.getValue().value());
        assertEquals(headers, sent.getValue().headers());

        PowerMock.verifyAll();
    }
"
"    @Test
    public void testHeadersWithCustomConverter() throws Exception {
        StringConverter stringConverter = new StringConverter();
        TestConverterWithHeaders testConverter = new TestConverterWithHeaders();

        createWorkerTask(TargetState.STARTED, stringConverter, testConverter, stringConverter);

        List<SourceRecord> records = new ArrayList<>();

        String stringA = ""ÃrvÃ­ztÅ±rÅ tÃ¼kÃ¶rfÃºrÃ³gÃ©p"";
        org.apache.kafka.connect.header.Headers headersA = new ConnectHeaders();
        String encodingA = ""latin2"";
        headersA.addString(""encoding"", encodingA);

        records.add(new SourceRecord(PARTITION, OFFSET, ""topic"", null, Schema.STRING_SCHEMA, ""a"", Schema.STRING_SCHEMA, stringA, null, headersA));

        String stringB = ""Ð¢ÐµÑÑÐ¾Ð²Ð¾Ðµ ÑÐ¾Ð¾Ð±ÑÐµÐ½Ð¸Ðµ"";
        org.apache.kafka.connect.header.Headers headersB = new ConnectHeaders();
        String encodingB = ""koi8_r"";
        headersB.addString(""encoding"", encodingB);

        records.add(new SourceRecord(PARTITION, OFFSET, ""topic"", null, Schema.STRING_SCHEMA, ""b"", Schema.STRING_SCHEMA, stringB, null, headersB));

        Capture<ProducerRecord<byte[], byte[]>> sentRecordA = expectSendRecord(false, false, true, true, false, null);
        Capture<ProducerRecord<byte[], byte[]>> sentRecordB = expectSendRecord(false, false, true, true, false, null);

        PowerMock.replayAll();

        Whitebox.setInternalState(workerTask, ""toSend"", records);
        Whitebox.invokeMethod(workerTask, ""sendRecords"");

        assertEquals(ByteBuffer.wrap(""a"".getBytes()), ByteBuffer.wrap(sentRecordA.getValue().key()));
        assertEquals(
            ByteBuffer.wrap(stringA.getBytes(encodingA)),
            ByteBuffer.wrap(sentRecordA.getValue().value())
        );
        assertEquals(encodingA, new String(sentRecordA.getValue().headers().lastHeader(""encoding"").value()));

        assertEquals(ByteBuffer.wrap(""b"".getBytes()), ByteBuffer.wrap(sentRecordB.getValue().key()));
        assertEquals(
            ByteBuffer.wrap(stringB.getBytes(encodingB)),
            ByteBuffer.wrap(sentRecordB.getValue().value())
        );
        assertEquals(encodingB, new String(sentRecordB.getValue().headers().lastHeader(""encoding"").value()));

        PowerMock.verifyAll();
    }
"
"    @Test
    public void testEmbeddedConfigCast() {
        // Validate that we can construct a Connector config containing the extended config for the transform
        HashMap<String, String> connProps = new HashMap<>();
        connProps.put(""name"", ""foo"");
        connProps.put(""connector.class"", MockConnector.class.getName());
        connProps.put(""transforms"", ""example"");
        connProps.put(""transforms.example.type"", Cast.Value.class.getName());
        connProps.put(""transforms.example.spec"", ""int8"");

        Plugins plugins = null; // Safe when we're only constructing the config
        new ConnectorConfig(plugins, connProps);
    }
"
"    @Test
    public void testEmbeddedConfigExtractField() {
        // Validate that we can construct a Connector config containing the extended config for the transform
        HashMap<String, String> connProps = new HashMap<>();
        connProps.put(""name"", ""foo"");
        connProps.put(""connector.class"", MockConnector.class.getName());
        connProps.put(""transforms"", ""example"");
        connProps.put(""transforms.example.type"", ExtractField.Value.class.getName());
        connProps.put(""transforms.example.field"", ""field"");

        Plugins plugins = null; // Safe when we're only constructing the config
        new ConnectorConfig(plugins, connProps);
    }
"
"    @Test
    public void testEmbeddedConfigFlatten() {
        // Validate that we can construct a Connector config containing the extended config for the transform
        HashMap<String, String> connProps = new HashMap<>();
        connProps.put(""name"", ""foo"");
        connProps.put(""connector.class"", MockConnector.class.getName());
        connProps.put(""transforms"", ""example"");
        connProps.put(""transforms.example.type"", Flatten.Value.class.getName());

        Plugins plugins = null; // Safe when we're only constructing the config
        new ConnectorConfig(plugins, connProps);
    }
"
"    @Test
    public void testEmbeddedConfigHoistField() {
        // Validate that we can construct a Connector config containing the extended config for the transform
        HashMap<String, String> connProps = new HashMap<>();
        connProps.put(""name"", ""foo"");
        connProps.put(""connector.class"", MockConnector.class.getName());
        connProps.put(""transforms"", ""example"");
        connProps.put(""transforms.example.type"", HoistField.Value.class.getName());
        connProps.put(""transforms.example.field"", ""field"");

        Plugins plugins = null; // Safe when we're only constructing the config
        new ConnectorConfig(plugins, connProps);
    }
"
"    @Test
    public void testEmbeddedConfigInsertField() {
        // Validate that we can construct a Connector config containing the extended config for the transform
        HashMap<String, String> connProps = new HashMap<>();
        connProps.put(""name"", ""foo"");
        connProps.put(""connector.class"", MockConnector.class.getName());
        connProps.put(""transforms"", ""example"");
        connProps.put(""transforms.example.type"", InsertField.Value.class.getName());

        Plugins plugins = null; // Safe when we're only constructing the config
        new ConnectorConfig(plugins, connProps);
    }
"
"    @Test
    public void testEmbeddedConfigMaskField() {
        // Validate that we can construct a Connector config containing the extended config for the transform
        HashMap<String, String> connProps = new HashMap<>();
        connProps.put(""name"", ""foo"");
        connProps.put(""connector.class"", MockConnector.class.getName());
        connProps.put(""transforms"", ""example"");
        connProps.put(""transforms.example.type"", MaskField.Value.class.getName());
        connProps.put(""transforms.example.fields"", ""field"");


        Plugins plugins = null; // Safe when we're only constructing the config
        new ConnectorConfig(plugins, connProps);
    }
"
"    @Test
    public void testEmbeddedConfigRegexRouter() {
        // Validate that we can construct a Connector config containing the extended config for the transform
        HashMap<String, String> connProps = new HashMap<>();
        connProps.put(""name"", ""foo"");
        connProps.put(""connector.class"", MockConnector.class.getName());
        connProps.put(""transforms"", ""example"");
        connProps.put(""transforms.example.type"", RegexRouter.class.getName());
        connProps.put(""transforms.example.regex"", ""(.*)"");
        connProps.put(""transforms.example.replacement"", ""prefix-$1"");

        Plugins plugins = null; // Safe when we're only constructing the config
        new ConnectorConfig(plugins, connProps);
    }
"
"    @Test
    public void testEmbeddedConfigReplaceField() {
        // Validate that we can construct a Connector config containing the extended config for the transform
        HashMap<String, String> connProps = new HashMap<>();
        connProps.put(""name"", ""foo"");
        connProps.put(""connector.class"", MockConnector.class.getName());
        connProps.put(""transforms"", ""example"");
        connProps.put(""transforms.example.type"", ReplaceField.Value.class.getName());

        Plugins plugins = null; // Safe when we're only constructing the config
        new ConnectorConfig(plugins, connProps);
    }
"
"    @Test
    public void testEmbeddedConfigSetSchemaMetadata() {
        // Validate that we can construct a Connector config containing the extended config for the transform
        HashMap<String, String> connProps = new HashMap<>();
        connProps.put(""name"", ""foo"");
        connProps.put(""connector.class"", MockConnector.class.getName());
        connProps.put(""transforms"", ""example"");
        connProps.put(""transforms.example.type"", SetSchemaMetadata.Value.class.getName());

        Plugins plugins = null; // Safe when we're only constructing the config
        new ConnectorConfig(plugins, connProps);
    }
"
"    @Test
    public void testEmbeddedConfigTimestampConverter() {
        // Validate that we can construct a Connector config containing the extended config for the transform
        HashMap<String, String> connProps = new HashMap<>();
        connProps.put(""name"", ""foo"");
        connProps.put(""connector.class"", MockConnector.class.getName());
        connProps.put(""transforms"", ""example"");
        connProps.put(""transforms.example.type"", TimestampConverter.Value.class.getName());
        connProps.put(""transforms.example.target.type"", ""unix"");

        Plugins plugins = null; // Safe when we're only constructing the config
        new ConnectorConfig(plugins, connProps);
    }
"
"    @Test
    public void testEmbeddedConfigTimestampRouter() {
        // Validate that we can construct a Connector config containing the extended config for the transform
        HashMap<String, String> connProps = new HashMap<>();
        connProps.put(""name"", ""foo"");
        connProps.put(""connector.class"", MockConnector.class.getName());
        connProps.put(""transforms"", ""example"");
        connProps.put(""transforms.example.type"", TimestampRouter.class.getName());

        Plugins plugins = null; // Safe when we're only constructing the config
        new ConnectorConfig(plugins, connProps);
    }
"
"    @Test
    public void testEmbeddedConfigValueToKey() {
        // Validate that we can construct a Connector config containing the extended config for the transform
        HashMap<String, String> connProps = new HashMap<>();
        connProps.put(""name"", ""foo"");
        connProps.put(""connector.class"", MockConnector.class.getName());
        connProps.put(""transforms"", ""example"");
        connProps.put(""transforms.example.type"", ValueToKey.class.getName());
        connProps.put(""transforms.example.fields"", ""field"");

        Plugins plugins = null; // Safe when we're only constructing the config
        new ConnectorConfig(plugins, connProps);
    }
"
"    @Test
    public void currentStateIsNullWhenNotInitialized() {
        assertNull(tracker.currentState());
    }
"
"    @Test
    public void currentState() {
        for (State state : State.values()) {
            tracker.changeState(state, time.milliseconds());
            assertEquals(state, tracker.currentState());
        }
    }
"
"	@Test // #112
	public void createsLocalDateTimeFromTimestamp() {

		DefaultRevisionEntity entity = new DefaultRevisionEntity();
		entity.setTimestamp(NOW.toEpochMilli());

		DefaultRevisionMetadata metadata = new DefaultRevisionMetadata(entity);

		assertThat(metadata.getRevisionDate()).hasValue(LocalDateTime.ofInstant(NOW, ZoneOffset.systemDefault()));
	}
"
"	@Test
	public void testWithQueryDsl() {

		Country de = new Country();
		de.code = ""de"";
		de.name = ""Deutschland"";

		countryRepository.save(de);

		Country found = countryRepository.findOne(QCountry.country.name.eq(""Deutschland"")).get();

		assertThat(found).isNotNull();
		assertThat(found.id).isEqualTo(de.id);
	}
"
"	@Test
	public void testWithRevisions() {

		Country de = new Country();
		de.code = ""de"";
		de.name = ""Deutschland"";

		countryRepository.save(de);

		de.name = ""Germany"";

		countryRepository.save(de);

		Revisions<Integer, Country> revisions = countryRepository.findRevisions(de.id);

		assertThat(revisions).hasSize(2);

		Iterator<Revision<Integer, Country>> iterator = revisions.iterator();

		Integer firstRevisionNumber = iterator.next().getRevisionNumber().get();
		Integer secondRevisionNumber = iterator.next().getRevisionNumber().get();

		assertThat(countryRepository.findRevision(de.id, firstRevisionNumber).get().getEntity().name)
				.isEqualTo(""Deutschland"");
		assertThat(countryRepository.findRevision(de.id, secondRevisionNumber).get().getEntity().name).isEqualTo(""Germany"");
	}
"
"	@Test // #146
	public void findRevisionShortCircuitsOnEmptyRevisionList() {

		failOnEmptyRevisions();

		EnversRevisionRepositoryImplUnderTest<?, Object, ?> repository = new EnversRevisionRepositoryImplUnderTest<>(entityInformation, revisionEntityInformation, entityManager);

		repository.findRevisions(-999, PageRequest.of(0, 5));
	}
"
"	@Test
	public void testLifeCycle() {

		License license = new License();
		license.name = ""Schnitzel"";

		licenseRepository.save(license);

		Country de = new Country();
		de.code = ""de"";
		de.name = ""Deutschland"";

		countryRepository.save(de);

		Country se = new Country();
		se.code = ""se"";
		se.name = ""Schweden"";

		countryRepository.save(se);

		license.laender = new HashSet<Country>();
		license.laender.addAll(Arrays.asList(de, se));

		licenseRepository.save(license);

		de.name = ""Daenemark"";

		countryRepository.save(de);

		Optional<Revision<Integer, License>> revision = licenseRepository.findLastChangeRevision(license.id);

		assertThat(revision).hasValueSatisfying(it -> {

			Page<Revision<Integer, License>> page = licenseRepository.findRevisions(license.id, PageRequest.of(0, 10));
			Revisions<Integer, License> revisions = Revisions.of(page.getContent());
			assertThat(revisions.getLatestRevision()).isEqualTo(it);
		});
	}
"
"	@Test // #1
	public void returnsEmptyRevisionsForUnrevisionedEntity() {
		assertThat(countryRepository.findRevisions(100L)).isEmpty();
	}
"
"	@Test // #31
	public void returnsParticularRevisionForAnEntity() {

		Country de = new Country();
		de.code = ""de"";
		de.name = ""Deutschland"";

		countryRepository.save(de);

		de.name = ""Germany"";

		countryRepository.save(de);

		Revisions<Integer, Country> revisions = countryRepository.findRevisions(de.id);

		assertThat(revisions).hasSize(2);

		Iterator<Revision<Integer, Country>> iterator = revisions.iterator();
		Revision<Integer, Country> first = iterator.next();
		Revision<Integer, Country> second = iterator.next();

		assertThat(countryRepository.findRevision(de.id, first.getRequiredRevisionNumber())).hasValueSatisfying(it -> {
			assertThat(it.getEntity().name).isEqualTo(""Deutschland"");
		});

		assertThat(countryRepository.findRevision(de.id, second.getRequiredRevisionNumber())).hasValueSatisfying(it -> {
			assertThat(it.getEntity().name).isEqualTo(""Germany"");
		});
	}
"
"	@Test // #55
	public void considersRevisionNumberSortOrder() {

		Country de = new Country();
		de.code = ""de"";
		de.name = ""Deutschland"";

		countryRepository.save(de);

		de.name = ""Germany"";

		countryRepository.save(de);

		Page<Revision<Integer, Country>> page = countryRepository.findRevisions(de.id,
				PageRequest.of(0, 10, RevisionSort.desc()));

		assertThat(page).hasSize(2);
		assertThat(page.getContent().get(0).getRequiredRevisionNumber())
				.isGreaterThan(page.getContent().get(1).getRequiredRevisionNumber());
	}
"
"	@Test // #21
	public void findsDeletedRevisions() {

		Country de = new Country();
		de.code = ""de"";
		de.name = ""Deutschland"";

		countryRepository.save(de);

		countryRepository.delete(de);

		Revisions<Integer, Country> revisions = countryRepository.findRevisions(de.id);

		assertThat(revisions).hasSize(2);
		assertThat(revisions.getLatestRevision().getEntity()) //
				.isNotNull() //
				.extracting(c -> c.name, c -> c.code) //
				.containsExactly(null, null);
	}
"
"	@Test // #146
	public void shortCurcuitingWhenOffsetIsToLarge() {
		Country de = new Country();
		de.code = ""de"";
		de.name = ""Deutschland"";

		countryRepository.save(de);

		countryRepository.delete(de);

		check(de, 0, 1);
		check(de, 1, 1);
		check(de, 2, 0);
	}
"
"    @Test
    public void testPreloadingNotHappening() {
        final JsonSchemaFactory factory = JsonSchemaFactory.getInstance(SpecVersion.VersionFlag.V7);
        final JsonSchema schema = factory.getSchema(INVALID_$REF_SCHEMA);
        // not breaking - pass
        Assertions.assertNotNull(schema);
    }
"
"    @Test
    public void testPreloadingHappening() {
        final JsonSchemaFactory factory = JsonSchemaFactory.getInstance(SpecVersion.VersionFlag.V7);
        final JsonSchema schema = factory.getSchema(INVALID_$REF_SCHEMA);
        Assertions.assertThrows(JsonSchemaException.class,
                            new Executable() {
                                @Override
                                public void execute() {
                                    schema.initializeValidators();
                                }
"
"    @Test
    public void testPreloadingHappeningForCircularDependency() {
        final JsonSchemaFactory factory = JsonSchemaFactory.getInstance(SpecVersion.VersionFlag.V7);
        final JsonSchema schema = factory.getSchema(CIRCULAR_$REF_SCHEMA);
        schema.initializeValidators();
    }
"
"    @ParameterizedTest
    public void dataIsValid(boolean failFast) throws Exception {
        String schemaPath = ""/schema/issue386-v7.json"";
        String dataPath = ""/data/issue386.json"";
        JsonSchema schema = getJsonSchemaFromPathV7(schemaPath, failFast);
        JsonNode node = getJsonNodeFromPath(dataPath).get(""valid"");
        node.forEach(testNode -> {
            Set<ValidationMessage> errors = schema.validate(testNode.get(""data""));
            Assertions.assertEquals(0, errors.size(), ""Expected no errors for "" + testNode.get(""data""));
        });
    }
"
"    @Test
    public void dataIsInvalidFailFast() throws Exception {
        String schemaPath = ""/schema/issue386-v7.json"";
        String dataPath = ""/data/issue386.json"";
        JsonSchema schema = getJsonSchemaFromPathV7(schemaPath, true);
        JsonNode node = getJsonNodeFromPath(dataPath).get(""invalid"");
        node.forEach(testNode -> {
            try {
                schema.validate(testNode.get(""data""));
                Assertions.fail();
            } catch (JsonSchemaException e) {
                Assertions.assertEquals(testNode.get(""expectedErrors"").get(0).asText(), e.getMessage());
            }
        });
    }
"
"    @Test
    public void dataIsInvalidFailSlow() throws Exception {
        String schemaPath = ""/schema/issue386-v7.json"";
        String dataPath = ""/data/issue386.json"";
        JsonSchema schema = getJsonSchemaFromPathV7(schemaPath, false);
        JsonNode node = getJsonNodeFromPath(dataPath).get(""invalid"");
        node.forEach(testNode -> {
            Set<ValidationMessage> errors = schema.validate(testNode.get(""data""));
            List<String> errorMessages = errors.stream().map(x -> x.getMessage()).collect(Collectors.toList());
            testNode.get(""expectedErrors"").forEach(expectedError -> {
                Assertions.assertTrue(errorMessages.contains(expectedError.asText()));
            });
        });
    }
"
"    @Test
    public void testNullableOneOf() throws Exception {
        runTestFile(""data/issue425.json"");
    }
"
"  @BeforeEach
  public void setup() throws IOException {
    setupSchema();
  }
"
"  @Test
  public void firstOneValid() throws Exception {
    String dataPath = ""/data/issue366.json"";

    InputStream dataInputStream = getClass().getResourceAsStream(dataPath);
    JsonNode node = getJsonNodeFromStreamContent(dataInputStream);
    List<JsonNode> testNodes = node.findValues(""tests"");
    JsonNode testNode = testNodes.get(0).get(0);
    JsonNode dataNode = testNode.get(""data"");
    Set<ValidationMessage> errors = jsonSchema.validate(dataNode);
    assertTrue(errors.isEmpty());
  }
"
"  @Test
  public void secondOneValid() throws Exception {
    String dataPath = ""/data/issue366.json"";

    InputStream dataInputStream = getClass().getResourceAsStream(dataPath);
    JsonNode node = getJsonNodeFromStreamContent(dataInputStream);
    List<JsonNode> testNodes = node.findValues(""tests"");
    JsonNode testNode = testNodes.get(0).get(1);
    JsonNode dataNode = testNode.get(""data"");
    Set<ValidationMessage> errors = jsonSchema.validate(dataNode);
    assertTrue(errors.isEmpty());
  }
"
"  @Test
  public void bothValid() throws Exception {
    String dataPath = ""/data/issue366.json"";

    assertThrows(JsonSchemaException.class, () -> {
        InputStream dataInputStream = getClass().getResourceAsStream(dataPath);
        JsonNode node = getJsonNodeFromStreamContent(dataInputStream);
        List<JsonNode> testNodes = node.findValues(""tests"");
        JsonNode testNode = testNodes.get(0).get(2);
        JsonNode dataNode = testNode.get(""data"");
        jsonSchema.validate(dataNode);
    });
  }
"
"  @Test
  public void neitherValid() throws Exception {
    String dataPath = ""/data/issue366.json"";

    assertThrows(JsonSchemaException.class, () -> {
        InputStream dataInputStream = getClass().getResourceAsStream(dataPath);
        JsonNode node = getJsonNodeFromStreamContent(dataInputStream);
        List<JsonNode> testNodes = node.findValues(""tests"");
        JsonNode testNode = testNodes.get(0).get(3);
        JsonNode dataNode = testNode.get(""data"");
        jsonSchema.validate(dataNode);
    });
  }
"
"    @Test
    public void testValidIntegralValuesWithJavaSemantics() {
        schemaValidatorsConfig.setJavaSemantics(true);
        for (String validValue : validIntegralValues) {
            assertSame(JsonType.INTEGER,
                    getValueNodeType(DecimalNode.valueOf(new BigDecimal(validValue)), schemaValidatorsConfig),
                    validValue);
        }
    }
"
"    @Test
    public void testValidIntegralValuesWithoutJavaSemantics() {
        schemaValidatorsConfig.setJavaSemantics(false);
        for (String validValue : validIntegralValues) {
            assertSame(JsonType.NUMBER,
                    getValueNodeType(DecimalNode.valueOf(new BigDecimal(validValue)), schemaValidatorsConfig),
                    validValue);
        }
    }
"
"    @Test
    public void testWithLosslessNarrowing() {
        schemaValidatorsConfig.setLosslessNarrowing(true);
        for (String validValue : validIntegralValues) {
            assertSame(JsonType.INTEGER,
                    getValueNodeType(DecimalNode.valueOf(new BigDecimal(""1.0"")), schemaValidatorsConfig),
                    validValue);

            assertSame(JsonType.NUMBER,
                    getValueNodeType(DecimalNode.valueOf(new BigDecimal(""1.5"")), schemaValidatorsConfig),
                    validValue);
        }
    }
"
"    @Test
    public void testWithoutLosslessNarrowing() {
        schemaValidatorsConfig.setLosslessNarrowing(false);
        for (String validValue : validIntegralValues) {
            assertSame(JsonType.NUMBER,
                    getValueNodeType(DecimalNode.valueOf(new BigDecimal(""1.0"")), schemaValidatorsConfig),
                    validValue);

            assertSame(JsonType.NUMBER,
                    getValueNodeType(DecimalNode.valueOf(new BigDecimal(""1.5"")), schemaValidatorsConfig),
                    validValue);
        }

    }
"
"    @Test
    public void shouldWalkWithValidation() throws URISyntaxException, IOException {
        JsonSchema schema = getJsonSchemaFromStreamContentV7(new URI(""http://json-schema"" +
                "".org/draft-07/schema#""));
        JsonNode data = mapper.readTree(Issue461Test.class.getResource(""/data/issue461-v7.json""));
        ValidationResult result = schema.walk(data, true);
        Assertions.assertTrue(result.getValidationMessages().isEmpty());
    }
"
"    @Test
    public void testNumeicValues() {
        for (String validValue : validNumericValues) {
            assertTrue(isNumeric(validValue), validValue);
        }
    }
"
"    @Test
    public void testNonNumeicValues() {
        for (String invalidValue : invalidNumericValues) {
            assertFalse(isNumeric(invalidValue), invalidValue);
        }
    }
"
"    @Test
    public void testGetVersionValue() {
        SpecVersion ds = new SpecVersion();
        Set versionFlags = EnumSet.of(
                SpecVersion.VersionFlag.V4,
                SpecVersion.VersionFlag.V201909);
        Assertions.assertEquals(ds.getVersionValue(versionFlags), 9); // 0001|1000
    }
"
"    @Test
    public void testGetVersionFlags() {
        SpecVersion ds = new SpecVersion();

        long numericVersionCode = SpecVersion.VersionFlag.V201909.getVersionFlagValue()
                | SpecVersion.VersionFlag.V6.getVersionFlagValue()
                | SpecVersion.VersionFlag.V7.getVersionFlagValue();  // 14

        Set versionFlags = ds.getVersionFlags(numericVersionCode);

        assert !versionFlags.contains(SpecVersion.VersionFlag.V4);
        assert versionFlags.contains(SpecVersion.VersionFlag.V6);
        assert versionFlags.contains(SpecVersion.VersionFlag.V7);
        assert versionFlags.contains(SpecVersion.VersionFlag.V201909);

    }
"
"    @Test
    public void testAllVersionValue() {
        long numericVersionCode =
                SpecVersion.VersionFlag.V201909.getVersionFlagValue()
                        | SpecVersion.VersionFlag.V4.getVersionFlagValue()
                        | SpecVersion.VersionFlag.V6.getVersionFlagValue()
                        | SpecVersion.VersionFlag.V7.getVersionFlagValue();  // 15
        Assertions.assertEquals(numericVersionCode, 15);

    }
"
"    @Test
    public void nestedOneOfsShouldStillMatchV7() throws Exception {
        String schemaPath = ""/schema/issue383-v7.json"";
        String dataPath = ""/data/issue383.json"";
        InputStream schemaInputStream = getClass().getResourceAsStream(schemaPath);
        JsonSchema schema = getJsonSchemaFromStreamContentV7(schemaInputStream);
        InputStream dataInputStream = getClass().getResourceAsStream(dataPath);
        JsonNode node = getJsonNodeFromStreamContent(dataInputStream);
        Set<ValidationMessage> errors = schema.validate(node);
        Assertions.assertEquals(0, errors.size());
    }
"
"    @Test
    public void testComplexPropertyNamesV7() throws Exception {
        String schemaPath = ""/schema/issue396-v7.json"";
        String dataPath = ""/data/issue396.json"";
        InputStream schemaInputStream = getClass().getResourceAsStream(schemaPath);
        JsonSchema schema = getJsonSchemaFromStreamContentV7(schemaInputStream);
        InputStream dataInputStream = getClass().getResourceAsStream(dataPath);
        JsonNode node = getJsonNodeFromStreamContent(dataInputStream);

        final Set<String> invalidPaths = new HashSet<>();
        node.fields().forEachRemaining(entry -> {
            if (!entry.getValue().asBoolean())
                invalidPaths.add(""$."" + entry.getKey());
        });

        Set<ValidationMessage> errors = schema.validate(node);
        final Set<String> failedPaths = errors.stream().map(ValidationMessage::getPath).collect(Collectors.toSet());
        Assertions.assertEquals(failedPaths, invalidPaths);
    }
"
"    @Test
    public void shouldFailWhenRequiredPropertiesDoNotExistInReferencedSubSchema() throws Exception {
        String schemaPath = ""/draft2019-09/issue255.json"";
        String dataPath = ""/data/issue255.json"";
        InputStream schemaInputStream = getClass().getResourceAsStream(schemaPath);
        JsonSchema schema = getJsonSchemaFromStreamContent(schemaInputStream);
        InputStream dataInputStream = getClass().getResourceAsStream(dataPath);
        JsonNode node = getJsonNodeFromStreamContent(dataInputStream);
        Set<ValidationMessage> errors = schema.validate(node);
        Assertions.assertEquals(2, errors.size());
    }
"
"    @Test
    public void propertyNameEnumShouldFailV7() throws Exception {
        String schemaPath = ""/schema/issue342-v7.json"";
        String dataPath = ""/data/issue342.json"";
        InputStream schemaInputStream = getClass().getResourceAsStream(schemaPath);
        JsonSchema schema = getJsonSchemaFromStreamContentV7(schemaInputStream);
        InputStream dataInputStream = getClass().getResourceAsStream(dataPath);
        JsonNode node = getJsonNodeFromStreamContent(dataInputStream);
        Set<ValidationMessage> errors = schema.validate(node);
        Assertions.assertEquals(1, errors.size());
        final ValidationMessage error = errors.iterator().next();
        Assertions.assertEquals(""$.z"", error.getPath());
        Assertions.assertEquals(""Property name $.z is not valid for validation: does not have a value in the enumeration [a, b, c]"", error.getMessage());
    }
"
"    @Test
    public void nestedValidation() throws IOException {
        JsonSchema jsonSchema = schemaFactory.getSchema(schemaStr);
        Set<ValidationMessage> validationMessages = jsonSchema.validate(mapper.readTree(person));

        System.err.println(""\n"" + Arrays.toString(validationMessages.toArray()));

        assertFalse(validationMessages.isEmpty());


    }
"
"    @Test
    public void nestedTypeValidation() throws IOException, URISyntaxException {
        URI uri = new URI(""https://json-schema.org/draft/2019-09/schema"");
        JsonSchema jsonSchema = schemaFactory.getSchema(uri);
        Set<ValidationMessage> validationMessages = jsonSchema.validate(mapper.readTree(invalidNestedSchema));

        System.err.println(""\n"" + Arrays.toString(validationMessages.toArray()));

        assertFalse(validationMessages.isEmpty());
    }
"
"    @Test
    public void typeValidation() throws IOException, URISyntaxException {
        URI uri = new URI(""https://json-schema.org/draft/2019-09/schema"");
        JsonSchema jsonSchema = schemaFactory.getSchema(uri);
        Set<ValidationMessage> validationMessages = jsonSchema.validate(mapper.readTree(invalidSchema));

        System.err.println(""\n"" + Arrays.toString(validationMessages.toArray()));

        assertFalse(validationMessages.isEmpty());
    }
"
"    @Test
    public void shouldWorkV7() throws Exception {
        String schemaPath = ""/schema/issue426-v7.json"";
        String dataPath = ""/data/issue426.json"";
        InputStream schemaInputStream = getClass().getResourceAsStream(schemaPath);
        JsonSchema schema = getJsonSchemaFromStreamContentV7(schemaInputStream);
        InputStream dataInputStream = getClass().getResourceAsStream(dataPath);
        JsonNode node = getJsonNodeFromStreamContent(dataInputStream);
        Set<ValidationMessage> errors = schema.validate(node);
        Assertions.assertEquals(2, errors.size());
        final JsonNode message = schema.schemaNode.get(""message"");
        for(ValidationMessage error : errors) {
            //validating custom message
            Assertions.assertEquals(message.get(error.getType()).asText(),  error.getMessage());
        }
    }
"
"    @Test
    public void expectObjectNotIntegerV7() throws Exception {
        String schemaPath = ""/schema/issue404-v7.json"";
        String dataPath = ""/data/issue404.json"";
        InputStream schemaInputStream = getClass().getResourceAsStream(schemaPath);
        JsonSchema schema = getJsonSchemaFromStreamContentV7(schemaInputStream);
        InputStream dataInputStream = getClass().getResourceAsStream(dataPath);
        JsonNode node = getJsonNodeFromStreamContent(dataInputStream);
        Set<ValidationMessage> errors = schema.validate(node);
        Assertions.assertEquals(0, errors.size());
    }
"
"    @Test(/*expected = java.lang.StackOverflowError.class*/)
    public void testLoadingWithId() throws Exception {
        URL url = new URL(""http://localhost:1234/self_ref/selfRef.json"");
        JsonNode schemaJson = mapper.readTree(url);
        JsonSchemaFactory factory = JsonSchemaFactory.getInstance(SpecVersion.VersionFlag.V4);
        @SuppressWarnings(""unused"")
        JsonSchema schema = factory.getSchema(schemaJson);
    }
"
"    @Test
    public void testBignumValidator() throws Exception {
        runTestFile(""draft4/optional/bignum.json"");
    }
"
"    @Test
    public void testFormatValidator() throws Exception {
        runTestFile(""draft4/optional/format.json"");
    }
"
"    @Test
    public void testComplexSchema() throws Exception {
        runTestFile(""draft4/optional/complex.json"");
    }
"
"    @Test
    public void testZeroTerminatedFloatsValidator() throws Exception {
        runTestFile(""draft4/optional/zeroTerminatedFloats.json"");
    }
"
"    @Test
    public void testAdditionalItemsValidator() throws Exception {
        runTestFile(""draft4/additionalItems.json"");
    }
"
"    @Test
    public void testAdditionalPropertiesValidator() throws Exception {
        runTestFile(""draft4/additionalProperties.json"");
    }
"
"    @Test
    public void testAllOfValidator() throws Exception {
        runTestFile(""draft4/allOf.json"");
    }
"
"    @Test
    public void testAnyOFValidator() throws Exception {
        runTestFile(""draft4/anyOf.json"");
    }
"
"    @Test
    public void testDefaultValidator() throws Exception {
        runTestFile(""draft4/default.json"");
    }
"
"    @Test
    public void testDefinitionsValidator() throws Exception {
        runTestFile(""draft4/definitions.json"");
    }
"
"    @Test
    public void testDependenciesValidator() throws Exception {
        runTestFile(""draft4/dependencies.json"");
    }
"
"    @Test
    public void testEnumValidator() throws Exception {
        runTestFile(""draft4/enum.json"");
    }
"
"    @Test
    public void testItemsValidator() throws Exception {
        runTestFile(""draft4/items.json"");
    }
"
"    @Test
    public void testMaximumValidator() throws Exception {
        runTestFile(""draft4/maximum.json"");
    }
"
"    @Test
    public void testMaxItemsValidator() throws Exception {
        runTestFile(""draft4/maxItems.json"");
    }
"
"    @Test
    public void testMaxLengthValidator() throws Exception {
        runTestFile(""draft4/maxLength.json"");
    }
"
"    @Test
    public void testMaxPropertiesValidator() throws Exception {
        runTestFile(""draft4/maxProperties.json"");
    }
"
"    @Test
    public void testMinimumValidator() throws Exception {
        runTestFile(""draft4/minimum.json"");
    }
"
"    @Test
    public void testMinItemsValidator() throws Exception {
        runTestFile(""draft4/minItems.json"");
    }
"
"    @Test
    public void testMinLengthValidator() throws Exception {
        runTestFile(""draft4/minLength.json"");
    }
"
"    @Test
    public void testMinPropertiesValidator() throws Exception {
        runTestFile(""draft4/minProperties.json"");
    }
"
"    @Test
    public void testMultipleOfValidator() throws Exception {
        runTestFile(""draft4/multipleOf.json"");
    }
"
"    @Test
    public void testNotValidator() throws Exception {
        runTestFile(""draft4/not.json"");
    }
"
"    @Test
    public void testOneOfValidator() throws Exception {
        runTestFile(""draft4/oneOf.json"");
    }
"
"    @Test
    public void testPatternValidator() throws Exception {
        runTestFile(""draft4/pattern.json"");
    }
"
"    @Test
    public void testPatternPropertiesValidator() throws Exception {
        runTestFile(""draft4/patternProperties.json"");
    }
"
"    @Test
    public void testPropertiesValidator() throws Exception {
        runTestFile(""draft4/properties.json"");
    }
"
"    @Test
    public void testRefValidator() throws Exception {
        runTestFile(""draft4/ref.json"");
    }
"
"    @Test
    public void testRefRemoteValidator() throws Exception {
        runTestFile(""draft4/refRemote.json"");
    }
"
"    @Test
    public void testRefIdReference() throws Exception {
        runTestFile(""draft4/idRef.json"");
    }
"
"    @Test
    public void testRelativeRefRemoteValidator() throws Exception {
        runTestFile(""draft4/relativeRefRemote.json"");
    }
"
"    @Test
    public void testRequiredValidator() throws Exception {
        runTestFile(""draft4/required.json"");
    }
"
"    @Test
    public void testTypeValidator() throws Exception {
        runTestFile(""draft4/type.json"");
    }
"
"    @Test
    public void testUnionTypeValidator() throws Exception {
        runTestFile(""draft4/union_type.json"");
    }
"
"    @Test
    public void testUniqueItemsValidator() throws Exception {
        runTestFile(""draft4/uniqueItems.json"");
    }
"
"    @Test
    public void testEnumObject() throws Exception {
        runTestFile(""draft4/enumObject.json"");
    }
"
"    @Test
    public void testIdSchemaWithUrl() throws Exception {
        runTestFile(""draft4/property.json"");
    }
"
"    @Test
    public void testSchemaFromClasspath() throws Exception {
        runTestFile(""draft4/classpath/schema.json"");
    }
"
"    @Test
    public void testUUIDValidator() throws Exception {
        runTestFile(""draft4/uuid.json"");
    }
"
"    @Test
    public void testFailFast_AllErrors() throws IOException {
        try {
            validateFailingFastSchemaFor(""product.schema.json"", ""product-all-errors-data.json"");
            fail(""Exception must be thrown"");
        } catch (JsonSchemaException e) {
            final Set<ValidationMessage> messages = e.getValidationMessages();
            assertEquals(1, messages.size());
        }
    }
"
"    @Test
    public void testFailFast_OneErrors() throws IOException {
        try {
            validateFailingFastSchemaFor(""product.schema.json"", ""product-one-error-data.json"");
            fail(""Exception must be thrown"");
        } catch (JsonSchemaException e) {
            final Set<ValidationMessage> messages = e.getValidationMessages();
            assertEquals(1, messages.size());
        }
    }
"
"    @Test
    public void testFailFast_TwoErrors() throws IOException {
        try {
            validateFailingFastSchemaFor(""product.schema.json"", ""product-two-errors-data.json"");
            fail(""Exception must be thrown"");
        } catch (JsonSchemaException e) {
            final Set<ValidationMessage> messages = e.getValidationMessages();
            assertEquals(1, messages.size());
        }
    }
"
"    @Test
    public void testFailFast_NoErrors() throws IOException {
        try {
            final Set<ValidationMessage> messages = validateFailingFastSchemaFor(""product.schema.json"", ""product-no-errors-data.json"");
            assertTrue(messages.isEmpty());
        } catch (JsonSchemaException e) {
            fail(""Must not get an errors"");
        }
    }
"
"    @AfterEach
    public void cleanup() {
        reset();
    }
"
"    @Test
    public void shouldWalkAnyOfProperties() {
        walk(null, false);
    }
"
"    @Test
    public void shouldWalkAnyOfPropertiesWithWithPayloadAndValidation() throws Exception {
        JsonNode data = getJsonNodeFromStreamContent(Issue451Test.class.getResourceAsStream(
                ""/data/issue451.json""));
        walk(data,true);
    }
"
"    @Test
    public void shouldWalkAnyOfPropertiesWithWithPayload() throws Exception {
        JsonNode data = getJsonNodeFromStreamContent(Issue451Test.class.getResourceAsStream(
                ""/data/issue451.json""));
        walk(data, false);
    }
"
"    @Test
    public void shouldWorkT2() throws Exception {
        String schemaPath = ""/schema/issue456-v7.json"";
        String dataPath = ""/data/issue456-T2.json"";
        String dataT3Path = ""/data/issue456-T3.json"";
        InputStream schemaInputStream = getClass().getResourceAsStream(schemaPath);
        JsonSchema schema = getJsonSchemaFromStreamContentV7(schemaInputStream);
        InputStream dataInputStream = getClass().getResourceAsStream(dataPath);
        JsonNode node = getJsonNodeFromStreamContent(dataInputStream);
        Set<ValidationMessage> errors = schema.validate(node);
        Assertions.assertEquals(0, errors.size());
    }
"
"    @Test
    public void shouldWorkT3() throws Exception {
        String schemaPath = ""/schema/issue456-v7.json"";
        String dataPath = ""/data/issue456-T3.json"";
        InputStream schemaInputStream = getClass().getResourceAsStream(schemaPath);
        JsonSchema schema = getJsonSchemaFromStreamContentV7(schemaInputStream);
        InputStream dataInputStream = getClass().getResourceAsStream(dataPath);
        JsonNode node = getJsonNodeFromStreamContent(dataInputStream);
        Set<ValidationMessage> errors = schema.validate(node);
        Assertions.assertEquals(0, errors.size());
    }
"
"    @BeforeEach
    public void setup() {
        setupSchema();
    }
"
"    @AfterEach
    public void cleanup() {
       CollectorContext.getInstance().reset();
    }
"
"    @Test
    public void testWalk() throws IOException {
        ObjectMapper objectMapper = new ObjectMapper();
        ValidationResult result = jsonSchema.walk(
                objectMapper.readTree(getClass().getClassLoader().getResourceAsStream(""data/walk-data.json"")), false);
        JsonNode collectedNode = (JsonNode) result.getCollectorContext().get(SAMPLE_WALK_COLLECTOR_TYPE);
        assertEquals(collectedNode, (objectMapper.readTree(""{"" +
                ""    \""PROPERTY1\"": \""sample1\"",""
                + ""    \""PROPERTY2\"": \""sample2\"",""
                + ""    \""property3\"": {""
                + ""        \""street_address\"":\""test-address\"",""
                + ""        \""phone_number\"": {""
                + ""            \""country-code\"": \""091\"",""
                + ""            \""number\"": \""123456789\""""
                + ""          }""
                + ""     }""
                + ""}"")));
    }
"
"    @Test
    public void testWalkWithDifferentListeners() throws IOException {
        ObjectMapper objectMapper = new ObjectMapper();
        // This instance of schema contains all listeners.
        ValidationResult result = jsonSchema.walk(
                objectMapper.readTree(getClass().getClassLoader().getResourceAsStream(""data/walk-data.json"")), false);
        JsonNode collectedNode = (JsonNode) result.getCollectorContext().get(SAMPLE_WALK_COLLECTOR_TYPE);
        assertEquals(collectedNode, (objectMapper.readTree(""{"" +
                ""    \""PROPERTY1\"": \""sample1\"",""
                + ""    \""PROPERTY2\"": \""sample2\"",""
                + ""    \""property3\"": {""
                + ""        \""street_address\"":\""test-address\"",""
                + ""        \""phone_number\"": {""
                + ""            \""country-code\"": \""091\"",""
                + ""            \""number\"": \""123456789\""""
                + ""          }""
                + ""     }""
                + ""}"")));
        // This instance of schema contains one listener removed.
        CollectorContext collectorContext = result.getCollectorContext();
        collectorContext.reset();
        result = jsonSchema1.walk(
                objectMapper.readTree(getClass().getClassLoader().getResourceAsStream(""data/walk-data.json"")), false);
        collectedNode = (JsonNode) result.getCollectorContext().get(SAMPLE_WALK_COLLECTOR_TYPE);
        assertEquals(collectedNode, (objectMapper.readTree(""{""
                + ""    \""property3\"": {""
                + ""        \""street_address\"":\""test-address\"",""
                + ""        \""phone_number\"": {""
                + ""            \""country-code\"": \""091\"",""
                + ""            \""number\"": \""123456789\""""
                + ""          }""
                + ""     }""
                + ""}"")));
    }
"
"    @Test
    public void shouldFailV201909() throws Exception {
        String schemaPath = ""/schema/issue313-2019-09.json"";
        String dataPath = ""/data/issue313.json"";
        InputStream schemaInputStream = getClass().getResourceAsStream(schemaPath);
        JsonSchema schema = getJsonSchemaFromStreamContentV201909(schemaInputStream);
        InputStream dataInputStream = getClass().getResourceAsStream(dataPath);
        JsonNode node = getJsonNodeFromStreamContent(dataInputStream);
        Set<ValidationMessage> errors = schema.validate(node);
        Assertions.assertEquals(2, errors.size());
    }
"
"    @Test
    public void shouldFailV7() throws Exception {
        String schemaPath = ""/schema/issue313-v7.json"";
        String dataPath = ""/data/issue313.json"";
        InputStream schemaInputStream = getClass().getResourceAsStream(schemaPath);
        JsonSchema schema = getJsonSchemaFromStreamContentV7(schemaInputStream);
        InputStream dataInputStream = getClass().getResourceAsStream(dataPath);
        JsonNode node = getJsonNodeFromStreamContent(dataInputStream);
        Set<ValidationMessage> errors = schema.validate(node);
        Assertions.assertEquals(2, errors.size());
    }
"
"    @Test
    public void testNullableOneOf() throws Exception {
        runTestFile(""data/issue428.json"");
    }
"
"    @Test
    public void positiveNumber() throws IOException {
        String[][] values = augmentWithQuotes(new String[][]{
//            maximum,                       value
                {""1000.1"", ""1000""},
                {""1000"", ""1E3""},
        });

        expectNoMessages(values, NUMBER);

    }
"
"    @Test
    public void negativeNumber() throws IOException {
        String[][] values = augmentWithQuotes(new String[][]{
//            maximum,                           value
//            These values overflow 64bit IEEE 754
                {""1.7976931348623157e+308"", ""1.7976931348623159e+308""},
                {""1.7976931348623156e+308"", ""1.7976931348623157e+308""},

//            Here, threshold is parsed as integral number, yet payload is 'number'
                {""1000"", ""1000.1""},

//          See a {@link #doubleValueCoarsing() doubleValueCoarsing} test notes below
//            {""1.7976931348623157e+308"",         ""1.7976931348623158e+308""},
        });

        expectSomeMessages(values, NUMBER);

        expectSomeMessages(values, NUMBER, mapper, bigDecimalMapper);

        expectSomeMessages(values, NUMBER, bigDecimalMapper, bigDecimalMapper);
    }
"
"    @Test
    public void positiveInteger() throws IOException {
        String[][] values = augmentWithQuotes(new String[][]{
//            maximum,                       value
                {""9223372036854775807"", ""9223372036854775807""},
                {""9223372036854775808"", ""9223372036854775808""},

//                testIntegerTypeWithFloatMaxPositive
                {""37.7"", ""37""},

//                testMaximumDoubleValue
                {""1E39"", ""1000""},
        });

        expectNoMessages(values, INTEGER);

        expectNoMessages(values, INTEGER, bigIntegerMapper);
    }
"
"    @Test
    public void negativeInteger() throws IOException {
        String[][] values = augmentWithQuotes(new String[][]{
//            maximum,                value
                {""9223372036854775800"", ""9223372036854775855""},
                {""9223372036854775807"", ""9223372036854775808""},
                {""9223372036854775807"", new BigDecimal(String.valueOf(Double.MAX_VALUE)).add(BigDecimal.ONE).toString()},
                {""9223372036854775806"", new BigDecimal(String.valueOf(Double.MAX_VALUE)).add(BigDecimal.ONE).toString()},
                {""9223372036854776000"", ""9223372036854776001""},
                {""1000"", ""1E39""},
                {""37.7"", ""38""},
        });

        expectSomeMessages(values, INTEGER);

        expectSomeMessages(values, INTEGER, mapper, bigIntegerMapper);
    }
"
"    @Test
    public void positiveExclusiveInteger() throws IOException {
        String[][] values = augmentWithQuotes(new String[][]{
//            maximum,                       value
                {""9223372036854775000"", ""9223372036854774988""},
                {""20"", ""10""},

//                threshold outside long range
                {""9223372036854775809"", ""9223372036854775806""},

//                both threshold and value are outside long range
                {""9223372036854775809"", ""9223372036854775808""},
        });

        expectNoMessages(values, EXCLUSIVE_INTEGER);

        expectNoMessages(values, EXCLUSIVE_INTEGER, bigIntegerMapper);
    }
"
"    @Test
    public void negativeExclusiveInteger() throws IOException {
        String[][] values = augmentWithQuotes(new String[][]{
//            maximum,                       value
                {""10"", ""20""},

//                value outside long range
                {""9223372036854775806"", ""9223372036854775808""},

//                both threshold and value are outside long range
                {""9223372036854775808"", ""9223372036854775809""},
        });

        expectSomeMessages(values, EXCLUSIVE_INTEGER);

        expectSomeMessages(values, EXCLUSIVE_INTEGER, mapper, bigIntegerMapper);
    }
"
"    @Test
    public void negativeDoubleOverflowTest() throws IOException {
        String[][] values = new String[][]{
//            maximum,                           value
//                both of these get parsed into double (with a precision loss) as  1.7976931348623157E+308
                {""1.79769313486231571E+308"", ""1.79769313486231572e+308""},
//                while underflow in not captures in previous case (unquoted number is parsed as double)
//                it is captured if value is passed as string, which is correctly parsed by BidDecimal
//                thus effective comparison is between
//                maximum 1.7976931348623157E+308  and
//                value   1.79769313486231572e+308
//                {""1.79769313486231571E+308"",        ""\""1.79769313486231572e+308\""""},
                {""1.7976931348623157E+309"", ""1.7976931348623157e+309""},
                {""1.7976931348623157E+309"", ""\""1.7976931348623157e+309\""""},
                {""1.000000000000000000000001E+400"", ""1.000000000000000000000001E+401""},
                {""1.000000000000000000000001E+400"", ""\""1.000000000000000000000001E+401\""""},
                {""1.000000000000000000000001E+400"", ""1.000000000000000000000002E+400""},
                {""1.000000000000000000000001E+400"", ""\""1.000000000000000000000002E+400\""""},
                {""1.000000000000000000000001E+400"", ""1.0000000000000000000000011E+400""},
                {""1.000000000000000000000001E+400"", ""\""1.0000000000000000000000011E+400\""""},
        };

        for (String[] aTestCycle : values) {
            String maximum = aTestCycle[0];
            String value = aTestCycle[1];
            String schema = format(NUMBER, maximum);
            SchemaValidatorsConfig config = new SchemaValidatorsConfig();
            config.setTypeLoose(true);
            // Schema and document parsed with just double
            JsonSchema v = factory.getSchema(mapper.readTree(schema), config);
            JsonNode doc = mapper.readTree(value);
            Set<ValidationMessage> messages = v.validate(doc);
            assertTrue(messages.isEmpty(), format(""Maximum %s and value %s are interpreted as Infinity, thus no schema violation should be reported"", maximum, value));

            // document parsed with BigDecimal

            doc = bigDecimalMapper.readTree(value);
            Set<ValidationMessage> messages2 = v.validate(doc);
            if (Double.valueOf(maximum).equals(Double.POSITIVE_INFINITY)) {
                assertTrue(messages2.isEmpty(), format(""Maximum %s and value %s are equal, thus no schema violation should be reported"", maximum, value));
            } else {
                assertFalse(messages2.isEmpty(), format(""Maximum %s is smaller than value %s ,  should be validation error reported"", maximum, value));
            }


            // schema and document parsed with BigDecimal
            v = factory.getSchema(bigDecimalMapper.readTree(schema), config);
            Set<ValidationMessage> messages3 = v.validate(doc);
            //when the schema and value are both using BigDecimal, the value should be parsed in same mechanism.
            if (maximum.toLowerCase().equals(value.toLowerCase()) || Double.valueOf(maximum).equals(Double.POSITIVE_INFINITY)) {
                assertTrue(messages3.isEmpty(), format(""Maximum %s and value %s are equal, thus no schema violation should be reported"", maximum, value));
            } else {
                assertFalse(messages3.isEmpty(), format(""Maximum %s is smaller than value %s ,  should be validation error reported"", maximum, value));
            }
        }
    }
"
"    @Test
    public void doubleValueCoarsing() throws IOException {
        String schema = ""{ \""$schema\"":\""http://json-schema.org/draft-04/schema#\"", \""type\"": \""number\"", \""maximum\"": 1.7976931348623157e+308 }"";
        String content = ""1.7976931348623158e+308"";

        JsonNode doc = mapper.readTree(content);
        JsonSchema v = factory.getSchema(mapper.readTree(schema));

        Set<ValidationMessage> messages = v.validate(doc);
        assertTrue(messages.isEmpty(), ""Validation should succeed as by default double values are used by mapper"");

        doc = bigDecimalMapper.readTree(content);
        messages = v.validate(doc);
        // ""1.7976931348623158e+308"" == ""1.7976931348623157e+308"" == Double.MAX_VALUE
        // new BigDecimal(""1.7976931348623158e+308"").compareTo(new BigDecimal(""1.7976931348623157e+308"")) > 0
        assertFalse(messages.isEmpty(), ""Validation should not succeed because content is using bigDecimalMapper, and bigger than the maximum"");

        /*
         * Note: technically this is where 1.7976931348623158e+308 rounding to 1.7976931348623157e+308 could be spotted,
         *       yet it requires a dedicated case of comparison BigDecimal to BigDecimal. Since values above
         *       1.7976931348623158e+308 are parsed as Infinity anyways (jackson uses double as primary type with later
         *       ""upcasting"" to BigDecimal, if property is set) adding a dedicated code block just for this one case
         *       seems infeasible.
         */
        v = factory.getSchema(bigDecimalMapper.readTree(schema));
        messages = v.validate(doc);
        assertFalse(messages.isEmpty(), ""Validation should succeed as by default double values are used by mapper"");
    }
"
"    @Test
    public void doubleValueCoarsingExceedRange() throws IOException {
        String schema = ""{ \""$schema\"":\""http://json-schema.org/draft-04/schema#\"", \""type\"": \""number\"", \""maximum\"": 1.7976931348623159e+308 }"";
        String content = ""1.7976931348623160e+308"";

        JsonNode doc = mapper.readTree(content);
        JsonSchema v = factory.getSchema(mapper.readTree(schema));

        Set<ValidationMessage> messages = v.validate(doc);
        assertTrue(messages.isEmpty(), ""Validation should succeed as by default double values are used by mapper"");

        doc = bigDecimalMapper.readTree(content);
        messages = v.validate(doc);
        // ""1.7976931348623158e+308"" == ""1.7976931348623157e+308"" == Double.MAX_VALUE
        // new BigDecimal(""1.7976931348623158e+308"").compareTo(new BigDecimal(""1.7976931348623157e+308"")) > 0
        assertTrue(messages.isEmpty(), ""Validation should success because the bug of bigDecimalMapper, it will treat 1.7976931348623159e+308 as INFINITY"");

        /*
         * Note: technically this is where 1.7976931348623158e+308 rounding to 1.7976931348623157e+308 could be spotted,
         *       yet it requires a dedicated case of comparison BigDecimal to BigDecimal. Since values above
         *       1.7976931348623158e+308 are parsed as Infinity anyways (jackson uses double as primary type with later
         *       ""upcasting"" to BigDecimal, if property is set) adding a dedicated code block just for this one case
         *       seems infeasible.
         */
        v = factory.getSchema(bigDecimalMapper.readTree(schema));
        messages = v.validate(doc);
        assertTrue(messages.isEmpty(), ""Validation should success because the bug of bigDecimalMapper, it will treat 1.7976931348623159e+308 as INFINITY"");
    }
"
"    @BeforeEach
    public void setUp() {
        mapper = new ObjectMapper();
        // due to a jackson bug, a float number which is larger than Double.POSITIVE_INFINITY cannot be convert to BigDecimal correctly
        // https://github.com/FasterXML/jackson-databind/issues/1770
        // https://github.com/FasterXML/jackson-databind/issues/2087
        bigDecimalMapper = new ObjectMapper().enable(DeserializationFeature.USE_BIG_DECIMAL_FOR_FLOATS);
        bigIntegerMapper = new ObjectMapper().enable(DeserializationFeature.USE_BIG_INTEGER_FOR_INTS);

    }
"
"    @Test
    public void positiveNumber() throws IOException {
        String[][] values = augmentWithQuotes(new String[][]{
//            minimum,                       value
                {""1000"", ""1000.1""},
        });

        expectNoMessages(values, NUMBER, mapper);
    }
"
"    @Test
    public void negativeNumber() throws IOException {
        String[][] values = augmentWithQuotes(new String[][]{
//            minimum,                           value
                {""-1.7976931348623157e+308"", ""-1.7976931348623159e+308""},
                {""-1.7976931348623156e+308"", ""-1.7976931348623157e+308""},
                {""-1000"", ""-1E309""},
                {""1000.1"", ""1000""},
//          See a {@link #doubleValueCoarsing() doubleValueCoarsing} test notes below
//            {""-1.7976931348623157e+308"",         ""-1.7976931348623158e+308""},
        });

        expectSomeMessages(values, NUMBER, mapper, mapper);

        expectSomeMessages(values, NUMBER, mapper, bigDecimalMapper);

        expectSomeMessages(values, NUMBER, bigDecimalMapper, bigDecimalMapper);
    }
"
"    @Test
    public void testSharedRDDExample() throws Exception {
        SharedRDDExample.main(EMPTY_ARGS);
    }
"
"    @Test
    public void testCatalogExample() throws Exception {
        JavaIgniteCatalogExample.main(EMPTY_ARGS);
    }
"
"    @Test
    public void testDataFrameExample() throws Exception {
        JavaIgniteDataFrameExample.main(EMPTY_ARGS);
    }
"
"    @Test
    public void testDataFrameWriteExample() throws Exception {
        JavaIgniteDataFrameWriteExample.main(EMPTY_ARGS);
    }
"
"    @Test
    public void testCatalogExample() throws Exception {
        IgniteCatalogExample.main(EMPTY_ARGS);
    }
"
"    @Test
    public void testDataFrameExample() throws Exception {
        IgniteDataFrameExample.main(EMPTY_ARGS);
    }
"
"    @Test
    public void testDataFrameWriteExample() throws Exception {
        IgniteDataFrameWriteExample.main(EMPTY_ARGS);
    }
"
"    @Test
    public void testServiceExposedAndCallbacksInvoked() throws Exception {
        assertNotNull(ignite);
        assertEquals(""testGrid"", ignite.name());

        TestOsgiFlags flags = (TestOsgiFlags) bundleCtx.getService(
            bundleCtx.getAllServiceReferences(TestOsgiFlags.class.getName(), null)[0]);

        assertNotNull(flags);
        assertEquals(Boolean.TRUE, flags.getOnBeforeStartInvoked());
        assertEquals(Boolean.TRUE, flags.getOnAfterStartInvoked());

        // The bundle is still not stopped, therefore these callbacks cannot be tested.
        assertNull(flags.getOnBeforeStopInvoked());
        assertNull(flags.getOnAfterStopInvoked());

        // No exceptions.
        assertNull(flags.getOnAfterStartThrowable());
        assertNull(flags.getOnAfterStopThrowable());
    }
"
"    @Test
    public void testAllBundlesActiveAndFeaturesInstalled() throws Exception {
        // Asssert all bundles except fragments are ACTIVE.
        for (Bundle b : bundleCtx.getBundles()) {
            System.out.println(String.format(""Checking state of bundle [symbolicName=%s, state=%s]"",
                b.getSymbolicName(), b.getState()));

            if (b.getHeaders().get(Constants.FRAGMENT_HOST) == null)
                assertTrue(b.getState() == Bundle.ACTIVE);
        }

        // Check that according to the FeaturesService, all Ignite features except ignite-log4j are installed.
        Feature[] features = featuresSvc.getFeatures(IGNITE_FEATURES_NAME_REGEX);

        assertNotNull(features);
        assertEquals(EXPECTED_FEATURES, features.length);

        for (Feature f : features) {
            if (IGNORED_FEATURES.contains(f.getName()))
                continue;

            boolean installed = featuresSvc.isInstalled(f);

            System.out.println(String.format(""Checking if feature is installed [featureName=%s, installed=%s]"",
                f.getName(), installed));

            assertTrue(installed);
            assertEquals(PROJECT_VERSION.replaceAll(""-"", "".""), f.getVersion().replaceAll(""-"", "".""));
        }
    }
"
"    @Test
    public void testStart() {
        UUID nodeId = UUID.randomUUID();
        UUID procId = UUID.randomUUID();

        Ignite ignite = mock(Ignite.class);
        IgniteCluster cluster = mock(IgniteCluster.class);
        ClusterGroup clusterGrp = mock(ClusterGroup.class);
        IgniteCompute igniteCompute = mock(IgniteCompute.class);
        doReturn(cluster).when(ignite).cluster();
        doReturn(igniteCompute).when(ignite).compute(eq(clusterGrp));
        doReturn(clusterGrp).when(cluster).forNodeId(eq(nodeId));
        doReturn(Collections.singletonList(procId)).when(igniteCompute).call(any(IgniteCallable.class));

        List<LongRunningProcess> list = Collections.singletonList(new LongRunningProcess(nodeId, () -> {}));

        LongRunningProcessManager mgr = new LongRunningProcessManager(ignite);
        Map<UUID, List<UUID>> res = mgr.start(list);

        assertEquals(1, res.size());
        assertTrue(res.containsKey(nodeId));
        assertEquals(procId, res.get(nodeId).iterator().next());

        verify(igniteCompute).call(any(LongRunningProcessStartTask.class));
    }
"
"    @Test
    public void testPing() {
        UUID nodeId = UUID.randomUUID();
        UUID procId = UUID.randomUUID();

        Ignite ignite = mock(Ignite.class);
        IgniteCluster cluster = mock(IgniteCluster.class);
        ClusterGroup clusterGrp = mock(ClusterGroup.class);
        IgniteCompute igniteCompute = mock(IgniteCompute.class);
        doReturn(cluster).when(ignite).cluster();
        doReturn(igniteCompute).when(ignite).compute(eq(clusterGrp));
        doReturn(clusterGrp).when(cluster).forNodeId(eq(nodeId));
        doReturn(Collections.singletonList(new LongRunningProcessStatus(LongRunningProcessState.RUNNING)))
            .when(igniteCompute).call(any(IgniteCallable.class));

        Map<UUID, List<UUID>> procIds = new HashMap<>();
        procIds.put(nodeId, Collections.singletonList(procId));

        LongRunningProcessManager mgr = new LongRunningProcessManager(ignite);
        Map<UUID, List<LongRunningProcessStatus>> res = mgr.ping(procIds);

        assertEquals(1, res.size());
        assertTrue(res.containsKey(nodeId));
        assertEquals(LongRunningProcessState.RUNNING, res.get(nodeId).iterator().next().getState());

        verify(igniteCompute).call(any(LongRunningProcessPingTask.class));
    }
"
"    @Test
    public void testStop() {
        UUID nodeId = UUID.randomUUID();
        UUID procId = UUID.randomUUID();

        Ignite ignite = mock(Ignite.class);
        IgniteCluster cluster = mock(IgniteCluster.class);
        ClusterGroup clusterGrp = mock(ClusterGroup.class);
        IgniteCompute igniteCompute = mock(IgniteCompute.class);
        doReturn(cluster).when(ignite).cluster();
        doReturn(igniteCompute).when(ignite).compute(eq(clusterGrp));
        doReturn(clusterGrp).when(cluster).forNodeId(eq(nodeId));
        doReturn(Collections.singletonList(new LongRunningProcessStatus(LongRunningProcessState.RUNNING)))
            .when(igniteCompute).call(any(IgniteCallable.class));

        Map<UUID, List<UUID>> procIds = new HashMap<>();
        procIds.put(nodeId, Collections.singletonList(procId));

        LongRunningProcessManager mgr = new LongRunningProcessManager(ignite);
        Map<UUID, List<LongRunningProcessStatus>> res = mgr.stop(procIds, true);

        assertEquals(1, res.size());
        assertTrue(res.containsKey(nodeId));
        assertEquals(LongRunningProcessState.RUNNING, res.get(nodeId).iterator().next().getState());

        verify(igniteCompute).call(any(LongRunningProcessStopTask.class));
    }
"
"    @Test
    public void testClear() {
        UUID nodeId = UUID.randomUUID();
        UUID procId = UUID.randomUUID();

        Ignite ignite = mock(Ignite.class);
        IgniteCluster cluster = mock(IgniteCluster.class);
        ClusterGroup clusterGrp = mock(ClusterGroup.class);
        IgniteCompute igniteCompute = mock(IgniteCompute.class);
        doReturn(cluster).when(ignite).cluster();
        doReturn(igniteCompute).when(ignite).compute(eq(clusterGrp));
        doReturn(clusterGrp).when(cluster).forNodeId(eq(nodeId));
        doReturn(Collections.singletonList(new LongRunningProcessStatus(LongRunningProcessState.RUNNING)))
            .when(igniteCompute).call(any(IgniteCallable.class));

        Map<UUID, List<UUID>> procIds = new HashMap<>();
        procIds.put(nodeId, Collections.singletonList(procId));

        LongRunningProcessManager mgr = new LongRunningProcessManager(ignite);
        Map<UUID, List<LongRunningProcessStatus>> res = mgr.clear(procIds);

        assertEquals(1, res.size());
        assertTrue(res.containsKey(nodeId));
        assertEquals(LongRunningProcessState.RUNNING, res.get(nodeId).iterator().next().getState());

        verify(igniteCompute).call(any(LongRunningProcessClearTask.class));
    }
"
"    @Test
    public void testCallProcessNotFound() {
        LongRunningProcessClearTask clearTask = createTask(UUID.randomUUID());

        List<LongRunningProcessStatus> statuses = clearTask.call();

        assertEquals(1, statuses.size());

        LongRunningProcessStatus status = statuses.get(0);
        assertEquals(LongRunningProcessState.NOT_FOUND, status.getState());
        assertNull(status.getException());

        assertEquals(0, metadataStorage.size());
    }
"
"    @Test(expected = IllegalStateException.class)
    public void testCallProcessIsRunning() {
        UUID procId = UUID.randomUUID();

        Future<?> fut = mock(Future.class);
        doReturn(false).when(fut).isDone();
        metadataStorage.put(procId, fut);

        LongRunningProcessClearTask clearTask = createTask(procId);

        clearTask.call();
    }
"
"    @Test
    public void testCallProcessIsDone() {
        UUID procId = UUID.randomUUID();

        Future<?> fut = mock(Future.class);
        doReturn(true).when(fut).isDone();
        metadataStorage.put(procId, fut);

        LongRunningProcessClearTask clearTask = createTask(procId);

        List<LongRunningProcessStatus> statuses = clearTask.call();

        assertEquals(1, statuses.size());

        LongRunningProcessStatus status = statuses.get(0);
        assertEquals(LongRunningProcessState.DONE, status.getState());
        assertNull(status.getException());

        assertEquals(0, metadataStorage.size());
    }
"
"    @Test
    public void testCallProcessIsDoneWithException() throws ExecutionException, InterruptedException {
        UUID procId = UUID.randomUUID();

        Future<?> fut = mock(Future.class);
        doReturn(true).when(fut).isDone();
        doThrow(RuntimeException.class).when(fut).get();
        metadataStorage.put(procId, fut);

        LongRunningProcessClearTask clearTask = createTask(procId);

        List<LongRunningProcessStatus> statuses = clearTask.call();

        assertEquals(1, statuses.size());

        LongRunningProcessStatus status = statuses.get(0);
        assertEquals(LongRunningProcessState.DONE, status.getState());
        assertNotNull(status.getException());
        assertTrue(status.getException() instanceof RuntimeException);

        assertEquals(0, metadataStorage.size());
    }
"
"    @Test
    public void testCallProcessNotFound() {
        LongRunningProcessStopTask stopTask = createTask(UUID.randomUUID(), true);

        List<LongRunningProcessStatus> statuses = stopTask.call();

        assertEquals(1, statuses.size());

        LongRunningProcessStatus status = statuses.get(0);
        assertEquals(LongRunningProcessState.NOT_FOUND, status.getState());
        assertNull(status.getException());

        assertEquals(0, metadataStorage.size());
    }
"
"    @Test
    public void testCallProcessIsRunning() {
        UUID procId = UUID.randomUUID();

        Future<?> fut = mock(Future.class);
        doReturn(false).when(fut).isDone();
        metadataStorage.put(procId, fut);

        LongRunningProcessStopTask stopTask = createTask(procId, true);

        List<LongRunningProcessStatus> statuses = stopTask.call();

        assertEquals(1, statuses.size());
        verify(fut).cancel(eq(true));

        LongRunningProcessStatus status = statuses.get(0);
        assertEquals(LongRunningProcessState.DONE, status.getState());
        assertNull(status.getException());

        assertEquals(0, metadataStorage.size());
    }
"
"    @Test
    public void testCallProcessIsDone() {
        UUID procId = UUID.randomUUID();

        Future<?> fut = mock(Future.class);
        doReturn(true).when(fut).isDone();
        metadataStorage.put(procId, fut);

        LongRunningProcessStopTask stopTask = createTask(procId, true);

        List<LongRunningProcessStatus> statuses = stopTask.call();

        assertEquals(1, statuses.size());
        verify(fut).cancel(eq(true));

        LongRunningProcessStatus status = statuses.get(0);
        assertEquals(LongRunningProcessState.DONE, status.getState());
        assertNull(status.getException());

        assertEquals(0, metadataStorage.size());
    }
"
"    @Test
    public void testCallProcessIsDoneWithException() throws ExecutionException, InterruptedException {
        UUID procId = UUID.randomUUID();

        Future<?> fut = mock(Future.class);
        doReturn(true).when(fut).isDone();
        doThrow(RuntimeException.class).when(fut).get();
        metadataStorage.put(procId, fut);

        LongRunningProcessStopTask stopTask = createTask(procId, true);

        List<LongRunningProcessStatus> statuses = stopTask.call();

        assertEquals(1, statuses.size());
        verify(fut).cancel(eq(true));

        LongRunningProcessStatus status = statuses.get(0);
        assertEquals(LongRunningProcessState.DONE, status.getState());
        assertNotNull(status.getException());
        assertTrue(status.getException() instanceof RuntimeException);

        assertEquals(0, metadataStorage.size());
    }
"
"    @Test
    public void testCall() throws ExecutionException, InterruptedException {
        LongRunningProcess proc = new LongRunningProcess(UUID.randomUUID(), () -> {});
        LongRunningProcessStartTask task = createTask(proc);
        List<UUID> procIds = task.call();

        assertEquals(1, procIds.size());

        UUID procId = procIds.get(0);

        assertNotNull(metadataStorage.get(procId));

        Future<?> fut = metadataStorage.get(procId);
        fut.get();

        assertEquals(true, fut.isDone());
    }
"
"    @Test(expected = ExecutionException.class)
    public void testCallWithException() throws ExecutionException, InterruptedException {
        LongRunningProcess proc = new LongRunningProcess(UUID.randomUUID(), () -> {
            throw new RuntimeException();
        });
        LongRunningProcessStartTask task = createTask(proc);
        List<UUID> procIds = task.call();

        assertEquals(1, procIds.size());

        UUID procId = procIds.get(0);

        assertNotNull(metadataStorage.get(procId));

        Future<?> fut = metadataStorage.get(procId);
        fut.get();
    }
"
"    @Test
    public void testCallProcessNotFound() {
        LongRunningProcessPingTask pingTask = createTask(UUID.randomUUID());

        List<LongRunningProcessStatus> statuses = pingTask.call();

        assertEquals(1, statuses.size());

        LongRunningProcessStatus status = statuses.get(0);
        assertEquals(LongRunningProcessState.NOT_FOUND, status.getState());
        assertNull(status.getException());

        assertEquals(0, metadataStorage.size());
    }
"
"    @Test
    public void testCallProcessIsRunning() {
        UUID procId = UUID.randomUUID();

        Future<?> fut = mock(Future.class);
        doReturn(false).when(fut).isDone();
        metadataStorage.put(procId, fut);

        LongRunningProcessPingTask pingTask = createTask(procId);

        List<LongRunningProcessStatus> statuses = pingTask.call();

        assertEquals(1, statuses.size());

        LongRunningProcessStatus status = statuses.get(0);
        assertEquals(LongRunningProcessState.RUNNING, status.getState());
        assertNull(status.getException());

        assertEquals(1, metadataStorage.size());
    }
"
"    @Test
    public void testCallProcessIsDone() {
        UUID procId = UUID.randomUUID();

        Future<?> fut = mock(Future.class);
        doReturn(true).when(fut).isDone();
        metadataStorage.put(procId, fut);

        LongRunningProcessPingTask pingTask = createTask(procId);

        List<LongRunningProcessStatus> statuses = pingTask.call();

        assertEquals(1, statuses.size());

        LongRunningProcessStatus status = statuses.get(0);
        assertEquals(LongRunningProcessState.DONE, status.getState());
        assertNull(status.getException());

        assertEquals(1, metadataStorage.size());
    }
"
"    @Test
    public void testCallProcessIsDoneWithException() throws ExecutionException, InterruptedException {
        UUID procId = UUID.randomUUID();

        Future<?> fut = mock(Future.class);
        doReturn(true).when(fut).isDone();
        doThrow(RuntimeException.class).when(fut).get();
        metadataStorage.put(procId, fut);

        LongRunningProcessPingTask pingTask = createTask(procId);

        List<LongRunningProcessStatus> statuses = pingTask.call();

        assertEquals(1, statuses.size());

        LongRunningProcessStatus status = statuses.get(0);
        assertEquals(LongRunningProcessState.DONE, status.getState());
        assertNotNull(status.getException());
        assertTrue(status.getException() instanceof RuntimeException);

        assertEquals(1, metadataStorage.size());
    }
"
"    @Test
    public void testStart() {
        wrapper.start(Arrays.asList(1, 2, 3));

        verify(delegate).start(eq(Arrays.asList(""1"", ""2"", ""3"")));
    }
"
"    @Test
    public void testPing() {
        Map<UUID, List<UUID>> procIds = Collections.emptyMap();
        wrapper.ping(procIds);

        verify(delegate).ping(eq(procIds));
    }
"
"    @Test
    public void testStop() {
        Map<UUID, List<UUID>> procIds = Collections.emptyMap();
        wrapper.stop(procIds, true);

        verify(delegate).stop(eq(procIds), eq(true));
    }
"
"    @Test
    public void testClear() {
        Map<UUID, List<UUID>> procIds = Collections.emptyMap();
        wrapper.clear(procIds);

        verify(delegate).clear(eq(procIds));
    }
"
"    @Test
    public void testMultithreading() throws Exception {
        final int THREAD_CNT = 8;
        final int ITERATION_CNT = 20;
        final int BATCH_SIZE = 1000;
        final int PAGE_CNT = 3;

        IgniteConfiguration srvCfg = Config.getServerConfiguration();

        // No peer class loading from thin clients: we need the server to know about this class to deserialize
        // ScanQuery filter.
        srvCfg.setBinaryConfiguration(new BinaryConfiguration().setTypeConfigurations(Arrays.asList(
            new BinaryTypeConfiguration(getClass().getName()),
            new BinaryTypeConfiguration(SerializedLambda.class.getName())
        )));

        try (Ignite ignored = Ignition.start(srvCfg);
             IgniteClient client = Ignition.startClient(new ClientConfiguration().setAddresses(Config.SERVER))
        ) {
            ClientCache<Integer, String> cache = client.createCache(""testMultithreading"");

            AtomicInteger cnt = new AtomicInteger(1);

            AtomicReference<Throwable> error = new AtomicReference<>();

            Runnable assertion = () -> {
                try {
                    int rangeStart = cnt.getAndAdd(BATCH_SIZE);
                    int rangeEnd = rangeStart + BATCH_SIZE;

                    Map<Integer, String> data = IntStream.range(rangeStart, rangeEnd).boxed()
                        .collect(Collectors.toMap(i -> i, i -> String.format(""String %s"", i)));

                    cache.putAll(data);

                    Query<Cache.Entry<Integer, String>> qry = new ScanQuery<Integer, String>()
                        .setPageSize(data.size() / PAGE_CNT)
                        .setFilter((i, s) -> i >= rangeStart && i < rangeEnd);

                    try (QueryCursor<Cache.Entry<Integer, String>> cur = cache.query(qry)) {
                        List<Cache.Entry<Integer, String>> res = cur.getAll();

                        assertEquals(""Unexpected number of entries"", data.size(), res.size());

                        Map<Integer, String> act = res.stream()
                            .collect(Collectors.toMap(Cache.Entry::getKey, Cache.Entry::getValue));

                        assertEquals(""Unexpected entries"", data, act);
                    }
                }
                catch (Throwable ex) {
                    error.set(ex);
                }
            };

            CountDownLatch complete = new CountDownLatch(THREAD_CNT);

            Runnable manyAssertions = () -> {
                for (int i = 0; i < ITERATION_CNT && error.get() == null; i++)
                    assertion.run();

                complete.countDown();
            };

            ExecutorService threadPool = Executors.newFixedThreadPool(THREAD_CNT);

            IntStream.range(0, THREAD_CNT).forEach(t -> threadPool.submit(manyAssertions));

            assertTrue(""Timeout"", complete.await(180, TimeUnit.SECONDS));

            String errMsg = error.get() == null ? """" : error.get().getMessage();

            assertNull(errMsg, error.get());
        }
    }
"
"    @Test
    public void testFailover() throws Exception {
        final int CLUSTER_SIZE = 3;

        try (LocalIgniteCluster cluster = LocalIgniteCluster.start(CLUSTER_SIZE);
             IgniteClient client = Ignition.startClient(new ClientConfiguration()
                 .setAddresses(cluster.clientAddresses().toArray(new String[CLUSTER_SIZE]))
             )
        ) {
            final Random rnd = new Random();

            final ClientCache<Integer, String> cache = client.getOrCreateCache(
                new ClientCacheConfiguration().setName(""testFailover"").setCacheMode(CacheMode.REPLICATED)
            );

            // Simple operation failover: put/get
            assertOnUnstableCluster(cluster, () -> {
                Integer key = rnd.nextInt();
                String val = key.toString();

                cache.put(key, val);

                String cachedVal = cache.get(key);

                assertEquals(val, cachedVal);
            });

            // Composite operation failover: query
            Map<Integer, String> data = IntStream.rangeClosed(1, 1000).boxed()
                .collect(Collectors.toMap(i -> i, i -> String.format(""String %s"", i)));

            assertOnUnstableCluster(cluster, () -> {
                cache.putAll(data);

                Query<Cache.Entry<Integer, String>> qry =
                    new ScanQuery<Integer, String>().setPageSize(data.size() / 10);

                try (QueryCursor<Cache.Entry<Integer, String>> cur = cache.query(qry)) {
                    List<Cache.Entry<Integer, String>> res = cur.getAll();

                    assertEquals(""Unexpected number of entries"", data.size(), res.size());

                    Map<Integer, String> act = res.stream()
                        .collect(Collectors.toMap(Cache.Entry::getKey, Cache.Entry::getValue));

                    assertEquals(""Unexpected entries"", data, act);
                }
            });

            // Client fails if all nodes go down
            cluster.close();

            boolean igniteUnavailable = false;

            try {
                cache.put(1, ""1"");
            }
            catch (ClientConnectionException ex) {
                igniteUnavailable = true;

                Throwable[] suppressed = ex.getSuppressed();

                assertEquals(suppressed.length, CLUSTER_SIZE - 1);

                assertTrue(Stream.of(suppressed).allMatch(t -> t instanceof ClientConnectionException));
            }

            assertTrue(igniteUnavailable);
        }
    }
"
"    @Test
    public void testUnmarshalSchemalessIgniteBinaries() throws Exception {
        int key = 1;
        Person val = new Person(key, ""Joe"");

        try (Ignite srv = Ignition.start(Config.getServerConfiguration())) {
            // Add an entry directly to the Ignite server. This stores a schema-less object in the cache and
            // does not register schema in the client's metadata cache.
            srv.cache(Config.DEFAULT_CACHE_NAME).put(key, val);

            try (IgniteClient client = Ignition.startClient(new ClientConfiguration().setAddresses(Config.SERVER))) {
                ClientCache<Integer, Person> cache = client.cache(Config.DEFAULT_CACHE_NAME);

                Person cachedVal = cache.get(key);

                assertEquals(val, cachedVal);
            }
        }
    }
"
"    @Test
    public void testReadingSchemalessIgniteBinaries() throws Exception {
        int key = 1;
        Person val = new Person(key, ""Joe"");

        try (Ignite srv = Ignition.start(Config.getServerConfiguration())) {
            // Add an entry directly to the Ignite server. This stores a schema-less object in the cache and
            // does not register schema in the client's metadata cache.
            srv.cache(Config.DEFAULT_CACHE_NAME).put(key, val);

            try (IgniteClient client = Ignition.startClient(new ClientConfiguration().setAddresses(Config.SERVER))) {
                ClientCache<Integer, BinaryObject> cache = client.cache(Config.DEFAULT_CACHE_NAME).withKeepBinary();

                BinaryObject cachedVal = cache.get(key);

                assertEquals(val.getId(), cachedVal.field(""id""));
                assertEquals(val.getName(), cachedVal.field(""name""));
            }
        }
    }
"
"    @Test
    public void testBinaryObjectPutGet() throws Exception {
        int key = 1;

        try (Ignite ignored = Ignition.start(Config.getServerConfiguration())) {
            try (IgniteClient client =
                     Ignition.startClient(new ClientConfiguration().setAddresses(Config.SERVER))
            ) {
                IgniteBinary binary = client.binary();

                BinaryObject val = binary.builder(""Person"")
                    .setField(""id"", 1, int.class)
                    .setField(""name"", ""Joe"", String.class)
                    .build();

                ClientCache<Integer, BinaryObject> cache = client.cache(Config.DEFAULT_CACHE_NAME).withKeepBinary();

                cache.put(key, val);

                BinaryObject cachedVal =
                    client.cache(Config.DEFAULT_CACHE_NAME).<Integer, BinaryObject>withKeepBinary().get(key);

                assertBinaryObjectsEqual(val, cachedVal);
            }
        }
    }
"
"    @Test
    public void testBinaryObjectApi() throws Exception {
        try (Ignite srv = Ignition.start(Config.getServerConfiguration())) {
            try (IgniteClient client = Ignition.startClient(new ClientConfiguration().setAddresses(Config.SERVER))) {
                // Use ""server-side"" IgniteBinary as a reference to test the thin client IgniteBinary against
                IgniteBinary refBinary = srv.binary();

                IgniteBinary binary = client.binary();

                Person obj = new Person(1, ""Joe"");

                int refTypeId = refBinary.typeId(Person.class.getName());
                int typeId = binary.typeId(Person.class.getName());

                assertEquals(refTypeId, typeId);

                BinaryObject refBinObj = refBinary.toBinary(obj);
                BinaryObject binObj = binary.toBinary(obj);

                assertBinaryObjectsEqual(refBinObj, binObj);

                assertBinaryTypesEqual(refBinary.type(typeId), binary.type(typeId));

                assertBinaryTypesEqual(refBinary.type(Person.class), binary.type(Person.class));

                assertBinaryTypesEqual(refBinary.type(Person.class.getName()), binary.type(Person.class.getName()));

                Collection<BinaryType> refTypes = refBinary.types();
                Collection<BinaryType> types = binary.types();

                assertEquals(refTypes.size(), types.size());

                BinaryObject refEnm = refBinary.buildEnum(Enum.class.getName(), Enum.DEFAULT.ordinal());
                BinaryObject enm = binary.buildEnum(Enum.class.getName(), Enum.DEFAULT.ordinal());

                assertBinaryObjectsEqual(refEnm, enm);

                Map<String, Integer> enumMap = Arrays.stream(Enum.values())
                    .collect(Collectors.toMap(java.lang.Enum::name, java.lang.Enum::ordinal));

                BinaryType refEnumType = refBinary.registerEnum(Enum.class.getName(), enumMap);
                BinaryType enumType = binary.registerEnum(Enum.class.getName(), enumMap);

                assertBinaryTypesEqual(refEnumType, enumType);

                refEnm = refBinary.buildEnum(Enum.class.getName(), Enum.DEFAULT.name());
                enm = binary.buildEnum(Enum.class.getName(), Enum.DEFAULT.name());

                assertBinaryObjectsEqual(refEnm, enm);
            }
        }
    }
"
"    @Test
    public void testCacheManagement() throws Exception {
        try (LocalIgniteCluster ignored = LocalIgniteCluster.start(2);
             IgniteClient client = Ignition.startClient(getClientConfiguration())
        ) {
            final String CACHE_NAME = ""testCacheManagement"";

            ClientCacheConfiguration cacheCfg = new ClientCacheConfiguration().setName(CACHE_NAME)
                .setCacheMode(CacheMode.REPLICATED)
                .setWriteSynchronizationMode(CacheWriteSynchronizationMode.FULL_SYNC);

            int key = 1;
            Person val = new Person(key, Integer.toString(key));

            ClientCache<Integer, Person> cache = client.getOrCreateCache(cacheCfg);

            cache.put(key, val);

            assertEquals(1, cache.size());
            assertEquals(2, cache.size(CachePeekMode.ALL));

            cache = client.cache(CACHE_NAME);

            Person cachedVal = cache.get(key);

            assertEquals(val, cachedVal);

            Object[] cacheNames = new TreeSet<>(client.cacheNames()).toArray();

            assertArrayEquals(new TreeSet<>(Arrays.asList(Config.DEFAULT_CACHE_NAME, CACHE_NAME)).toArray(), cacheNames);

            client.destroyCache(CACHE_NAME);

            cacheNames = client.cacheNames().toArray();

            assertArrayEquals(new Object[] {Config.DEFAULT_CACHE_NAME}, cacheNames);

            cache = client.createCache(CACHE_NAME);

            assertFalse(cache.containsKey(key));

            cacheNames = client.cacheNames().toArray();

            assertArrayEquals(new TreeSet<>(Arrays.asList(Config.DEFAULT_CACHE_NAME, CACHE_NAME)).toArray(), cacheNames);

            client.destroyCache(CACHE_NAME);

            cache = client.createCache(cacheCfg);

            assertFalse(cache.containsKey(key));

            assertArrayEquals(new TreeSet<>(Arrays.asList(Config.DEFAULT_CACHE_NAME, CACHE_NAME)).toArray(), cacheNames);
        }
    }
"
"    @Test
    public void testCacheConfiguration() throws Exception {
        try (Ignite ignored = Ignition.start(Config.getServerConfiguration());
             IgniteClient client = Ignition.startClient(getClientConfiguration())
        ) {
            final String CACHE_NAME = ""testCacheConfiguration"";

            ClientCacheConfiguration cacheCfg = new ClientCacheConfiguration().setName(CACHE_NAME)
                .setAtomicityMode(CacheAtomicityMode.TRANSACTIONAL)
                .setBackups(3)
                .setCacheMode(CacheMode.PARTITIONED)
                .setWriteSynchronizationMode(CacheWriteSynchronizationMode.FULL_SYNC)
                .setEagerTtl(false)
                .setGroupName(""FunctionalTest"")
                .setDefaultLockTimeout(12345)
                .setPartitionLossPolicy(PartitionLossPolicy.READ_WRITE_ALL)
                .setReadFromBackup(true)
                .setRebalanceBatchSize(67890)
                .setRebalanceBatchesPrefetchCount(102938)
                .setRebalanceDelay(54321)
                .setRebalanceMode(CacheRebalanceMode.SYNC)
                .setRebalanceOrder(2)
                .setRebalanceThrottle(564738)
                .setRebalanceTimeout(142536)
                .setKeyConfiguration(new CacheKeyConfiguration(""Employee"", ""orgId""))
                .setQueryEntities(new QueryEntity(int.class.getName(), ""Employee"")
                    .setTableName(""EMPLOYEE"")
                    .setFields(
                        Stream.of(
                            new SimpleEntry<>(""id"", Integer.class.getName()),
                            new SimpleEntry<>(""orgId"", Integer.class.getName())
                        ).collect(Collectors.toMap(
                            SimpleEntry::getKey, SimpleEntry::getValue, (a, b) -> a, LinkedHashMap::new
                        ))
                    )
                    .setKeyFields(Collections.singleton(""id""))
                    .setNotNullFields(Collections.singleton(""id""))
                    .setDefaultFieldValues(Collections.singletonMap(""id"", 0))
                    .setIndexes(Collections.singletonList(new QueryIndex(""id"", true, ""IDX_EMPLOYEE_ID"")))
                    .setAliases(Stream.of(""id"", ""orgId"").collect(Collectors.toMap(f -> f, String::toUpperCase)))
                );

            ClientCache cache = client.createCache(cacheCfg);

            assertEquals(CACHE_NAME, cache.getName());

            assertTrue(Comparers.equal(cacheCfg, cache.getConfiguration()));
        }
    }
"
"    @Test
    public void testPutGet() throws Exception {
        // Existing cache, primitive key and object value
        try (Ignite ignored = Ignition.start(Config.getServerConfiguration());
             IgniteClient client = Ignition.startClient(getClientConfiguration())
        ) {
            ClientCache<Integer, Person> cache = client.getOrCreateCache(Config.DEFAULT_CACHE_NAME);

            Integer key = 1;
            Person val = new Person(key, ""Joe"");

            cache.put(key, val);

            assertTrue(cache.containsKey(key));

            Person cachedVal = cache.get(key);

            assertEquals(val, cachedVal);
        }

        // Non-existing cache, object key and primitive value
        try (Ignite ignored = Ignition.start(Config.getServerConfiguration());
             IgniteClient client = Ignition.startClient(getClientConfiguration())
        ) {
            ClientCache<Person, Integer> cache = client.getOrCreateCache(""testPutGet"");

            Integer val = 1;

            Person key = new Person(val, ""Joe"");

            cache.put(key, val);

            Integer cachedVal = cache.get(key);

            assertEquals(val, cachedVal);
        }

        // Object key and Object value
        try (Ignite ignored = Ignition.start(Config.getServerConfiguration());
             IgniteClient client = Ignition.startClient(getClientConfiguration())
        ) {
            ClientCache<Person, Person> cache = client.getOrCreateCache(""testPutGet"");

            Person key = new Person(1, ""Joe Key"");

            Person val = new Person(1, ""Joe Value"");

            cache.put(key, val);

            Person cachedVal = cache.get(key);

            assertEquals(val, cachedVal);
        }
    }
"
"    @Test
    public void testBatchPutGet() throws Exception {
        // Existing cache, primitive key and object value
        try (Ignite ignored = Ignition.start(Config.getServerConfiguration());
             IgniteClient client = Ignition.startClient(getClientConfiguration())
        ) {
            ClientCache<Integer, Person> cache = client.cache(Config.DEFAULT_CACHE_NAME);

            Map<Integer, Person> data = IntStream
                .rangeClosed(1, 1000).boxed()
                .collect(Collectors.toMap(i -> i, i -> new Person(i, String.format(""Person %s"", i))));

            cache.putAll(data);

            Map<Integer, Person> cachedData = cache.getAll(data.keySet());

            assertEquals(data, cachedData);
        }

        // Non-existing cache, object key and primitive value
        try (Ignite ignored = Ignition.start(Config.getServerConfiguration());
             IgniteClient client = Ignition.startClient(getClientConfiguration())
        ) {
            ClientCache<Person, Integer> cache = client.createCache(""testBatchPutGet"");

            Map<Person, Integer> data = IntStream
                .rangeClosed(1, 1000).boxed()
                .collect(Collectors.toMap(i -> new Person(i, String.format(""Person %s"", i)), i -> i));

            cache.putAll(data);

            Map<Person, Integer> cachedData = cache.getAll(data.keySet());

            assertEquals(data, cachedData);

            cache.clear();

            assertEquals(0, cache.size(CachePeekMode.ALL));
        }
    }
"
"    @Test
    public void testAtomicPutGet() throws Exception {
        try (Ignite ignored = Ignition.start(Config.getServerConfiguration());
             IgniteClient client = Ignition.startClient(getClientConfiguration())
        ) {
            ClientCache<Integer, String> cache = client.createCache(""testRemoveReplace"");

            assertNull(cache.getAndPut(1, ""1""));
            assertEquals(""1"", cache.getAndPut(1, ""1.1""));

            assertEquals(""1.1"", cache.getAndRemove(1));
            assertNull(cache.getAndRemove(1));

            assertTrue(cache.putIfAbsent(1, ""1""));
            assertFalse(cache.putIfAbsent(1, ""1.1""));

            assertEquals(""1"", cache.getAndReplace(1, ""1.1""));
            assertEquals(""1.1"", cache.getAndReplace(1, ""1""));
            assertNull(cache.getAndReplace(2, ""2""));
        }
    }
"
"    @Test
    public void testRemoveReplace() throws Exception {
        try (Ignite ignored = Ignition.start(Config.getServerConfiguration());
             IgniteClient client = Ignition.startClient(getClientConfiguration())
        ) {
            ClientCache<Integer, String> cache = client.createCache(""testRemoveReplace"");

            Map<Integer, String> data = IntStream.rangeClosed(1, 100).boxed()
                .collect(Collectors.toMap(i -> i, Object::toString));

            cache.putAll(data);

            assertFalse(cache.replace(1, ""2"", ""3""));
            assertEquals(""1"", cache.get(1));
            assertTrue(cache.replace(1, ""1"", ""3""));
            assertEquals(""3"", cache.get(1));

            assertFalse(cache.replace(101, ""101""));
            assertNull(cache.get(101));
            assertTrue(cache.replace(100, ""101""));
            assertEquals(""101"", cache.get(100));

            assertFalse(cache.remove(101));
            assertTrue(cache.remove(100));
            assertNull(cache.get(100));

            assertFalse(cache.remove(99, ""100""));
            assertEquals(""99"", cache.get(99));
            assertTrue(cache.remove(99, ""99""));
            assertNull(cache.get(99));

            cache.put(101, ""101"");

            cache.removeAll(data.keySet());
            assertEquals(1, cache.size());
            assertEquals(""101"", cache.get(101));

            cache.removeAll();
            assertEquals(0, cache.size());
        }
    }
"
"    @Test
    public void testClientFailsOnStart() {
        ClientConnectionException expEx = null;

        try (IgniteClient ignored = Ignition.startClient(getClientConfiguration())) {
            // No-op.
        }
        catch (ClientConnectionException connEx) {
            expEx = connEx;
        }
        catch (Exception ex) {
            fail(String.format(
                ""%s expected but %s was received: %s"",
                ClientConnectionException.class.getName(),
                ex.getClass().getName(),
                ex
            ));
        }

        assertNotNull(
            String.format(""%s expected but no exception was received"", ClientConnectionException.class.getName()),
            expEx
        );
    }
"
"    @Test
    public void testSerialization() throws IOException, ClassNotFoundException {
        ClientCacheConfiguration target = new ClientCacheConfiguration().setName(""Person"")
            .setAtomicityMode(CacheAtomicityMode.TRANSACTIONAL)
            .setBackups(3)
            .setCacheMode(CacheMode.PARTITIONED)
            .setWriteSynchronizationMode(CacheWriteSynchronizationMode.FULL_SYNC)
            .setEagerTtl(false)
            .setGroupName(""FunctionalTest"")
            .setDefaultLockTimeout(12345)
            .setPartitionLossPolicy(PartitionLossPolicy.READ_WRITE_ALL)
            .setReadFromBackup(true)
            .setRebalanceBatchSize(67890)
            .setRebalanceBatchesPrefetchCount(102938)
            .setRebalanceDelay(54321)
            .setRebalanceMode(CacheRebalanceMode.SYNC)
            .setRebalanceOrder(2)
            .setRebalanceThrottle(564738)
            .setRebalanceTimeout(142536)
            .setKeyConfiguration(new CacheKeyConfiguration(""Employee"", ""orgId""))
            .setQueryEntities(new QueryEntity(int.class.getName(), ""Employee"")
                .setTableName(""EMPLOYEE"")
                .setFields(
                    Stream.of(
                        new SimpleEntry<>(""id"", Integer.class.getName()),
                        new SimpleEntry<>(""orgId"", Integer.class.getName())
                    ).collect(Collectors.toMap(
                        SimpleEntry::getKey, SimpleEntry::getValue, (a, b) -> a, LinkedHashMap::new
                    ))
                )
                .setKeyFields(Collections.singleton(""id""))
                .setNotNullFields(Collections.singleton(""id""))
                .setDefaultFieldValues(Collections.singletonMap(""id"", 0))
                .setIndexes(Collections.singletonList(new QueryIndex(""id"", true, ""IDX_EMPLOYEE_ID"")))
                .setAliases(Stream.of(""id"", ""orgId"").collect(Collectors.toMap(f -> f, String::toUpperCase)))
            );

        ByteArrayOutputStream outBytes = new ByteArrayOutputStream();

        ObjectOutput out = new ObjectOutputStream(outBytes);

        out.writeObject(target);
        out.flush();

        ObjectInput in = new ObjectInputStream(new ByteArrayInputStream(outBytes.toByteArray()));

        Object desTarget = in.readObject();

        assertTrue(Comparers.equal(target, desTarget));
    }
"
"    @Test
    public void testSerialization() throws IOException, ClassNotFoundException {
        ClientConfiguration target = new ClientConfiguration()
            .setAddresses(""127.0.0.1:10800"", ""127.0.0.1:10801"")
            .setTimeout(123)
            .setBinaryConfiguration(new BinaryConfiguration()
                .setClassNames(Collections.singleton(""Person""))
            )
            .setSslMode(SslMode.REQUIRED)
            .setSslClientCertificateKeyStorePath(""client.jks"")
            .setSslClientCertificateKeyStoreType(""JKS"")
            .setSslClientCertificateKeyStorePassword(""123456"")
            .setSslTrustCertificateKeyStorePath(""trust.jks"")
            .setSslTrustCertificateKeyStoreType(""JKS"")
            .setSslTrustCertificateKeyStorePassword(""123456"")
            .setSslKeyAlgorithm(""SunX509"");

        ByteArrayOutputStream outBytes = new ByteArrayOutputStream();

        ObjectOutput out = new ObjectOutputStream(outBytes);

        out.writeObject(target);
        out.flush();

        ObjectInput in = new ObjectInputStream(new ByteArrayInputStream(outBytes.toByteArray()));

        Object desTarget = in.readObject();

        assertTrue(Comparers.equal(target, desTarget));
    }
"
"    @Test
    public void testFind() {
        BitSet[] matrix = provider.provide();

        int nodes = matrix.length;

        BitSet all = new BitSet(nodes);
        for (int i = 0; i < nodes; i++)
            all.set(i);

        FullyConnectedComponentSearcher searcher = new FullyConnectedComponentSearcher(matrix);

        BitSet res = searcher.findLargest(all);
        int size = res.cardinality();

        Assert.assertTrue(""Actual = "" + size + "", Expected = "" + minAcceptableRes,
            size >= minAcceptableRes);
    }
"
"    @Test
    public void testReplacementWithDelayCausesLockForRead() throws IgniteCheckedException {
        IgniteConfiguration cfg = getConfiguration(16 * MB);

        AtomicInteger totalEvicted = new AtomicInteger();

        ReplacedPageWriter pageWriter = (FullPageId fullPageId, ByteBuffer byteBuf, int tag) -> {
            log.info(""Evicting "" + fullPageId);

            assert getLockedPages(fullPageId).contains(fullPageId);

            assert !getSegment(fullPageId).writeLock().isHeldByCurrentThread();

            totalEvicted.incrementAndGet();
        };

        int pageSize = 4096;
        PageMemoryImpl memory = createPageMemory(cfg, pageWriter, pageSize);

        this.pageMemory = memory;

        long pagesTotal = cfg.getDataStorageConfiguration().getDefaultDataRegionConfiguration().getMaxSize() / pageSize;
        long markDirty = pagesTotal * 2 / 3;
        for (int i = 0; i < markDirty; i++) {
            long pageId = memory.allocatePage(1, 1, PageIdAllocator.FLAG_DATA);
            long ptr = memory.acquirePage(1, pageId);

            memory.releasePage(1, pageId, ptr);
        }

        GridMultiCollectionWrapper<FullPageId> ids = memory.beginCheckpoint();
        int cpPages = ids.size();
        log.info(""Started CP with ["" + cpPages + ""] pages in it, created ["" + markDirty + ""] pages"");

        for (int i = 0; i < cpPages; i++) {
            long pageId = memory.allocatePage(1, 1, PageIdAllocator.FLAG_DATA);
            long ptr = memory.acquirePage(1, pageId);
            memory.releasePage(1, pageId, ptr);
        }

        List<Collection<FullPageId>> stripes = getAllLockedPages();

        assert !stripes.isEmpty();

        for (Collection<FullPageId> pageIds : stripes) {
            assert pageIds.isEmpty();
        }

        assert totalEvicted.get() > 0;

        memory.stop(true);
    }
"
"    @Test
    public void testBackwardCompatibilityMode() throws IgniteCheckedException {
        IgniteConfiguration cfg = getConfiguration(16 * MB);

        AtomicInteger totalEvicted = new AtomicInteger();

        ReplacedPageWriter pageWriter = (FullPageId fullPageId, ByteBuffer byteBuf, int tag) -> {
            log.info(""Evicting "" + fullPageId);

            assert getSegment(fullPageId).writeLock().isHeldByCurrentThread();

            totalEvicted.incrementAndGet();
        };

        System.setProperty(IgniteSystemProperties.IGNITE_DELAYED_REPLACED_PAGE_WRITE, ""false"");
        int pageSize = 4096;
        PageMemoryImpl memory;

        try {
            memory = createPageMemory(cfg, pageWriter, pageSize);
        }
        finally {
            System.clearProperty(IgniteSystemProperties.IGNITE_DELAYED_REPLACED_PAGE_WRITE);
        }

        this.pageMemory = memory;

        long pagesTotal = cfg.getDataStorageConfiguration().getDefaultDataRegionConfiguration().getMaxSize() / pageSize;
        long markDirty = pagesTotal * 2 / 3;
        for (int i = 0; i < markDirty; i++) {
            long pageId = memory.allocatePage(1, 1, PageIdAllocator.FLAG_DATA);
            long ptr = memory.acquirePage(1, pageId);

            memory.releasePage(1, pageId, ptr);
        }

        GridMultiCollectionWrapper<FullPageId> ids = memory.beginCheckpoint();
        int cpPages = ids.size();
        log.info(""Started CP with ["" + cpPages + ""] pages in it, created ["" + markDirty + ""] pages"");

        for (int i = 0; i < cpPages; i++) {
            long pageId = memory.allocatePage(1, 1, PageIdAllocator.FLAG_DATA);
            long ptr = memory.acquirePage(1, pageId);
            memory.releasePage(1, pageId, ptr);
        }

        assert totalEvicted.get() > 0;

        memory.stop(true);
    }
"
"    @Test
    public void breakInCaseTooFast() {
        PagesWriteSpeedBasedThrottle throttle = new PagesWriteSpeedBasedThrottle(pageMemory2g, null, stateChecker, log);

        long time = throttle.getParkTime(0.67,
            (362584 + 67064) / 2,
            328787,
            1,
            60184,
            23103);

        assertTrue(time > 0);
    }
"
"    @Test
    public void noBreakIfNotFastWrite() {
        PagesWriteSpeedBasedThrottle throttle = new PagesWriteSpeedBasedThrottle(pageMemory2g, null, stateChecker, log);

        long time = throttle.getParkTime(0.47,
            ((362584 + 67064) / 2),
            328787,
            1,
            20103,
            23103);

        assertTrue(time == 0);
    }
"
"    @Test
    public void averageCalculation() throws InterruptedException {
        IntervalBasedMeasurement measurement = new IntervalBasedMeasurement(100, 1);

        for (int i = 0; i < 1000; i++)
            measurement.addMeasurementForAverageCalculation(100);

        assertEquals(100, measurement.getAverage());

        Thread.sleep(220);

        assertEquals(0, measurement.getAverage());

        assertEquals(0, measurement.getSpeedOpsPerSec(System.nanoTime()));
    }
"
"    @Test
    public void speedCalculation() throws InterruptedException {
        IntervalBasedMeasurement measurement = new IntervalBasedMeasurement(100, 1);

        for (int i = 0; i < 1000; i++)
            measurement.setCounter(i, System.nanoTime());

        long speed = measurement.getSpeedOpsPerSec(System.nanoTime());
        System.out.println(""speed measured "" + speed);
        assertTrue(speed > 1000);

        Thread.sleep(230);

        assertEquals(0, measurement.getSpeedOpsPerSec(System.nanoTime()));
    }
"
"    @Test
    public void speedWithDelayCalculation() throws InterruptedException {
        IntervalBasedMeasurement measurement = new IntervalBasedMeasurement(100, 1);

        int runs = 10;
        int nanosPark = 100;
        int multiplier = 100000;
        for (int i = 0; i < runs; i++) {
            measurement.setCounter(i * multiplier, System.nanoTime());

            LockSupport.parkNanos(nanosPark);
        }

        long speed = measurement.getSpeedOpsPerSec(System.nanoTime());

        assertTrue(speed > 0);
        long maxSpeed = (TimeUnit.SECONDS.toNanos(1) * multiplier * runs) / ((long)(runs * nanosPark));
        assertTrue(speed < maxSpeed);

        Thread.sleep(200);

        assertEquals(0, measurement.getSpeedOpsPerSec(System.nanoTime()));
    }
"
"    @Test
    public void beginOfCp() {
        PagesWriteSpeedBasedThrottle throttle = new PagesWriteSpeedBasedThrottle(pageMemory2g, null, stateChecker, log);

        assertTrue(throttle.getParkTime(0.01, 100,400000,
            1,
            20103,
            23103) == 0);

        //mark speed 22413 for mark all remaining as dirty
        long time = throttle.getParkTime(0.024, 100, 400000,
            1,
            24000,
            23103);
        assertTrue(time > 0);

        assertTrue(throttle.getParkTime(0.01,
            100,
            400000,
            1,
            22412,
            23103) == 0);
    }
"
"    @Test
    public void enforceThrottleAtTheEndOfCp() {
        PagesWriteSpeedBasedThrottle throttle = new PagesWriteSpeedBasedThrottle(pageMemory2g, null, stateChecker, log);

        long time1 = throttle.getParkTime(0.70, 300000, 400000,
            1, 20200, 23000);
        long time2 = throttle.getParkTime(0.71, 300000, 400000,
            1, 20200, 23000);

        assertTrue(time2 >= time1 * 2); // extra slowdown should be applied.

        long time3 = throttle.getParkTime(0.73, 300000, 400000,
            1, 20200, 23000);
        long time4 = throttle.getParkTime(0.74, 300000, 400000,
            1, 20200, 23000);

        assertTrue(time3 > time2);
        assertTrue(time4 > time3);
    }
"
"    @Test
    public void tooMuchPagesMarkedDirty() {
        PagesWriteSpeedBasedThrottle throttle = new PagesWriteSpeedBasedThrottle(pageMemory2g, null, stateChecker, log);

       // 363308	350004	348976	10604
        long time = throttle.getParkTime(0.75,
            ((350004 + 348976) / 2),
            350004-10604,
            4,
            279,
            23933);

        System.err.println(time);

        assertTrue(time == 0);
    }
"
"    @Test
    public void warningInCaseTooMuchThrottling() {
        AtomicInteger warnings = new AtomicInteger(0);
        IgniteLogger log = mock(IgniteLogger.class);

        doAnswer(invocation -> {
            Object[] args = invocation.getArguments();

            System.out.println(""log.info() called with arguments: "" + Arrays.toString(args));

            warnings.incrementAndGet();

            return null;
        }).when(log).info(anyString());

        AtomicInteger written = new AtomicInteger();
        CheckpointWriteProgressSupplier cpProgress = mock(CheckpointWriteProgressSupplier.class);
        when(cpProgress.writtenPagesCounter()).thenReturn(written);

        PagesWriteSpeedBasedThrottle throttle = new PagesWriteSpeedBasedThrottle(pageMemory2g, cpProgress, stateChecker, log) {
            @Override protected void doPark(long throttleParkTimeNs) {
                //do nothing
            }
        };
        throttle.onBeginCheckpoint();
        written.set(200); //emulating some pages written

        for (int i = 0; i < 100000; i++) {
            //emulating high load on marking
            throttle.onMarkDirty(false);

            if (throttle.throttleWeight() > PagesWriteSpeedBasedThrottle.WARN_THRESHOLD)
                break;
        }

        for (int i = 0; i < 1000; i++) {
            //emulating additional page writes to be sure log message is generated

            throttle.onMarkDirty(false);

            if(warnings.get()>0)
                break;
        }

        System.out.println(throttle.throttleWeight());

        assertTrue(warnings.get() > 0);
    }
"
"    @Test
    public void testShortSize() throws Exception {
        withMap(map -> {
            map.put(1, 1, 0, 0);
            map.put(2, 0, 1, 1);
            map.remove(1, 1);
        }, 2);
    }
"
"    @Test
    public void testSimplestPutGet() throws Exception {
        int cnt = 100;
        withMap(map -> {
                for (int i = 0; i < cnt; i++) {
                    int grpId = i + 1;
                    int val = grpId * grpId;

                    assertSizeChanged(""Unique put should be successful "" + grpId,
                        map, () -> map.put(grpId, 1, val, 1));
                    assertEquals(val, map.get(grpId, 1, 0, -1, -2));

                    assertSizeNotChanged(""Duplicate put for "" + grpId,
                        map, () -> map.put(grpId, 1, 1, 1));
                    assertEquals(1, map.get(grpId, 1, 0, -1, -2));
                }

                assertEquals(cnt, map.size());
            }
            , cnt);
    }
"
"    @Test(expected = IgniteOutOfMemoryException.class)
    public void testSimplestOverflow() throws Exception {
        withMap(map -> {
                for (int i = 0; i < 10; i++) {
                    int grpId = i + 1;
                    int val = grpId * grpId;
                    assertSizeChanged(""Unique put should be successful ["" + grpId + ""]"", map, () -> map.put(grpId, 1, val, 1));

                    assertEquals(val, map.get(grpId, 1, 0, -1, -2));

                    assertSizeNotChanged(""Duplicate put for "" + grpId, map, () -> map.put(grpId, 1, 1, 1));
                    assertEquals(1, map.get(grpId, 1, 0, -1, -2));
                }

                map.put(11, 1, 11, 1);
            }
            , 10);
    }
"
"    @Test
    public void testPutRemoveOnSamePlaces() throws Exception {
        withMap(map -> {
                doAddRemove(map);

                //fill with 1 space left;
                for (int i = 0; i < 99; i++) {
                    int grpId = i + 1;
                    int val = grpId * grpId;
                    assertSizeChanged(""Unique put should be successful "" + grpId, map,
                        () -> map.put(grpId, 1, val, 1));
                }

                doAddRemove(map);
            }
            , 100);
    }
"
"    @Test
    public void testCollisionOnRemove() {
        Map<FullPageId, Long> ctrl = new LinkedHashMap<>();
        int cap = 10;
        FullPageId baseId = new FullPageId(0, 1);

        withMap(map -> {
            for (int i = 0; i < cap; i++) {
                int grpId = i + 1;
                int pageId = findPageIdForCollision(grpId, baseId, cap);
                ctrl.put(new FullPageId(pageId, grpId), (long)grpId);
                map.put(grpId, pageId, (long)grpId, 1);
            }
            for (FullPageId next : ctrl.keySet()) {
                assertTrue(map.remove(next.groupId(), next.pageId()));
            }
        }, cap);
    }
"
"    @Test
    public void testRandomOpsPutRemove() {
        doPutRemoveTest(System.currentTimeMillis());
    }
"
"    @Test
    public void testPutAndCantGetOutdatedValue() throws Exception {
        withMap(map -> {
            //fill with 1 space left;
            for (int i = 0; i < 99; i++) {
                int ver = i;
                int grpId = ver + 1;
                int val = grpId * grpId;
                map.put(grpId, 1, val, ver);

                assertEquals(val, map.get(grpId, 1, ver, -1, -2));

                assertEquals(-2, map.get(grpId, 1, ver + 1, -1, -2));
            }
        }, 100);
    }
"
"    @Test
    public void testPutAndRefreshValue() throws Exception {
        withMap(map -> {
            //fill with 1 space left;
            for (int i = 0; i < 99; i++) {
                int ver = i;
                int grpId = ver + 1;
                int val = grpId * grpId;
                int pageId = 1;
                map.put(grpId, pageId, val, ver);

                map.refresh(grpId, pageId, ver + 1);

                assertEquals(val, map.get(grpId, pageId, ver + 1, -1, -2));

            }

            doAddRemove(map);
        }, 100);
    }
"
"    @Test
    public void testClearAtWithControlMap3() throws Exception {
        int cap = 100;

        doRemovalTests(cap, (grpId, pageId) -> {
            int hc = Integer.hashCode(grpId) + 31 * Long.hashCode(pageId);

            return hc % 3 == 0;
        });
    }
"
"    @Test
    public void testClearAtWithControlMap7() throws Exception {
        int cap = 100;

        doRemovalTests(cap, (grpId, pageId) -> {
            int hc = Integer.hashCode(grpId) + 31 * Long.hashCode(pageId);

            return hc % 7 == 0;
        });
    }
"
"    @Test
    public void testClearAllWithControlMap() throws Exception {
        int cap = 100;

        doRemovalTests(cap, (grpId, pageId) -> true);
    }
"
"    @Test
    public void testRandomOperations() throws Exception {
        int cnt = CACHE_ID_RANGE * PAGE_ID_RANGE;

        long mem = FullPageIdTable.requiredMemory(cnt);

        UnsafeMemoryProvider prov = new UnsafeMemoryProvider(log);

        prov.initialize(new long[] {mem});

        DirectMemoryRegion region = prov.nextRegion();

        try {
            long seed = U.currentTimeMillis();

            info(""Seed: "" + seed + ""L; //"");

            Random rnd = new Random(seed);

            LoadedPagesMap tbl = new FullPageIdTable(region.address(), region.size(), true);

            Map<FullPageId, Long> check = new HashMap<>();

            for (int i = 0; i < 10_000; i++) {
                int cacheId = rnd.nextInt(CACHE_ID_RANGE) + 1;
                int pageId = rnd.nextInt(PAGE_ID_RANGE);

                FullPageId fullId = new FullPageId(pageId, cacheId);

                boolean put = rnd.nextInt(3) != -1;

                if (put) {
                    long val = rnd.nextLong();

                    tbl.put(cacheId, pageId, val, 0);
                    check.put(fullId, val);
                }
                else {
                    tbl.remove(cacheId, pageId);
                    check.remove(fullId);
                }

                verifyLinear(tbl, check);

                if (i > 0 && i % 1000 == 0)
                    info(""Done: "" + i);
            }
        }
        finally {
            prov.shutdown(true);
        }
    }
"
"    @Test
    public void putRemoveScenario() throws Exception {
        long seed = U.currentTimeMillis();

        doPutRemoveTest(seed, false, 1_000_000);
    }
"
"    @Test
    public void putRemoveScenarioNewMap() throws Exception {
        long seed = U.currentTimeMillis();
        doPutRemoveTest(seed, true, 30_000_000);
    }
"
"    @Test
    public void testBuckets() {
        testBuckets(hist1, new int[] {0, 1, 2, 3, 4, 5}, new int[] {4, 3, 2, 1, 1, 1});
        testBuckets(hist2, new int[] {0, 1, 5, 6}, new int[] {6, 5, 1, 1});
    }
"
"    @Test
    public void testAdd() {
        double val = 100.0;
        hist1.addElement(val);
        Optional<Double> cntr = hist1.getValue(computeBucket(val));

        assertTrue(cntr.isPresent());
        assertEquals(1, cntr.get().intValue());
    }
"
"    @Test
    public void testAddHist() {
        ObjectHistogram<Double> res = hist1.plus(hist2);
        testBuckets(res, new int[] {0, 1, 2, 3, 4, 5, 6}, new int[] {10, 8, 2, 1, 1, 2, 1});
    }
"
"    @Test
    public void testDistributionFunction() {
        TreeMap<Integer, Double> distribution = hist1.computeDistributionFunction();

        int[] buckets = new int[distribution.size()];
        double[] sums = new double[distribution.size()];

        int ptr = 0;
        for(int bucket : distribution.keySet()) {
            sums[ptr] = distribution.get(bucket);
            buckets[ptr++] = bucket;
        }

        assertArrayEquals(new int[] {0, 1, 2, 3, 4, 5}, buckets);
        assertArrayEquals(new double[] {4., 7., 9., 10., 11., 12.}, sums, 0.01);
    }
"
"    @Test
    public void testOfSum() {
        IgniteFunction<Double, Integer> bucketMap = x -> (int) (Math.ceil(x * 100) % 100);
        IgniteFunction<Double, Double> cntrMap = x -> Math.pow(x, 2);

        ObjectHistogram<Double> forAllHistogram = new ObjectHistogram<>(bucketMap, cntrMap);
        Random rnd = new Random();
        List<ObjectHistogram<Double>> partitions = new ArrayList<>();
        int cntOfPartitions = rnd.nextInt(100);
        int sizeOfDataset = rnd.nextInt(10000);
        for(int i = 0; i < cntOfPartitions; i++)
            partitions.add(new ObjectHistogram<>(bucketMap, cntrMap));

        for(int i = 0; i < sizeOfDataset; i++) {
            double objVal = rnd.nextDouble();
            forAllHistogram.addElement(objVal);
            partitions.get(rnd.nextInt(partitions.size())).addElement(objVal);
        }

        Optional<ObjectHistogram<Double>> leftSum = partitions.stream().reduce(ObjectHistogram::plus);
        Optional<ObjectHistogram<Double>> rightSum = partitions.stream().reduce((x,y) -> y.plus(x));
        assertTrue(leftSum.isPresent());
        assertTrue(rightSum.isPresent());
        assertTrue(forAllHistogram.isEqualTo(leftSum.get()));
        assertTrue(forAllHistogram.isEqualTo(rightSum.get()));
        assertTrue(leftSum.get().isEqualTo(rightSum.get()));
    }
"
"    @Test
    public void basicTest() throws Exception {
        Map<Integer, DataPoint> dataPoints = new HashMap<Integer, DataPoint>() {{
            put(1, new DataPoint(42, 10000));
            put(2, new DataPoint(32, 64000));
            put(3, new DataPoint(53, 120000));
            put(4, new DataPoint(24, 70000));
        }};

        // Creates a local simple dataset containing features and providing standard dataset API.
        try (SimpleDataset<?> dataset = DatasetFactory.createSimpleDataset(
            dataPoints,
            2,
            (k, v) -> VectorUtils.of(v.getAge(), v.getSalary())
        )) {
            assertArrayEquals(""Mean values."", new double[] {37.75, 66000.0}, dataset.mean(), 0);

            assertArrayEquals(""Standard deviation values."",
                new double[] {10.871407452579449, 38961.519477556314}, dataset.std(), 0);

            double[][] covExp = new double[][] {
                new double[] {118.1875, 135500.0},
                new double[] {135500.0, 1.518E9}
            };
            double[][] cov = dataset.cov();
            int rowCov = 0;
            for (double[] row : cov)
                assertArrayEquals(""Covariance matrix row "" + rowCov,
                    covExp[rowCov++], row, 0);


            double[][] corrExp = new double[][] {
                new double[] {1.0000000000000002, 0.31990250167874007},
                new double[] {0.31990250167874007, 1.0}
            };
            double[][] corr = dataset.corr();
            int rowCorr = 0;
            for (double[] row : corr)
                assertArrayEquals(""Correlation matrix row "" + rowCorr,
                    corrExp[rowCorr++], row, 0);
        }
    }
"
"    @Test
    public void basicTest() throws Exception {
        Map<Integer, DataPoint> dataPoints = new HashMap<Integer, DataPoint>() {{
            put(5, new DataPoint(42, 10000));
            put(6, new DataPoint(32, 64000));
            put(7, new DataPoint(53, 120000));
            put(8, new DataPoint(24, 70000));
        }};

        double[][] actualFeatures = new double[2][];
        double[][] actualLabels = new double[2][];
        int[] actualRows = new int[2];

        // Creates a local simple dataset containing features and providing standard dataset API.
        try (SimpleLabeledDataset<?> dataset = DatasetFactory.createSimpleLabeledDataset(
            dataPoints,
            2,
            (k, v) -> VectorUtils.of(v.getAge(), v.getSalary()),
            (k, v) -> new double[] {k, v.getAge(), v.getSalary()}
        )) {
            assertNull(dataset.compute((data, partIdx) -> {
                actualFeatures[partIdx] = data.getFeatures();
                actualLabels[partIdx] = data.getLabels();
                actualRows[partIdx] = data.getRows();
                return null;
            }, (k, v) -> null));
        }

        double[][] expFeatures = new double[][] {
            new double[] {42.0, 32.0, 10000.0, 64000.0},
            new double[] {53.0, 24.0, 120000.0, 70000.0}
        };
        int rowFeat = 0;
        for (double[] row : actualFeatures)
            assertArrayEquals(""Features partition index "" + rowFeat,
                expFeatures[rowFeat++], row, 0);

        double[][] expLabels = new double[][] {
            new double[] {5.0, 6.0, 42.0, 32.0, 10000.0, 64000.0},
            new double[] {7.0, 8.0, 53.0, 24.0, 120000.0, 70000.0}
        };
        int rowLbl = 0;
        for (double[] row : actualLabels)
            assertArrayEquals(""Labels partition index "" + rowLbl,
                expLabels[rowLbl++], row, 0);

        assertArrayEquals(""Rows per partitions"", new int[] {2, 2}, actualRows);
    }
"
"    @Test
    public void testComputeWithCtx() {
        doReturn(42).when(dataset).computeWithCtx(any(IgniteTriFunction.class), any(), any());

        Integer res = (Integer) wrapper.computeWithCtx(mock(IgniteTriFunction.class), mock(IgniteBinaryOperator.class),
            null);

        assertEquals(42, res.intValue());

        verify(dataset, times(1)).computeWithCtx(any(IgniteTriFunction.class), any(), any());
    }
"
"    @Test
    public void testComputeWithCtx2() {
        doReturn(42).when(dataset).computeWithCtx(any(IgniteTriFunction.class), any(), any());

        Integer res = (Integer) wrapper.computeWithCtx(mock(IgniteBiFunction.class), mock(IgniteBinaryOperator.class),
            null);

        assertEquals(42, res.intValue());

        verify(dataset, times(1)).computeWithCtx(any(IgniteTriFunction.class), any(), any());
    }
"
"    @Test
    public void testComputeWithCtx3() {
        wrapper.computeWithCtx((ctx, data) -> {
            assertNotNull(ctx);
            assertNotNull(data);
        });

        verify(dataset, times(1)).computeWithCtx(any(IgniteTriFunction.class),
            any(IgniteBinaryOperator.class), any());
    }
"
"    @Test
    public void testCompute() {
        doReturn(42).when(dataset).compute(any(IgniteBiFunction.class), any(), any());

        Integer res = (Integer) wrapper.compute(mock(IgniteBiFunction.class), mock(IgniteBinaryOperator.class),
            null);

        assertEquals(42, res.intValue());

        verify(dataset, times(1)).compute(any(IgniteBiFunction.class), any(), any());
    }
"
"    @Test
    public void testCompute2() {
        doReturn(42).when(dataset).compute(any(IgniteBiFunction.class), any(IgniteBinaryOperator.class), any());

        Integer res = (Integer) wrapper.compute(mock(IgniteFunction.class), mock(IgniteBinaryOperator.class),
            null);

        assertEquals(42, res.intValue());

        verify(dataset, times(1)).compute(any(IgniteBiFunction.class), any(IgniteBinaryOperator.class), any());
    }
"
"    @Test
    public void testClose() throws Exception {
        wrapper.close();

        verify(dataset, times(1)).close();
    }
"
"    @Test
    public void testBuild() {
        Map<Integer, Integer> data = new HashMap<>();
        for (int i = 0; i < 100; i++)
            data.put(i, i);

        LocalDatasetBuilder<Integer, Integer> builder = new LocalDatasetBuilder<>(data, 10);

        LocalDataset<Serializable, TestPartitionData> dataset = buildDataset(builder);

        assertEquals(10, dataset.getCtx().size());
        assertEquals(10, dataset.getData().size());

        AtomicLong cnt = new AtomicLong();

        dataset.compute((partData, partIdx) -> {
           cnt.incrementAndGet();

           int[] arr = partData.data;

           assertEquals(10, arr.length);

           for (int i = 0; i < 10; i++)
               assertEquals(partIdx * 10 + i, arr[i]);
        });

        assertEquals(10, cnt.intValue());
    }
"
"    @Test
    public void testBuildWithPredicate() {
        Map<Integer, Integer> data = new HashMap<>();
        for (int i = 0; i < 100; i++)
            data.put(i, i);

        LocalDatasetBuilder<Integer, Integer> builder = new LocalDatasetBuilder<>(data, (k, v) -> k % 2 == 0,10);

        LocalDataset<Serializable, TestPartitionData> dataset = buildDataset(builder);

        AtomicLong cnt = new AtomicLong();

        dataset.compute((partData, partIdx) -> {
            cnt.incrementAndGet();

            int[] arr = partData.data;

            assertEquals(5, arr.length);

            for (int i = 0; i < 5; i++)
                assertEquals((partIdx * 5 + i) * 2, arr[i]);
        });

        assertEquals(10, cnt.intValue());
    }
"
"    @Test(expected = ConcurrentModificationException.class)
    public void testNextWhenIteratorHasLessElementsThanExpected() {
        List<Integer> list = Arrays.asList(1, 2, 3);

        Iterator<Integer> iter = new IteratorWithConcurrentModificationChecker<>(list.iterator(), 4, ""Exception"");

        assertEquals(Integer.valueOf(1), iter.next());
        assertEquals(Integer.valueOf(2), iter.next());
        assertEquals(Integer.valueOf(3), iter.next());

        iter.next(); // Should throw an exception.
    }
"
"    @Test(expected = ConcurrentModificationException.class)
    public void testNextWhenIteratorHasMoreElementsThanExpected() {
        List<Integer> list = Arrays.asList(1, 2, 3);

        Iterator<Integer> iter = new IteratorWithConcurrentModificationChecker<>(list.iterator(), 2, ""Exception"");

        assertEquals(Integer.valueOf(1), iter.next());
        assertEquals(Integer.valueOf(2), iter.next());

        iter.next(); // Should throw an exception.
    }
"
"    @Test(expected = ConcurrentModificationException.class)
    public void testHasNextWhenIteratorHasLessElementsThanExpected() {
        List<Integer> list = Arrays.asList(1, 2, 3);

        Iterator<Integer> iter = new IteratorWithConcurrentModificationChecker<>(list.iterator(), 4, ""Exception"");

        assertTrue(iter.hasNext());
        iter.next();
        assertTrue(iter.hasNext());
        iter.next();
        assertTrue(iter.hasNext());
        iter.next();

        iter.hasNext(); // Should throw an exception.
    }
"
"    @Test(expected = ConcurrentModificationException.class)
    public void testHasNextWhenIteratorHasMoreElementsThanExpected() {
        List<Integer> list = Arrays.asList(1, 2, 3);

        Iterator<Integer> iter = new IteratorWithConcurrentModificationChecker<>(list.iterator(), 2, ""Exception"");

        assertTrue(iter.hasNext());
        iter.next();
        assertTrue(iter.hasNext());
        iter.next();

        iter.hasNext(); // Should throw an exception.
    }
"
"    @Test
    public void testReset() {
        wrapper.reset();

        verify(affinityFunction, times(1)).reset();
    }
"
"    @Test
    public void testPartitions() {
        doReturn(42).when(affinityFunction).partitions();

        int partitions = wrapper.partitions();

        assertEquals(42, partitions);
        verify(affinityFunction, times(1)).partitions();
    }
"
"    @Test
    public void testPartition() {
        doReturn(0).when(affinityFunction).partition(eq(42));

        int part = wrapper.partition(42);

        assertEquals(42, part);
        verify(affinityFunction, times(0)).partition(any());
    }
"
"    @Test
    public void testAssignPartitions() {
        List<List<ClusterNode>> nodes = Collections.singletonList(Collections.singletonList(mock(ClusterNode.class)));

        doReturn(nodes).when(affinityFunction).assignPartitions(any());

        List<List<ClusterNode>> resNodes = wrapper.assignPartitions(mock(AffinityFunctionContext.class));

        assertEquals(nodes, resNodes);
        verify(affinityFunction, times(1)).assignPartitions(any());
    }
"
"    @Test
    public void testRemoveNode() {
        UUID nodeId = UUID.randomUUID();

        wrapper.removeNode(nodeId);

        verify(affinityFunction, times(1)).removeNode(eq(nodeId));
    }
"
"    @Test
    public void testComputeDataIfAbsent() {
        AtomicLong cnt = new AtomicLong();

        for (int i = 0; i < 10; i++) {
            Integer res = (Integer) dataStorage.computeDataIfAbsent(0, () -> {
                cnt.incrementAndGet();

                return 42;
            });

            assertEquals(42, res.intValue());
        }

        assertEquals(1, cnt.intValue());
    }
"
"    @Test
    public void testPredict() {
        Vector weights = new DenseVector(new double[] {2.0, 3.0});

        verifyPredict(getMdl(new LogisticRegressionModel(weights, 1.0).withRawLabels(true)));
    }
"
"    @Test
    public void testTrainWithTheLinearlySeparableCase() {
        Map<Integer, Double[]> cacheMock = new HashMap<>();

        for (int i = 0; i < twoLinearlySeparableClasses.length; i++) {
            double[] row = twoLinearlySeparableClasses[i];
            Double[] convertedRow = new Double[row.length];
            for (int j = 0; j < row.length; j++)
                convertedRow[j] = row[j];
            cacheMock.put(i, convertedRow);
        }

        LogisticRegressionSGDTrainer<?> trainer = new LogisticRegressionSGDTrainer<>()
            .withUpdatesStgy(new UpdatesStrategy<>(new SimpleGDUpdateCalculator(0.2),
                SimpleGDParameterUpdate::sumLocal, SimpleGDParameterUpdate::avg))
            .withMaxIterations(100000)
            .withLocIterations(100)
            .withBatchSize(10)
            .withSeed(123L);

        PipelineMdl<Integer, Double[]> mdl = new Pipeline<Integer, Double[], Vector>()
            .addFeatureExtractor((k, v) -> VectorUtils.of(Arrays.copyOfRange(v, 1, v.length)))
            .addLabelExtractor((k, v) -> v[0])
            .addPreprocessor(new MinMaxScalerTrainer<Integer, Object[]>())
            .addPreprocessor(new NormalizationTrainer<Integer, Object[]>()
                .withP(1))
            .addTrainer(trainer)
            .fit(
                cacheMock,
                parts
            );

        TestUtils.assertEquals(0, mdl.apply(VectorUtils.of(100, 10)), PRECISION);
        TestUtils.assertEquals(1, mdl.apply(VectorUtils.of(10, 100)), PRECISION);
    }
"
"    @Test(expected = IllegalStateException.class)
    public void testTrainWithMissedFinalStage() {
        Map<Integer, Double[]> cacheMock = new HashMap<>();

        for (int i = 0; i < twoLinearlySeparableClasses.length; i++) {
            double[] row = twoLinearlySeparableClasses[i];
            Double[] convertedRow = new Double[row.length];
            for (int j = 0; j < row.length; j++)
                convertedRow[j] = row[j];
            cacheMock.put(i, convertedRow);
        }

        PipelineMdl<Integer, Double[]> mdl = new Pipeline<Integer, Double[], Vector>()
            .addFeatureExtractor((k, v) -> VectorUtils.of(Arrays.copyOfRange(v, 1, v.length)))
            .addLabelExtractor((k, v) -> v[0])
            .addPreprocessor(new MinMaxScalerTrainer<Integer, Object[]>())
            .addPreprocessor(new NormalizationTrainer<Integer, Object[]>()
                .withP(1))
            .fit(
                cacheMock,
                parts
            );

        TestUtils.assertEquals(0, mdl.apply(VectorUtils.of(100, 10)), PRECISION);
        TestUtils.assertEquals(1, mdl.apply(VectorUtils.of(10, 100)), PRECISION);
    }
"
"  @Test
  public void testStartShouldStartTheMetricsReportersAndServer() throws Exception {
    NetworkConnector connector = Mockito.mock(NetworkConnector.class);
    int testServerPort = 100;
    Mockito.doReturn(testServerPort).when(connector).getPort();
    Mockito.when(server.getConnectors()).thenReturn(new NetworkConnector[]{connector});
    Mockito.doNothing().when(server).start();
    samzaRestService.start();
    Mockito.verify(metricsReporter).start();
    Mockito.verify(metricsReporter).register(""SamzaRest"", metricsRegistry);
    Mockito.verify(server).start();
  }
"
"  @Test
  public void testStopShouldStopTheMetricsReportersAndStopTheServer() throws Exception {
    samzaRestService.stop();
    Mockito.verify(metricsReporter).stop();
    Mockito.verify(server).stop();
  }
"
"  @Test
  public void testGetJobStatuses() throws IOException, InterruptedException {
    doReturn(APPS_RESPONSE.getBytes()).when(provider).httpGet(anyString());

    List<Job> jobs = Lists.newArrayList(
        new Job(""job1"", ""1""),  // Job with multiple applications, 1 RUNNING
        new Job(""job2"", ""1""),  // Job with 1 KILLED application
        new Job(""job3"", ""1""),  // Job with 1 RUNNING application
        new Job(""job4"", ""1"")); // Job not found in YARN
    provider.getJobStatuses(jobs);

    Collections.sort(jobs, (o1, o2) -> o1.getJobName().compareTo(o2.getJobName()));

    assertEquals(4, jobs.size());
    verifyJobStatus(jobs.get(0), ""job1"", JobStatus.STARTED, ""RUNNING"");
    verifyJobStatus(jobs.get(1), ""job2"", JobStatus.STOPPED, ""KILLED"");
    verifyJobStatus(jobs.get(2), ""job3"", JobStatus.STARTED, ""RUNNING"");
    verifyJobStatus(jobs.get(3), ""job4"", JobStatus.UNKNOWN, null);
  }
"
"  @Test
   public void testGetJobs()
      throws IOException {

    Response resp = target(""v1/jobs"").request().get();
    assertEquals(200, resp.getStatus());
    final Job[] jobs = objectMapper.readValue(resp.readEntity(String.class), Job[].class);
    assertEquals(4, jobs.length);

    assertEquals(MockJobProxy.JOB_INSTANCE_1_NAME, jobs[0].getJobName());
    assertEquals(MockJobProxy.JOB_INSTANCE_1_ID, jobs[0].getJobId());
    assertStatusNotDefault(jobs[0]);
    assertEquals(MockJobProxy.JOB_INSTANCE_2_NAME, jobs[1].getJobName());
    assertEquals(MockJobProxy.JOB_INSTANCE_2_ID, jobs[1].getJobId());
    assertStatusNotDefault(jobs[1]);
    assertEquals(MockJobProxy.JOB_INSTANCE_3_NAME, jobs[2].getJobName());
    assertEquals(MockJobProxy.JOB_INSTANCE_3_ID, jobs[2].getJobId());
    assertStatusNotDefault(jobs[2]);
    assertEquals(MockJobProxy.JOB_INSTANCE_4_NAME, jobs[3].getJobName());
    assertEquals(MockJobProxy.JOB_INSTANCE_4_ID, jobs[3].getJobId());
    assertStatusNotDefault(jobs[3]);
    resp.close();
  }
"
"  @Test
   public void testPostJobs()
      throws IOException {
    Response resp = target(""v1/jobs"").request().post(Entity.text(""""));
    assertEquals(405, resp.getStatus());
    resp.close();
  }
"
"  @Test
  public void testPutJobs()
      throws IOException {
    Response resp = target(""v1/jobs"").request().put(Entity.text(""""));
    assertEquals(405, resp.getStatus());
    resp.close();
  }
"
"  @Test
  public void testGetJob()
      throws IOException {
    Response resp = target(String.format(""v1/jobs/%s/%s"", MockJobProxy.JOB_INSTANCE_2_NAME, MockJobProxy.JOB_INSTANCE_2_ID)).request().get();
    assertEquals(200, resp.getStatus());
    final Job job2 = objectMapper.readValue(resp.readEntity(String.class), Job.class);

    assertEquals(MockJobProxy.JOB_INSTANCE_2_NAME, job2.getJobName());
    assertEquals(MockJobProxy.JOB_INSTANCE_2_ID, job2.getJobId());
    assertStatusNotDefault(job2);
    resp.close();
  }
"
"  @Test
  public void testPostJob()
      throws IOException {
    Response resp = target(String.format(""v1/jobs/%s/%s"", MockJobProxy.JOB_INSTANCE_2_NAME, MockJobProxy.JOB_INSTANCE_2_ID)).request().post(
        Entity.text(""""));
    assertEquals(405, resp.getStatus());
    resp.close();
  }
"
"  @Test
  public void testGetJobNameNotFound()
      throws IOException {
    Response resp = target(String.format(""v1/jobs/%s/%s"", ""BadJobName"", MockJobProxy.JOB_INSTANCE_2_ID)).request().get();
    assertEquals(404, resp.getStatus());

    final Map<String, String> errorMessage = objectMapper.readValue(resp.readEntity(String.class), new TypeReference<Map<String, String>>() { });
    assertTrue(errorMessage.get(""message""), errorMessage.get(""message"").contains(""does not exist""));
    resp.close();
  }
"
"  @Test
  public void testGetJobIdNotFound()
      throws IOException {
    Response resp = target(String.format(""v1/jobs/%s/%s"", MockJobProxy.JOB_INSTANCE_2_NAME, ""BadJobId"")).request().get();
    assertEquals(404, resp.getStatus());

    final Map<String, String> errorMessage = objectMapper.readValue(resp.readEntity(String.class), new TypeReference<Map<String, String>>() { });
    assertTrue(errorMessage.get(""message""), errorMessage.get(""message"").contains(""does not exist""));
    resp.close();
  }
"
"  @Test
  public void testGetJobNameWithoutId()
      throws IOException {
    Response resp = target(String.format(""v1/jobs/%s"", MockJobProxy.JOB_INSTANCE_2_NAME)).request().get();
    assertEquals(404, resp.getStatus());
    resp.close();
  }
"
"  @Test
  public void testStartJob()
      throws IOException {
    Response resp = target(String.format(""v1/jobs/%s/%s"", MockJobProxy.JOB_INSTANCE_2_NAME, MockJobProxy.JOB_INSTANCE_2_ID))
        .queryParam(""status"", ""started"").request().put(Entity.form(new Form()));
    assertEquals(202, resp.getStatus());

    final Job job2 = objectMapper.readValue(resp.readEntity(String.class), Job.class);
    assertEquals(MockJobProxy.JOB_INSTANCE_2_NAME, job2.getJobName());
    assertEquals(MockJobProxy.JOB_INSTANCE_2_ID, job2.getJobId());
    assertStatusNotDefault(job2);
    resp.close();
  }
"
"  @Test
  public void testStopJob()
      throws IOException {
    Response resp = target(String.format(""v1/jobs/%s/%s"", MockJobProxy.JOB_INSTANCE_2_NAME, MockJobProxy.JOB_INSTANCE_2_ID))
        .queryParam(""status"", ""stopped"").request().put(Entity.form(new Form()));
    assertEquals(202, resp.getStatus());

    final Job job2 = objectMapper.readValue(resp.readEntity(String.class), Job.class);
    assertEquals(MockJobProxy.JOB_INSTANCE_2_NAME, job2.getJobName());
    assertEquals(MockJobProxy.JOB_INSTANCE_2_ID, job2.getJobId());
    assertStatusNotDefault(job2);
    resp.close();
  }
"
"  @Test
  public void testPutBadJobStatus()
      throws IOException {
    Response resp = target(String.format(""v1/jobs/%s/%s"", MockJobProxy.JOB_INSTANCE_2_NAME, MockJobProxy.JOB_INSTANCE_2_ID))
        .queryParam(""status"", ""BADSTATUS"").request().put(Entity.form(new Form()));
    assertEquals(400, resp.getStatus());

    final Map<String, String> errorMessage = objectMapper.readValue(resp.readEntity(String.class), new TypeReference<Map<String, String>>() { });
    assertTrue(errorMessage.get(""message"").contains(""BADSTATUS""));
    resp.close();
  }
"
"  @Test
  public void testPutMissingStatus()
      throws IOException {
    Response resp = target(String.format(""v1/jobs/%s/%s"", MockJobProxy.JOB_INSTANCE_2_NAME, MockJobProxy.JOB_INSTANCE_2_ID)).request()
        .put(Entity.form(new Form()));
    assertEquals(400, resp.getStatus());

    final Map<String, String> errorMessage = objectMapper.readValue(resp.readEntity(String.class), new TypeReference<Map<String, String>>() { });
    assertTrue(errorMessage.get(""message"").contains(""status""));
    resp.close();
  }
"
"  @Test
  public void testGetTasks() throws IOException {
    String requestUrl = String.format(""v1/jobs/%s/%s/tasks"", ""testJobName"", ""testJobId"");
    Response response = target(requestUrl).request().get();
    assertEquals(200, response.getStatus());
    Task[] tasks = objectMapper.readValue(response.readEntity(String.class), Task[].class);
    assertEquals(2, tasks.length);

    assertEquals(MockTaskProxy.TASK_1_PREFERRED_HOST, tasks[0].getPreferredHost());
    assertEquals(MockTaskProxy.TASK_1_CONTAINER_ID, tasks[0].getContainerId());
    assertEquals(MockTaskProxy.TASK_1_NAME, tasks[0].getTaskName());
    assertEquals(MockTaskProxy.PARTITIONS, tasks[0].getPartitions());

    assertEquals(MockTaskProxy.TASK_2_PREFERRED_HOST, tasks[1].getPreferredHost());
    assertEquals(MockTaskProxy.TASK_2_CONTAINER_ID, tasks[1].getContainerId());
    assertEquals(MockTaskProxy.TASK_2_NAME, tasks[1].getTaskName());
    assertEquals(MockTaskProxy.PARTITIONS, tasks[1].getPartitions());
  }
"
"  @Test
  public void testGetTasksWithInvalidJobName() throws IOException {
    String requestUrl = String.format(""v1/jobs/%s/%s/tasks"", ""BadJobName"", MockJobProxy.JOB_INSTANCE_4_ID);
    Response resp = target(requestUrl).request().get();
    assertEquals(400, resp.getStatus());
    final Map<String, String> errorMessage = objectMapper.readValue(resp.readEntity(String.class), new TypeReference<Map<String, String>>() { });
    assertTrue(errorMessage.get(""message""), errorMessage.get(""message"").contains(""Invalid arguments for getTasks. ""));
    resp.close();
  }
"
"  @Test
  public void testGetTasksWithInvalidJobId() throws IOException {
    String requestUrl = String.format(""v1/jobs/%s/%s/tasks"", MockJobProxy.JOB_INSTANCE_1_NAME, ""BadJobId"");
    Response resp = target(requestUrl).request().get();
    assertEquals(400, resp.getStatus());
    final Map<String, String> errorMessage = objectMapper.readValue(resp.readEntity(String.class), new TypeReference<Map<String, String>>() { });
    assertTrue(errorMessage.get(""message""), errorMessage.get(""message"").contains(""Invalid arguments for getTasks. ""));
    resp.close();
  }
"
"  @Test
  public void shouldDeleteLocalTaskStoreWhenItHasNoOffsetFile() throws Exception {
    localStoreMonitor.monitor();
    assertTrue(""Task store directory should not exist."", !taskStoreDir.exists());
    assertEquals(taskStoreSize, localStoreMonitorMetrics.diskSpaceFreedInBytes.getCount());
    assertEquals(1, localStoreMonitorMetrics.noOfDeletedTaskPartitionStores.getCount());
  }
"
"  @Test
  public void shouldDeleteLocalStoreWhenLastModifiedTimeOfOffsetFileIsGreaterThanOffsetTTL() throws Exception {
    File offsetFile = createOffsetFile(taskStoreDir);
    offsetFile.setLastModified(0);
    localStoreMonitor.monitor();
    assertTrue(""Offset file should not exist."", !offsetFile.exists());
    assertEquals(0, localStoreMonitorMetrics.diskSpaceFreedInBytes.getCount());
  }
"
"  @Test
  public void shouldDeleteInActiveLocalStoresOfTheJob() throws Exception {
    File inActiveStoreDir = new File(jobDir, ""inActiveStore"");
    FileUtils.forceMkdir(inActiveStoreDir);
    File inActiveTaskDir = new File(inActiveStoreDir, ""test-task"");
    FileUtils.forceMkdir(inActiveTaskDir);
    long inActiveTaskDirSize = inActiveTaskDir.getTotalSpace();
    localStoreMonitor.monitor();
    assertTrue(""Inactive task store directory should not exist."", !inActiveTaskDir.exists());
    assertEquals(taskStoreSize + inActiveTaskDirSize, localStoreMonitorMetrics.diskSpaceFreedInBytes.getCount());
    assertEquals(2, localStoreMonitorMetrics.noOfDeletedTaskPartitionStores.getCount());
    FileUtils.deleteDirectory(inActiveStoreDir);
  }
"
"  @Test
  public void shouldDoNothingWhenLastModifiedTimeOfOffsetFileIsLessThanOffsetTTL() throws Exception {
    File offsetFile = createOffsetFile(taskStoreDir);
    localStoreMonitor.monitor();
    assertTrue(""Offset file should exist."", offsetFile.exists());
    assertEquals(0, localStoreMonitorMetrics.diskSpaceFreedInBytes.getCount());
  }
"
"  @Test
  public void shouldDoNothingWhenTheJobIsRunning() throws Exception {
    Mockito.when(jobsClientMock.getJobStatus(Mockito.any())).thenReturn(JobStatus.STARTED);
    File offsetFile = createOffsetFile(taskStoreDir);
    localStoreMonitor.monitor();
    assertTrue(""Offset file should exist."", offsetFile.exists());
    assertEquals(0, localStoreMonitorMetrics.diskSpaceFreedInBytes.getCount());
  }
"
"  @Test
  public void shouldDeleteTaskStoreWhenTaskPreferredStoreIsNotLocalHost() throws Exception {
    Task task = new Task(""notLocalHost"", ""test-task"", ""0"", new ArrayList<>(), ImmutableList.of(""test-store""));
    Mockito.when(jobsClientMock.getTasks(Mockito.any())).thenReturn(ImmutableList.of(task));
    localStoreMonitor.monitor();
    assertTrue(""Task store directory should not exist."", !taskStoreDir.exists());
    assertEquals(taskStoreSize, localStoreMonitorMetrics.diskSpaceFreedInBytes.getCount());
    assertEquals(1, localStoreMonitorMetrics.noOfDeletedTaskPartitionStores.getCount());
  }
"
"  @Test
  public void shouldContinueLocalStoreCleanUpAfterFailureToCleanUpStoreOfAJob() throws Exception {
    File testFailingJobDir = new File(localStoreDir, ""test-jobName-jobId-1"");

    File testFailingTaskStoreDir = new File(new File(testFailingJobDir, ""test-store""), ""test-task"");

    FileUtils.forceMkdir(testFailingTaskStoreDir);

    // For job: test-jobName-jobId-1, throw up in getTasks call and
    // expect the cleanup to succeed for other job: test-jobName-jobId.
    Mockito.doThrow(new RuntimeException(""Dummy exception message.""))
        .when(jobsClientMock)
        .getTasks(new JobInstance(""test-jobName"", ""jobId-1""));

    Task task = new Task(""notLocalHost"", ""test-task"", ""0"", new ArrayList<>(), ImmutableList.of(""test-store""));

    Mockito.when(jobsClientMock.getTasks(new JobInstance(""test-jobName"", ""jobId""))).thenReturn(ImmutableList.of(task));

    Map<String, String> configMap = new HashMap<>(config);
    configMap.put(LocalStoreMonitorConfig.CONFIG_IGNORE_FAILURES, ""true"");

    LocalStoreMonitor localStoreMonitor =
        new LocalStoreMonitor(new LocalStoreMonitorConfig(new MapConfig(configMap)), localStoreMonitorMetrics,
            jobsClientMock);

    localStoreMonitor.monitor();

    // Non failing job directory should be cleaned up.
    assertTrue(""Task store directory should not exist."", !taskStoreDir.exists());
    FileUtils.deleteDirectory(testFailingJobDir);
  }
"
"  @Test
  public void testMonitorsShouldBeInstantiatedProperly() {
    // Test that a monitor should be instantiated properly by invoking
    // the appropriate factory method.
    Map<String, String> configMap = ImmutableMap.of(CONFIG_MONITOR_FACTORY_CLASS,
                                                    DummyMonitorFactory.class.getCanonicalName());
    Monitor monitor = null;
    try {
      monitor = MonitorLoader.instantiateMonitor(""testMonitor"", new MonitorConfig(new MapConfig(configMap)),
          METRICS_REGISTRY);
    } catch (InstantiationException e) {
      fail();
    }
    assertNotNull(monitor);
    // Object should implement the monitor().
    try {
      monitor.monitor();
    } catch (Exception e) {
      fail();
    }
  }
"
"  @Test
  public void testShouldGroupRelevantMonitorConfigTogether() {
    // Test that Monitor Loader groups relevant config together.
    Map<String, String> firstMonitorConfig = ImmutableMap.of(""monitor.monitor1.factory.class"",
                                                             ""org.apache.samza.monitor.DummyMonitor"",
                                                             ""monitor.monitor1.scheduling.interval.ms"",
                                                             ""100"");
    Map<String, String> secondMonitorConfig = ImmutableMap.of(""monitor.monitor2.factory.class"",
                                                              ""org.apache.samza.monitor.DummyMonitor"",
                                                              ""monitor.monitor2.scheduling.interval.ms"",
                                                              ""200"");
    MapConfig mapConfig = new MapConfig(ImmutableList.of(firstMonitorConfig, secondMonitorConfig));
    MonitorConfig expectedFirstConfig = new MonitorConfig(new MapConfig(firstMonitorConfig).subset(""monitor.monitor1.""));
    MonitorConfig expectedSecondConfig = new MonitorConfig(new MapConfig(secondMonitorConfig).subset(""monitor.monitor2.""));
    Map<String, MonitorConfig> expected = ImmutableMap.of(""monitor1"", expectedFirstConfig, ""monitor2"", expectedSecondConfig);
    assertEquals(expected, MonitorConfig.getMonitorConfigs(mapConfig));
  }
"
"  @Test
  public void testMonitorExceptionIsolation() {
    // Test that an exception from a monitor doesn't bubble up out of the scheduler.
    Map<String, String> configMap =
        ImmutableMap.of(String.format(""monitor.name.%s"", CONFIG_MONITOR_FACTORY_CLASS),
                        ExceptionThrowingMonitorFactory.class.getCanonicalName());
    SamzaRestConfig config = new SamzaRestConfig(new MapConfig(configMap));
    SamzaMonitorService monitorService = new SamzaMonitorService(config,
                                                                 METRICS_REGISTRY);

    // This will throw if the exception isn't caught within the provider.
    monitorService.start();
    monitorService.stop();
  }
"
"  @Test
  public void testShouldNotFailWhenTheMonitorFactoryClassIsNotDefined()
      throws Exception {
    // Test that when MonitorFactoryClass is not defined in the config, monitor service
    // should not fail.
    Map<String, String> configMap = ImmutableMap.of(""monitor.monitor1.config.key1"", ""configValue1"",
                                                    ""monitor.monitor1.config.key2"", ""configValue2"",
                                                    String.format(""monitor.MOCK_MONITOR.%s"", CONFIG_MONITOR_FACTORY_CLASS),
                                                    MockMonitorFactory.class.getCanonicalName());

    SamzaRestConfig config = new SamzaRestConfig(new MapConfig(configMap));

    class SamzaMonitorServiceTest extends SamzaMonitorService {
      MetricsRegistry metricsRegistry;
      public SamzaMonitorServiceTest(SamzaRestConfig config, MetricsRegistry metricsRegistry) {
        super(config, metricsRegistry);
        this.metricsRegistry = metricsRegistry;
      }

      @Override
      public void createSchedulerAndScheduleMonitor(String monitorName, MonitorConfig monitorConfig, long schedulingIntervalInMs) {
        try {
          // immediately run monitor, without scheduling
          instantiateMonitor(monitorName, monitorConfig, metricsRegistry).monitor();
        } catch (Exception e) {
          fail();
        }
      }
"
"  @Test(expected = SamzaException.class)
  public void testShouldFailWhenTheMonitorFactoryClassIsInvalid() {
    // Test that when MonitorFactoryClass is defined in the config and is invalid,
    // monitor service should fail. Should throw back SamzaException.
    Map<String, String> configMap = ImmutableMap.of(String.format(""monitor.name.%s"", CONFIG_MONITOR_FACTORY_CLASS),
                                                    ""RandomClassName"");
    SamzaRestConfig config = new SamzaRestConfig(new MapConfig(configMap));
    SamzaMonitorService monitorService = new SamzaMonitorService(config,
                                                                 METRICS_REGISTRY);
    monitorService.start();
  }
"
"  @Test
  public void testScheduledExecutorSchedulingProvider() {
    // Test that the monitor is scheduled by the ScheduledExecutorSchedulingProvider
    ScheduledExecutorService executorService = Executors.newScheduledThreadPool(1);

    // notifyingMonitor.monitor() should be called repeatedly.
    final CountDownLatch wasCalledLatch = new CountDownLatch(3);

    final Monitor notifyingMonitor = new Monitor() {
      @Override
      public void monitor() {
        wasCalledLatch.countDown();
      }
"
"  @Test
  public void testKafkaSystemConsumerMetrics() {
    String systemName = ""system"";
    TopicPartition tp1 = new TopicPartition(""topic1"", 1);
    TopicPartition tp2 = new TopicPartition(""topic2"", 2);
    String clientName = ""clientName"";

    // record expected values for further comparison
    Map<String, String> expectedValues = new HashMap<>();

    ReadableMetricsRegistry registry = new MetricsRegistryMap();
    KafkaSystemConsumerMetrics metrics = new KafkaSystemConsumerMetrics(systemName, registry);

    // initialize the metrics for the partitions
    metrics.registerTopicPartition(tp1);
    metrics.registerTopicPartition(tp2);

    // initialize the metrics for the host:port
    metrics.registerClientProxy(clientName);

    metrics.setOffsets(tp1, 1001);
    metrics.setOffsets(tp2, 1002);
    expectedValues.put(metrics.offsets().get(tp1).getName(), ""1001"");
    expectedValues.put(metrics.offsets().get(tp2).getName(), ""1002"");

    metrics.incBytesReads(tp1, 10);
    metrics.incBytesReads(tp1, 5); // total 15
    expectedValues.put(metrics.bytesRead().get(tp1).getName(), ""15"");

    metrics.incReads(tp1);
    metrics.incReads(tp1); // total 2
    expectedValues.put(metrics.reads().get(tp1).getName(), ""2"");

    metrics.setHighWatermarkValue(tp2, 1000);
    metrics.setHighWatermarkValue(tp2, 1001); // final value 1001
    expectedValues.put(metrics.highWatermark().get(tp2).getName(), ""1001"");

    metrics.setLagValue(tp1, 200);
    metrics.setLagValue(tp1, 201); // final value 201
    expectedValues.put(metrics.lag().get(tp1).getName(), ""201"");

    metrics.incClientBytesReads(clientName, 100); // broker-bytes-read
    metrics.incClientBytesReads(clientName, 110); // total 210
    expectedValues.put(metrics.clientBytesRead().get(clientName).getName(), ""210"");

    metrics.incClientReads(clientName); // messages-read
    metrics.incClientReads(clientName); // total 2
    expectedValues.put(metrics.clientReads().get(clientName).getName(), ""2"");

    metrics.setNumTopicPartitions(clientName, 2); // ""topic-partitions""
    metrics.setNumTopicPartitions(clientName, 3); // final value 3
    expectedValues.put(metrics.topicPartitions().get(clientName).getName(), ""3"");


    String groupName = metrics.group();
    Assert.assertEquals(groupName, KafkaSystemConsumerMetrics.class.getName());
    Assert.assertEquals(metrics.systemName(), systemName);

    Map<String, Metric> metricMap = registry.getGroup(groupName);
    validate(metricMap, expectedValues);
  }
"
"  @Test
  public void testGetSystemStreamMetaDataWithValidTopic() {
    System.out.println(""STARTING"");
    Map<String, SystemStreamMetadata> metadataMap =
        kafkaSystemAdmin.getSystemStreamMetadata(ImmutableSet.of(VALID_TOPIC));

    // verify metadata size
    assertEquals(""metadata should return for 1 topic"", metadataMap.size(), 1);
    System.out.println(""STARTING1"");
    // verify the metadata streamName
    assertEquals(""the stream name should be "" + VALID_TOPIC, metadataMap.get(VALID_TOPIC).getStreamName(), VALID_TOPIC);
    System.out.println(""STARTING2"");
    // verify the offset for each partition
    Map<Partition, SystemStreamMetadata.SystemStreamPartitionMetadata> systemStreamPartitionMetadata =
        metadataMap.get(VALID_TOPIC).getSystemStreamPartitionMetadata();
    assertEquals(""there are 2 partitions"", systemStreamPartitionMetadata.size(), 2);
    System.out.println(""STARTING3"");
    SystemStreamMetadata.SystemStreamPartitionMetadata partition0Metadata =
        systemStreamPartitionMetadata.get(new Partition(0));
    assertEquals(""oldest offset for partition 0"", partition0Metadata.getOldestOffset(),
        KAFKA_BEGINNING_OFFSET_FOR_PARTITION0.toString());
    assertEquals(""upcoming offset for partition 0"", partition0Metadata.getUpcomingOffset(),
        KAFKA_END_OFFSET_FOR_PARTITION0.toString());
    assertEquals(""newest offset for partition 0"", partition0Metadata.getNewestOffset(),
        Long.toString(KAFKA_END_OFFSET_FOR_PARTITION0 - 1));
    System.out.println(""STARTING4"");
    SystemStreamMetadata.SystemStreamPartitionMetadata partition1Metadata =
        systemStreamPartitionMetadata.get(new Partition(1));
    assertEquals(""oldest offset for partition 1"", partition1Metadata.getOldestOffset(),
        KAFKA_BEGINNING_OFFSET_FOR_PARTITION1.toString());
    assertEquals(""upcoming offset for partition 1"", partition1Metadata.getUpcomingOffset(),
        KAFKA_END_OFFSET_FOR_PARTITION1.toString());
    assertEquals(""newest offset for partition 1"", partition1Metadata.getNewestOffset(),
        Long.toString(KAFKA_END_OFFSET_FOR_PARTITION1 - 1));
  }
"
"  @Test
  public void testGetSystemStreamMetaDataWithInvalidTopic() {
    Map<String, SystemStreamMetadata> metadataMap =
        kafkaSystemAdmin.getSystemStreamMetadata(ImmutableSet.of(INVALID_TOPIC));
    assertEquals(""empty metadata for invalid topic"", metadataMap.size(), 0);
  }
"
"  @Test
  public void testGetSystemStreamMetaDataWithNoTopic() {
    Map<String, SystemStreamMetadata> metadataMap = kafkaSystemAdmin.getSystemStreamMetadata(Collections.emptySet());
    assertEquals(""empty metadata for no topic"", metadataMap.size(), 0);
  }
"
"  @Test
  public void testGetSystemStreamMetaDataForTopicWithNoMessage() {
    // The topic with no messages will have beginningOffset = 0 and endOffset = 0
    when(mockKafkaConsumer.beginningOffsets(ImmutableList.of(testTopicPartition0, testTopicPartition1))).thenReturn(
        ImmutableMap.of(testTopicPartition0, 0L, testTopicPartition1, 0L));
    when(mockKafkaConsumer.endOffsets(ImmutableList.of(testTopicPartition0, testTopicPartition1))).thenReturn(
        ImmutableMap.of(testTopicPartition0, 0L, testTopicPartition1, 0L));

    Map<String, SystemStreamMetadata> metadataMap =
        kafkaSystemAdmin.getSystemStreamMetadata(ImmutableSet.of(VALID_TOPIC));
    assertEquals(""metadata should return for 1 topic"", metadataMap.size(), 1);

    // verify the metadata streamName
    assertEquals(""the stream name should be "" + VALID_TOPIC, metadataMap.get(VALID_TOPIC).getStreamName(), VALID_TOPIC);

    // verify the offset for each partition
    Map<Partition, SystemStreamMetadata.SystemStreamPartitionMetadata> systemStreamPartitionMetadata =
        metadataMap.get(VALID_TOPIC).getSystemStreamPartitionMetadata();
    assertEquals(""there are 2 partitions"", systemStreamPartitionMetadata.size(), 2);

    SystemStreamMetadata.SystemStreamPartitionMetadata partition0Metadata =
        systemStreamPartitionMetadata.get(new Partition(0));
    assertEquals(""oldest offset for partition 0"", partition0Metadata.getOldestOffset(), ""0"");
    assertEquals(""upcoming offset for partition 0"", partition0Metadata.getUpcomingOffset(), ""0"");
    assertEquals(""newest offset is not set due to abnormal upcoming offset"", partition0Metadata.getNewestOffset(),
        null);

    SystemStreamMetadata.SystemStreamPartitionMetadata partition1Metadata =
        systemStreamPartitionMetadata.get(new Partition(1));
    assertEquals(""oldest offset for partition 1"", partition1Metadata.getOldestOffset(), ""0"");
    assertEquals(""upcoming offset for partition 1"", partition1Metadata.getUpcomingOffset(), ""0"");
    assertEquals(""newest offset is not set due to abnormal upcoming offset"", partition1Metadata.getNewestOffset(),
        null);
  }
"
"  @Test
  public void testGetSSPMetadata() {
    SystemStreamPartition ssp = new SystemStreamPartition(TEST_SYSTEM, VALID_TOPIC, new Partition(0));
    SystemStreamPartition otherSSP = new SystemStreamPartition(TEST_SYSTEM, ""otherTopic"", new Partition(1));
    TopicPartition topicPartition = new TopicPartition(VALID_TOPIC, 0);
    TopicPartition otherTopicPartition = new TopicPartition(""otherTopic"", 1);
    when(mockKafkaConsumer.beginningOffsets(ImmutableList.of(topicPartition, otherTopicPartition))).thenReturn(
        ImmutableMap.of(topicPartition, 1L, otherTopicPartition, 2L));
    when(mockKafkaConsumer.endOffsets(ImmutableList.of(topicPartition, otherTopicPartition))).thenReturn(
        ImmutableMap.of(topicPartition, 11L, otherTopicPartition, 12L));
    Map<SystemStreamPartition, SystemStreamMetadata.SystemStreamPartitionMetadata> expected =
        ImmutableMap.of(ssp, new SystemStreamMetadata.SystemStreamPartitionMetadata(""1"", ""10"", ""11""), otherSSP,
            new SystemStreamMetadata.SystemStreamPartitionMetadata(""2"", ""11"", ""12""));
    assertEquals(kafkaSystemAdmin.getSSPMetadata(ImmutableSet.of(ssp, otherSSP)), expected);
  }
"
"  @Test
  public void testGetSSPMetadataEmptyPartition() {
    SystemStreamPartition ssp = new SystemStreamPartition(TEST_SYSTEM, VALID_TOPIC, new Partition(0));
    SystemStreamPartition otherSSP = new SystemStreamPartition(TEST_SYSTEM, ""otherTopic"", new Partition(1));
    TopicPartition topicPartition = new TopicPartition(VALID_TOPIC, 0);
    TopicPartition otherTopicPartition = new TopicPartition(""otherTopic"", 1);
    when(mockKafkaConsumer.beginningOffsets(ImmutableList.of(topicPartition, otherTopicPartition))).thenReturn(
        ImmutableMap.of(topicPartition, 1L));
    when(mockKafkaConsumer.endOffsets(ImmutableList.of(topicPartition, otherTopicPartition))).thenReturn(
        ImmutableMap.of(topicPartition, 11L));

    Map<SystemStreamPartition, SystemStreamMetadata.SystemStreamPartitionMetadata> expected =
        ImmutableMap.of(ssp, new SystemStreamMetadata.SystemStreamPartitionMetadata(""1"", ""10"", ""11""), otherSSP,
            new SystemStreamMetadata.SystemStreamPartitionMetadata(null, null, null));
    assertEquals(expected, kafkaSystemAdmin.getSSPMetadata(ImmutableSet.of(ssp, otherSSP)));
  }
"
"  @Test
  public void testGetSSPMetadataEmptyUpcomingOffset() {
    SystemStreamPartition ssp = new SystemStreamPartition(TEST_SYSTEM, VALID_TOPIC, new Partition(0));
    TopicPartition topicPartition = new TopicPartition(VALID_TOPIC, 0);
    when(mockKafkaConsumer.beginningOffsets(ImmutableList.of(topicPartition))).thenReturn(
        ImmutableMap.of(topicPartition, 0L));
    when(mockKafkaConsumer.endOffsets(ImmutableList.of(topicPartition))).thenReturn(ImmutableMap.of());
    Map<SystemStreamPartition, SystemStreamMetadata.SystemStreamPartitionMetadata> expected =
        ImmutableMap.of(ssp, new SystemStreamMetadata.SystemStreamPartitionMetadata(""0"", null, null));
    assertEquals(kafkaSystemAdmin.getSSPMetadata(ImmutableSet.of(ssp)), expected);
  }
"
"  @Test
  public void testGetSSPMetadataZeroUpcomingOffset() {
    SystemStreamPartition ssp = new SystemStreamPartition(TEST_SYSTEM, VALID_TOPIC, new Partition(0));
    TopicPartition topicPartition = new TopicPartition(VALID_TOPIC, 0);
    when(mockKafkaConsumer.beginningOffsets(ImmutableList.of(topicPartition))).thenReturn(
        ImmutableMap.of(topicPartition, -1L));
    when(mockKafkaConsumer.endOffsets(ImmutableList.of(topicPartition))).thenReturn(
        ImmutableMap.of(topicPartition, 0L));
    Map<SystemStreamPartition, SystemStreamMetadata.SystemStreamPartitionMetadata> expected =
        ImmutableMap.of(ssp, new SystemStreamMetadata.SystemStreamPartitionMetadata(""0"", null, ""0""));
    assertEquals(kafkaSystemAdmin.getSSPMetadata(ImmutableSet.of(ssp)), expected);
  }
"
"  @Test
  public void testGetSystemStreamMetaDataWithRetry() {
    final List<PartitionInfo> partitionInfosForTopic = ImmutableList.of(mockPartitionInfo0, mockPartitionInfo1);
    when(mockKafkaConsumer.partitionsFor(VALID_TOPIC)).thenThrow(new RuntimeException())
        .thenReturn(partitionInfosForTopic);

    Map<String, SystemStreamMetadata> metadataMap =
        kafkaSystemAdmin.getSystemStreamMetadata(ImmutableSet.of(VALID_TOPIC));
    assertEquals(""metadata should return for 1 topic"", metadataMap.size(), 1);

    // retried twice because the first fails and the second succeeds
    Mockito.verify(mockKafkaConsumer, Mockito.times(2)).partitionsFor(VALID_TOPIC);

    final List<TopicPartition> topicPartitions =
        Arrays.asList(new TopicPartition(mockPartitionInfo0.topic(), mockPartitionInfo0.partition()),
            new TopicPartition(mockPartitionInfo1.topic(), mockPartitionInfo1.partition()));
    // the following methods thereafter are only called once
    Mockito.verify(mockKafkaConsumer, Mockito.times(1)).beginningOffsets(topicPartitions);
    Mockito.verify(mockKafkaConsumer, Mockito.times(1)).endOffsets(topicPartitions);
  }
"
"  @Test(expected = SamzaException.class)
  public void testGetSystemStreamMetadataShouldTerminateAfterFiniteRetriesOnException() {
    when(mockKafkaConsumer.partitionsFor(VALID_TOPIC)).thenThrow(new RuntimeException())
        .thenThrow(new RuntimeException())
        .thenThrow(new RuntimeException())
        .thenThrow(new RuntimeException())
        .thenThrow(new RuntimeException());

    kafkaSystemAdmin.getSystemStreamMetadata(ImmutableSet.of(VALID_TOPIC));
  }
"
"  @Test(expected = SamzaException.class)
  public void testGetSystemStreamPartitionCountsShouldTerminateAfterFiniteRetriesOnException() throws Exception {
    final Set<String> streamNames = ImmutableSet.of(VALID_TOPIC);
    final long cacheTTL = 100L;

    when(mockKafkaConsumer.partitionsFor(VALID_TOPIC)).thenThrow(new RuntimeException())
        .thenThrow(new RuntimeException())
        .thenThrow(new RuntimeException())
        .thenThrow(new RuntimeException())
        .thenThrow(new RuntimeException());

    kafkaSystemAdmin.getSystemStreamPartitionCounts(streamNames, cacheTTL);
  }
"
"  @Test
  public void testGetSSPMetadataWithRetry() {
    SystemStreamPartition oneSSP = new SystemStreamPartition(TEST_SYSTEM, VALID_TOPIC, new Partition(0));
    SystemStreamPartition otherSSP = new SystemStreamPartition(TEST_SYSTEM, ""otherTopic"", new Partition(1));
    ImmutableSet<SystemStreamPartition> ssps = ImmutableSet.of(oneSSP, otherSSP);
    List<TopicPartition> topicPartitions = ssps.stream()
        .map(ssp -> new TopicPartition(ssp.getStream(), ssp.getPartition().getPartitionId()))
        .collect(Collectors.toList());
    Map<TopicPartition, Long> testBeginningOffsets =
        ImmutableMap.of(testTopicPartition0, KAFKA_BEGINNING_OFFSET_FOR_PARTITION0, testTopicPartition1,
            KAFKA_BEGINNING_OFFSET_FOR_PARTITION1);

    when(mockKafkaConsumer.beginningOffsets(topicPartitions)).thenThrow(new RuntimeException())
        .thenReturn(testBeginningOffsets);
    Map<SystemStreamPartition, SystemStreamMetadata.SystemStreamPartitionMetadata> sspMetadata =
        kafkaSystemAdmin.getSSPMetadata(ssps, new ExponentialSleepStrategy(2,
            1, 1));

    assertEquals(""metadata should return for 2 topics"", sspMetadata.size(), 2);

    // retried twice because the first fails and the second succeeds
    Mockito.verify(mockKafkaConsumer, Mockito.times(2)).beginningOffsets(topicPartitions);
  }
"
"  @Test(expected = SamzaException.class)
  public void testGetSSPMetadataShouldTerminateAfterFiniteRetriesOnException() throws Exception {
    SystemStreamPartition oneSSP = new SystemStreamPartition(TEST_SYSTEM, VALID_TOPIC, new Partition(0));
    SystemStreamPartition otherSSP = new SystemStreamPartition(TEST_SYSTEM, ""otherTopic"", new Partition(1));

    ImmutableSet<SystemStreamPartition> ssps = ImmutableSet.of(oneSSP, otherSSP);
    List<TopicPartition> topicPartitions = ssps.stream()
        .map(ssp -> new TopicPartition(ssp.getStream(), ssp.getPartition().getPartitionId()))
        .collect(Collectors.toList());

    when(mockKafkaConsumer.beginningOffsets(topicPartitions)).thenThrow(new RuntimeException())
        .thenThrow(new RuntimeException());

    kafkaSystemAdmin.getSSPMetadata(ssps, new ExponentialSleepStrategy(2,
        1, 1));
  }
"
"  @Test
  public void testConfigValidations() {

    final KafkaSystemConsumer consumer = createConsumer(FETCH_THRESHOLD_MSGS, FETCH_THRESHOLD_BYTES);

    consumer.start();
    // should be no failures
  }
"
"  @Test
  public void testFetchThresholdShouldDivideEvenlyAmongPartitions() {
    final KafkaSystemConsumer consumer = createConsumer(FETCH_THRESHOLD_MSGS, FETCH_THRESHOLD_BYTES);
    final int partitionsNum = 50;
    for (int i = 0; i < partitionsNum; i++) {
      consumer.register(new SystemStreamPartition(TEST_SYSTEM, TEST_STREAM, new Partition(i)), ""0"");
    }

    consumer.start();

    Assert.assertEquals(Long.valueOf(FETCH_THRESHOLD_MSGS) / partitionsNum, consumer.perPartitionFetchThreshold);
    Assert.assertEquals(Long.valueOf(FETCH_THRESHOLD_BYTES) / 2 / partitionsNum,
        consumer.perPartitionFetchThresholdBytes);

    consumer.stop();
  }
"
"  @Test
  public void testConsumerRegisterOlderOffsetOfTheSamzaSSP() {

    KafkaSystemConsumer consumer = createConsumer(FETCH_THRESHOLD_MSGS, FETCH_THRESHOLD_BYTES);

    SystemStreamPartition ssp0 = new SystemStreamPartition(TEST_SYSTEM, TEST_STREAM, new Partition(0));
    SystemStreamPartition ssp1 = new SystemStreamPartition(TEST_SYSTEM, TEST_STREAM, new Partition(1));
    SystemStreamPartition ssp2 = new SystemStreamPartition(TEST_SYSTEM, TEST_STREAM, new Partition(2));

    consumer.register(ssp0, ""0"");
    consumer.register(ssp0, ""5"");
    consumer.register(ssp1, ""2"");
    consumer.register(ssp1, ""3"");
    consumer.register(ssp2, ""0"");

    assertEquals(""0"", consumer.topicPartitionsToOffset.get(KafkaSystemConsumer.toTopicPartition(ssp0)));
    assertEquals(""2"", consumer.topicPartitionsToOffset.get(KafkaSystemConsumer.toTopicPartition(ssp1)));
    assertEquals(""0"", consumer.topicPartitionsToOffset.get(KafkaSystemConsumer.toTopicPartition(ssp2)));
  }
"
"  @Test
  public void testFetchThresholdBytes() {

    SystemStreamPartition ssp0 = new SystemStreamPartition(TEST_SYSTEM, TEST_STREAM, new Partition(0));
    SystemStreamPartition ssp1 = new SystemStreamPartition(TEST_SYSTEM, TEST_STREAM, new Partition(1));
    int partitionsNum = 2;
    int ime0Size = Integer.valueOf(FETCH_THRESHOLD_MSGS) / partitionsNum; // fake size
    int ime1Size = Integer.valueOf(FETCH_THRESHOLD_MSGS) / partitionsNum - 1; // fake size
    int ime11Size = 20;
    ByteArraySerializer bytesSerde = new ByteArraySerializer();
    IncomingMessageEnvelope ime0 = new IncomingMessageEnvelope(ssp0, ""0"", bytesSerde.serialize("""", ""key0"".getBytes()),
        bytesSerde.serialize("""", ""value0"".getBytes()), ime0Size);
    IncomingMessageEnvelope ime1 = new IncomingMessageEnvelope(ssp1, ""0"", bytesSerde.serialize("""", ""key1"".getBytes()),
        bytesSerde.serialize("""", ""value1"".getBytes()), ime1Size);
    IncomingMessageEnvelope ime11 = new IncomingMessageEnvelope(ssp1, ""0"", bytesSerde.serialize("""", ""key11"".getBytes()),
        bytesSerde.serialize("""", ""value11"".getBytes()), ime11Size);
    KafkaSystemConsumer consumer = createConsumer(FETCH_THRESHOLD_MSGS, FETCH_THRESHOLD_BYTES);

    consumer.register(ssp0, ""0"");
    consumer.register(ssp1, ""0"");
    consumer.start();
    consumer.messageSink.addMessage(ssp0, ime0);
    // queue for ssp0 should be full now, because we added message of size FETCH_THRESHOLD_MSGS/partitionsNum
    Assert.assertFalse(consumer.messageSink.needsMoreMessages(ssp0));
    consumer.messageSink.addMessage(ssp1, ime1);
    // queue for ssp1 should be less then full now, because we added message of size (FETCH_THRESHOLD_MSGS/partitionsNum - 1)
    Assert.assertTrue(consumer.messageSink.needsMoreMessages(ssp1));
    consumer.messageSink.addMessage(ssp1, ime11);
    // queue for ssp1 should full now, because we added message of size 20 on top
    Assert.assertFalse(consumer.messageSink.needsMoreMessages(ssp1));

    Assert.assertEquals(1, consumer.getNumMessagesInQueue(ssp0));
    Assert.assertEquals(2, consumer.getNumMessagesInQueue(ssp1));
    Assert.assertEquals(ime0Size, consumer.getMessagesSizeInQueue(ssp0));
    Assert.assertEquals(ime1Size + ime11Size, consumer.getMessagesSizeInQueue(ssp1));

    consumer.stop();
  }
"
"  @Test
  public void testFetchThresholdBytesDiabled() {
    // Pass 0 as fetchThresholdByBytes, which disables checking for limit by size

    SystemStreamPartition ssp0 = new SystemStreamPartition(TEST_SYSTEM, TEST_STREAM, new Partition(0));
    SystemStreamPartition ssp1 = new SystemStreamPartition(TEST_SYSTEM, TEST_STREAM, new Partition(1));
    int partitionsNum = 2;
    int ime0Size = Integer.valueOf(FETCH_THRESHOLD_MSGS) / partitionsNum; // fake size, upto the limit
    int ime1Size = Integer.valueOf(FETCH_THRESHOLD_MSGS) / partitionsNum - 100; // fake size, below the limit
    int ime11Size = 20; // event with the second message still below the size limit
    ByteArraySerializer bytesSerde = new ByteArraySerializer();
    IncomingMessageEnvelope ime0 = new IncomingMessageEnvelope(ssp0, ""0"", bytesSerde.serialize("""", ""key0"".getBytes()),
        bytesSerde.serialize("""", ""value0"".getBytes()), ime0Size);
    IncomingMessageEnvelope ime1 = new IncomingMessageEnvelope(ssp1, ""0"", bytesSerde.serialize("""", ""key1"".getBytes()),
        bytesSerde.serialize("""", ""value1"".getBytes()), ime1Size);
    IncomingMessageEnvelope ime11 = new IncomingMessageEnvelope(ssp1, ""0"", bytesSerde.serialize("""", ""key11"".getBytes()),
        bytesSerde.serialize("""", ""value11"".getBytes()), ime11Size);

    // limit by number of messages 4/2 = 2 per partition
    // limit by number of bytes - disabled
    KafkaSystemConsumer consumer = createConsumer(""4"", ""0""); // should disable

    consumer.register(ssp0, ""0"");
    consumer.register(ssp1, ""0"");
    consumer.start();
    consumer.messageSink.addMessage(ssp0, ime0);
    // should be full by size, but not full by number of messages (1 of 2)
    Assert.assertTrue(consumer.messageSink.needsMoreMessages(ssp0));
    consumer.messageSink.addMessage(ssp1, ime1);
    // not full neither by size nor by messages
    Assert.assertTrue(consumer.messageSink.needsMoreMessages(ssp1));
    consumer.messageSink.addMessage(ssp1, ime11);
    // not full by size, but should be full by messages
    Assert.assertFalse(consumer.messageSink.needsMoreMessages(ssp1));

    Assert.assertEquals(1, consumer.getNumMessagesInQueue(ssp0));
    Assert.assertEquals(2, consumer.getNumMessagesInQueue(ssp1));
    Assert.assertEquals(ime0Size, consumer.getMessagesSizeInQueue(ssp0));
    Assert.assertEquals(ime1Size + ime11Size, consumer.getMessagesSizeInQueue(ssp1));

    consumer.stop();
  }
"
"  @Test
  public void testStartConsumer() {
    final Consumer consumer = Mockito.mock(Consumer.class);
    final KafkaConsumerProxyFactory kafkaConsumerProxyFactory = Mockito.mock(KafkaConsumerProxyFactory.class);

    final KafkaSystemConsumerMetrics kafkaSystemConsumerMetrics = new KafkaSystemConsumerMetrics(TEST_SYSTEM, new NoOpMetricsRegistry());
    final SystemStreamPartition testSystemStreamPartition1 = new SystemStreamPartition(TEST_SYSTEM, TEST_STREAM, new Partition(0));
    final SystemStreamPartition testSystemStreamPartition2 = new SystemStreamPartition(TEST_SYSTEM, TEST_STREAM, new Partition(1));
    final String testOffset = ""1"";
    final KafkaConsumerProxy kafkaConsumerProxy = Mockito.mock(KafkaConsumerProxy.class);

    Mockito.when(kafkaConsumerProxyFactory.create(Mockito.anyObject())).thenReturn(kafkaConsumerProxy);
    Mockito.doNothing().when(consumer).seek(new TopicPartition(TEST_STREAM, 0), 1);
    Mockito.doNothing().when(consumer).seek(new TopicPartition(TEST_STREAM, 1), 1);

    KafkaSystemConsumer kafkaSystemConsumer = new KafkaSystemConsumer(consumer, TEST_SYSTEM, new MapConfig(), TEST_CLIENT_ID, kafkaConsumerProxyFactory,
                                                                      kafkaSystemConsumerMetrics, new SystemClock());
    kafkaSystemConsumer.register(testSystemStreamPartition1, testOffset);
    kafkaSystemConsumer.register(testSystemStreamPartition2, testOffset);

    kafkaSystemConsumer.startConsumer();

    Mockito.verify(consumer).seek(new TopicPartition(TEST_STREAM, 0), 1);
    Mockito.verify(consumer).seek(new TopicPartition(TEST_STREAM, 1), 1);
    Mockito.verify(kafkaConsumerProxy).start();
    Mockito.verify(kafkaConsumerProxy).addTopicPartition(testSystemStreamPartition1, Long.valueOf(testOffset));
    Mockito.verify(kafkaConsumerProxy).addTopicPartition(testSystemStreamPartition2, Long.valueOf(testOffset));
  }
"
"  @Test
  public void testCreateStreamShouldCoordinatorStreamWithCorrectTopicProperties() throws Exception {
    String coordinatorTopicName = String.format(""topic-name-%s"", RandomStringUtils.randomAlphabetic(5));
    StreamSpec coordinatorStreamSpec = KafkaStreamSpec.createCoordinatorStreamSpec(coordinatorTopicName, SYSTEM());

    boolean hasCreatedStream = systemAdmin().createStream(coordinatorStreamSpec);

    assertTrue(hasCreatedStream);

    Map<String, String> coordinatorTopicProperties = getTopicConfigFromKafkaBroker(coordinatorTopicName);

    assertEquals(""compact"", coordinatorTopicProperties.get(TopicConfig.CLEANUP_POLICY_CONFIG));
    assertEquals(""26214400"", coordinatorTopicProperties.get(TopicConfig.SEGMENT_BYTES_CONFIG));
    assertEquals(""86400000"", coordinatorTopicProperties.get(TopicConfig.DELETE_RETENTION_MS_CONFIG));
  }
"
"  @Test
  public void testGetOffsetsAfter() {
    SystemStreamPartition ssp1 = new SystemStreamPartition(SYSTEM, TOPIC, new Partition(0));
    SystemStreamPartition ssp2 = new SystemStreamPartition(SYSTEM, TOPIC, new Partition(1));
    Map<SystemStreamPartition, String> offsets = new HashMap<>();
    offsets.put(ssp1, ""1"");
    offsets.put(ssp2, ""2"");

    offsets = systemAdmin().getOffsetsAfter(offsets);

    Assert.assertEquals(""2"", offsets.get(ssp1));
    Assert.assertEquals(""3"", offsets.get(ssp2));
  }
"
"  @Test
  public void testToKafkaSpecForCheckpointStreamShouldReturnTheCorrectStreamSpecByPreservingTheConfig() {
    String topicName = ""testStream"";
    String streamId = ""samza-internal-checkpoint-stream-id"";
    int partitionCount = 1;
    Map<String, String> map = new HashMap<>();
    map.put(""cleanup.policy"", ""compact"");
    map.put(""replication.factor"", ""3"");
    map.put(""segment.bytes"", ""536870912"");
    map.put(""delete.retention.ms"", ""86400000"");

    Config config = new MapConfig(map);

    StreamSpec spec = new StreamSpec(streamId, topicName, SYSTEM, partitionCount, config);
    KafkaSystemAdmin kafkaSystemAdmin = systemAdmin();
    KafkaStreamSpec kafkaStreamSpec = kafkaSystemAdmin.toKafkaSpec(spec);
    System.out.println(kafkaStreamSpec);
    assertEquals(streamId, kafkaStreamSpec.getId());
    assertEquals(topicName, kafkaStreamSpec.getPhysicalName());
    assertEquals(partitionCount, kafkaStreamSpec.getPartitionCount());
    assertEquals(3, kafkaStreamSpec.getReplicationFactor());
    assertEquals(""compact"", kafkaStreamSpec.getConfig().get(""cleanup.policy""));
    assertEquals(""536870912"", kafkaStreamSpec.getConfig().get(""segment.bytes""));
    assertEquals(""86400000"", kafkaStreamSpec.getConfig().get(""delete.retention.ms""));
  }
"
"  @Test
  public void testToKafkaSpec() {
    String topicName = ""testStream"";

    int defaultPartitionCount = 2;
    int changeLogPartitionFactor = 5;
    Map<String, String> map = new HashMap<>();
    Config config = new MapConfig(map);
    StreamSpec spec = new StreamSpec(""id"", topicName, SYSTEM, defaultPartitionCount, config);

    KafkaSystemAdmin kafkaAdmin = systemAdmin();
    KafkaStreamSpec kafkaSpec = kafkaAdmin.toKafkaSpec(spec);

    Assert.assertEquals(""id"", kafkaSpec.getId());
    Assert.assertEquals(topicName, kafkaSpec.getPhysicalName());
    Assert.assertEquals(SYSTEM, kafkaSpec.getSystemName());
    Assert.assertEquals(defaultPartitionCount, kafkaSpec.getPartitionCount());

    // validate that conversion is using coordination metadata
    map.put(""job.coordinator.segment.bytes"", ""123"");
    map.put(""job.coordinator.cleanup.policy"", ""superCompact"");
    int coordReplicatonFactor = 4;
    map.put(org.apache.samza.config.KafkaConfig.JOB_COORDINATOR_REPLICATION_FACTOR(),
        String.valueOf(coordReplicatonFactor));

    KafkaSystemAdmin admin = Mockito.spy(createSystemAdmin(SYSTEM, map));
    spec = StreamSpec.createCoordinatorStreamSpec(topicName, SYSTEM);
    kafkaSpec = admin.toKafkaSpec(spec);
    Assert.assertEquals(coordReplicatonFactor, kafkaSpec.getReplicationFactor());
    Assert.assertEquals(""123"", kafkaSpec.getProperties().getProperty(""segment.bytes""));
    // cleanup policy is overridden in the KafkaAdmin
    Assert.assertEquals(""compact"", kafkaSpec.getProperties().getProperty(""cleanup.policy""));

    // validate that conversion is using changeLog metadata
    map = new HashMap<>();
    map.put(JobConfig.JOB_DEFAULT_SYSTEM, SYSTEM);

    map.put(String.format(""stores.%s.changelog"", ""fakeStore""), topicName);
    int changeLogReplicationFactor = 3;
    map.put(String.format(""stores.%s.changelog.replication.factor"", ""fakeStore""),
        String.valueOf(changeLogReplicationFactor));
    admin = Mockito.spy(createSystemAdmin(SYSTEM, map));
    spec = StreamSpec.createChangeLogStreamSpec(topicName, SYSTEM, changeLogPartitionFactor);
    kafkaSpec = admin.toKafkaSpec(spec);
    Assert.assertEquals(changeLogReplicationFactor, kafkaSpec.getReplicationFactor());

    // same, but with missing topic info
    try {
      admin = Mockito.spy(createSystemAdmin(SYSTEM, map));
      spec = StreamSpec.createChangeLogStreamSpec(""anotherTopic"", SYSTEM, changeLogPartitionFactor);
      kafkaSpec = admin.toKafkaSpec(spec);
      Assert.fail(""toKafkaSpec should've failed for missing topic"");
    } catch (StreamValidationException e) {
      // expected
    }

    // validate that conversion is using intermediate streams properties
    String interStreamId = ""isId"";

    Map<String, String> interStreamMap = new HashMap<>();
    interStreamMap.put(""app.mode"", ApplicationConfig.ApplicationMode.BATCH.toString());
    interStreamMap.put(String.format(""streams.%s.samza.intermediate"", interStreamId), ""true"");
    interStreamMap.put(String.format(""streams.%s.samza.system"", interStreamId), ""testSystem"");
    interStreamMap.put(String.format(""streams.%s.p1"", interStreamId), ""v1"");
    interStreamMap.put(String.format(""streams.%s.retention.ms"", interStreamId), ""123"");
    // legacy format
    interStreamMap.put(String.format(""systems.%s.streams.%s.p2"", ""testSystem"", interStreamId), ""v2"");

    admin = Mockito.spy(createSystemAdmin(SYSTEM, interStreamMap));
    spec = new StreamSpec(interStreamId, topicName, SYSTEM, defaultPartitionCount, config);
    kafkaSpec = admin.toKafkaSpec(spec);
    Assert.assertEquals(""v1"", kafkaSpec.getProperties().getProperty(""p1""));
    Assert.assertEquals(""v2"", kafkaSpec.getProperties().getProperty(""p2""));
    Assert.assertEquals(""123"", kafkaSpec.getProperties().getProperty(""retention.ms""));
    Assert.assertEquals(defaultPartitionCount, kafkaSpec.getPartitionCount());
  }
"
"  @Test
  public void testCreateCoordinatorStream() {
    SystemAdmin admin = Mockito.spy(systemAdmin());
    StreamSpec spec = StreamSpec.createCoordinatorStreamSpec(""testCoordinatorStream"", ""testSystem"");

    admin.createStream(spec);
    admin.validateStream(spec);
    Mockito.verify(admin).createStream(Mockito.any());
  }
"
"  @Test
  public void testCreateCoordinatorStreamWithSpecialCharsInTopicName() {
    final String stream = ""test.coordinator_test.Stream"";

    Map<String, String> map = new HashMap<>();
    map.put(""job.coordinator.segment.bytes"", ""123"");
    map.put(""job.coordinator.cleanup.policy"", ""compact"");
    int coordReplicatonFactor = 2;
    map.put(org.apache.samza.config.KafkaConfig.JOB_COORDINATOR_REPLICATION_FACTOR(),
        String.valueOf(coordReplicatonFactor));

    KafkaSystemAdmin admin = Mockito.spy(createSystemAdmin(SYSTEM, map));
    StreamSpec spec = StreamSpec.createCoordinatorStreamSpec(stream, SYSTEM);

    Mockito.doAnswer(invocationOnMock -> {
      StreamSpec internalSpec = (StreamSpec) invocationOnMock.callRealMethod();
      assertTrue(internalSpec instanceof KafkaStreamSpec);  // KafkaStreamSpec is used to carry replication factor
      assertTrue(internalSpec.isCoordinatorStream());
      assertEquals(SYSTEM, internalSpec.getSystemName());
      assertEquals(stream, internalSpec.getPhysicalName());
      assertEquals(1, internalSpec.getPartitionCount());
      Assert.assertEquals(coordReplicatonFactor, ((KafkaStreamSpec) internalSpec).getReplicationFactor());
      Assert.assertEquals(""123"", ((KafkaStreamSpec) internalSpec).getProperties().getProperty(""segment.bytes""));
      // cleanup policy is overridden in the KafkaAdmin
      Assert.assertEquals(""compact"", ((KafkaStreamSpec) internalSpec).getProperties().getProperty(""cleanup.policy""));

      return internalSpec;
    }).when(admin).toKafkaSpec(Mockito.any());

    admin.createStream(spec);
    admin.validateStream(spec);
  }
"
"  @Test
  public void testCreateChangelogStreamHelp() {
    testCreateChangelogStreamHelp(""testChangeLogStream"");
  }
"
"  @Test
  public void testCreateChangelogStreamWithSpecialCharsInTopicName() {
    // cannot contain period
    testCreateChangelogStreamHelp(""test-Change_Log-Stream"");
  }
"
"  @Test
  public void testCreateStream() {
    StreamSpec spec = new StreamSpec(""testId"", ""testStream"", ""testSystem"", 8);
    KafkaSystemAdmin admin = systemAdmin();
    assertTrue(""createStream should return true if the stream does not exist and then is created."",
        admin.createStream(spec));
    admin.validateStream(spec);

    assertFalse(""createStream should return false if the stream already exists."", systemAdmin().createStream(spec));
  }
"
"  @Test(expected = StreamValidationException.class)
  public void testValidateStreamDoesNotExist() {

    StreamSpec spec = new StreamSpec(""testId"", ""testStreamNameExist"", ""testSystem"", 8);

    systemAdmin().validateStream(spec);
  }
"
"  @Test(expected = StreamValidationException.class)
  public void testValidateStreamWrongPartitionCount() {
    StreamSpec spec1 = new StreamSpec(""testId"", ""testStreamPartition"", ""testSystem"", 8);
    StreamSpec spec2 = new StreamSpec(""testId"", ""testStreamPartition"", ""testSystem"", 4);

    assertTrue(""createStream should return true if the stream does not exist and then is created."",
        systemAdmin().createStream(spec1));

    systemAdmin().validateStream(spec2);
  }
"
"  @Test(expected = StreamValidationException.class)
  public void testValidateStreamWrongName() {
    StreamSpec spec1 = new StreamSpec(""testId"", ""testStreamName1"", ""testSystem"", 8);
    StreamSpec spec2 = new StreamSpec(""testId"", ""testStreamName2"", ""testSystem"", 8);

    assertTrue(""createStream should return true if the stream does not exist and then is created."",
        systemAdmin().createStream(spec1));

    systemAdmin().validateStream(spec2);
  }
"
"  @Test
  public void testClearStream() {
    StreamSpec spec = new StreamSpec(""testId"", ""testStreamClear"", ""testSystem"", 8);

    KafkaSystemAdmin admin = systemAdmin();
    String topicName = spec.getPhysicalName();

    assertTrue(""createStream should return true if the stream does not exist and then is created."", admin.createStream(spec));
    // validate topic exists
    assertTrue(admin.clearStream(spec));

    // validate that topic was removed
    DescribeTopicsResult dtr = admin.adminClient.describeTopics(ImmutableSet.of(topicName));
    try {
      TopicDescription td = dtr.all().get().get(topicName);
      Assert.fail(""topic "" + topicName + "" should've been removed. td="" + td);
    } catch (Exception e) {
      if (!(e.getCause() instanceof org.apache.kafka.common.errors.UnknownTopicOrPartitionException)) {
        Assert.fail(""topic "" + topicName + "" should've been removed. Expected UnknownTopicOrPartitionException."");
      }
    }
  }
"
"  @Test
  public void testShouldAssembleMetadata() {
    Map<SystemStreamPartition, String> oldestOffsets = new ImmutableMap.Builder<SystemStreamPartition, String>()
        .put(new SystemStreamPartition(SYSTEM, ""stream1"", new Partition(0)), ""o1"")
        .put(new SystemStreamPartition(SYSTEM, ""stream2"", new Partition(0)), ""o2"")
        .put(new SystemStreamPartition(SYSTEM, ""stream1"", new Partition(1)), ""o3"")
        .put(new SystemStreamPartition(SYSTEM, ""stream2"", new Partition(1)), ""o4"")
        .build();

    Map<SystemStreamPartition, String> newestOffsets = new ImmutableMap.Builder<SystemStreamPartition, String>()
        .put(new SystemStreamPartition(SYSTEM, ""stream1"", new Partition(0)), ""n1"")
        .put(new SystemStreamPartition(SYSTEM, ""stream2"", new Partition(0)), ""n2"")
        .put(new SystemStreamPartition(SYSTEM, ""stream1"", new Partition(1)), ""n3"")
        .put(new SystemStreamPartition(SYSTEM, ""stream2"", new Partition(1)), ""n4"")
        .build();

    Map<SystemStreamPartition, String> upcomingOffsets = new ImmutableMap.Builder<SystemStreamPartition, String>()
        .put(new SystemStreamPartition(SYSTEM, ""stream1"", new Partition(0)), ""u1"")
        .put(new SystemStreamPartition(SYSTEM, ""stream2"", new Partition(0)), ""u2"")
        .put(new SystemStreamPartition(SYSTEM, ""stream1"", new Partition(1)), ""u3"")
        .put(new SystemStreamPartition(SYSTEM, ""stream2"", new Partition(1)), ""u4"")
        .build();

    Map<String, SystemStreamMetadata> metadata = assembleMetadata(oldestOffsets, newestOffsets, upcomingOffsets);
    assertNotNull(metadata);
    assertEquals(2, metadata.size());
    assertTrue(metadata.containsKey(""stream1""));
    assertTrue(metadata.containsKey(""stream2""));
    SystemStreamMetadata stream1Metadata = metadata.get(""stream1"");
    SystemStreamMetadata stream2Metadata = metadata.get(""stream2"");
    assertNotNull(stream1Metadata);
    assertNotNull(stream2Metadata);
    assertEquals(""stream1"", stream1Metadata.getStreamName());
    assertEquals(""stream2"", stream2Metadata.getStreamName());
    SystemStreamMetadata.SystemStreamPartitionMetadata expectedSystemStream1Partition0Metadata =
        new SystemStreamMetadata.SystemStreamPartitionMetadata(""o1"", ""n1"", ""u1"");
    SystemStreamMetadata.SystemStreamPartitionMetadata expectedSystemStream1Partition1Metadata =
        new SystemStreamMetadata.SystemStreamPartitionMetadata(""o3"", ""n3"", ""u3"");
    SystemStreamMetadata.SystemStreamPartitionMetadata expectedSystemStream2Partition0Metadata =
        new SystemStreamMetadata.SystemStreamPartitionMetadata(""o2"", ""n2"", ""u2"");
    SystemStreamMetadata.SystemStreamPartitionMetadata expectedSystemStream2Partition1Metadata =
        new SystemStreamMetadata.SystemStreamPartitionMetadata(""o4"", ""n4"", ""u4"");
    Map<Partition, SystemStreamMetadata.SystemStreamPartitionMetadata> stream1PartitionMetadata =
        stream1Metadata.getSystemStreamPartitionMetadata();
    Map<Partition, SystemStreamMetadata.SystemStreamPartitionMetadata> stream2PartitionMetadata =
        stream2Metadata.getSystemStreamPartitionMetadata();
    assertEquals(expectedSystemStream1Partition0Metadata, stream1PartitionMetadata.get(new Partition(0)));
    assertEquals(expectedSystemStream1Partition1Metadata, stream1PartitionMetadata.get(new Partition(1)));
    assertEquals(expectedSystemStream2Partition0Metadata, stream2PartitionMetadata.get(new Partition(0)));
    assertEquals(expectedSystemStream2Partition1Metadata, stream2PartitionMetadata.get(new Partition(1)));
  }
"
"  @Test
  public void testStartpointSpecificOffsetVisitorShouldResolveToCorrectOffset() {
    final KafkaConsumer consumer = Mockito.mock(KafkaConsumer.class);
    final KafkaStartpointToOffsetResolver kafkaStartpointToOffsetResolver = new KafkaStartpointToOffsetResolver(consumer);

    final StartpointSpecific testStartpointSpecific = new StartpointSpecific(TEST_OFFSET);

    // Invoke the consumer with startpoint.
    String resolvedOffset = kafkaStartpointToOffsetResolver.visit(TEST_SYSTEM_STREAM_PARTITION, testStartpointSpecific);
    Assert.assertEquals(TEST_OFFSET, resolvedOffset);
  }
"
"  @Test
  public void testStartpointTimestampVisitorShouldResolveToCorrectOffset() {
    // Define dummy variables for testing.
    final Long testTimeStamp = 10L;

    final KafkaConsumer consumer = Mockito.mock(KafkaConsumer.class);

    final KafkaStartpointToOffsetResolver kafkaStartpointToOffsetResolver = new KafkaStartpointToOffsetResolver(consumer);

    final StartpointTimestamp startpointTimestamp = new StartpointTimestamp(testTimeStamp);
    final Map<TopicPartition, OffsetAndTimestamp> offsetForTimesResult = ImmutableMap.of(
        TEST_TOPIC_PARTITION, new OffsetAndTimestamp(Long.valueOf(TEST_OFFSET), testTimeStamp));

    // Mock the consumer interactions.
    Mockito.when(consumer.offsetsForTimes(ImmutableMap.of(TEST_TOPIC_PARTITION, testTimeStamp))).thenReturn(offsetForTimesResult);
    Mockito.when(consumer.position(TEST_TOPIC_PARTITION)).thenReturn(Long.valueOf(TEST_OFFSET));

    String resolvedOffset = kafkaStartpointToOffsetResolver.visit(TEST_SYSTEM_STREAM_PARTITION, startpointTimestamp);
    Assert.assertEquals(TEST_OFFSET, resolvedOffset);
  }
"
"  @Test
  public void testStartpointTimestampVisitorShouldResolveToCorrectOffsetWhenTimestampDoesNotExist() {
    final KafkaConsumer consumer = Mockito.mock(KafkaConsumer.class);
    final KafkaStartpointToOffsetResolver kafkaStartpointToOffsetResolver = new KafkaStartpointToOffsetResolver(consumer);

    final StartpointTimestamp startpointTimestamp = new StartpointTimestamp(0L);
    final Map<TopicPartition, OffsetAndTimestamp> offsetForTimesResult = new HashMap<>();
    offsetForTimesResult.put(TEST_TOPIC_PARTITION, null);

    // Mock the consumer interactions.
    Mockito.when(consumer.offsetsForTimes(ImmutableMap.of(TEST_TOPIC_PARTITION, 0L))).thenReturn(offsetForTimesResult);
    Mockito.when(consumer.endOffsets(ImmutableSet.of(TEST_TOPIC_PARTITION))).thenReturn(ImmutableMap.of(TEST_TOPIC_PARTITION, 10L));

    String resolvedOffset = kafkaStartpointToOffsetResolver.visit(TEST_SYSTEM_STREAM_PARTITION, startpointTimestamp);
    Assert.assertEquals(TEST_OFFSET, resolvedOffset);

    // Mock verifications.
    Mockito.verify(consumer).offsetsForTimes(ImmutableMap.of(TEST_TOPIC_PARTITION, 0L));
  }
"
"  @Test
  public void testStartpointOldestVisitorShouldResolveToCorrectOffset() {
    // Define dummy variables for testing.
    final KafkaConsumer consumer = Mockito.mock(KafkaConsumer.class);
    final KafkaStartpointToOffsetResolver kafkaStartpointToOffsetResolver = new KafkaStartpointToOffsetResolver(consumer);

    final StartpointOldest testStartpointSpecific = new StartpointOldest();

    // Mock the consumer interactions.
    Mockito.when(consumer.beginningOffsets(ImmutableSet.of(TEST_TOPIC_PARTITION))).thenReturn(ImmutableMap.of(TEST_TOPIC_PARTITION, 10L));

    // Invoke the consumer with startpoint.
    String resolvedOffset = kafkaStartpointToOffsetResolver.visit(TEST_SYSTEM_STREAM_PARTITION, testStartpointSpecific);
    Assert.assertEquals(TEST_OFFSET, resolvedOffset);
  }
"
"  @Test
  public void testStartpointUpcomingVisitorShouldResolveToCorrectOffset() {
    // Define dummy variables for testing.
    final KafkaConsumer consumer = Mockito.mock(KafkaConsumer.class);

    final KafkaStartpointToOffsetResolver kafkaStartpointToOffsetResolver = new KafkaStartpointToOffsetResolver(consumer);

    final StartpointUpcoming testStartpointSpecific = new StartpointUpcoming();

    // Mock the consumer interactions.
    Mockito.when(consumer.endOffsets(ImmutableSet.of(TEST_TOPIC_PARTITION))).thenReturn(ImmutableMap.of(TEST_TOPIC_PARTITION, 10L));

    // Invoke the consumer with startpoint.
    String resolvedOffset = kafkaStartpointToOffsetResolver.visit(TEST_SYSTEM_STREAM_PARTITION, testStartpointSpecific);
    Assert.assertEquals(TEST_OFFSET, resolvedOffset);
  }
"
"  @Test
  public void testUnsupportedConfigStrippedFromProperties() {
    StreamSpec original = new StreamSpec(""dummyId"", ""dummyPhysicalName"", ""dummySystemName"",
        ImmutableMap.of(""segment.bytes"", ""4"", ""replication.factor"", ""7""));

    // First verify the original
    assertEquals(""7"", original.get(""replication.factor""));
    assertEquals(""4"", original.get(""segment.bytes""));

    Map<String, String> config = original.getConfig();
    assertEquals(""7"", config.get(""replication.factor""));
    assertEquals(""4"", config.get(""segment.bytes""));


    // Now verify the Kafka spec
    KafkaStreamSpec spec = KafkaStreamSpec.fromSpec(original);
    assertNull(spec.get(""replication.factor""));
    assertEquals(""4"", spec.get(""segment.bytes""));

    Properties kafkaProperties = spec.getProperties();
    Map<String, String> kafkaConfig = spec.getConfig();
    assertNull(kafkaProperties.get(""replication.factor""));
    assertEquals(""4"", kafkaProperties.get(""segment.bytes""));

    assertNull(kafkaConfig.get(""replication.factor""));
    assertEquals(""4"", kafkaConfig.get(""segment.bytes""));
  }
"
"  @Test(expected = IllegalArgumentException.class)
  public void testInvalidPartitionCount() {
    new KafkaStreamSpec(""dummyId"", ""dummyPhysicalName"", ""dummySystemName"", 0);
  }
"
"  @Test
  public void testInstantiateProducer() {
    KafkaSystemProducer ksp = new KafkaSystemProducer(""SysName"", new ExponentialSleepStrategy(2.0, 200, 10000),
      new AbstractFunction0<Producer<byte[], byte[]>>() {
        @Override
        public Producer<byte[], byte[]> apply() {
          return new KafkaProducer<>(new HashMap<String, Object>());
        }
      }, new KafkaSystemProducerMetrics(""SysName"", new MetricsRegistryMap()), new AbstractFunction0<Object>() {
        @Override
        public Object apply() {
          return System.currentTimeMillis();
        }
"
"  @Test
  public void testISDConfigsWithOverrides() {
    KafkaSystemDescriptor sd = new KafkaSystemDescriptor(""kafka"");

    KafkaInputDescriptor<KV<String, Integer>> isd =
        sd.getInputDescriptor(""input-stream"", KVSerde.of(new StringSerde(), new IntegerSerde()))
            .withConsumerAutoOffsetReset(""largest"")
            .withConsumerFetchMessageMaxBytes(1024 * 1024);

    Map<String, String> generatedConfigs = isd.toConfig();
    assertEquals(""kafka"", generatedConfigs.get(""streams.input-stream.samza.system""));
    assertEquals(""largest"", generatedConfigs.get(""systems.kafka.streams.input-stream.consumer.auto.offset.reset""));
    assertEquals(""1048576"", generatedConfigs.get(""systems.kafka.streams.input-stream.consumer.fetch.message.max.bytes""));
  }
"
"  @Test
  public void testISDConfigsWithDefaults() {
    KafkaSystemDescriptor sd = new KafkaSystemDescriptor(""kafka"")
        .withConsumerZkConnect(ImmutableList.of(""localhost:123""))
        .withProducerBootstrapServers(ImmutableList.of(""localhost:567"", ""localhost:890""));

    KafkaInputDescriptor<KV<String, Integer>> isd =
        sd.getInputDescriptor(""input-stream"", KVSerde.of(new StringSerde(), new IntegerSerde()));

    Map<String, String> generatedConfigs = isd.toConfig();
    assertEquals(""kafka"", generatedConfigs.get(""streams.input-stream.samza.system""));
    assertEquals(1, generatedConfigs.size()); // verify that there are no other configs
  }
"
"  @Test
  public void testSDConfigsWithOverrides() {
    KafkaSystemDescriptor sd =
        new KafkaSystemDescriptor(""kafka"")
            .withConsumerZkConnect(ImmutableList.of(""localhost:1234""))
            .withProducerBootstrapServers(ImmutableList.of(""localhost:567"", ""localhost:890""))
            .withDefaultStreamOffsetDefault(SystemStreamMetadata.OffsetType.OLDEST)
            .withConsumerAutoOffsetReset(""smallest"")
            .withConsumerFetchMessageMaxBytes(1024 * 1024)
            .withSamzaFetchThreshold(10000)
            .withSamzaFetchThresholdBytes(1024 * 1024)
            .withConsumerConfigs(ImmutableMap.of(""custom-consumer-config-key"", ""custom-consumer-config-value""))
            .withProducerConfigs(ImmutableMap.of(""custom-producer-config-key"", ""custom-producer-config-value""))
            .withDefaultStreamConfigs(ImmutableMap.of(""custom-stream-config-key"", ""custom-stream-config-value""));

    Map<String, String> generatedConfigs = sd.toConfig();
    assertEquals(""org.apache.samza.system.kafka.KafkaSystemFactory"", generatedConfigs.get(""systems.kafka.samza.factory""));
    assertEquals(""localhost:1234"", generatedConfigs.get(""systems.kafka.consumer.zookeeper.connect""));
    assertEquals(""localhost:567,localhost:890"", generatedConfigs.get(""systems.kafka.producer.bootstrap.servers""));
    assertEquals(""smallest"", generatedConfigs.get(""systems.kafka.consumer.auto.offset.reset""));
    assertEquals(""1048576"", generatedConfigs.get(""systems.kafka.consumer.fetch.message.max.bytes""));
    assertEquals(""10000"", generatedConfigs.get(""systems.kafka.samza.fetch.threshold""));
    assertEquals(""1048576"", generatedConfigs.get(""systems.kafka.samza.fetch.threshold.bytes""));
    assertEquals(""custom-consumer-config-value"", generatedConfigs.get(""systems.kafka.consumer.custom-consumer-config-key""));
    assertEquals(""custom-producer-config-value"", generatedConfigs.get(""systems.kafka.producer.custom-producer-config-key""));
    assertEquals(""custom-stream-config-value"", generatedConfigs.get(""systems.kafka.default.stream.custom-stream-config-key""));
    assertEquals(""oldest"", generatedConfigs.get(""systems.kafka.default.stream.samza.offset.default""));
    assertEquals(11, generatedConfigs.size());
  }
"
"  @Test
  public void testSDConfigsWithoutOverrides() {
    KafkaSystemDescriptor sd = new KafkaSystemDescriptor(""kafka"");

    Map<String, String> generatedConfigs = sd.toConfig();
    assertEquals(""org.apache.samza.system.kafka.KafkaSystemFactory"", generatedConfigs.get(""systems.kafka.samza.factory""));
    assertEquals(1, generatedConfigs.size()); // verify that there are no other configs
  }
"
"  @Test
  public void testGetIntermediateStreamProperties() {
    Map<String, String> config = new HashMap<>();
    KafkaSystemFactory factory = new KafkaSystemFactory();
    Map<String, Properties> properties = JavaConversions.mapAsJavaMap(
        factory.getIntermediateStreamProperties(new MapConfig(config)));
    assertTrue(properties.isEmpty());

    // no properties for stream
    config.put(""streams.test.samza.intermediate"", ""true"");
    config.put(""streams.test.compression.type"", ""lz4""); //some random config
    properties = JavaConversions.mapAsJavaMap(
        factory.getIntermediateStreamProperties(new MapConfig(config)));
    assertTrue(properties.isEmpty());

    config.put(ApplicationConfig.APP_MODE, ApplicationConfig.ApplicationMode.BATCH.name());

    KafkaSystemAdmin admin = createSystemAdmin(SYSTEM(), config);
    StreamSpec spec = new StreamSpec(""test"", ""test"", SYSTEM(),
        Collections.singletonMap(""replication.factor"", ""1""));
    KafkaStreamSpec kspec = admin.toKafkaSpec(spec);

    Properties prop = kspec.getProperties();
    assertEquals(prop.getProperty(""retention.ms""), String.valueOf(KafkaConfig.DEFAULT_RETENTION_MS_FOR_BATCH()));
    assertEquals(prop.getProperty(""compression.type""), ""lz4"");

    // replication.factor should be removed from the properties and set on the spec directly
    assertEquals(kspec.getReplicationFactor(), 1);
    assertNull(prop.getProperty(""replication.factor""));
  }
"
"  @Test
  public void testGetCheckpointTopicProperties() {
    Map<String, String> config = new HashMap<>();
    Properties properties = new KafkaConfig(new MapConfig(config)).getCheckpointTopicProperties();

    assertEquals(properties.getProperty(""cleanup.policy""), ""compact"");
    assertEquals(properties.getProperty(""segment.bytes""), String.valueOf(KafkaConfig.DEFAULT_CHECKPOINT_SEGMENT_BYTES()));

    config.put(ApplicationConfig.APP_MODE, ApplicationConfig.ApplicationMode.BATCH.name());
    properties = new KafkaConfig(new MapConfig(config)).getCheckpointTopicProperties();

    assertEquals(properties.getProperty(""cleanup.policy""), ""compact,delete"");
    assertEquals(properties.getProperty(""segment.bytes""), String.valueOf(KafkaConfig.DEFAULT_CHECKPOINT_SEGMENT_BYTES()));
    assertEquals(properties.getProperty(""retention.ms""), String.valueOf(KafkaConfig.DEFAULT_RETENTION_MS_FOR_BATCH()));
  }
"
"  @Test
  public void testBinaryCompatibility() {
    KafkaCheckpointLogKey logKey1 = new KafkaCheckpointLogKey(KafkaCheckpointLogKey.CHECKPOINT_V1_KEY_TYPE,
        new TaskName(""Partition 0""), GroupByPartitionFactory.class.getCanonicalName());
    KafkaCheckpointLogKeySerde checkpointSerde = new KafkaCheckpointLogKeySerde();

    byte[] bytes = (""{\""systemstreampartition-grouper-factory\"""" +
        "":\""org.apache.samza.container.grouper.stream.GroupByPartitionFactory\"",\""taskName\"":\""Partition 0\"","" +
        ""\""type\"":\""checkpoint\""}"").getBytes();

    // test that the checkpoints returned by the Serde are byte-wise identical to an actual checkpoint in Kafka
    Assert.assertEquals(true, Arrays.equals(bytes, checkpointSerde.toBytes(logKey1)));
  }
"
"  @Test
  public void testSerde() {
    KafkaCheckpointLogKey key = new KafkaCheckpointLogKey(KafkaCheckpointLogKey.CHECKPOINT_V1_KEY_TYPE,
        new TaskName(""Partition 0""), GroupByPartitionFactory.class.getCanonicalName());
    KafkaCheckpointLogKeySerde checkpointSerde = new KafkaCheckpointLogKeySerde();

    // test that deserialize(serialize(k)) == k
    Assert.assertEquals(key, checkpointSerde.fromBytes(checkpointSerde.toBytes(key)));
  }
"
"  @Test
  public void testCheckpointTypeV2() {
    KafkaCheckpointLogKey keyV2 = new KafkaCheckpointLogKey(KafkaCheckpointLogKey.CHECKPOINT_V2_KEY_TYPE, new TaskName(""Partition 0""),
        GroupByPartitionFactory.class.getCanonicalName());
    KafkaCheckpointLogKeySerde checkpointKeySerde = new KafkaCheckpointLogKeySerde();

    // test that deserialize(serialize(k)) == k
    Assert.assertEquals(keyV2, checkpointKeySerde.fromBytes(checkpointKeySerde.toBytes(keyV2)));
  }
"
"  @Test
  public void testForwardsCompatibility() {
    // Set the key to another value, this is for the future if we want to support multiple checkpoint keys
    // we do not want to throw in the Serdes layer, but must be validated in the CheckpointManager
    KafkaCheckpointLogKey key = new KafkaCheckpointLogKey(""checkpoint-v2"",
        new TaskName(""Partition 0""), GroupByPartitionFactory.class.getCanonicalName());
    KafkaCheckpointLogKeySerde checkpointSerde = new KafkaCheckpointLogKeySerde();

    // test that deserialize(serialize(k)) == k
    Assert.assertEquals(key, checkpointSerde.fromBytes(checkpointSerde.toBytes(key)));
  }
"
"  @Test(expected = TopicAlreadyMarkedForDeletionException.class)
  public void testCreateResourcesTopicCreationError() {
    setupSystemFactory(config());
    // throw an exception during createStream
    doThrow(new TopicAlreadyMarkedForDeletionException(""invalid stream"")).when(this.createResourcesSystemAdmin)
        .createStream(CHECKPOINT_SPEC);
    KafkaCheckpointManager checkpointManager = buildKafkaCheckpointManager(true, config());
    // expect an exception during startup
    checkpointManager.createResources();
  }
"
"  @Test(expected = StreamValidationException.class)
  public void testCreateResourcesTopicValidationError() {
    setupSystemFactory(config());
    // throw an exception during validateStream
    doThrow(new StreamValidationException(""invalid stream"")).when(this.createResourcesSystemAdmin)
        .validateStream(CHECKPOINT_SPEC);
    KafkaCheckpointManager checkpointManager = buildKafkaCheckpointManager(true, config());
    // expect an exception during startup
    checkpointManager.createResources();
  }
"
"  @Test(expected = SamzaException.class)
  public void testReadFailsOnSerdeExceptions() throws InterruptedException {
    setupSystemFactory(config());
    List<IncomingMessageEnvelope> checkpointEnvelopes =
        ImmutableList.of(newCheckpointV1Envelope(TASK0, buildCheckpointV1(INPUT_SSP0, ""0""), ""0""));
    setupConsumer(checkpointEnvelopes);
    // wire up an exception throwing serde with the checkpointManager
    CheckpointV1Serde checkpointV1Serde = mock(CheckpointV1Serde.class);
    doThrow(new RuntimeException(""serde failed"")).when(checkpointV1Serde).fromBytes(any());
    KafkaCheckpointManager checkpointManager =
        new KafkaCheckpointManager(CHECKPOINT_SPEC, this.systemFactory, true, config(), this.metricsRegistry,
            checkpointV1Serde, CHECKPOINT_V2_SERDE, KAFKA_CHECKPOINT_LOG_KEY_SERDE);
    checkpointManager.register(TASK0);

    // expect an exception
    checkpointManager.readLastCheckpoint(TASK0);
  }
"
"  @Test
  public void testReadSucceedsOnKeySerdeExceptionsWhenValidationIsDisabled() throws InterruptedException {
    setupSystemFactory(config());
    List<IncomingMessageEnvelope> checkpointEnvelopes =
        ImmutableList.of(newCheckpointV1Envelope(TASK0, buildCheckpointV1(INPUT_SSP0, ""0""), ""0""));
    setupConsumer(checkpointEnvelopes);
    // wire up an exception throwing serde with the checkpointManager
    CheckpointV1Serde checkpointV1Serde = mock(CheckpointV1Serde.class);
    doThrow(new RuntimeException(""serde failed"")).when(checkpointV1Serde).fromBytes(any());
    KafkaCheckpointManager checkpointManager =
        new KafkaCheckpointManager(CHECKPOINT_SPEC, this.systemFactory, false, config(), this.metricsRegistry,
            checkpointV1Serde, CHECKPOINT_V2_SERDE, KAFKA_CHECKPOINT_LOG_KEY_SERDE);
    checkpointManager.register(TASK0);

    // expect the read to succeed in spite of the exception from ExceptionThrowingSerde
    assertNull(checkpointManager.readLastCheckpoint(TASK0));
  }
"
"  @Test
  public void testStart() {
    setupSystemFactory(config());
    String oldestOffset = ""1"";
    String newestOffset = ""2"";
    SystemStreamMetadata checkpointTopicMetadata = new SystemStreamMetadata(CHECKPOINT_TOPIC,
        ImmutableMap.of(new Partition(0), new SystemStreamPartitionMetadata(oldestOffset, newestOffset,
            Integer.toString(Integer.parseInt(newestOffset) + 1))));
    when(this.systemAdmin.getSystemStreamMetadata(Collections.singleton(CHECKPOINT_TOPIC))).thenReturn(
        ImmutableMap.of(CHECKPOINT_TOPIC, checkpointTopicMetadata));

    KafkaCheckpointManager checkpointManager = buildKafkaCheckpointManager(true, config());

    checkpointManager.start();

    verify(this.systemProducer).start();
    verify(this.systemAdmin).start();
    verify(this.systemConsumer).register(CHECKPOINT_SSP, oldestOffset);
    verify(this.systemConsumer).start();
  }
"
"  @Test
  public void testRegister() {
    setupSystemFactory(config());
    KafkaCheckpointManager kafkaCheckpointManager = buildKafkaCheckpointManager(true, config());
    kafkaCheckpointManager.register(TASK0);
    verify(this.systemProducer).register(TASK0.getTaskName());
  }
"
"  @Test
  public void testStop() {
    setupSystemFactory(config());
    KafkaCheckpointManager checkpointManager = buildKafkaCheckpointManager(true, config());
    checkpointManager.stop();
    verify(this.systemProducer).stop();
    // default configuration for stopConsumerAfterFirstRead means that consumer is not stopped here
    verify(this.systemConsumer, never()).stop();
    verify(this.systemAdmin).stop();
  }
"
"  @Test
  public void testWriteCheckpointShouldRecreateSystemProducerOnFailure() {
    setupSystemFactory(config());
    SystemProducer secondKafkaProducer = mock(SystemProducer.class);
    // override default mock behavior to return a second producer on the second call to create a producer
    when(this.systemFactory.getProducer(CHECKPOINT_SYSTEM, config(), this.metricsRegistry,
        KafkaCheckpointManager.class.getSimpleName())).thenReturn(this.systemProducer, secondKafkaProducer);
    // first producer throws an exception on flush
    doThrow(new RuntimeException(""flush failed"")).when(this.systemProducer).flush(TASK0.getTaskName());
    KafkaCheckpointManager kafkaCheckpointManager = buildKafkaCheckpointManager(true, config());
    kafkaCheckpointManager.register(TASK0);

    CheckpointV1 checkpointV1 = buildCheckpointV1(INPUT_SSP0, ""0"");
    kafkaCheckpointManager.writeCheckpoint(TASK0, checkpointV1);

    // first producer should be stopped
    verify(this.systemProducer).stop();
    // register and start the second producer
    verify(secondKafkaProducer).register(TASK0.getTaskName());
    verify(secondKafkaProducer).start();
    // check that the second producer was given the message to send out
    ArgumentCaptor<OutgoingMessageEnvelope> outgoingMessageEnvelopeArgumentCaptor =
        ArgumentCaptor.forClass(OutgoingMessageEnvelope.class);
    verify(secondKafkaProducer).send(eq(TASK0.getTaskName()), outgoingMessageEnvelopeArgumentCaptor.capture());
    assertEquals(CHECKPOINT_SSP, outgoingMessageEnvelopeArgumentCaptor.getValue().getSystemStream());
    assertEquals(new KafkaCheckpointLogKey(KafkaCheckpointLogKey.CHECKPOINT_V1_KEY_TYPE, TASK0, GROUPER_FACTORY_CLASS),
        KAFKA_CHECKPOINT_LOG_KEY_SERDE.fromBytes((byte[]) outgoingMessageEnvelopeArgumentCaptor.getValue().getKey()));
    assertEquals(checkpointV1,
        CHECKPOINT_V1_SERDE.fromBytes((byte[]) outgoingMessageEnvelopeArgumentCaptor.getValue().getMessage()));
    verify(secondKafkaProducer).flush(TASK0.getTaskName());
  }
"
"  @Test
  public void testCreateResources() {
    setupSystemFactory(config());
    KafkaCheckpointManager kafkaCheckpointManager = buildKafkaCheckpointManager(true, config());
    kafkaCheckpointManager.createResources();

    verify(this.createResourcesSystemAdmin).start();
    verify(this.createResourcesSystemAdmin).createStream(CHECKPOINT_SPEC);
    verify(this.createResourcesSystemAdmin).validateStream(CHECKPOINT_SPEC);
    verify(this.createResourcesSystemAdmin).stop();
  }
"
"  @Test
  public void testCreateResourcesSkipValidation() {
    setupSystemFactory(config());
    KafkaCheckpointManager kafkaCheckpointManager = buildKafkaCheckpointManager(false, config());
    kafkaCheckpointManager.createResources();

    verify(this.createResourcesSystemAdmin).start();
    verify(this.createResourcesSystemAdmin).createStream(CHECKPOINT_SPEC);
    verify(this.createResourcesSystemAdmin, never()).validateStream(CHECKPOINT_SPEC);
    verify(this.createResourcesSystemAdmin).stop();
  }
"
"  @Test
  public void testReadEmpty() throws InterruptedException {
    setupSystemFactory(config());
    setupConsumer(ImmutableList.of());
    KafkaCheckpointManager kafkaCheckpointManager = buildKafkaCheckpointManager(true, config());
    kafkaCheckpointManager.register(TASK0);
    assertNull(kafkaCheckpointManager.readLastCheckpoint(TASK0));
  }
"
"  @Test
  public void testReadCheckpointV1() throws InterruptedException {
    setupSystemFactory(config());
    CheckpointV1 checkpointV1 = buildCheckpointV1(INPUT_SSP0, ""0"");
    List<IncomingMessageEnvelope> checkpointEnvelopes =
        ImmutableList.of(newCheckpointV1Envelope(TASK0, checkpointV1, ""0""));
    setupConsumer(checkpointEnvelopes);
    KafkaCheckpointManager kafkaCheckpointManager = buildKafkaCheckpointManager(true, config());
    kafkaCheckpointManager.register(TASK0);
    Checkpoint actualCheckpoint = kafkaCheckpointManager.readLastCheckpoint(TASK0);
    assertEquals(checkpointV1, actualCheckpoint);
  }
"
"  @Test
  public void testReadIgnoreCheckpointV2WhenV1Enabled() throws InterruptedException {
    setupSystemFactory(config());
    CheckpointV1 checkpointV1 = buildCheckpointV1(INPUT_SSP0, ""0"");
    List<IncomingMessageEnvelope> checkpointEnvelopes =
        ImmutableList.of(newCheckpointV1Envelope(TASK0, checkpointV1, ""0""),
            newCheckpointV2Envelope(TASK0, buildCheckpointV2(INPUT_SSP0, ""1""), ""1""));
    setupConsumer(checkpointEnvelopes);
    // default is to only read CheckpointV1
    KafkaCheckpointManager kafkaCheckpointManager = buildKafkaCheckpointManager(true, config());
    kafkaCheckpointManager.register(TASK0);
    Checkpoint actualCheckpoint = kafkaCheckpointManager.readLastCheckpoint(TASK0);
    assertEquals(checkpointV1, actualCheckpoint);
  }
"
"  @Test
  public void testReadCheckpointV2() throws InterruptedException {
    Config config = config(ImmutableMap.of(TaskConfig.CHECKPOINT_READ_VERSIONS, ""1,2""));
    setupSystemFactory(config);
    CheckpointV2 checkpointV2 = buildCheckpointV2(INPUT_SSP0, ""0"");
    List<IncomingMessageEnvelope> checkpointEnvelopes =
        ImmutableList.of(newCheckpointV2Envelope(TASK0, checkpointV2, ""0""));
    setupConsumer(checkpointEnvelopes);
    KafkaCheckpointManager kafkaCheckpointManager = buildKafkaCheckpointManager(true, config);
    kafkaCheckpointManager.register(TASK0);
    Checkpoint actualCheckpoint = kafkaCheckpointManager.readLastCheckpoint(TASK0);
    assertEquals(checkpointV2, actualCheckpoint);
  }
"
"  @Test
  public void testReadCheckpointPriority() throws InterruptedException {
    Config config = config(ImmutableMap.of(TaskConfig.CHECKPOINT_READ_VERSIONS, ""2,1""));
    setupSystemFactory(config);
    CheckpointV2 checkpointV2 = buildCheckpointV2(INPUT_SSP0, ""1"");
    List<IncomingMessageEnvelope> checkpointEnvelopes =
        ImmutableList.of(newCheckpointV1Envelope(TASK0, buildCheckpointV1(INPUT_SSP0, ""0""), ""0""),
            newCheckpointV2Envelope(TASK0, checkpointV2, ""1""));
    setupConsumer(checkpointEnvelopes);
    KafkaCheckpointManager kafkaCheckpointManager = buildKafkaCheckpointManager(true, config);
    kafkaCheckpointManager.register(TASK0);
    Checkpoint actualCheckpoint = kafkaCheckpointManager.readLastCheckpoint(TASK0);
    assertEquals(checkpointV2, actualCheckpoint);
  }
"
"  @Test
  public void testReadMultipleCheckpointsMultipleSSP() throws InterruptedException {
    setupSystemFactory(config());
    KafkaCheckpointManager checkpointManager = buildKafkaCheckpointManager(true, config());
    checkpointManager.register(TASK0);
    checkpointManager.register(TASK1);

    // mock out a consumer that returns 5 checkpoint IMEs for each SSP
    int newestOffset = 5;
    int checkpointOffsetCounter = 0;
    List<List<IncomingMessageEnvelope>> pollOutputs = new ArrayList<>();
    for (int offset = 1; offset <= newestOffset; offset++) {
      pollOutputs.add(ImmutableList.of(
          // use regular offset value for INPUT_SSP0
          newCheckpointV1Envelope(TASK0, buildCheckpointV1(INPUT_SSP0, Integer.toString(offset)),
              Integer.toString(checkpointOffsetCounter++)),
          // use (offset * 2) value for INPUT_SSP1 so offsets are different from INPUT_SSP0
          newCheckpointV1Envelope(TASK1, buildCheckpointV1(INPUT_SSP1, Integer.toString(offset * 2)),
              Integer.toString(checkpointOffsetCounter++))));
    }
    setupConsumerMultiplePoll(pollOutputs);

    assertEquals(buildCheckpointV1(INPUT_SSP0, Integer.toString(newestOffset)),
        checkpointManager.readLastCheckpoint(TASK0));
    assertEquals(buildCheckpointV1(INPUT_SSP1, Integer.toString(newestOffset * 2)),
        checkpointManager.readLastCheckpoint(TASK1));
    // check expected number of polls (+1 is for the final empty poll), and the checkpoint is the newest message
    verify(this.systemConsumer, times(newestOffset + 1)).poll(ImmutableSet.of(CHECKPOINT_SSP),
        SystemConsumer.BLOCK_ON_OUTSTANDING_MESSAGES);
  }
"
"  @Test
  public void testReadMultipleCheckpointsUpgradeCheckpointVersion() throws InterruptedException {
    Config config = config(ImmutableMap.of(TaskConfig.CHECKPOINT_READ_VERSIONS, ""2,1""));
    setupSystemFactory(config);
    KafkaCheckpointManager kafkaCheckpointManager = buildKafkaCheckpointManager(true, config);
    kafkaCheckpointManager.register(TASK0);
    kafkaCheckpointManager.register(TASK1);

    List<IncomingMessageEnvelope> checkpointEnvelopesV1 =
        ImmutableList.of(newCheckpointV1Envelope(TASK0, buildCheckpointV1(INPUT_SSP0, ""0""), ""0""),
            newCheckpointV1Envelope(TASK1, buildCheckpointV1(INPUT_SSP1, ""0""), ""1""));
    CheckpointV2 ssp0CheckpointV2 = buildCheckpointV2(INPUT_SSP0, ""10"");
    CheckpointV2 ssp1CheckpointV2 = buildCheckpointV2(INPUT_SSP1, ""11"");
    List<IncomingMessageEnvelope> checkpointEnvelopesV2 =
        ImmutableList.of(newCheckpointV2Envelope(TASK0, ssp0CheckpointV2, ""2""),
            newCheckpointV2Envelope(TASK1, ssp1CheckpointV2, ""3""));
    setupConsumerMultiplePoll(ImmutableList.of(checkpointEnvelopesV1, checkpointEnvelopesV2));
    assertEquals(ssp0CheckpointV2, kafkaCheckpointManager.readLastCheckpoint(TASK0));
    assertEquals(ssp1CheckpointV2, kafkaCheckpointManager.readLastCheckpoint(TASK1));
    // 2 polls for actual checkpoints, 1 final empty poll
    verify(this.systemConsumer, times(3)).poll(ImmutableSet.of(CHECKPOINT_SSP),
        SystemConsumer.BLOCK_ON_OUTSTANDING_MESSAGES);
  }
"
"    @Test
    public void testPluginInstalled() {
        try (TransportClient client = new PreBuiltXPackTransportClient(Settings.EMPTY)) {
            Settings settings = client.settings();
            assertEquals(SecurityField.NAME4, NetworkModule.TRANSPORT_TYPE_SETTING.get(settings));
        }
    }
"
"@TestLogging(""org.elasticsearch.client:TRACE,tracer:TRACE"")
    public void cleanExporters() throws Exception {
        Request request = new Request(""PUT"", ""/_cluster/settings"");
        request.setJsonEntity(Strings.toString(jsonBuilder().startObject()
                .startObject(""transient"")
                    .nullField(""xpack.monitoring.exporters.*"")
                .endObject().endObject()));
        adminClient().performRequest(request);
        adminClient().performRequest(new Request(""DELETE"", ""/.watch*""));
    }
"
"@TestLogging(""org.elasticsearch.client:TRACE"")
    public void waitForSecuritySetup() throws Exception {

        String masterNode = null;
        String catNodesResponse = EntityUtils.toString(
                client().performRequest(""GET"", ""/_cat/nodes?h=id,master"").getEntity(),
                StandardCharsets.UTF_8
        );
        for (String line : catNodesResponse.split(""\n"")) {
            int indexOfStar = line.indexOf('*'); // * in the node's output denotes it is master
            if (indexOfStar != -1) {
                masterNode = line.substring(0, indexOfStar).trim();
                break;
            }
        }
        assertNotNull(masterNode);
        final String masterNodeId = masterNode;

        assertBusy(() -> {
            try {
                Response nodeDetailsResponse = client().performRequest(""GET"", ""/_nodes"");
                ObjectPath path = ObjectPath.createFromResponse(nodeDetailsResponse);
                Map<String, Object> nodes = path.evaluate(""nodes"");
                assertThat(nodes.size(), greaterThanOrEqualTo(2));
                String masterVersion = null;
                for (String key : nodes.keySet()) {
                    // get the ES version number master is on
                    if (key.startsWith(masterNodeId)) {
                        masterVersion = path.evaluate(""nodes."" + key + "".version"");
                        break;
                    }
                }
                assertNotNull(masterVersion);
                final String masterTemplateVersion = masterVersion;

                Response response = client().performRequest(""GET"", ""/_cluster/state/metadata"");
                ObjectPath objectPath = ObjectPath.createFromResponse(response);
                final String mappingsPath = ""metadata.templates.security-index-template.mappings"";
                Map<String, Object> mappings = objectPath.evaluate(mappingsPath);
                assertNotNull(mappings);
                assertThat(mappings.size(), greaterThanOrEqualTo(1));
                for (String key : mappings.keySet()) {
                    String templateVersion = objectPath.evaluate(mappingsPath + ""."" + key + """" +
                            ""._meta.security-version"");
                    final Version mVersion = Version.fromString(masterTemplateVersion);
                    final Version tVersion = Version.fromString(templateVersion);
                    assertEquals(mVersion, tVersion);
                }
            } catch (Exception e) {
                throw new AssertionError(""failed to get cluster state"", e);
            }
        });

        nodes = buildNodeAndVersions();
        logger.info(""Nodes in cluster before test: bwc [{}], new [{}], master [{}]"", nodes.getBWCNodes(), nodes.getNewNodes(),
                nodes.getMaster());

        Map<String, String> params = Collections.singletonMap(""error_trace"", ""true"");
        executeAgainstMasterNode(client -> {
            // create a watch before each test, most of the time this is just overwriting...
            assertOK(client.performRequest(""PUT"", ""/_xpack/watcher/watch/my-watch"", params, entity));
            // just a check to see if we can execute a watch, purely optional
            if (randomBoolean()) {
                assertOK(client.performRequest(""POST"", ""/_xpack/watcher/watch/my-watch/_execute"", params,
                        new StringEntity(""{ \""record_execution\"" : true }"", ContentType.APPLICATION_JSON)));
            }
            if (randomBoolean()) {
                Map<String, String> ignore404Params = MapBuilder.newMapBuilder(params).put(""ignore"", ""404"").immutableMap();
                Response indexExistsResponse = client.performRequest(""HEAD"", ""/.triggered_watches"", ignore404Params);
                if (indexExistsResponse.getStatusLine().getStatusCode() == 404) {
                    logger.info(""Created triggered watches index to ensure it gets upgraded"");
                    client.performRequest(""PUT"", ""/.triggered_watches"");
                }
            }
        });

        // helping debugging output
        executeAgainstMasterNode(client -> {
            Map<String, String> filterPathParams = MapBuilder.newMapBuilder(params)
                    .put(""filter_path"", ""*.template,*.index_patterns"").immutableMap();
            Response r = client.performRequest(""GET"", ""_template/*watch*"", filterPathParams);
            logger.info(""existing watcher templates response [{}]"", EntityUtils.toString(r.getEntity(), StandardCharsets.UTF_8));
        });

        // set logging to debug
//        executeAgainstMasterNode(client -> {
//            StringEntity entity = new StringEntity(""{ \""transient\"" : { \""logger.org.elasticsearch.xpack.watcher\"" : \""TRACE\"" } }"",
//                    ContentType.APPLICATION_JSON);
//            Response response = client.performRequest(""PUT"", ""_cluster/settings"", params, entity);
//            logger.info(""cluster update settings response [{}]"", EntityUtils.toString(response.getEntity(), StandardCharsets.UTF_8));
//        });
    }
"
"@TestLogging(""org.elasticsearch.xpack.core.ssl.SSLService:TRACE"")
    public void init() throws Exception {
        Path caPath = getDataPath(LDAPCACERT_PATH);
        /*
         * Prior to each test we reinitialize the socket factory with a new SSLService so that we get a new SSLContext.
         * If we re-use a SSLContext, previously connected sessions can get re-established which breaks hostname
         * verification tests since a re-established connection does not perform hostname verification.
         */
        globalSettings = Settings.builder()
            .put(""path.home"", createTempDir())
            .put(""xpack.ssl.certificate_authorities"", caPath)
            .build();
        threadPool = new TestThreadPool(""LdapUserSearchSessionFactoryTests"");
    }
"
"    @TestGroup(enabled = true, sysProperty = ESRestTestCase.TESTS_REST)
            public int compare(RestTestCandidate o1, RestTestCandidate o2) {
                return o1.getTestPath().compareTo(o2.getTestPath());
            }
"
"@TestLogging(""org.elasticsearch.xpack.security.authc.ldap.support:DEBUG"")
    public void init() throws Exception {
        threadPool = new TestThreadPool(""SessionFactoryLoadBalancingTests thread pool"");
    }
"
"    @TestLogging(""org.elasticsearch.xpack.security.authc:DEBUG"")
    public void testExpiredTokensDeletedAfterExpiration() throws Exception {
        final Client client = client().filterWithHeader(Collections.singletonMap(""Authorization"",
                UsernamePasswordToken.basicAuthHeaderValue(SecuritySettingsSource.TEST_SUPERUSER,
                        SecuritySettingsSourceField.TEST_PASSWORD_SECURE_STRING)));
        SecurityClient securityClient = new SecurityClient(client);
        CreateTokenResponse response = securityClient.prepareCreateToken()
                .setGrantType(""password"")
                .setUsername(SecuritySettingsSource.TEST_USER_NAME)
                .setPassword(new SecureString(SecuritySettingsSourceField.TEST_PASSWORD.toCharArray()))
                .get();

        Instant created = Instant.now();

        InvalidateTokenResponse invalidateResponse = securityClient
                .prepareInvalidateToken(response.getTokenString())
                .setType(InvalidateTokenRequest.Type.ACCESS_TOKEN)
                .get();
        assertTrue(invalidateResponse.isCreated());
        AtomicReference<String> docId = new AtomicReference<>();
        assertBusy(() -> {
            SearchResponse searchResponse = client.prepareSearch(SecurityIndexManager.SECURITY_INDEX_NAME)
                    .setSource(SearchSourceBuilder.searchSource()
                            .query(QueryBuilders.termQuery(""doc_type"", TokenService.INVALIDATED_TOKEN_DOC_TYPE)))
                    .setSize(1)
                    .setTerminateAfter(1)
                    .get();
            assertThat(searchResponse.getHits().getTotalHits(), equalTo(1L));
            docId.set(searchResponse.getHits().getAt(0).getId());
        });

        // hack doc to modify the time to the day before
        Instant dayBefore = created.minus(1L, ChronoUnit.DAYS);
        assertTrue(Instant.now().isAfter(dayBefore));
        client.prepareUpdate(SecurityIndexManager.SECURITY_INDEX_NAME, ""doc"", docId.get())
                .setDoc(""expiration_time"", dayBefore.toEpochMilli())
                .setRefreshPolicy(WriteRequest.RefreshPolicy.IMMEDIATE)
                .get();

        AtomicBoolean deleteTriggered = new AtomicBoolean(false);
        assertBusy(() -> {
            if (deleteTriggered.compareAndSet(false, true)) {
                // invalidate a invalid token... doesn't matter that it is bad... we just want this action to trigger the deletion
                try {
                    securityClient.prepareInvalidateToken(""fooobar"")
                            .setType(randomFrom(InvalidateTokenRequest.Type.values()))
                            .execute()
                            .actionGet();
                } catch (ElasticsearchSecurityException e) {
                    assertEquals(""token malformed"", e.getMessage());
                }
            }
            client.admin().indices().prepareRefresh(SecurityIndexManager.SECURITY_INDEX_NAME).get();
            SearchResponse searchResponse = client.prepareSearch(SecurityIndexManager.SECURITY_INDEX_NAME)
                    .setSource(SearchSourceBuilder.searchSource()
                            .query(QueryBuilders.termQuery(""doc_type"", TokenService.INVALIDATED_TOKEN_DOC_TYPE)))
                    .setSize(0)
                    .setTerminateAfter(1)
                    .get();
            assertThat(searchResponse.getHits().getTotalHits(), equalTo(0L));
        }, 30, TimeUnit.SECONDS);
    }
"
"@TestLogging(""org.elasticsearch.xpack.security.authz.store.NativePrivilegeStore:TRACE"")
    public void setup() {
        requests = new ArrayList<>();
        listener = new AtomicReference<>();
        client = new NoOpClient(getTestName()) {
            @Override
            protected <Request extends ActionRequest,
                Response extends ActionResponse,
                RequestBuilder extends ActionRequestBuilder<Request, Response, RequestBuilder>>
            void doExecute(Action<Request, Response, RequestBuilder> action, Request request, ActionListener<Response> listener) {
                NativePrivilegeStoreTests.this.requests.add(request);
                NativePrivilegeStoreTests.this.listener.set(listener);
            }
        };
        final SecurityIndexManager securityIndex = mock(SecurityIndexManager.class);
        when(securityIndex.freeze()).thenReturn(securityIndex);
        when(securityIndex.indexExists()).thenReturn(true);
        when(securityIndex.isAvailable()).thenReturn(true);
        Mockito.doAnswer(invocationOnMock -> {
            assertThat(invocationOnMock.getArguments().length, equalTo(2));
            assertThat(invocationOnMock.getArguments()[1], instanceOf(Runnable.class));
            ((Runnable) invocationOnMock.getArguments()[1]).run();
            return null;
        }).when(securityIndex).prepareIndexIfNeededThenExecute(any(Consumer.class), any(Runnable.class));
        Mockito.doAnswer(invocationOnMock -> {
            assertThat(invocationOnMock.getArguments().length, equalTo(2));
            assertThat(invocationOnMock.getArguments()[1], instanceOf(Runnable.class));
            ((Runnable) invocationOnMock.getArguments()[1]).run();
            return null;
        }).when(securityIndex).checkIndexVersionThenExecute(any(Consumer.class), any(Runnable.class));
        store = new NativePrivilegeStore(Settings.EMPTY, client, securityIndex);
    }
"
"@TestLogging(""org.elasticsearch.xpack.security.action.user.TransportHasPrivilegesAction:TRACE,"" +
    public void setup() {
        final Settings settings = Settings.builder().build();
        user = new User(randomAlphaOfLengthBetween(4, 12));
        final ThreadPool threadPool = mock(ThreadPool.class);
        final ThreadContext threadContext = new ThreadContext(Settings.EMPTY);
        final TransportService transportService = new TransportService(Settings.EMPTY, mock(Transport.class), null,
            TransportService.NOOP_TRANSPORT_INTERCEPTOR, x -> null, null, Collections.emptySet());

        final Authentication authentication = mock(Authentication.class);
        threadContext.putTransient(AuthenticationField.AUTHENTICATION_KEY, authentication);
        when(threadPool.getThreadContext()).thenReturn(threadContext);

        when(authentication.getUser()).thenReturn(user);

        AuthorizationService authorizationService = mock(AuthorizationService.class);
        Mockito.doAnswer(invocationOnMock -> {
            ActionListener<Role> listener = (ActionListener<Role>) invocationOnMock.getArguments()[1];
            listener.onResponse(role);
            return null;
        }).when(authorizationService).roles(eq(user), any(ActionListener.class));

        applicationPrivileges = new ArrayList<>();
        NativePrivilegeStore privilegeStore = mock(NativePrivilegeStore.class);
        Mockito.doAnswer(inv -> {
            assertThat(inv.getArguments(), arrayWithSize(3));
            ActionListener<List<ApplicationPrivilegeDescriptor>> listener
                = (ActionListener<List<ApplicationPrivilegeDescriptor>>) inv.getArguments()[2];
            logger.info(""Privileges for ({}) are {}"", Arrays.toString(inv.getArguments()), applicationPrivileges);
            listener.onResponse(applicationPrivileges);
            return null;
        }).when(privilegeStore).getPrivileges(any(Collection.class), any(Collection.class), any(ActionListener.class));

        action = new TransportHasPrivilegesAction(settings, threadPool, transportService, mock(ActionFilters.class),
            mock(IndexNameExpressionResolver.class), authorizationService, privilegeStore);
    }
"
"@TestLogging(""org.elasticsearch.xpack.security.audit.index:TRACE"")
    public boolean transportSSLEnabled() {
        return sslEnabled;
    }
"
"@TestLogging(""org.elasticsearch.xpack.ssl.RestrictedTrustManager:DEBUG"")
    public Settings nodeSettings(int nodeOrdinal) {

        Settings parentSettings = super.nodeSettings(nodeOrdinal);
        Settings.Builder builder = Settings.builder()
                .put(parentSettings.filter((s) -> s.startsWith(""xpack.ssl."") == false))
                .put(nodeSSL);

        restrictionsPath = configPath.resolve(""trust_restrictions.yml"");
        restrictionsTmpPath = configPath.resolve(""trust_restrictions.tmp"");

        writeRestrictions(""*.trusted"");
        builder.put(""xpack.ssl.trust_restrictions.path"", restrictionsPath);
        builder.put(""resource.reload.interval.high"", RESOURCE_RELOAD_MILLIS + ""ms"");

        return builder.build();
    }
"
"@TestLogging(""org.elasticsearch.cluster.service:TRACE,org.elasticsearch.discovery.zen:TRACE,org.elasticsearch.action.search:TRACE,"" +
    public Settings nodeSettings(int nodeOrdinal) {
        return Settings.builder().put(super.nodeSettings(nodeOrdinal))
                .put(NetworkModule.HTTP_ENABLED.getKey(), true)
            .put(TestZenDiscovery.USE_MOCK_PINGS.getKey(), false)
                .build();
    }
"
"    @TestLogging(""org.elasticsearch.xpack.persistent:TRACE,org.elasticsearch.cluster.service:DEBUG,org.elasticsearch.xpack.ml.action:DEBUG"")
    public void testDedicatedMlNode() throws Exception {
        internalCluster().ensureAtMostNumDataNodes(0);
        // start 2 non ml node that will never get a job allocated. (but ml apis are accessible from this node)
        internalCluster().startNode(Settings.builder().put(MachineLearning.ML_ENABLED.getKey(), false));
        internalCluster().startNode(Settings.builder().put(MachineLearning.ML_ENABLED.getKey(), false));
        // start ml node
        if (randomBoolean()) {
            internalCluster().startNode(Settings.builder().put(MachineLearning.ML_ENABLED.getKey(), true));
        } else {
            // the default is based on 'xpack.ml.enabled', which is enabled in base test class.
            internalCluster().startNode();
        }
        ensureStableCluster(3);

        String jobId = ""dedicated-ml-node-job"";
        Job.Builder job = createJob(jobId, new ByteSizeValue(2, ByteSizeUnit.MB));
        PutJobAction.Request putJobRequest = new PutJobAction.Request(job);
        client().execute(PutJobAction.INSTANCE, putJobRequest).actionGet();

        OpenJobAction.Request openJobRequest = new OpenJobAction.Request(job.getId());
        client().execute(OpenJobAction.INSTANCE, openJobRequest).actionGet();
        assertBusy(() -> {
            ClusterState clusterState = client().admin().cluster().prepareState().get().getState();
            PersistentTasksCustomMetaData tasks = clusterState.getMetaData().custom(PersistentTasksCustomMetaData.TYPE);
            PersistentTask<?> task = tasks.getTask(MlTasks.jobTaskId(jobId));
            DiscoveryNode node = clusterState.nodes().resolveNode(task.getExecutorNode());
            assertThat(node.getAttributes(), hasEntry(MachineLearning.ML_ENABLED_NODE_ATTR, ""true""));
            assertThat(node.getAttributes(), hasEntry(MachineLearning.MAX_OPEN_JOBS_NODE_ATTR, ""20""));
            JobTaskState jobTaskState = (JobTaskState) task.getState();
            assertNotNull(jobTaskState);
            assertEquals(JobState.OPENED, jobTaskState.getState());
        });

        logger.info(""stop the only running ml node"");
        internalCluster().stopRandomNode(settings -> settings.getAsBoolean(MachineLearning.ML_ENABLED.getKey(), true));
        ensureStableCluster(2);
        assertBusy(() -> {
            // job should get and remain in a failed state and
            // the status remains to be opened as from ml we didn't had the chance to set the status to failed:
            assertJobTask(jobId, JobState.OPENED, false);
        });

        logger.info(""start ml node"");
        internalCluster().startNode(Settings.builder().put(MachineLearning.ML_ENABLED.getKey(), true));
        ensureStableCluster(3);
        assertBusy(() -> {
            // job should be re-opened:
            assertJobTask(jobId, JobState.OPENED, true);
        });
    }
"
"    @TestLogging(""org.elasticsearch.xpack.ml.action:DEBUG,org.elasticsearch.xpack.persistent:TRACE,"" +
    public void testLoseDedicatedMasterNode() throws Exception {
        internalCluster().ensureAtMostNumDataNodes(0);
        logger.info(""Starting dedicated master node..."");
        internalCluster().startNode(Settings.builder()
                .put(""node.master"", true)
                .put(""node.data"", false)
                .put(""node.ml"", false)
                .build());
        logger.info(""Starting ml and data node..."");
        String mlAndDataNode = internalCluster().startNode(Settings.builder()
                .put(""node.master"", false)
                .build());
        ensureStableClusterOnAllNodes(2);
        run(""lose-dedicated-master-node-job"", () -> {
            logger.info(""Stopping dedicated master node"");
            internalCluster().stopRandomNode(settings -> settings.getAsBoolean(""node.master"", false));
            assertBusy(() -> {
                ClusterState state = client(mlAndDataNode).admin().cluster().prepareState()
                        .setLocal(true).get().getState();
                assertNull(state.nodes().getMasterNodeId());
            });
            logger.info(""Restarting dedicated master node"");
            internalCluster().startNode(Settings.builder()
                    .put(""node.master"", true)
                    .put(""node.data"", false)
                    .put(""node.ml"", false)
                    .build());
            ensureStableClusterOnAllNodes(2);
        });
    }
"
"    @TestLogging(""org.elasticsearch.xpack.ml.job.process.autodetect:DEBUG"")
    public void testCanCloseClosingJob() throws Exception {
        AutodetectCommunicator communicator = mock(AutodetectCommunicator.class);
        AtomicInteger numberOfCommunicatorCloses = new AtomicInteger(0);
        doAnswer(invocationOnMock -> {
            numberOfCommunicatorCloses.incrementAndGet();
            // This increases the chance of the two threads both getting into
            // the middle of the AutodetectProcessManager.close() method
            Thread.yield();
            return null;
        }).when(communicator).close(anyBoolean(), anyString());
        AutodetectProcessManager manager = createManager(communicator);
        assertEquals(0, manager.numberOfOpenJobs());

        JobTask jobTask = mock(JobTask.class);
        when(jobTask.getJobId()).thenReturn(""foo"");
        manager.openJob(jobTask, e -> {});
        manager.processData(jobTask, analysisRegistry, createInputStream(""""), randomFrom(XContentType.values()),
                mock(DataLoadParams.class), (dataCounts1, e) -> {});

        assertEquals(1, manager.numberOfOpenJobs());

        // Close the job in a separate thread
        Thread closeThread = new Thread(() -> manager.closeJob(jobTask, false, ""in separate thread""));
        closeThread.start();
        Thread.yield();

        // Also close the job in the current thread, so that we have two simultaneous close requests
        manager.closeJob(jobTask, false, ""in main test thread"");

        // The 10 second timeout here is usually far in excess of what is required.  In the vast
        // majority of cases the other thread will exit within a few milliseconds.  However, it
        // has been observed that on some VMs the test can fail because the VM stalls at the
        // wrong moment.  A 10 second timeout is on a par with the length of time assertBusy()
        // would wait under these circumstances.
        closeThread.join(10000);
        assertFalse(closeThread.isAlive());

        // Only one of the threads should have called AutodetectCommunicator.close()
        assertEquals(1, numberOfCommunicatorCloses.get());
        assertEquals(0, manager.numberOfOpenJobs());
    }
"
"@TestLogging(""org.elasticsearch.xpack.ml.action:DEBUG"")
    public void resetLicensing() {
        enableLicensing();

        ensureStableCluster(1);
        ensureYellow();
    }
"
"@TestLogging(""org.elasticsearch.xpack.watcher:DEBUG,org.elasticsearch.xpack.watcher.WatcherIndexingListener:TRACE"")
    public void testActionConditionWithHardFailures() throws Exception {
        final String id = ""testActionConditionWithHardFailures"";

        final ExecutableCondition scriptConditionFailsHard = mockScriptCondition(""throw new IllegalStateException('failed');"");
        final List<ExecutableCondition> actionConditionsWithFailure =
                Arrays.asList(scriptConditionFailsHard, conditionPasses, InternalAlwaysCondition.INSTANCE);

        Collections.shuffle(actionConditionsWithFailure, random());

        final int failedIndex = actionConditionsWithFailure.indexOf(scriptConditionFailsHard);

        putAndTriggerWatch(id, input, actionConditionsWithFailure.toArray(new Condition[actionConditionsWithFailure.size()]));

        flush();

        assertWatchWithMinimumActionsCount(id, ExecutionState.EXECUTED, 1);

        // only one action should have failed via condition
        final SearchResponse response = searchHistory(SearchSourceBuilder.searchSource().query(termQuery(""watch_id"", id)));
        assertThat(response.getHits().getTotalHits(), is(1L));

        final SearchHit hit = response.getHits().getAt(0);
        final List<Object> actions = getActionsFromHit(hit.getSourceAsMap());

        for (int i = 0; i < actionConditionsWithFailure.size(); ++i) {
            final Map<String, Object> action = (Map<String, Object>)actions.get(i);
            final Map<String, Object> condition = (Map<String, Object>)action.get(""condition"");
            final Map<String, Object> logging = (Map<String, Object>)action.get(""logging"");

            assertThat(action.get(""id""), is(""action"" + i));

            if (i == failedIndex) {
                assertThat(action.get(""status""), is(""condition_failed""));
                assertThat(action.get(""reason""), is(""condition failed. skipping: [expected] failed hard""));
                assertThat(condition, nullValue());
                assertThat(logging, nullValue());
            } else {
                assertThat(condition.get(""type""), is(actionConditionsWithFailure.get(i).type()));

                assertThat(action.get(""status""), is(""success""));
                assertThat(condition.get(""met""), is(true));
                assertThat(action.get(""reason""), nullValue());
                assertThat(logging.get(""logged_text""), is(Integer.toString(i)));
            }
        }
    }
"
"@TestLogging(""org.elasticsearch.xpack.watcher:DEBUG,"" +
    public void setUp() throws Exception {
        super.setUp();
        server = EmailServer.localhost(logger);
    }
"
"@TestLogging(""org.elasticsearch.xpack.watcher:DEBUG,org.elasticsearch.xpack.watcher.WatcherIndexingListener:TRACE"")
    public void testHttpInput() throws Exception {
        createIndex(""index"");
        client().prepareIndex(""index"", ""type"", ""id"").setSource(""{}"", XContentType.JSON).setRefreshPolicy(IMMEDIATE).get();

        InetSocketAddress address = internalCluster().httpAddresses()[0];
        watcherClient().preparePutWatch(""_name"")
                .setSource(watchBuilder()
                        .trigger(schedule(interval(""5s"")))
                        .input(httpInput(HttpRequestTemplate.builder(address.getHostString(), address.getPort())
                                .path(""/index/_search"")
                                .body(Strings.toString(jsonBuilder().startObject().field(""size"", 1).endObject()))
                                .putHeader(""Content-Type"", new TextTemplate(""application/json""))))
                        .condition(new CompareCondition(""ctx.payload.hits.total"", CompareCondition.Op.EQ, 1L))
                        .addAction(""_id"", loggingAction(""anything"")))
                .get();

        timeWarp().trigger(""_name"");
        refresh();
        assertWatchWithMinimumPerformedActionsCount(""_name"", 1, false);
    }
"
"@TestLogging(""org.elasticsearch.xpack.watcher:DEBUG,"" +
    public void testIndexWatch() throws Exception {
        WatcherClient watcherClient = watcherClient();
        createIndex(""idx"");
        // Have a sample document in the index, the watch is going to evaluate
        client().prepareIndex(""idx"", ""type"").setSource(""field"", ""foo"").get();
        refresh();
        WatcherSearchTemplateRequest request = templateRequest(searchSource().query(termQuery(""field"", ""foo"")), ""idx"");
        watcherClient.preparePutWatch(""_name"")
                .setSource(watchBuilder()
                        .trigger(schedule(interval(5, IntervalSchedule.Interval.Unit.SECONDS)))
                        .input(searchInput(request))
                        .condition(new CompareCondition(""ctx.payload.hits.total"", CompareCondition.Op.EQ, 1L))
                        .addAction(""_logger"", loggingAction(""_logging"")
                                .setCategory(""_category"")))
                .get();

        timeWarp().trigger(""_name"");
        assertWatchWithMinimumPerformedActionsCount(""_name"", 1);

        GetWatchResponse getWatchResponse = watcherClient().prepareGetWatch().setId(""_name"").get();
        assertThat(getWatchResponse.isFound(), is(true));
        assertThat(getWatchResponse.getSource(), notNullValue());
    }
"
"    @TestLogging(""org.elasticsearch.xpack.watcher:DEBUG"")
    public void testModifyWatches() throws Exception {
        createIndex(""idx"");
        WatcherSearchTemplateRequest searchRequest = templateRequest(searchSource().query(matchAllQuery()), ""idx"");

        WatchSourceBuilder source = watchBuilder()
                .trigger(schedule(interval(""5s"")))
                .input(searchInput(searchRequest))
                .addAction(""_id"", indexAction(""idx"", ""action""));

        watcherClient().preparePutWatch(""_name"")
                .setSource(source.condition(new CompareCondition(""ctx.payload.hits.total"", CompareCondition.Op.EQ, 1L)))
                .get();

        timeWarp().clock().fastForwardSeconds(5);
        timeWarp().trigger(""_name"");
        assertWatchWithMinimumPerformedActionsCount(""_name"", 0, false);

        watcherClient().preparePutWatch(""_name"")
                .setSource(source.condition(new CompareCondition(""ctx.payload.hits.total"", CompareCondition.Op.EQ, 0L)))
                .get();

        timeWarp().clock().fastForwardSeconds(5);
        timeWarp().trigger(""_name"");
        refresh();
        assertWatchWithMinimumPerformedActionsCount(""_name"", 1, false);

        watcherClient().preparePutWatch(""_name"")
                .setSource(source
                        .trigger(schedule(Schedules.cron(""0/1 * * * * ? 2020"")))
                        .condition(new CompareCondition(""ctx.payload.hits.total"", CompareCondition.Op.EQ, 0L)))
                .get();

        timeWarp().clock().fastForwardSeconds(5);
        timeWarp().trigger(""_name"");
        long count = findNumberOfPerformedActions(""_name"");

        timeWarp().clock().fastForwardSeconds(5);
        timeWarp().trigger(""_name"");
        assertThat(count, equalTo(findNumberOfPerformedActions(""_name"")));
    }
"
"@TestLogging(""org.elasticsearch.xpack.watcher:DEBUG"")
    public void indexTestDocument() {
        IndexResponse eventIndexResponse = client().prepareIndex(""events"", ""event"", id)
                .setRefreshPolicy(WriteRequest.RefreshPolicy.IMMEDIATE)
                .setSource(""level"", ""error"")
                .get();
        assertEquals(DocWriteResponse.Result.CREATED, eventIndexResponse.getResult());
    }
"
"@TestLogging(""org.elasticsearch.xpack.watcher:DEBUG,"" +
    public void testTimeThrottle(){
        String id = randomAlphaOfLength(20);
        PutWatchResponse putWatchResponse = watcherClient().preparePutWatch()
                .setId(id)
                .setSource(watchBuilder()
                        .trigger(schedule(interval(""5s"")))
                        .input(simpleInput())
                        .addAction(""my-logging-action"", loggingAction(""foo""))
                        .defaultThrottlePeriod(TimeValue.timeValueSeconds(30)))
                .get();
        assertThat(putWatchResponse.isCreated(), is(true));

        timeWarp().trigger(id);
        assertHistoryEntryExecuted(id);

        timeWarp().clock().fastForward(TimeValue.timeValueMillis(4000));
        timeWarp().trigger(id);
        assertHistoryEntryThrottled(id);

        timeWarp().clock().fastForwardSeconds(30);
        timeWarp().trigger(id);
        assertHistoryEntryExecuted(id);

        assertTotalHistoryEntries(id, 3);
    }
"
"@TestLogging(""org.elasticsearch.xpack.watcher:DEBUG,org.elasticsearch.xpack.watcher.WatcherIndexingListener:TRACE"")
    public void testDeactivateAndActivate() throws Exception {
        PutWatchResponse putWatchResponse = watcherClient().preparePutWatch()
                .setId(""_id"")
                .setSource(watchBuilder()
                        .trigger(schedule(interval(""1s"")))
                        .input(simpleInput(""foo"", ""bar""))
                        .addAction(""_a1"", indexAction(""actions"", ""action1""))
                        .defaultThrottlePeriod(new TimeValue(0, TimeUnit.SECONDS)))
                .get();

        assertThat(putWatchResponse.isCreated(), is(true));

        GetWatchResponse getWatchResponse = watcherClient().prepareGetWatch(""_id"").get();
        assertThat(getWatchResponse, notNullValue());
        assertThat(getWatchResponse.getStatus().state().isActive(), is(true));

        logger.info(""Waiting for watch to be executed at least once"");
        assertWatchWithMinimumActionsCount(""_id"", ExecutionState.EXECUTED, 1);

        // we now know the watch is executing... lets deactivate it
        ActivateWatchResponse activateWatchResponse = watcherClient().prepareActivateWatch(""_id"", false).get();
        assertThat(activateWatchResponse, notNullValue());
        assertThat(activateWatchResponse.getStatus().state().isActive(), is(false));

        getWatchResponse = watcherClient().prepareGetWatch(""_id"").get();
        assertThat(getWatchResponse, notNullValue());
        assertThat(getWatchResponse.getStatus().state().isActive(), is(false));

        // wait until no watch is executing
        assertBusy(() -> {
            WatcherStatsResponse statsResponse = watcherClient().prepareWatcherStats().setIncludeCurrentWatches(true).get();
            int sum = statsResponse.getNodes().stream().map(WatcherStatsResponse.Node::getSnapshots).mapToInt(List::size).sum();
            assertThat(sum, is(0));
        });

        logger.info(""Ensured no more watches are being executed"");
        refresh();
        long count1 = docCount("".watcher-history*"", ""doc"", matchAllQuery());

        logger.info(""Sleeping for 5 seconds, watch history count [{}]"", count1);
        Thread.sleep(5000);

        refresh();
        long count2 = docCount("".watcher-history*"", ""doc"", matchAllQuery());

        assertThat(count2, is(count1));

        // lets activate it again
        logger.info(""Activating watch again"");

        activateWatchResponse = watcherClient().prepareActivateWatch(""_id"", true).get();
        assertThat(activateWatchResponse, notNullValue());
        assertThat(activateWatchResponse.getStatus().state().isActive(), is(true));

        getWatchResponse = watcherClient().prepareGetWatch(""_id"").get();
        assertThat(getWatchResponse, notNullValue());
        assertThat(getWatchResponse.getStatus().state().isActive(), is(true));

        logger.info(""Sleeping for another five seconds, ensuring that watch is executed"");
        Thread.sleep(5000);
        refresh();
        long count3 = docCount("".watcher-history*"", ""doc"", matchAllQuery());
        assertThat(count3, greaterThan(count1));
    }
"
"    @Test
    public void testPluginInstalled() {
        try (TransportClient client = new PreBuiltTransportClient(Settings.EMPTY)) {
            Settings settings = client.settings();
            assertEquals(Netty4Plugin.NETTY_TRANSPORT_NAME, NetworkModule.HTTP_DEFAULT_TYPE_SETTING.get(settings));
            assertEquals(Netty4Plugin.NETTY_TRANSPORT_NAME, NetworkModule.TRANSPORT_DEFAULT_TYPE_SETTING.get(settings));
        }
    }
"
"    @Test
    public void testInstallPluginTwice() {
        for (Class<? extends Plugin> plugin :
                Arrays.asList(ParentJoinPlugin.class, ReindexPlugin.class, PercolatorPlugin.class,
                    MustachePlugin.class)) {
            try {
                new PreBuiltTransportClient(Settings.EMPTY, plugin);
                fail(""exception expected"");
            } catch (IllegalArgumentException ex) {
                assertTrue(""Expected message to start with [plugin already exists: ] but was instead ["" + ex.getMessage() + ""]"",
                        ex.getMessage().startsWith(""plugin already exists: ""));
            }
        }
    }
"
"@TestMethodProviders({
    public void logTestNameBefore() {
        logger.info(""["" + testNameRule.getMethodName() + ""]: before test"");
    }
"
"@TestCaseOrdering(TestCaseOrdering.AlphabeticOrder.class)
    public void onlyCompatibleDistributions() {
        assumeTrue(""only rpm platforms"", isRPM());
        assumeTrue(""only compatible distributions"", distribution().packaging.compatible);
    }
"
"@TestCaseOrdering(TestCaseOrdering.AlphabeticOrder.class)
    public void onlyCompatibleDistributions() {
        assumeTrue(""only compatible distributions"", distribution().packaging.compatible);
    }
"
"@TestCaseOrdering(TestCaseOrdering.AlphabeticOrder.class)
    public void onlyCompatibleDistributions() {
        assumeTrue(""only compatible distributions"", distribution().packaging.compatible);
    }
"
"@TestCaseOrdering(TestCaseOrdering.AlphabeticOrder.class)
    public void onlyCompatibleDistributions() {
        assumeTrue(""only dpkg platforms"", isDPKG());
        assumeTrue(""only compatible distributions"", distribution().packaging.compatible);
    }
"
"@TestRuleLimitSysouts.Limit(bytes = 14000)
    public void testTransportClient() throws URISyntaxException, IOException {
        try (CloseableHttpClient client = HttpClientBuilder.create().build()) {
            final String str = String.format(
                    Locale.ROOT,
                    ""http://localhost:%d/wildfly-%s%s/transport/employees/1"",
                    Integer.parseInt(System.getProperty(""tests.jboss.http.port"")),
                    Version.CURRENT,
                    Build.CURRENT.isSnapshot() ? ""-SNAPSHOT"" : """");
            final HttpPut put = new HttpPut(new URI(str));
            final String body;
            try (XContentBuilder builder = jsonBuilder()) {
                builder.startObject();
                {
                    builder.field(""first_name"", ""John"");
                    builder.field(""last_name"", ""Smith"");
                    builder.field(""age"", 25);
                    builder.field(""about"", ""I love to go rock climbing"");
                    builder.startArray(""interests"");
                    {
                        builder.value(""sports"");
                        builder.value(""music"");
                    }
                    builder.endArray();
                }
                builder.endObject();
                body = Strings.toString(builder);
            }
            put.setEntity(new StringEntity(body, ContentType.APPLICATION_JSON));
            try (CloseableHttpResponse response = client.execute(put)) {
                int status = response.getStatusLine().getStatusCode();
                assertThat(""expected a 201 response but got: "" + status + "" - body: "" + EntityUtils.toString(response.getEntity()),
                        status, equalTo(201));
            }

            final HttpGet get = new HttpGet(new URI(str));
            try (
                    CloseableHttpResponse response = client.execute(get);
                    XContentParser parser =
                            JsonXContent.jsonXContent.createParser(
                                    new NamedXContentRegistry(ClusterModule.getNamedXWriteables()),
                                    DeprecationHandler.THROW_UNSUPPORTED_OPERATION,
                                    response.getEntity().getContent())) {
                final Map<String, Object> map = parser.map();
                assertThat(map.get(""first_name""), equalTo(""John""));
                assertThat(map.get(""last_name""), equalTo(""Smith""));
                assertThat(map.get(""age""), equalTo(25));
                assertThat(map.get(""about""), equalTo(""I love to go rock climbing""));
                final Object interests = map.get(""interests"");
                assertThat(interests, instanceOf(List.class));
                @SuppressWarnings(""unchecked"") final List<String> interestsAsList = (List<String>) interests;
                assertThat(interestsAsList, containsInAnyOrder(""sports"", ""music""));
            }
        }
    }
"
"    @TestGroup(enabled = false, sysProperty = ESIntegTestCase.SYSPROP_THIRDPARTY)
    public void randomIndexTemplate() throws IOException {

        // TODO move settings for random directory etc here into the index based randomized settings.
        if (cluster().size() > 0) {
            Settings.Builder randomSettingsBuilder =
                setRandomIndexSettings(random(), Settings.builder());
            if (isInternalCluster()) {
                // this is only used by mock plugins and if the cluster is not internal we just can't set it
                randomSettingsBuilder.put(INDEX_TEST_SEED_SETTING.getKey(), random().nextLong());
            }

            randomSettingsBuilder.put(SETTING_NUMBER_OF_SHARDS, numberOfShards())
                .put(SETTING_NUMBER_OF_REPLICAS, numberOfReplicas());

            // if the test class is annotated with SuppressCodecs(""*""), it means don't use lucene's codec randomization
            // otherwise, use it, it has assertions and so on that can find bugs.
            SuppressCodecs annotation = getClass().getAnnotation(SuppressCodecs.class);
            if (annotation != null && annotation.value().length == 1 && ""*"".equals(annotation.value()[0])) {
                randomSettingsBuilder.put(""index.codec"", randomFrom(CodecService.DEFAULT_CODEC, CodecService.BEST_COMPRESSION_CODEC));
            } else {
                randomSettingsBuilder.put(""index.codec"", CodecService.LUCENE_DEFAULT_CODEC);
            }

            for (String setting : randomSettingsBuilder.keys()) {
                assertThat(""non index. prefix setting set on index template, its a node setting..."", setting, startsWith(""index.""));
            }
            // always default delayed allocation to 0 to make sure we have tests are not delayed
            randomSettingsBuilder.put(UnassignedInfo.INDEX_DELAYED_NODE_LEFT_TIMEOUT_SETTING.getKey(), 0);
            if (randomBoolean()) {
                randomSettingsBuilder.put(IndexModule.INDEX_QUERY_CACHE_ENABLED_SETTING.getKey(), randomBoolean());
            }
            PutIndexTemplateRequestBuilder putTemplate = client().admin().indices()
                .preparePutTemplate(""random_index_template"")
                .setPatterns(Collections.singletonList(""*""))
                .setOrder(0)
                .setSettings(randomSettingsBuilder);
            assertAcked(putTemplate.execute().actionGet());
        }
    }
"
"    @Test
    public void testReproducible() throws IOException {
        if (ITER++ == 0) {
            CLUSTER_SEED = cluster().seed();
            for (int i = 0; i < SEQUENCE.length; i++) {
                SEQUENCE[i] = randomLong();
            }
        } else {
            assertEquals(CLUSTER_SEED, Long.valueOf(cluster().seed()));
            for (int i = 0; i < SEQUENCE.length; i++) {
                assertThat(SEQUENCE[i], equalTo(randomLong()));
            }
        }
    }
"
"        @TestLogging(""xyz:TRACE,foo:WARN,foo.bar:ERROR"")
        public void annotatedTestMethod() {

        }
"
"        @TestLogging(""abc:TRACE,xyz:DEBUG"")
        public void annotatedTestMethod2() {

        }
"
"        @TestLogging(""abc:INFO:WARN"")
        public void invalidMethod() {

        }
"
"    @TestLogging(""org.elasticsearch.index.shard:TRACE,org.elasticsearch.index.engine:TRACE"")
    public void testStressMaybeFlushOrRollTranslogGeneration() throws Exception {
        createIndex(""test"");
        ensureGreen();
        IndicesService indicesService = getInstanceFromNode(IndicesService.class);
        IndexService test = indicesService.indexService(resolveIndex(""test""));
        final IndexShard shard = test.getShardOrNull(0);
        assertFalse(shard.shouldPeriodicallyFlush());
        final boolean flush = randomBoolean();
        final Settings settings;
        if (flush) {
            // size of the operation plus two generations of overhead.
            settings = Settings.builder().put(""index.translog.flush_threshold_size"", ""180b"").build();
        } else {
            // size of the operation plus header and footer
            settings = Settings.builder().put(""index.translog.generation_threshold_size"", ""117b"").build();
        }
        client().admin().indices().prepareUpdateSettings(""test"").setSettings(settings).get();
        client().prepareIndex(""test"", ""test"", ""0"")
                .setSource(""{}"", XContentType.JSON)
                .setRefreshPolicy(randomBoolean() ? IMMEDIATE : NONE)
                .get();
        assertFalse(shard.shouldPeriodicallyFlush());
        final AtomicBoolean running = new AtomicBoolean(true);
        final int numThreads = randomIntBetween(2, 4);
        final Thread[] threads = new Thread[numThreads];
        final CyclicBarrier barrier = new CyclicBarrier(numThreads + 1);
        for (int i = 0; i < threads.length; i++) {
            threads[i] = new Thread(() -> {
                try {
                    barrier.await();
                } catch (final InterruptedException | BrokenBarrierException e) {
                    throw new RuntimeException(e);
                }
                while (running.get()) {
                    shard.afterWriteOperation();
                }
            });
            threads[i].start();
        }
        barrier.await();
        final CheckedRunnable<Exception> check;
        if (flush) {
            final FlushStats initialStats = shard.flushStats();
            client().prepareIndex(""test"", ""test"", ""1"").setSource(""{}"", XContentType.JSON).get();
            check = () -> {
                final FlushStats currentStats = shard.flushStats();
                String msg = String.format(Locale.ROOT, ""flush stats: total=[%d vs %d], periodic=[%d vs %d]"",
                    initialStats.getTotal(), currentStats.getTotal(), initialStats.getPeriodic(), currentStats.getPeriodic());
                assertThat(msg, currentStats.getPeriodic(), equalTo(initialStats.getPeriodic() + 1));
                assertThat(msg, currentStats.getTotal(), equalTo(initialStats.getTotal() + 1));
            };
        } else {
            final long generation = getTranslog(shard).currentFileGeneration();
            client().prepareIndex(""test"", ""test"", ""1"").setSource(""{}"", XContentType.JSON).get();
            check = () -> assertEquals(
                    generation + 1,
                    getTranslog(shard).currentFileGeneration());
        }
        assertBusy(check);
        running.set(false);
        for (int i = 0; i < threads.length; i++) {
            threads[i].join();
        }
        check.run();
    }
"
"    @TestLogging(""org.elasticsearch.index.shard:TRACE,org.elasticsearch.indices.recovery:TRACE"")
    public void testRecoveryAfterPrimaryPromotion() throws Exception {
        try (ReplicationGroup shards = createGroup(2)) {
            shards.startAll();
            int totalDocs = shards.indexDocs(randomInt(10));
            int committedDocs = 0;
            if (randomBoolean()) {
                shards.flush();
                committedDocs = totalDocs;
            }

            final IndexShard oldPrimary = shards.getPrimary();
            final IndexShard newPrimary = shards.getReplicas().get(0);
            final IndexShard replica = shards.getReplicas().get(1);
            if (randomBoolean()) {
                // simulate docs that were inflight when primary failed, these will be rolled back
                final int rollbackDocs = randomIntBetween(1, 5);
                logger.info(""--> indexing {} rollback docs"", rollbackDocs);
                for (int i = 0; i < rollbackDocs; i++) {
                    final IndexRequest indexRequest = new IndexRequest(index.getName(), ""type"", ""rollback_"" + i)
                            .source(""{}"", XContentType.JSON);
                    final BulkShardRequest bulkShardRequest = indexOnPrimary(indexRequest, oldPrimary);
                    indexOnReplica(bulkShardRequest, shards, replica);
                }
                if (randomBoolean()) {
                    oldPrimary.flush(new FlushRequest(index.getName()));
                }
            }

            shards.promoteReplicaToPrimary(newPrimary).get();

            // check that local checkpoint of new primary is properly tracked after primary promotion
            assertThat(newPrimary.getLocalCheckpoint(), equalTo(totalDocs - 1L));
            assertThat(IndexShardTestCase.getReplicationTracker(newPrimary)
                .getTrackedLocalCheckpointForShard(newPrimary.routingEntry().allocationId().getId()).getLocalCheckpoint(),
                equalTo(totalDocs - 1L));

            // index some more
            int moreDocs = shards.indexDocs(randomIntBetween(0, 5));
            totalDocs += moreDocs;

            // As a replica keeps a safe commit, the file-based recovery only happens if the required translog
            // for the sequence based recovery are not fully retained and extra documents were added to the primary.
            boolean expectSeqNoRecovery = (moreDocs == 0 || randomBoolean());
            int uncommittedOpsOnPrimary = 0;
            if (expectSeqNoRecovery == false) {
                IndexMetaData.Builder builder = IndexMetaData.builder(newPrimary.indexSettings().getIndexMetaData());
                builder.settings(Settings.builder().put(newPrimary.indexSettings().getSettings())
                    .put(IndexSettings.INDEX_TRANSLOG_RETENTION_AGE_SETTING.getKey(), ""-1"")
                    .put(IndexSettings.INDEX_TRANSLOG_RETENTION_SIZE_SETTING.getKey(), ""-1"")
                    .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0)
                );
                newPrimary.indexSettings().updateIndexMetaData(builder.build());
                newPrimary.onSettingsChanged();
                // Make sure the global checkpoint on the new primary is persisted properly,
                // otherwise the deletion policy won't trim translog
                assertBusy(() -> {
                    shards.syncGlobalCheckpoint();
                    assertThat(newPrimary.getLastSyncedGlobalCheckpoint(), equalTo(newPrimary.seqNoStats().getMaxSeqNo()));
                });
                newPrimary.flush(new FlushRequest().force(true));
                if (replica.indexSettings().isSoftDeleteEnabled()) {
                    // We need an extra flush to advance the min_retained_seqno on the new primary so ops-based won't happen.
                    // The min_retained_seqno only advances when a merge asks for the retention query.
                    newPrimary.flush(new FlushRequest().force(true));
                }
                uncommittedOpsOnPrimary = shards.indexDocs(randomIntBetween(0, 10));
                totalDocs += uncommittedOpsOnPrimary;
            }

            if (randomBoolean()) {
                uncommittedOpsOnPrimary = 0;
                shards.syncGlobalCheckpoint();
                newPrimary.flush(new FlushRequest());
            }

            oldPrimary.close(""demoted"", false);
            oldPrimary.store().close();

            IndexShard newReplica = shards.addReplicaWithExistingPath(oldPrimary.shardPath(), oldPrimary.routingEntry().currentNodeId());
            shards.recoverReplica(newReplica);

            if (expectSeqNoRecovery) {
                assertThat(newReplica.recoveryState().getIndex().fileDetails(), empty());
                assertThat(newReplica.recoveryState().getTranslog().recoveredOperations(), equalTo(totalDocs - committedDocs));
            } else {
                assertThat(newReplica.recoveryState().getIndex().fileDetails(), not(empty()));
                assertThat(newReplica.recoveryState().getTranslog().recoveredOperations(), equalTo(uncommittedOpsOnPrimary));
            }
            // Make sure that flushing on a recovering shard is ok.
            shards.flush();
            shards.assertAllEqual(totalDocs);
        }
    }
"
"    @TestLogging(""org.elasticsearch.index.shard:TRACE,org.elasticsearch.action.resync:TRACE"")
    public void testResyncAfterPrimaryPromotion() throws Exception {
        // TODO: check translog trimming functionality once rollback is implemented in Lucene (ES trimming is done)
        Map<String, String> mappings =
            Collections.singletonMap(""type"", ""{ \""type\"": { \""properties\"": { \""f\"": { \""type\"": \""keyword\""} }}}"");
        try (ReplicationGroup shards = new ReplicationGroup(buildIndexMetaData(2, mappings))) {
            shards.startAll();
            int initialDocs = randomInt(10);

            for (int i = 0; i < initialDocs; i++) {
                final IndexRequest indexRequest = new IndexRequest(index.getName(), ""type"", ""initial_doc_"" + i)
                    .source(""{ \""f\"": \""normal\""}"", XContentType.JSON);
                shards.index(indexRequest);
            }

            boolean syncedGlobalCheckPoint = randomBoolean();
            if (syncedGlobalCheckPoint) {
                shards.syncGlobalCheckpoint();
            }

            final IndexShard oldPrimary = shards.getPrimary();
            final IndexShard newPrimary = shards.getReplicas().get(0);
            final IndexShard justReplica = shards.getReplicas().get(1);

            // simulate docs that were inflight when primary failed
            final int extraDocs = randomInt(5);
            logger.info(""--> indexing {} extra docs"", extraDocs);
            for (int i = 0; i < extraDocs; i++) {
                final IndexRequest indexRequest = new IndexRequest(index.getName(), ""type"", ""extra_doc_"" + i)
                    .source(""{ \""f\"": \""normal\""}"", XContentType.JSON);
                final BulkShardRequest bulkShardRequest = indexOnPrimary(indexRequest, oldPrimary);
                indexOnReplica(bulkShardRequest, shards, newPrimary);
            }

            final int extraDocsToBeTrimmed = randomIntBetween(0, 10);
            logger.info(""--> indexing {} extra docs to be trimmed"", extraDocsToBeTrimmed);
            for (int i = 0; i < extraDocsToBeTrimmed; i++) {
                final IndexRequest indexRequest = new IndexRequest(index.getName(), ""type"", ""extra_trimmed_"" + i)
                    .source(""{ \""f\"": \""trimmed\""}"", XContentType.JSON);
                final BulkShardRequest bulkShardRequest = indexOnPrimary(indexRequest, oldPrimary);
                // have to replicate to another replica != newPrimary one - the subject to trim
                indexOnReplica(bulkShardRequest, shards, justReplica);
            }

            logger.info(""--> resyncing replicas seqno_stats primary {} replica {}"", oldPrimary.seqNoStats(), newPrimary.seqNoStats());
            PrimaryReplicaSyncer.ResyncTask task = shards.promoteReplicaToPrimary(newPrimary).get();
            if (syncedGlobalCheckPoint) {
                assertEquals(extraDocs, task.getResyncedOperations());
            } else {
                assertThat(task.getResyncedOperations(), greaterThanOrEqualTo(extraDocs));
            }
            shards.assertAllEqual(initialDocs + extraDocs);
            for (IndexShard replica : shards.getReplicas()) {
                assertThat(replica.getMaxSeqNoOfUpdatesOrDeletes(),
                    greaterThanOrEqualTo(shards.getPrimary().getMaxSeqNoOfUpdatesOrDeletes()));
            }

            // check translog on replica is trimmed
            int translogOperations = 0;
            try(Translog.Snapshot snapshot = getTranslog(justReplica).newSnapshot()) {
                Translog.Operation next;
                while ((next = snapshot.next()) != null) {
                    translogOperations++;
                    assertThat(""unexpected op: "" + next, (int)next.seqNo(), lessThan(initialDocs + extraDocs));
                    assertThat(""unexpected primaryTerm: "" + next.primaryTerm(), next.primaryTerm(),
                        is(oldPrimary.getPendingPrimaryTerm()));
                    final Translog.Source source = next.getSource();
                    assertThat(source.source.utf8ToString(), is(""{ \""f\"": \""normal\""}""));
                }
            }
            assertThat(translogOperations, is(initialDocs + extraDocs));
        }
    }
"
"    @TestLogging(
    public void testWaitForPendingSeqNo() throws Exception {
        IndexMetaData metaData = buildIndexMetaData(1);

        final int pendingDocs = randomIntBetween(1, 5);
        final BlockingEngineFactory primaryEngineFactory = new BlockingEngineFactory();

        try (ReplicationGroup shards = new ReplicationGroup(metaData) {
            @Override
            protected EngineFactory getEngineFactory(ShardRouting routing) {
                if (routing.primary()) {
                    return primaryEngineFactory;
                } else {
                    return new InternalEngineFactory();
                }
            }
        }) {
            shards.startAll();
            int docs = shards.indexDocs(randomIntBetween(1, 10));
            // simulate a background global checkpoint sync at which point we expect the global checkpoint to advance on the replicas
            shards.syncGlobalCheckpoint();
            IndexShard replica = shards.getReplicas().get(0);
            shards.removeReplica(replica);
            closeShards(replica);

            docs += pendingDocs;
            primaryEngineFactory.latchIndexers(pendingDocs);
            CountDownLatch pendingDocsDone = new CountDownLatch(pendingDocs);
            for (int i = 0; i < pendingDocs; i++) {
                final String id = ""pending_"" + i;
                threadPool.generic().submit(() -> {
                    try {
                        shards.index(new IndexRequest(index.getName(), ""type"", id).source(""{}"", XContentType.JSON));
                    } catch (Exception e) {
                        throw new AssertionError(e);
                    } finally {
                        pendingDocsDone.countDown();
                    }
                });
            }

            // wait for the pending ops to ""hang""
            primaryEngineFactory.awaitIndexersLatch();

            primaryEngineFactory.allowIndexing();
            // index some more
            docs += shards.indexDocs(randomInt(5));

            IndexShard newReplica = shards.addReplicaWithExistingPath(replica.shardPath(), replica.routingEntry().currentNodeId());

            CountDownLatch recoveryStart = new CountDownLatch(1);
            AtomicBoolean opsSent = new AtomicBoolean(false);
            final Future<Void> recoveryFuture = shards.asyncRecoverReplica(newReplica, (indexShard, node) -> {
                recoveryStart.countDown();
                return new RecoveryTarget(indexShard, node, recoveryListener, l -> {
                }) {
                    @Override
                    public long indexTranslogOperations(List<Translog.Operation> operations, int totalTranslogOps,
                                                        long maxSeenAutoIdTimestamp, long maxSeqNoOfUpdates) throws IOException {
                        opsSent.set(true);
                        return super.indexTranslogOperations(operations, totalTranslogOps, maxSeenAutoIdTimestamp, maxSeqNoOfUpdates);
                    }
"
"    @TestLogging(
    public void testCheckpointsAndMarkingInSync() throws Exception {
        final IndexMetaData metaData = buildIndexMetaData(0);
        final BlockingEngineFactory replicaEngineFactory = new BlockingEngineFactory();
        try (
                ReplicationGroup shards = new ReplicationGroup(metaData) {
                    @Override
                    protected EngineFactory getEngineFactory(final ShardRouting routing) {
                        if (routing.primary()) {
                            return new InternalEngineFactory();
                        } else {
                            return replicaEngineFactory;
                        }
                    }
                };
                AutoCloseable ignored = replicaEngineFactory // make sure we release indexers before closing
        ) {
            shards.startPrimary();
            final int docs = shards.indexDocs(randomIntBetween(1, 10));
            logger.info(""indexed [{}] docs"", docs);
            final CountDownLatch pendingDocDone = new CountDownLatch(1);
            final CountDownLatch pendingDocActiveWithExtraDocIndexed = new CountDownLatch(1);
            final CountDownLatch phaseTwoStartLatch = new CountDownLatch(1);
            final IndexShard replica = shards.addReplica();
            final Future<Void> recoveryFuture = shards.asyncRecoverReplica(
                    replica,
                    (indexShard, node) -> new RecoveryTarget(indexShard, node, recoveryListener, l -> {}) {
                        @Override
                        public long indexTranslogOperations(final List<Translog.Operation> operations, final int totalTranslogOps,
                                                            final long maxAutoIdTimestamp, long maxSeqNoOfUpdates)
"
"    @TestLogging(""_root:DEBUG"")
    public void testMergesHappening() throws InterruptedException, IOException, ExecutionException {
        final int numOfShards = randomIntBetween(1, 5);
        // some settings to keep num segments low
        assertAcked(prepareCreate(""test"").setSettings(Settings.builder()
                .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, numOfShards)
                .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 0)
                .build()));
        long id = 0;
        final int rounds = scaledRandomIntBetween(50, 300);
        logger.info(""Starting rounds [{}] "", rounds);
        for (int i = 0; i < rounds; ++i) {
            final int numDocs = scaledRandomIntBetween(100, 1000);
            BulkRequestBuilder request = client().prepareBulk();
            for (int j = 0; j < numDocs; ++j) {
                request.add(Requests.indexRequest(""test"").type(""type1"").id(Long.toString(id++)).source(jsonBuilder().startObject().field(""l"", randomLong()).endObject()));
            }
            BulkResponse response = request.execute().actionGet();
            refresh();
            assertNoFailures(response);
            IndicesStatsResponse stats = client().admin().indices().prepareStats(""test"").setSegments(true).setMerge(true).get();
            logger.info(""index round [{}] - segments {}, total merges {}, current merge {}"", i, stats.getPrimaries().getSegments().getCount(), stats.getPrimaries().getMerge().getTotal(), stats.getPrimaries().getMerge().getCurrent());
        }
        final long upperNumberSegments = 2 * numOfShards * 10;
        awaitBusy(() -> {
            IndicesStatsResponse stats = client().admin().indices().prepareStats().setSegments(true).setMerge(true).get();
            logger.info(""numshards {}, segments {}, total merges {}, current merge {}"", numOfShards, stats.getPrimaries().getSegments().getCount(), stats.getPrimaries().getMerge().getTotal(), stats.getPrimaries().getMerge().getCurrent());
            long current = stats.getPrimaries().getMerge().getCurrent();
            long count = stats.getPrimaries().getSegments().getCount();
            return count < upperNumberSegments && current == 0;
        });
        IndicesStatsResponse stats = client().admin().indices().prepareStats().setSegments(true).setMerge(true).get();
        logger.info(""numshards {}, segments {}, total merges {}, current merge {}"", numOfShards, stats.getPrimaries().getSegments().getCount(), stats.getPrimaries().getMerge().getTotal(), stats.getPrimaries().getMerge().getCurrent());
        long count = stats.getPrimaries().getSegments().getCount();
        assertThat(count, Matchers.lessThanOrEqualTo(upperNumberSegments));
    }
"
"@TestLogging(""_root:DEBUG,org.elasticsearch.index.shard:TRACE,org.elasticsearch.cluster.service:TRACE,org.elasticsearch.index.seqno:TRACE,org.elasticsearch.indices.recovery:TRACE"")
    public void testRecoverWhileUnderLoadAllocateReplicasTest() throws Exception {
        logger.info(""--> creating test index ..."");
        int numberOfShards = numberOfShards();
        assertAcked(prepareCreate(""test"", 1, Settings.builder().put(SETTING_NUMBER_OF_SHARDS, numberOfShards).put(SETTING_NUMBER_OF_REPLICAS, 1).put(IndexSettings.INDEX_TRANSLOG_DURABILITY_SETTING.getKey(), Translog.Durability.ASYNC)));

        final int totalNumDocs = scaledRandomIntBetween(200, 10000);
        int waitFor = totalNumDocs / 10;
        int extraDocs = waitFor;
        try (BackgroundIndexer indexer = new BackgroundIndexer(""test"", ""type"", client(), extraDocs)) {
            logger.info(""--> waiting for {} docs to be indexed ..."", waitFor);
            waitForDocs(waitFor, indexer);
            indexer.assertNoFailures();
            logger.info(""--> {} docs indexed"", waitFor);

            extraDocs = totalNumDocs / 10;
            waitFor += extraDocs;
            indexer.continueIndexing(extraDocs);
            logger.info(""--> flushing the index ...."");
            // now flush, just to make sure we have some data in the index, not just translog
            client().admin().indices().prepareFlush().execute().actionGet();

            logger.info(""--> waiting for {} docs to be indexed ..."", waitFor);
            waitForDocs(waitFor, indexer);
            indexer.assertNoFailures();
            logger.info(""--> {} docs indexed"", waitFor);

            extraDocs = totalNumDocs - waitFor;
            indexer.continueIndexing(extraDocs);

            logger.info(""--> allow 2 nodes for index [test] ..."");
            // now start another node, while we index
            allowNodes(""test"", 2);

            logger.info(""--> waiting for GREEN health status ..."");
            // make sure the cluster state is green, and all has been recovered
            assertNoTimeout(client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setTimeout(""5m"").setWaitForGreenStatus());

            logger.info(""--> waiting for {} docs to be indexed ..."", totalNumDocs);
            waitForDocs(totalNumDocs, indexer);
            indexer.assertNoFailures();
            logger.info(""--> {} docs indexed"", totalNumDocs);

            logger.info(""--> marking and waiting for indexing threads to stop ..."");
            indexer.stop();
            logger.info(""--> indexing threads stopped"");

            logger.info(""--> refreshing the index"");
            refreshAndAssert();
            logger.info(""--> verifying indexed content"");
            iterateAssertCount(numberOfShards, 10, indexer.getIds());
        }
    }
"
"@TestLogging(""_root:DEBUG,org.elasticsearch.indices.recovery:TRACE,org.elasticsearch.index.shard.service:TRACE"")
    public void testSimpleRelocationNoIndexing() {
        logger.info(""--> starting [node1] ..."");
        final String node_1 = internalCluster().startNode();

        logger.info(""--> creating test index ..."");
        prepareCreate(""test"", Settings.builder()
                .put(""index.number_of_shards"", 1)
                .put(""index.number_of_replicas"", 0)
        ).get();

        logger.info(""--> index 10 docs"");
        for (int i = 0; i < 10; i++) {
            client().prepareIndex(""test"", ""type"", Integer.toString(i)).setSource(""field"", ""value"" + i).execute().actionGet();
        }
        logger.info(""--> flush so we have an actual index"");
        client().admin().indices().prepareFlush().execute().actionGet();
        logger.info(""--> index more docs so we have something in the translog"");
        for (int i = 10; i < 20; i++) {
            client().prepareIndex(""test"", ""type"", Integer.toString(i)).setSource(""field"", ""value"" + i).execute().actionGet();
        }

        logger.info(""--> verifying count"");
        client().admin().indices().prepareRefresh().execute().actionGet();
        assertThat(client().prepareSearch(""test"").setSize(0).execute().actionGet().getHits().getTotalHits(), equalTo(20L));

        logger.info(""--> start another node"");
        final String node_2 = internalCluster().startNode();
        ClusterHealthResponse clusterHealthResponse = client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForNodes(""2"").execute().actionGet();
        assertThat(clusterHealthResponse.isTimedOut(), equalTo(false));

        logger.info(""--> relocate the shard from node1 to node2"");
        client().admin().cluster().prepareReroute()
                .add(new MoveAllocationCommand(""test"", 0, node_1, node_2))
                .execute().actionGet();

        clusterHealthResponse = client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForNoRelocatingShards(true).setTimeout(ACCEPTABLE_RELOCATION_TIME).execute().actionGet();
        assertThat(clusterHealthResponse.isTimedOut(), equalTo(false));

        logger.info(""--> verifying count again..."");
        client().admin().indices().prepareRefresh().execute().actionGet();
        assertThat(client().prepareSearch(""test"").setSize(0).execute().actionGet().getHits().getTotalHits(), equalTo(20L));
    }
"
"    @TestLogging(""org.elasticsearch.action.bulk:TRACE,org.elasticsearch.action.search:TRACE"")
    public void testRelocationWhileIndexingRandom() throws Exception {
        int numberOfRelocations = scaledRandomIntBetween(1, rarely() ? 10 : 4);
        int numberOfReplicas = randomBoolean() ? 0 : 1;
        int numberOfNodes = numberOfReplicas == 0 ? 2 : 3;

        logger.info(""testRelocationWhileIndexingRandom(numRelocations={}, numberOfReplicas={}, numberOfNodes={})"", numberOfRelocations, numberOfReplicas, numberOfNodes);

        String[] nodes = new String[numberOfNodes];
        logger.info(""--> starting [node1] ..."");
        nodes[0] = internalCluster().startNode();

        logger.info(""--> creating test index ..."");
        prepareCreate(""test"", Settings.builder()
            .put(""index.number_of_shards"", 1)
            .put(""index.number_of_replicas"", numberOfReplicas)
        ).get();


        for (int i = 2; i <= numberOfNodes; i++) {
            logger.info(""--> starting [node{}] ..."", i);
            nodes[i - 1] = internalCluster().startNode();
            if (i != numberOfNodes) {
                ClusterHealthResponse healthResponse = client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID)
                        .setWaitForNodes(Integer.toString(i)).setWaitForGreenStatus().execute().actionGet();
                assertThat(healthResponse.isTimedOut(), equalTo(false));
            }
        }

        int numDocs = scaledRandomIntBetween(200, 2500);
        try (BackgroundIndexer indexer = new BackgroundIndexer(""test"", ""type1"", client(), numDocs)) {
            logger.info(""--> waiting for {} docs to be indexed ..."", numDocs);
            waitForDocs(numDocs, indexer);
            logger.info(""--> {} docs indexed"", numDocs);

            logger.info(""--> starting relocations..."");
            int nodeShiftBased = numberOfReplicas; // if we have replicas shift those
            for (int i = 0; i < numberOfRelocations; i++) {
                int fromNode = (i % 2);
                int toNode = fromNode == 0 ? 1 : 0;
                fromNode += nodeShiftBased;
                toNode += nodeShiftBased;
                numDocs = scaledRandomIntBetween(200, 1000);
                logger.debug(""--> Allow indexer to index [{}] documents"", numDocs);
                indexer.continueIndexing(numDocs);
                logger.info(""--> START relocate the shard from {} to {}"", nodes[fromNode], nodes[toNode]);
                client().admin().cluster().prepareReroute()
                        .add(new MoveAllocationCommand(""test"", 0, nodes[fromNode], nodes[toNode]))
                        .get();
                if (rarely()) {
                    logger.debug(""--> flushing"");
                    client().admin().indices().prepareFlush().get();
                }
                ClusterHealthResponse clusterHealthResponse = client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForNoRelocatingShards(true).setTimeout(ACCEPTABLE_RELOCATION_TIME).execute().actionGet();
                assertThat(clusterHealthResponse.isTimedOut(), equalTo(false));
                indexer.pauseIndexing();
                logger.info(""--> DONE relocate the shard from {} to {}"", fromNode, toNode);
            }
            logger.info(""--> done relocations"");
            logger.info(""--> waiting for indexing threads to stop ..."");
            indexer.stop();
            logger.info(""--> indexing threads stopped"");

            logger.info(""--> refreshing the index"");
            client().admin().indices().prepareRefresh(""test"").execute().actionGet();
            logger.info(""--> searching the index"");
            boolean ranOnce = false;
            for (int i = 0; i < 10; i++) {
                    logger.info(""--> START search test round {}"", i + 1);
                    SearchHits hits = client().prepareSearch(""test"").setQuery(matchAllQuery()).setSize((int) indexer.totalIndexedDocs()).storedFields().execute().actionGet().getHits();
                    ranOnce = true;
                    if (hits.getTotalHits() != indexer.totalIndexedDocs()) {
                        int[] hitIds = new int[(int) indexer.totalIndexedDocs()];
                        for (int hit = 0; hit < indexer.totalIndexedDocs(); hit++) {
                            hitIds[hit] = hit + 1;
                        }
                        IntHashSet set = IntHashSet.from(hitIds);
                        for (SearchHit hit : hits.getHits()) {
                            int id = Integer.parseInt(hit.getId());
                            if (!set.remove(id)) {
                                logger.error(""Extra id [{}]"", id);
                            }
                        }
                        set.forEach((IntProcedure) value -> {
                            logger.error(""Missing id [{}]"", value);
                        });
                    }
                    assertThat(hits.getTotalHits(), equalTo(indexer.totalIndexedDocs()));
                    logger.info(""--> DONE search test round {}"", i + 1);

            }
            if (!ranOnce) {
                fail();
            }
        }
    }
"
"    @TestLogging(""org.elasticsearch.action.bulk:TRACE,org.elasticsearch.action.search:TRACE"")
    public void testRelocationWhileRefreshing() throws Exception {
        int numberOfRelocations = scaledRandomIntBetween(1, rarely() ? 10 : 4);
        int numberOfReplicas = randomBoolean() ? 0 : 1;
        int numberOfNodes = numberOfReplicas == 0 ? 2 : 3;

        logger.info(""testRelocationWhileIndexingRandom(numRelocations={}, numberOfReplicas={}, numberOfNodes={})"", numberOfRelocations, numberOfReplicas, numberOfNodes);

        String[] nodes = new String[numberOfNodes];
        logger.info(""--> starting [node_0] ..."");
        nodes[0] = internalCluster().startNode();

        logger.info(""--> creating test index ..."");
        prepareCreate(
                ""test"",
                Settings.builder()
                        .put(""index.number_of_shards"", 1)
                        .put(""index.number_of_replicas"", numberOfReplicas)
                        .put(""index.refresh_interval"", -1) // we want to control refreshes
                        .put(IndexService.GLOBAL_CHECKPOINT_SYNC_INTERVAL_SETTING.getKey(), ""100ms""))
                .get();

        for (int i = 1; i < numberOfNodes; i++) {
            logger.info(""--> starting [node_{}] ..."", i);
            nodes[i] = internalCluster().startNode();
            if (i != numberOfNodes - 1) {
                ClusterHealthResponse healthResponse = client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID)
                        .setWaitForNodes(Integer.toString(i + 1)).setWaitForGreenStatus().execute().actionGet();
                assertThat(healthResponse.isTimedOut(), equalTo(false));
            }
        }

        final Semaphore postRecoveryShards = new Semaphore(0);
        final IndexEventListener listener = new IndexEventListener() {
            @Override
            public void indexShardStateChanged(IndexShard indexShard, @Nullable IndexShardState previousState, IndexShardState currentState, @Nullable String reason) {
                if (currentState == IndexShardState.POST_RECOVERY) {
                    postRecoveryShards.release();
                }
            }
"
"    @TestLogging(
    public void testIndexAndRelocateConcurrently() throws ExecutionException, InterruptedException {
        int halfNodes = randomIntBetween(1, 3);
        Settings[] nodeSettings = Stream.concat(
            Stream.generate(() -> Settings.builder().put(""node.attr.color"", ""blue"").build()).limit(halfNodes),
            Stream.generate(() -> Settings.builder().put(""node.attr.color"", ""red"").build()).limit(halfNodes)
            ).toArray(Settings[]::new);
        List<String> nodes = internalCluster().startNodes(nodeSettings);
        String[] blueNodes = nodes.subList(0, halfNodes).stream().toArray(String[]::new);
        String[] redNodes = nodes.subList(halfNodes, nodes.size()).stream().toArray(String[]::new);
        logger.info(""blue nodes: {}"", (Object)blueNodes);
        logger.info(""red nodes: {}"", (Object)redNodes);
        ensureStableCluster(halfNodes * 2);

        final Settings.Builder settings = Settings.builder()
                .put(""index.routing.allocation.exclude.color"", ""blue"")
                .put(indexSettings())
                .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, randomInt(halfNodes - 1))
                .put(IndexService.GLOBAL_CHECKPOINT_SYNC_INTERVAL_SETTING.getKey(), ""100ms"");
        assertAcked(prepareCreate(""test"", settings));
        assertAllShardsOnNodes(""test"", redNodes);
        int numDocs = randomIntBetween(100, 150);
        ArrayList<String> ids = new ArrayList<>();
        logger.info("" --> indexing [{}] docs"", numDocs);
        IndexRequestBuilder[] docs = new IndexRequestBuilder[numDocs];
        for (int i = 0; i < numDocs; i++) {
            String id = randomRealisticUnicodeOfLength(10) + String.valueOf(i);
            ids.add(id);
            docs[i] = client().prepareIndex(""test"", ""type1"", id).setSource(""field1"", English.intToEnglish(i));
        }
        indexRandom(true, docs);
        SearchResponse countResponse = client().prepareSearch(""test"").get();
        assertHitCount(countResponse, numDocs);

        logger.info("" --> moving index to new nodes"");
        Settings build = Settings.builder().put(""index.routing.allocation.exclude.color"", ""red"")
            .put(""index.routing.allocation.include.color"", ""blue"").build();
        client().admin().indices().prepareUpdateSettings(""test"").setSettings(build).execute().actionGet();

        // index while relocating
        logger.info("" --> indexing [{}] more docs"", numDocs);
        for (int i = 0; i < numDocs; i++) {
            String id = randomRealisticUnicodeOfLength(10) + String.valueOf(numDocs + i);
            ids.add(id);
            docs[i] = client().prepareIndex(""test"", ""type1"", id).setSource(""field1"", English.intToEnglish(numDocs + i));
        }
        indexRandom(true, docs);
        numDocs *= 2;

        logger.info("" --> waiting for relocation to complete"");
        ensureGreen(""test""); // move all shards to the new nodes (it waits on relocation)

        final int numIters = randomIntBetween(10, 20);
        for (int i = 0; i < numIters; i++) {
            logger.info("" --> checking iteration {}"", i);
            SearchResponse afterRelocation = client().prepareSearch().setSize(ids.size()).get();
            assertNoFailures(afterRelocation);
            assertSearchHits(afterRelocation, ids.toArray(new String[ids.size()]));
        }

    }
"
"@TestLogging(""org.elasticsearch.snapshot:TRACE"")
    public void testDisruptionOnSnapshotInitialization() throws Exception {
        final Settings settings = Settings.builder()
            .put(DEFAULT_SETTINGS)
            .put(DiscoverySettings.COMMIT_TIMEOUT_SETTING.getKey(), ""30s"") // wait till cluster state is committed
            .build();
        final String idxName = ""test"";
        configureCluster(settings, 4, null, 2);
        final List<String> allMasterEligibleNodes = internalCluster().startMasterOnlyNodes(3);
        final String dataNode = internalCluster().startDataOnlyNode();
        ensureStableCluster(4);

        createRandomIndex(idxName);

        logger.info(""-->  creating repository"");
        assertAcked(client().admin().cluster().preparePutRepository(""test-repo"")
            .setType(""fs"").setSettings(Settings.builder()
                .put(""location"", randomRepoPath())
                .put(""compress"", randomBoolean())
                .put(""chunk_size"", randomIntBetween(100, 1000), ByteSizeUnit.BYTES)));

        // Writing incompatible snapshot can cause this test to fail due to a race condition in repo initialization
        // by the current master and the former master. It is not causing any issues in real life scenario, but
        // might make this test to fail. We are going to complete initialization of the snapshot to prevent this failures.
        logger.info(""-->  initializing the repository"");
        assertEquals(SnapshotState.SUCCESS, client().admin().cluster().prepareCreateSnapshot(""test-repo"", ""test-snap-1"")
            .setWaitForCompletion(true).setIncludeGlobalState(true).setIndices().get().getSnapshotInfo().state());

        final String masterNode1 = internalCluster().getMasterName();
        Set<String> otherNodes = new HashSet<>();
        otherNodes.addAll(allMasterEligibleNodes);
        otherNodes.remove(masterNode1);
        otherNodes.add(dataNode);

        NetworkDisruption networkDisruption =
            new NetworkDisruption(new NetworkDisruption.TwoPartitions(Collections.singleton(masterNode1), otherNodes),
                new NetworkDisruption.NetworkUnresponsive());
        internalCluster().setDisruptionScheme(networkDisruption);

        ClusterService clusterService = internalCluster().clusterService(masterNode1);
        CountDownLatch disruptionStarted = new CountDownLatch(1);
        clusterService.addListener(new ClusterStateListener() {
            @Override
            public void clusterChanged(ClusterChangedEvent event) {
                SnapshotsInProgress snapshots = event.state().custom(SnapshotsInProgress.TYPE);
                if (snapshots != null && snapshots.entries().size() > 0) {
                    if (snapshots.entries().get(0).state() == SnapshotsInProgress.State.INIT) {
                        // The snapshot started, we can start disruption so the INIT state will arrive to another master node
                        logger.info(""--> starting disruption"");
                        networkDisruption.startDisrupting();
                        clusterService.removeListener(this);
                        disruptionStarted.countDown();
                    }
                }
            }
"
"@TestLogging(""_root:DEBUG,org.elasticsearch.cluster.service:TRACE"")
    public void testIsolatedUnicastNodes() throws Exception {
        List<String> nodes = startCluster(4, -1, new int[]{0});
        // Figure out what is the elected master node
        final String unicastTarget = nodes.get(0);

        Set<String> unicastTargetSide = new HashSet<>();
        unicastTargetSide.add(unicastTarget);

        Set<String> restOfClusterSide = new HashSet<>();
        restOfClusterSide.addAll(nodes);
        restOfClusterSide.remove(unicastTarget);

        // Forcefully clean temporal response lists on all nodes. Otherwise the node in the unicast host list
        // includes all the other nodes that have pinged it and the issue doesn't manifest
        ZenPing zenPing = ((TestZenDiscovery) internalCluster().getInstance(Discovery.class)).getZenPing();
        if (zenPing instanceof UnicastZenPing) {
            ((UnicastZenPing) zenPing).clearTemporalResponses();
        }

        // Simulate a network issue between the unicast target node and the rest of the cluster
        NetworkDisruption networkDisconnect = new NetworkDisruption(new TwoPartitions(unicastTargetSide, restOfClusterSide),
                new NetworkDisconnect());
        setDisruptionScheme(networkDisconnect);
        networkDisconnect.startDisrupting();
        // Wait until elected master has removed that the unlucky node...
        ensureStableCluster(3, nodes.get(1));

        // The isolate master node must report no master, so it starts with pinging
        assertNoMaster(unicastTarget);
        networkDisconnect.stopDisrupting();
        // Wait until the master node sees all 3 nodes again.
        ensureStableCluster(4);
    }
"
"@TestLogging(""org.elasticsearch.discovery.zen:TRACE,org.elasticsearch.cluster.service:TRACE"")
    public void setUp() throws Exception {
        super.setUp();
    }
"
"@TestLogging(""_root:DEBUG"")
    public void testNoShardRelocationsOccurWhenElectedMasterNodeFails() throws Exception {
        Settings defaultSettings = Settings.builder()
                .put(FaultDetection.PING_TIMEOUT_SETTING.getKey(), ""1s"")
                .put(FaultDetection.PING_RETRIES_SETTING.getKey(), ""1"")
                .build();

        Settings masterNodeSettings = Settings.builder()
                .put(Node.NODE_DATA_SETTING.getKey(), false)
                .put(defaultSettings)
                .build();
        internalCluster().startNodes(2, masterNodeSettings);
        Settings dateNodeSettings = Settings.builder()
                .put(Node.NODE_MASTER_SETTING.getKey(), false)
                .put(defaultSettings)
                .build();
        internalCluster().startNodes(2, dateNodeSettings);
        ClusterHealthResponse clusterHealthResponse = client().admin().cluster().prepareHealth()
                .setWaitForEvents(Priority.LANGUID)
                .setWaitForNodes(""4"")
                .setWaitForNoRelocatingShards(true)
                .get();
        assertThat(clusterHealthResponse.isTimedOut(), is(false));

        createIndex(""test"");
        ensureSearchable(""test"");
        RecoveryResponse r = client().admin().indices().prepareRecoveries(""test"").get();
        int numRecoveriesBeforeNewMaster = r.shardRecoveryStates().get(""test"").size();

        final String oldMaster = internalCluster().getMasterName();
        internalCluster().stopCurrentMasterNode();
        assertBusy(() -> {
            String current = internalCluster().getMasterName();
            assertThat(current, notNullValue());
            assertThat(current, not(equalTo(oldMaster)));
        });
        ensureSearchable(""test"");

        r = client().admin().indices().prepareRecoveries(""test"").get();
        int numRecoveriesAfterNewMaster = r.shardRecoveryStates().get(""test"").size();
        assertThat(numRecoveriesAfterNewMaster, equalTo(numRecoveriesBeforeNewMaster));
    }
"
"@TestLogging(""org.elasticsearch.discovery.zen.publish:TRACE"")
        public MockNode setAsMaster() {
            this.clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
                .masterNodeId(discoveryNode.getId())).build();
            return this;
        }
"
"@TestLogging(""_root:DEBUG,org.elasticsearch.cluster.service:TRACE"")
    public void testFailWithMinimumMasterNodesConfigured() throws Exception {
        List<String> nodes = startCluster(3);

        // Figure out what is the elected master node
        final String masterNode = internalCluster().getMasterName();
        logger.info(""---> legit elected master node={}"", masterNode);

        // Pick a node that isn't the elected master.
        Set<String> nonMasters = new HashSet<>(nodes);
        nonMasters.remove(masterNode);
        final String unluckyNode = randomFrom(nonMasters.toArray(Strings.EMPTY_ARRAY));


        // Simulate a network issue between the unlucky node and elected master node in both directions.

        NetworkDisruption networkDisconnect = new NetworkDisruption(
                new NetworkDisruption.TwoPartitions(masterNode, unluckyNode),
                new NetworkDisruption.NetworkDisconnect());
        setDisruptionScheme(networkDisconnect);
        networkDisconnect.startDisrupting();

        // Wait until elected master has removed that the unlucky node...
        ensureStableCluster(2, masterNode);

        // The unlucky node must report *no* master node, since it can't connect to master and in fact it should
        // continuously ping until network failures have been resolved. However
        // It may a take a bit before the node detects it has been cut off from the elected master
        assertNoMaster(unluckyNode);

        networkDisconnect.stopDisrupting();

        // Wait until the master node sees all 3 nodes again.
        ensureStableCluster(3);

        // The elected master shouldn't have changed, since the unlucky node never could have elected himself as
        // master since m_m_n of 2 could never be satisfied.
        assertMaster(masterNode, nodes);
    }
"
"    @TestLogging(""_root:DEBUG,org.elasticsearch.cluster.service:TRACE,org.elasticsearch.test.disruption:TRACE"")
    public void testStaleMasterNotHijackingMajority() throws Exception {
        // 3 node cluster with unicast discovery and minimum_master_nodes set to 2:
        final List<String> nodes = startCluster(3, 2);

        // Save the current master node as old master node, because that node will get frozen
        final String oldMasterNode = internalCluster().getMasterName();
        for (String node : nodes) {
            ensureStableCluster(3, node);
        }
        assertMaster(oldMasterNode, nodes);

        // Simulating a painful gc by suspending all threads for a long time on the current elected master node.
        SingleNodeDisruption masterNodeDisruption = new LongGCDisruption(random(), oldMasterNode);

        // Save the majority side
        final List<String> majoritySide = new ArrayList<>(nodes);
        majoritySide.remove(oldMasterNode);

        // Keeps track of the previous and current master when a master node transition took place on each node on the majority side:
        final Map<String, List<Tuple<String, String>>> masters = Collections.synchronizedMap(new HashMap<String, List<Tuple<String,
                        String>>>());
        for (final String node : majoritySide) {
            masters.put(node, new ArrayList<Tuple<String, String>>());
            internalCluster().getInstance(ClusterService.class, node).addListener(event -> {
                DiscoveryNode previousMaster = event.previousState().nodes().getMasterNode();
                DiscoveryNode currentMaster = event.state().nodes().getMasterNode();
                if (!Objects.equals(previousMaster, currentMaster)) {
                    logger.info(""node {} received new cluster state: {} \n and had previous cluster state: {}"", node, event.state(),
                            event.previousState());
                    String previousMasterNodeName = previousMaster != null ? previousMaster.getName() : null;
                    String currentMasterNodeName = currentMaster != null ? currentMaster.getName() : null;
                    masters.get(node).add(new Tuple<>(previousMasterNodeName, currentMasterNodeName));
                }
            });
        }

        final CountDownLatch oldMasterNodeSteppedDown = new CountDownLatch(1);
        internalCluster().getInstance(ClusterService.class, oldMasterNode).addListener(event -> {
            if (event.state().nodes().getMasterNodeId() == null) {
                oldMasterNodeSteppedDown.countDown();
            }
        });

        internalCluster().setDisruptionScheme(masterNodeDisruption);
        logger.info(""freezing node [{}]"", oldMasterNode);
        masterNodeDisruption.startDisrupting();

        // Wait for the majority side to get stable
        assertDifferentMaster(majoritySide.get(0), oldMasterNode);
        assertDifferentMaster(majoritySide.get(1), oldMasterNode);

        // the test is periodically tripping on the following assertion. To find out which threads are blocking the nodes from making
        // progress we print a stack dump
        boolean failed = true;
        try {
            assertDiscoveryCompleted(majoritySide);
            failed = false;
        } finally {
            if (failed) {
                logger.error(""discovery failed to complete, probably caused by a blocked thread: {}"",
                        new HotThreads().busiestThreads(Integer.MAX_VALUE).ignoreIdleThreads(false).detect());
            }
        }

        // The old master node is frozen, but here we submit a cluster state update task that doesn't get executed,
        // but will be queued and once the old master node un-freezes it gets executed.
        // The old master node will send this update + the cluster state where he is flagged as master to the other
        // nodes that follow the new master. These nodes should ignore this update.
        internalCluster().getInstance(ClusterService.class, oldMasterNode).submitStateUpdateTask(""sneaky-update"", new
                ClusterStateUpdateTask(Priority.IMMEDIATE) {
                    @Override
                    public ClusterState execute(ClusterState currentState) throws Exception {
                        return ClusterState.builder(currentState).build();
                    }
"
"    @TestLogging(
    public void testIsolateMasterAndVerifyClusterStateConsensus() throws Exception {
        final List<String> nodes = startCluster(3);

        assertAcked(prepareCreate(""test"")
                .setSettings(Settings.builder()
                        .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1 + randomInt(2))
                        .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, randomInt(2))
                ));

        ensureGreen();
        String isolatedNode = internalCluster().getMasterName();
        TwoPartitions partitions = isolateNode(isolatedNode);
        NetworkDisruption networkDisruption = addRandomDisruptionType(partitions);
        networkDisruption.startDisrupting();

        String nonIsolatedNode = partitions.getMajoritySide().iterator().next();

        // make sure cluster reforms
        ensureStableCluster(2, nonIsolatedNode);

        // make sure isolated need picks up on things.
        assertNoMaster(isolatedNode, TimeValue.timeValueSeconds(40));

        // restore isolation
        networkDisruption.stopDisrupting();

        for (String node : nodes) {
            ensureStableCluster(3, new TimeValue(DISRUPTION_HEALING_OVERHEAD.millis() + networkDisruption.expectedTimeToHeal().millis()),
                    true, node);
        }

        logger.info(""issue a reroute"");
        // trigger a reroute now, instead of waiting for the background reroute of RerouteService
        assertAcked(client().admin().cluster().prepareReroute());
        // and wait for it to finish and for the cluster to stabilize
        ensureGreen(""test"");

        // verify all cluster states are the same
        // use assert busy to wait for cluster states to be applied (as publish_timeout has low value)
        assertBusy(() -> {
            ClusterState state = null;
            for (String node : nodes) {
                ClusterState nodeState = getNodeClusterState(node);
                if (state == null) {
                    state = nodeState;
                    continue;
                }
                // assert nodes are identical
                try {
                    assertEquals(""unequal versions"", state.version(), nodeState.version());
                    assertEquals(""unequal node count"", state.nodes().getSize(), nodeState.nodes().getSize());
                    assertEquals(""different masters "", state.nodes().getMasterNodeId(), nodeState.nodes().getMasterNodeId());
                    assertEquals(""different meta data version"", state.metaData().version(), nodeState.metaData().version());
                    assertEquals(""different routing"", state.routingTable().toString(), nodeState.routingTable().toString());
                } catch (AssertionError t) {
                    fail(""failed comparing cluster state: "" + t.getMessage() + ""\n"" +
                            ""--- cluster state of node ["" + nodes.get(0) + ""]: ---\n"" + state +
                            ""\n--- cluster state ["" + node + ""]: ---\n"" + nodeState);
                }

            }
        });
    }
"
"    @TestLogging(
    public void testMappingTimeout() throws Exception {
        startCluster(3);
        createIndex(""test"", Settings.builder()
            .put(""index.number_of_shards"", 1)
            .put(""index.number_of_replicas"", 1)
            .put(""index.routing.allocation.exclude._name"", internalCluster().getMasterName())
        .build());

        // create one field
        index(""test"", ""doc"", ""1"", ""{ \""f\"": 1 }"");

        ensureGreen();

        assertAcked(client().admin().cluster().prepareUpdateSettings().setTransientSettings(
            Settings.builder().put(""indices.mapping.dynamic_timeout"", ""1ms"")));

        ServiceDisruptionScheme disruption = new BlockMasterServiceOnMaster(random());
        setDisruptionScheme(disruption);

        disruption.startDisrupting();

        BulkRequestBuilder bulk = client().prepareBulk();
        bulk.add(client().prepareIndex(""test"", ""doc"", ""2"").setSource(""{ \""f\"": 1 }"", XContentType.JSON));
        bulk.add(client().prepareIndex(""test"", ""doc"", ""3"").setSource(""{ \""g\"": 1 }"", XContentType.JSON));
        bulk.add(client().prepareIndex(""test"", ""doc"", ""4"").setSource(""{ \""f\"": 1 }"", XContentType.JSON));
        BulkResponse bulkResponse = bulk.get();
        assertTrue(bulkResponse.hasFailures());

        disruption.stopDisrupting();

        assertBusy(() -> {
            IndicesStatsResponse stats = client().admin().indices().prepareStats(""test"").clear().get();
            for (ShardStats shardStats : stats.getShards()) {
                assertThat(shardStats.getShardRouting().toString(),
                    shardStats.getSeqNoStats().getGlobalCheckpoint(), equalTo(shardStats.getSeqNoStats().getLocalCheckpoint()));
            }
        });

    }
"
"    @TestLogging(""_root:DEBUG,org.elasticsearch.action.bulk:TRACE,org.elasticsearch.action.get:TRACE,"" +
    public void testAckedIndexing() throws Exception {

        final int seconds = !(TEST_NIGHTLY && rarely()) ? 1 : 5;
        final String timeout = seconds + ""s"";

        final List<String> nodes = startCluster(rarely() ? 5 : 3);

        assertAcked(prepareCreate(""test"")
            .setSettings(Settings.builder()
                .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1 + randomInt(2))
                .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, randomInt(2))
            ));
        ensureGreen();

        ServiceDisruptionScheme disruptionScheme = addRandomDisruptionScheme();
        logger.info(""disruption scheme [{}] added"", disruptionScheme);

        final ConcurrentHashMap<String, String> ackedDocs = new ConcurrentHashMap<>(); // id -> node sent.

        final AtomicBoolean stop = new AtomicBoolean(false);
        List<Thread> indexers = new ArrayList<>(nodes.size());
        List<Semaphore> semaphores = new ArrayList<>(nodes.size());
        final AtomicInteger idGenerator = new AtomicInteger(0);
        final AtomicReference<CountDownLatch> countDownLatchRef = new AtomicReference<>();
        final List<Exception> exceptedExceptions = Collections.synchronizedList(new ArrayList<Exception>());

        logger.info(""starting indexers"");
        try {
            for (final String node : nodes) {
                final Semaphore semaphore = new Semaphore(0);
                semaphores.add(semaphore);
                final Client client = client(node);
                final String name = ""indexer_"" + indexers.size();
                final int numPrimaries = getNumShards(""test"").numPrimaries;
                Thread thread = new Thread(() -> {
                    while (!stop.get()) {
                        String id = null;
                        try {
                            if (!semaphore.tryAcquire(10, TimeUnit.SECONDS)) {
                                continue;
                            }
                            logger.info(""[{}] Acquired semaphore and it has {} permits left"", name, semaphore.availablePermits());
                            try {
                                id = Integer.toString(idGenerator.incrementAndGet());
                                int shard = Math.floorMod(Murmur3HashFunction.hash(id), numPrimaries);
                                logger.trace(""[{}] indexing id [{}] through node [{}] targeting shard [{}]"", name, id, node, shard);
                                IndexResponse response =
                                        client.prepareIndex(""test"", ""type"", id)
                                                .setSource(""{}"", XContentType.JSON)
                                                .setTimeout(timeout)
                                                .get(timeout);
                                assertEquals(DocWriteResponse.Result.CREATED, response.getResult());
                                ackedDocs.put(id, node);
                                logger.trace(""[{}] indexed id [{}] through node [{}], response [{}]"", name, id, node, response);
                            } catch (ElasticsearchException e) {
                                exceptedExceptions.add(e);
                                final String docId = id;
                                logger.trace(() -> new ParameterizedMessage(""[{}] failed id [{}] through node [{}]"", name, docId, node), e);
                            } finally {
                                countDownLatchRef.get().countDown();
                                logger.trace(""[{}] decreased counter : {}"", name, countDownLatchRef.get().getCount());
                            }
                        } catch (InterruptedException e) {
                            // fine - semaphore interrupt
                        } catch (AssertionError | Exception e) {
                            logger.info(() -> new ParameterizedMessage(""unexpected exception in background thread of [{}]"", node), e);
                        }
                    }
                });

                thread.setName(name);
                thread.start();
                indexers.add(thread);
            }

            int docsPerIndexer = randomInt(3);
            logger.info(""indexing {} docs per indexer before partition"", docsPerIndexer);
            countDownLatchRef.set(new CountDownLatch(docsPerIndexer * indexers.size()));
            for (Semaphore semaphore : semaphores) {
                semaphore.release(docsPerIndexer);
            }
            assertTrue(countDownLatchRef.get().await(1, TimeUnit.MINUTES));

            for (int iter = 1 + randomInt(2); iter > 0; iter--) {
                logger.info(""starting disruptions & indexing (iteration [{}])"", iter);
                disruptionScheme.startDisrupting();

                docsPerIndexer = 1 + randomInt(5);
                logger.info(""indexing {} docs per indexer during partition"", docsPerIndexer);
                countDownLatchRef.set(new CountDownLatch(docsPerIndexer * indexers.size()));
                Collections.shuffle(semaphores, random());
                for (Semaphore semaphore : semaphores) {
                    assertThat(semaphore.availablePermits(), equalTo(0));
                    semaphore.release(docsPerIndexer);
                }
                logger.info(""waiting for indexing requests to complete"");
                assertTrue(countDownLatchRef.get().await(docsPerIndexer * seconds * 1000 + 2000, TimeUnit.MILLISECONDS));

                logger.info(""stopping disruption"");
                disruptionScheme.stopDisrupting();
                for (String node : internalCluster().getNodeNames()) {
                    ensureStableCluster(nodes.size(), TimeValue.timeValueMillis(disruptionScheme.expectedTimeToHeal().millis() +
                        DISRUPTION_HEALING_OVERHEAD.millis()), true, node);
                }
                // in case of a bridge partition, shard allocation can fail ""index.allocation.max_retries"" times if the master
                // is the super-connected node and recovery source and target are on opposite sides of the bridge
                if (disruptionScheme instanceof NetworkDisruption &&
                    ((NetworkDisruption) disruptionScheme).getDisruptedLinks() instanceof Bridge) {
                    assertAcked(client().admin().cluster().prepareReroute().setRetryFailed(true));
                }
                ensureGreen(""test"");

                logger.info(""validating successful docs"");
                assertBusy(() -> {
                    for (String node : nodes) {
                        try {
                            logger.debug(""validating through node [{}] ([{}] acked docs)"", node, ackedDocs.size());
                            for (String id : ackedDocs.keySet()) {
                                assertTrue(""doc ["" + id + ""] indexed via node ["" + ackedDocs.get(id) + ""] not found"",
                                    client(node).prepareGet(""test"", ""type"", id).setPreference(""_local"").get().isExists());
                            }
                        } catch (AssertionError | NoShardAvailableActionException e) {
                            throw new AssertionError(e.getMessage() + "" (checked via node ["" + node + ""]"", e);
                        }
                    }
                }, 30, TimeUnit.SECONDS);

                logger.info(""done validating (iteration [{}])"", iter);
            }
        } finally {
            if (exceptedExceptions.size() > 0) {
                StringBuilder sb = new StringBuilder();
                for (Exception e : exceptedExceptions) {
                    sb.append(""\n"").append(e.getMessage());
                }
                logger.debug(""Indexing exceptions during disruption: {}"", sb);
            }
            logger.info(""shutting down indexers"");
            stop.set(true);
            for (Thread indexer : indexers) {
                indexer.interrupt();
                indexer.join(60000);
            }
        }
    }
"
"    @TestLogging(""org.elasticsearch.cluster.service:TRACE"") // To ensure that we log cluster state events on TRACE level
    public void testClusterStateUpdateLogging() throws Exception {
        MockLogAppender mockAppender = new MockLogAppender();
        mockAppender.start();
        mockAppender.addExpectation(
            new MockLogAppender.SeenEventExpectation(
                ""test1"",
                masterService.getClass().getCanonicalName(),
                Level.DEBUG,
                ""*processing [test1]: took [1s] no change in cluster state""));
        mockAppender.addExpectation(
            new MockLogAppender.SeenEventExpectation(
                ""test2"",
                masterService.getClass().getCanonicalName(),
                Level.TRACE,
                ""*failed to execute cluster state update in [2s]*""));
        mockAppender.addExpectation(
            new MockLogAppender.SeenEventExpectation(
                ""test3"",
                masterService.getClass().getCanonicalName(),
                Level.DEBUG,
                ""*processing [test3]: took [3s] done publishing updated cluster state (version: *, uuid: *)""));

        Logger clusterLogger = Loggers.getLogger(masterService.getClass().getPackage().getName());
        Loggers.addAppender(clusterLogger, mockAppender);
        try {
            final CountDownLatch latch = new CountDownLatch(4);
            masterService.currentTimeOverride = System.nanoTime();
            masterService.submitStateUpdateTask(""test1"", new ClusterStateUpdateTask() {
                @Override
                public ClusterState execute(ClusterState currentState) throws Exception {
                    masterService.currentTimeOverride += TimeValue.timeValueSeconds(1).nanos();
                    return currentState;
                }
"
"    @TestLogging(""org.elasticsearch.cluster.service:WARN"") // To ensure that we log cluster state events on WARN level
    public void testLongClusterStateUpdateLogging() throws Exception {
        MockLogAppender mockAppender = new MockLogAppender();
        mockAppender.start();
        mockAppender.addExpectation(
            new MockLogAppender.UnseenEventExpectation(
                ""test1 shouldn't see because setting is too low"",
                masterService.getClass().getCanonicalName(),
                Level.WARN,
                ""*cluster state update task [test1] took [*] above the warn threshold of *""));
        mockAppender.addExpectation(
            new MockLogAppender.SeenEventExpectation(
                ""test2"",
                masterService.getClass().getCanonicalName(),
                Level.WARN,
                ""*cluster state update task [test2] took [32s] above the warn threshold of *""));
        mockAppender.addExpectation(
            new MockLogAppender.SeenEventExpectation(
                ""test3"",
                masterService.getClass().getCanonicalName(),
                Level.WARN,
                ""*cluster state update task [test3] took [33s] above the warn threshold of *""));
        mockAppender.addExpectation(
            new MockLogAppender.SeenEventExpectation(
                ""test4"",
                masterService.getClass().getCanonicalName(),
                Level.WARN,
                ""*cluster state update task [test4] took [34s] above the warn threshold of *""));

        Logger clusterLogger = Loggers.getLogger(masterService.getClass().getPackage().getName());
        Loggers.addAppender(clusterLogger, mockAppender);
        try {
            final CountDownLatch latch = new CountDownLatch(5);
            final CountDownLatch processedFirstTask = new CountDownLatch(1);
            masterService.currentTimeOverride = System.nanoTime();
            masterService.submitStateUpdateTask(""test1"", new ClusterStateUpdateTask() {
                @Override
                public ClusterState execute(ClusterState currentState) throws Exception {
                    masterService.currentTimeOverride += TimeValue.timeValueSeconds(1).nanos();
                    return currentState;
                }
"
"    @TestLogging(""org.elasticsearch.cluster.service:TRACE"") // To ensure that we log cluster state events on TRACE level
    public void testClusterStateUpdateLogging() throws Exception {
        MockLogAppender mockAppender = new MockLogAppender();
        mockAppender.start();
        mockAppender.addExpectation(
                new MockLogAppender.SeenEventExpectation(
                        ""test1"",
                        clusterApplierService.getClass().getCanonicalName(),
                        Level.DEBUG,
                        ""*processing [test1]: took [1s] no change in cluster state""));
        mockAppender.addExpectation(
                new MockLogAppender.SeenEventExpectation(
                        ""test2"",
                        clusterApplierService.getClass().getCanonicalName(),
                        Level.TRACE,
                        ""*failed to execute cluster state applier in [2s]*""));

        Logger clusterLogger = Loggers.getLogger(""org.elasticsearch.cluster.service"");
        Loggers.addAppender(clusterLogger, mockAppender);
        try {
            final CountDownLatch latch = new CountDownLatch(3);
            clusterApplierService.currentTimeOverride = System.nanoTime();
            clusterApplierService.runOnApplierThread(""test1"",
                currentState -> clusterApplierService.currentTimeOverride += TimeValue.timeValueSeconds(1).nanos(),
                new ClusterApplyListener() {
                    @Override
                    public void onSuccess(String source) {
                        latch.countDown();
                    }
"
"    @TestLogging(""org.elasticsearch.cluster.service:WARN"") // To ensure that we log cluster state events on WARN level
    public void testLongClusterStateUpdateLogging() throws Exception {
        MockLogAppender mockAppender = new MockLogAppender();
        mockAppender.start();
        mockAppender.addExpectation(
                new MockLogAppender.UnseenEventExpectation(
                        ""test1 shouldn't see because setting is too low"",
                        clusterApplierService.getClass().getCanonicalName(),
                        Level.WARN,
                        ""*cluster state applier task [test1] took [*] above the warn threshold of *""));
        mockAppender.addExpectation(
                new MockLogAppender.SeenEventExpectation(
                        ""test2"",
                        clusterApplierService.getClass().getCanonicalName(),
                        Level.WARN,
                        ""*cluster state applier task [test2] took [32s] above the warn threshold of *""));
        mockAppender.addExpectation(
                new MockLogAppender.SeenEventExpectation(
                        ""test4"",
                        clusterApplierService.getClass().getCanonicalName(),
                        Level.WARN,
                        ""*cluster state applier task [test3] took [34s] above the warn threshold of *""));

        Logger clusterLogger = Loggers.getLogger(""org.elasticsearch.cluster.service"");
        Loggers.addAppender(clusterLogger, mockAppender);
        try {
            final CountDownLatch latch = new CountDownLatch(4);
            final CountDownLatch processedFirstTask = new CountDownLatch(1);
            clusterApplierService.currentTimeOverride = System.nanoTime();
            clusterApplierService.runOnApplierThread(""test1"",
                currentState -> clusterApplierService.currentTimeOverride += TimeValue.timeValueSeconds(1).nanos(),
                new ClusterApplyListener() {
                    @Override
                    public void onSuccess(String source) {
                        latch.countDown();
                        processedFirstTask.countDown();
                    }
"
"    @TestLogging(""_root:debug,org.elasticsearch.action.admin.cluster.tasks:trace"")
    public void testPendingUpdateTask() throws Exception {
        String node_0 = internalCluster().startNode();
        internalCluster().startCoordinatingOnlyNode(Settings.EMPTY);

        final ClusterService clusterService = internalCluster().getInstance(ClusterService.class, node_0);
        final CountDownLatch block1 = new CountDownLatch(1);
        final CountDownLatch invoked1 = new CountDownLatch(1);
        clusterService.submitStateUpdateTask(""1"", new ClusterStateUpdateTask() {
            @Override
            public ClusterState execute(ClusterState currentState) {
                invoked1.countDown();
                try {
                    block1.await();
                } catch (InterruptedException e) {
                    fail();
                }
                return currentState;
            }
"
"@TestLogging(""_root:DEBUG,org.elasticsearch.action.admin.cluster.state:TRACE"")
    public void testSimpleOnlyMasterNodeElection() throws IOException {
        logger.info(""--> start data node / non master node"");
        internalCluster().startNode(Settings.builder().put(Node.NODE_DATA_SETTING.getKey(), true).put(Node.NODE_MASTER_SETTING.getKey(), false)
            .put(""discovery.initial_state_timeout"", ""1s""));
        try {
            assertThat(client().admin().cluster().prepareState().setMasterNodeTimeout(""100ms"").execute().actionGet().getState().nodes().getMasterNodeId(), nullValue());
            fail(""should not be able to find master"");
        } catch (MasterNotDiscoveredException e) {
            // all is well, no master elected
        }
        logger.info(""--> start master node"");
        final String masterNodeName = internalCluster().startNode(Settings.builder().put(Node.NODE_DATA_SETTING.getKey(), false).put(Node.NODE_MASTER_SETTING.getKey(), true));
        assertThat(internalCluster().nonMasterClient().admin().cluster().prepareState().execute().actionGet().getState().nodes().getMasterNode().getName(), equalTo(masterNodeName));
        assertThat(internalCluster().masterClient().admin().cluster().prepareState().execute().actionGet().getState().nodes().getMasterNode().getName(), equalTo(masterNodeName));

        logger.info(""--> stop master node"");
        internalCluster().stopCurrentMasterNode();

        try {
            assertThat(client().admin().cluster().prepareState().setMasterNodeTimeout(""100ms"").execute().actionGet().getState().nodes().getMasterNodeId(), nullValue());
            fail(""should not be able to find master"");
        } catch (MasterNotDiscoveredException e) {
            // all is well, no master elected
        }

        logger.info(""--> start master node"");
        final String nextMasterEligibleNodeName = internalCluster().startNode(Settings.builder().put(Node.NODE_DATA_SETTING.getKey(), false).put(Node.NODE_MASTER_SETTING.getKey(), true));
        assertThat(internalCluster().nonMasterClient().admin().cluster().prepareState().execute().actionGet().getState().nodes().getMasterNode().getName(), equalTo(nextMasterEligibleNodeName));
        assertThat(internalCluster().masterClient().admin().cluster().prepareState().execute().actionGet().getState().nodes().getMasterNode().getName(), equalTo(nextMasterEligibleNodeName));
    }
"
"@TestLogging(""_root:DEBUG,org.elasticsearch.cluster.service:TRACE,org.elasticsearch.discovery.zen:TRACE"")
    public void testSimpleMinimumMasterNodes() throws Exception {

        Settings settings = Settings.builder()
                .put(""discovery.zen.minimum_master_nodes"", 2)
                .put(ZenDiscovery.PING_TIMEOUT_SETTING.getKey(), ""200ms"")
                .put(""discovery.initial_state_timeout"", ""500ms"")
                .build();

        logger.info(""--> start first node"");
        internalCluster().startNode(settings);

        logger.info(""--> should be blocked, no master..."");
        ClusterState state = client().admin().cluster().prepareState().setLocal(true).execute().actionGet().getState();
        assertThat(state.blocks().hasGlobalBlock(DiscoverySettings.NO_MASTER_BLOCK_ID), equalTo(true));
        assertThat(state.nodes().getSize(), equalTo(1)); // verify that we still see the local node in the cluster state

        logger.info(""--> start second node, cluster should be formed"");
        internalCluster().startNode(settings);

        ClusterHealthResponse clusterHealthResponse = client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForNodes(""2"").execute().actionGet();
        assertThat(clusterHealthResponse.isTimedOut(), equalTo(false));

        state = client().admin().cluster().prepareState().setLocal(true).execute().actionGet().getState();
        assertThat(state.blocks().hasGlobalBlock(DiscoverySettings.NO_MASTER_BLOCK_ID), equalTo(false));
        state = client().admin().cluster().prepareState().setLocal(true).execute().actionGet().getState();
        assertThat(state.blocks().hasGlobalBlock(DiscoverySettings.NO_MASTER_BLOCK_ID), equalTo(false));

        state = client().admin().cluster().prepareState().execute().actionGet().getState();
        assertThat(state.nodes().getSize(), equalTo(2));
        assertThat(state.metaData().indices().containsKey(""test""), equalTo(false));

        createIndex(""test"");
        NumShards numShards = getNumShards(""test"");
        logger.info(""--> indexing some data"");
        for (int i = 0; i < 100; i++) {
            client().prepareIndex(""test"", ""type1"", Integer.toString(i)).setSource(""field"", ""value"").execute().actionGet();
        }
        // make sure that all shards recovered before trying to flush
        assertThat(client().admin().cluster().prepareHealth(""test"").setWaitForActiveShards(numShards.totalNumShards).execute().actionGet().getActiveShards(), equalTo(numShards.totalNumShards));
        // flush for simpler debugging
        flushAndRefresh();

        logger.info(""--> verify we the data back"");
        for (int i = 0; i < 10; i++) {
            assertThat(client().prepareSearch().setSize(0).setQuery(QueryBuilders.matchAllQuery()).execute().actionGet().getHits().getTotalHits(), equalTo(100L));
        }

        internalCluster().stopCurrentMasterNode();
        awaitBusy(() -> {
            ClusterState clusterState = client().admin().cluster().prepareState().setLocal(true).execute().actionGet().getState();
            return clusterState.blocks().hasGlobalBlock(DiscoverySettings.NO_MASTER_BLOCK_ID);
        });
        state = client().admin().cluster().prepareState().setLocal(true).execute().actionGet().getState();
        assertThat(state.blocks().hasGlobalBlock(DiscoverySettings.NO_MASTER_BLOCK_ID), equalTo(true));
        // verify that both nodes are still in the cluster state but there is no master
        assertThat(state.nodes().getSize(), equalTo(2));
        assertThat(state.nodes().getMasterNode(), equalTo(null));

        logger.info(""--> starting the previous master node again..."");
        internalCluster().startNode(settings);

        clusterHealthResponse = client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForYellowStatus().setWaitForNodes(""2"").execute().actionGet();
        assertThat(clusterHealthResponse.isTimedOut(), equalTo(false));

        state = client().admin().cluster().prepareState().setLocal(true).execute().actionGet().getState();
        assertThat(state.blocks().hasGlobalBlock(DiscoverySettings.NO_MASTER_BLOCK_ID), equalTo(false));
        state = client().admin().cluster().prepareState().setLocal(true).execute().actionGet().getState();
        assertThat(state.blocks().hasGlobalBlock(DiscoverySettings.NO_MASTER_BLOCK_ID), equalTo(false));

        state = client().admin().cluster().prepareState().execute().actionGet().getState();
        assertThat(state.nodes().getSize(), equalTo(2));
        assertThat(state.metaData().indices().containsKey(""test""), equalTo(true));

        ensureGreen();

        logger.info(""--> verify we the data back after cluster reform"");
        for (int i = 0; i < 10; i++) {
            assertHitCount(client().prepareSearch().setSize(0).setQuery(QueryBuilders.matchAllQuery()).execute().actionGet(), 100);
        }

        internalCluster().stopRandomNonMasterNode();
        assertBusy(() -> {
            ClusterState state1 = client().admin().cluster().prepareState().setLocal(true).execute().actionGet().getState();
            assertThat(state1.blocks().hasGlobalBlock(DiscoverySettings.NO_MASTER_BLOCK_ID), equalTo(true));
        });

        logger.info(""--> starting the previous master node again..."");
        internalCluster().startNode(settings);

        ensureGreen();
        clusterHealthResponse = client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForNodes(""2"").setWaitForGreenStatus().execute().actionGet();
        assertThat(clusterHealthResponse.isTimedOut(), equalTo(false));

        state = client().admin().cluster().prepareState().setLocal(true).execute().actionGet().getState();
        assertThat(state.blocks().hasGlobalBlock(DiscoverySettings.NO_MASTER_BLOCK_ID), equalTo(false));
        state = client().admin().cluster().prepareState().setLocal(true).execute().actionGet().getState();
        assertThat(state.blocks().hasGlobalBlock(DiscoverySettings.NO_MASTER_BLOCK_ID), equalTo(false));

        state = client().admin().cluster().prepareState().execute().actionGet().getState();
        assertThat(state.nodes().getSize(), equalTo(2));
        assertThat(state.metaData().indices().containsKey(""test""), equalTo(true));

        logger.info(""Running Cluster Health"");
        ensureGreen();

        logger.info(""--> verify we the data back"");
        for (int i = 0; i < 10; i++) {
            assertHitCount(client().prepareSearch().setSize(0).setQuery(QueryBuilders.matchAllQuery()).execute().actionGet(), 100);
        }
    }
"
"    @TestLogging(""_root:DEBUG, org.elasticsearch.cluster.routing.allocation:TRACE, org.elasticsearch.cluster.action.shard:TRACE,"" +
    public void testPrimaryReplicaResyncFailed() throws Exception {
        String master = internalCluster().startMasterOnlyNode(Settings.EMPTY);
        final int numberOfReplicas = between(2, 3);
        final String oldPrimary = internalCluster().startDataOnlyNode();
        assertAcked(
            prepareCreate(""test"", Settings.builder().put(indexSettings())
                .put(SETTING_NUMBER_OF_SHARDS, 1)
                .put(SETTING_NUMBER_OF_REPLICAS, numberOfReplicas)));
        final ShardId shardId = new ShardId(clusterService().state().metaData().index(""test"").getIndex(), 0);
        final Set<String> replicaNodes = new HashSet<>(internalCluster().startDataOnlyNodes(numberOfReplicas));
        ensureGreen();
        assertAcked(
            client(master).admin().cluster().prepareUpdateSettings()
                .setTransientSettings(Settings.builder().put(""cluster.routing.allocation.enable"", ""none"")).get());
        logger.info(""--> Indexing with gap in seqno to ensure that some operations will be replayed in resync"");
        long numDocs = scaledRandomIntBetween(5, 50);
        for (int i = 0; i < numDocs; i++) {
            IndexResponse indexResult = index(""test"", ""doc"", Long.toString(i));
            assertThat(indexResult.getShardInfo().getSuccessful(), equalTo(numberOfReplicas + 1));
        }
        final IndexShard oldPrimaryShard = internalCluster().getInstance(IndicesService.class, oldPrimary).getShardOrNull(shardId);
        EngineTestCase.generateNewSeqNo(IndexShardTestCase.getEngine(oldPrimaryShard)); // Make gap in seqno.
        long moreDocs = scaledRandomIntBetween(1, 10);
        for (int i = 0; i < moreDocs; i++) {
            IndexResponse indexResult = index(""test"", ""doc"", Long.toString(numDocs + i));
            assertThat(indexResult.getShardInfo().getSuccessful(), equalTo(numberOfReplicas + 1));
        }
        final Set<String> replicasSide1 = Sets.newHashSet(randomSubsetOf(between(1, numberOfReplicas - 1), replicaNodes));
        final Set<String> replicasSide2 = Sets.difference(replicaNodes, replicasSide1);
        NetworkDisruption partition = new NetworkDisruption(new TwoPartitions(replicasSide1, replicasSide2), new NetworkDisconnect());
        internalCluster().setDisruptionScheme(partition);
        logger.info(""--> isolating some replicas during primary-replica resync"");
        partition.startDisrupting();
        internalCluster().stopRandomNode(InternalTestCluster.nameFilter(oldPrimary));
        // Checks that we fails replicas in one side but not mark them as stale.
        assertBusy(() -> {
            ClusterState state = client(master).admin().cluster().prepareState().get().getState();
            final IndexShardRoutingTable shardRoutingTable = state.routingTable().shardRoutingTable(shardId);
            final String newPrimaryNode = state.getRoutingNodes().node(shardRoutingTable.primary.currentNodeId()).node().getName();
            assertThat(newPrimaryNode, not(equalTo(oldPrimary)));
            Set<String> selectedPartition = replicasSide1.contains(newPrimaryNode) ? replicasSide1 : replicasSide2;
            assertThat(shardRoutingTable.activeShards(), hasSize(selectedPartition.size()));
            for (ShardRouting activeShard : shardRoutingTable.activeShards()) {
                assertThat(state.getRoutingNodes().node(activeShard.currentNodeId()).node().getName(), isIn(selectedPartition));
            }
            assertThat(state.metaData().index(""test"").inSyncAllocationIds(shardId.id()), hasSize(numberOfReplicas + 1));
        }, 1, TimeUnit.MINUTES);
        assertAcked(
            client(master).admin().cluster().prepareUpdateSettings()
                .setTransientSettings(Settings.builder().put(""cluster.routing.allocation.enable"", ""all"")).get());
        partition.stopDisrupting();
        partition.ensureHealthy(internalCluster());
        logger.info(""--> stop disrupting network and re-enable allocation"");
        assertBusy(() -> {
            ClusterState state = client(master).admin().cluster().prepareState().get().getState();
            assertThat(state.routingTable().shardRoutingTable(shardId).activeShards(), hasSize(numberOfReplicas));
            assertThat(state.metaData().index(""test"").inSyncAllocationIds(shardId.id()), hasSize(numberOfReplicas + 1));
            for (String node : replicaNodes) {
                IndexShard shard = internalCluster().getInstance(IndicesService.class, node).getShardOrNull(shardId);
                assertThat(shard.getLocalCheckpoint(), equalTo(numDocs + moreDocs));
            }
        }, 30, TimeUnit.SECONDS);
        internalCluster().assertConsistentHistoryBetweenTranslogAndLuceneIndex();
    }
"
"    @TestLogging(""org.elasticsearch.persistent:TRACE,org.elasticsearch.cluster.service:DEBUG"")
    public void testFullClusterRestart() throws Exception {
        PersistentTasksService service = internalCluster().getInstance(PersistentTasksService.class);
        int numberOfTasks = randomIntBetween(1, 10);
        String[] taskIds = new String[numberOfTasks];
        List<PlainActionFuture<PersistentTask<TestParams>>> futures = new ArrayList<>(numberOfTasks);

        for (int i = 0; i < numberOfTasks; i++) {
            PlainActionFuture<PersistentTask<TestParams>> future = new PlainActionFuture<>();
            futures.add(future);
            taskIds[i] = UUIDs.base64UUID();
            service.sendStartRequest(taskIds[i], TestPersistentTasksExecutor.NAME, new TestParams(""Blah""), future);
        }

        for (int i = 0; i < numberOfTasks; i++) {
            assertThat(futures.get(i).get().getId(), equalTo(taskIds[i]));
        }

        PersistentTasksCustomMetaData tasksInProgress = internalCluster().clusterService().state().getMetaData()
                .custom(PersistentTasksCustomMetaData.TYPE);
        assertThat(tasksInProgress.tasks().size(), equalTo(numberOfTasks));

        // Make sure that at least one of the tasks is running
        assertBusy(() -> {
            // Wait for the task to start
            assertThat(client().admin().cluster().prepareListTasks().setActions(TestPersistentTasksExecutor.NAME + ""[c]"").get()
                    .getTasks().size(), greaterThan(0));
        });

        // Restart cluster
        internalCluster().fullRestart();
        ensureYellow();

        tasksInProgress = internalCluster().clusterService().state().getMetaData().custom(PersistentTasksCustomMetaData.TYPE);
        assertThat(tasksInProgress.tasks().size(), equalTo(numberOfTasks));
        // Check that cluster state is correct
        for (int i = 0; i < numberOfTasks; i++) {
            PersistentTask<?> task = tasksInProgress.getTask(taskIds[i]);
            assertNotNull(task);
        }

        logger.info(""Waiting for {} tasks to start"", numberOfTasks);
        assertBusy(() -> {
            // Wait for all tasks to start
            assertThat(client().admin().cluster().prepareListTasks().setActions(TestPersistentTasksExecutor.NAME + ""[c]"").get()
                            .getTasks().size(), equalTo(numberOfTasks));
        });

        logger.info(""Complete all tasks"");
        // Complete the running task and make sure it finishes properly
        assertThat(new TestPersistentTasksPlugin.TestTasksRequestBuilder(client()).setOperation(""finish"").get().getTasks().size(),
                equalTo(numberOfTasks));

        assertBusy(() -> {
            // Make sure the task is removed from the cluster state
            assertThat(((PersistentTasksCustomMetaData) internalCluster().clusterService().state().getMetaData()
                    .custom(PersistentTasksCustomMetaData.TYPE)).tasks(), empty());
        });

    }
"
"@TestLogging(""_root:DEBUG"")
    public void testIndexCausesIndexCreation() throws Exception {
        searchWhileCreatingIndex(false, 1); // 1 replica in our default...
    }
"
"    @TestLogging(""_root:DEBUG"")  // this fails every now and then: https://github.com/elastic/elasticsearch/issues/18121 but without
    public void testReadonlyRepository() throws Exception {
        Client client = client();
        logger.info(""-->  creating repository"");
        Path repositoryLocation = randomRepoPath();
        assertAcked(client.admin().cluster().preparePutRepository(""test-repo"")
                .setType(""fs"").setSettings(Settings.builder()
                        .put(""location"", repositoryLocation)
                        .put(""compress"", randomBoolean())
                        .put(""chunk_size"", randomIntBetween(100, 1000), ByteSizeUnit.BYTES)));

        createIndex(""test-idx"");
        ensureGreen();

        logger.info(""--> indexing some data"");
        for (int i = 0; i < 100; i++) {
            index(""test-idx"", ""_doc"", Integer.toString(i), ""foo"", ""bar"" + i);
        }
        refresh();

        logger.info(""--> snapshot"");
        CreateSnapshotResponse createSnapshotResponse = client.admin().cluster().prepareCreateSnapshot(""test-repo"", ""test-snap"").setWaitForCompletion(true).setIndices(""test-idx"").get();
        assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), greaterThan(0));
        assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), equalTo(createSnapshotResponse.getSnapshotInfo().totalShards()));

        assertThat(client.admin().cluster().prepareGetSnapshots(""test-repo"").setSnapshots(""test-snap"").get().getSnapshots().get(0).state(), equalTo(SnapshotState.SUCCESS));

        logger.info(""--> delete index"");
        cluster().wipeIndices(""test-idx"");

        logger.info(""--> create read-only URL repository"");
        assertAcked(client.admin().cluster().preparePutRepository(""readonly-repo"")
                .setType(""fs"").setSettings(Settings.builder()
                        .put(""location"", repositoryLocation)
                        .put(""compress"", randomBoolean())
                        .put(""readonly"", true)
                        .put(""chunk_size"", randomIntBetween(100, 1000), ByteSizeUnit.BYTES)));
        logger.info(""--> restore index after deletion"");
        RestoreSnapshotResponse restoreSnapshotResponse = client.admin().cluster().prepareRestoreSnapshot(""readonly-repo"", ""test-snap"").setWaitForCompletion(true).setIndices(""test-idx"").execute().actionGet();
        assertThat(restoreSnapshotResponse.getRestoreInfo().totalShards(), greaterThan(0));

        assertThat(client.prepareSearch(""test-idx"").setSize(0).get().getHits().getTotalHits(), equalTo(100L));

        logger.info(""--> list available shapshots"");
        GetSnapshotsResponse getSnapshotsResponse = client.admin().cluster().prepareGetSnapshots(""readonly-repo"").get();
        assertThat(getSnapshotsResponse.getSnapshots(), notNullValue());
        assertThat(getSnapshotsResponse.getSnapshots().size(), equalTo(1));

        logger.info(""--> try deleting snapshot"");
        assertThrows(client.admin().cluster().prepareDeleteSnapshot(""readonly-repo"", ""test-snap""), RepositoryException.class, ""cannot delete snapshot from a readonly repository"");

        logger.info(""--> try making another snapshot"");
        assertThrows(client.admin().cluster().prepareCreateSnapshot(""readonly-repo"", ""test-snap-2"").setWaitForCompletion(true).setIndices(""test-idx""), RepositoryException.class, ""cannot create snapshot in a readonly repository"");
    }
"
"    @TestLogging(""org.elasticsearch.snapshots:TRACE"")
    public void testAbortedSnapshotDuringInitDoesNotStart() throws Exception {
        final Client client = client();

        // Blocks on initialization
        assertAcked(client.admin().cluster().preparePutRepository(""repository"")
            .setType(""mock"").setSettings(Settings.builder()
                .put(""location"", randomRepoPath())
                .put(""block_on_init"", true)
            ));

        createIndex(""test-idx"");
        final int nbDocs = scaledRandomIntBetween(100, 500);
        for (int i = 0; i < nbDocs; i++) {
            index(""test-idx"", ""_doc"", Integer.toString(i), ""foo"", ""bar"" + i);
        }
        flushAndRefresh(""test-idx"");
        assertThat(client.prepareSearch(""test-idx"").setSize(0).get().getHits().getTotalHits(), equalTo((long) nbDocs));

        // Create a snapshot
        client.admin().cluster().prepareCreateSnapshot(""repository"", ""snap"").execute();
        waitForBlock(internalCluster().getMasterName(), ""repository"", TimeValue.timeValueMinutes(1));
        boolean blocked = true;

        // Snapshot is initializing (and is blocked at this stage)
        SnapshotsStatusResponse snapshotsStatus = client.admin().cluster().prepareSnapshotStatus(""repository"").setSnapshots(""snap"").get();
        assertThat(snapshotsStatus.getSnapshots().iterator().next().getState(), equalTo(State.INIT));

        final List<State> states = new CopyOnWriteArrayList<>();
        final ClusterStateListener listener = event -> {
            SnapshotsInProgress snapshotsInProgress = event.state().custom(SnapshotsInProgress.TYPE);
            for (SnapshotsInProgress.Entry entry : snapshotsInProgress.entries()) {
                if (""snap"".equals(entry.snapshot().getSnapshotId().getName())) {
                    states.add(entry.state());
                }
            }
        };

        try {
            // Record the upcoming states of the snapshot on all nodes
            internalCluster().getInstances(ClusterService.class).forEach(clusterService -> clusterService.addListener(listener));

            // Delete the snapshot while it is being initialized
            ActionFuture<AcknowledgedResponse> delete = client.admin().cluster().prepareDeleteSnapshot(""repository"", ""snap"").execute();

            // The deletion must set the snapshot in the ABORTED state
            assertBusy(() -> {
                SnapshotsStatusResponse status = client.admin().cluster().prepareSnapshotStatus(""repository"").setSnapshots(""snap"").get();
                assertThat(status.getSnapshots().iterator().next().getState(), equalTo(State.ABORTED));
            });

            // Now unblock the repository
            unblockNode(""repository"", internalCluster().getMasterName());
            blocked = false;

            assertAcked(delete.get());
            expectThrows(SnapshotMissingException.class, () ->
                client.admin().cluster().prepareGetSnapshots(""repository"").setSnapshots(""snap"").get());

            assertFalse(""Expecting snapshot state to be updated"", states.isEmpty());
            assertFalse(""Expecting snapshot to be aborted and not started at all"", states.contains(State.STARTED));
        } finally {
            internalCluster().getInstances(ClusterService.class).forEach(clusterService -> clusterService.removeListener(listener));
            if (blocked) {
                unblockNode(""repository"", internalCluster().getMasterName());
            }
        }
    }
"
"    @TestLogging(""org.elasticsearch.common.util.concurrent:DEBUG"")
    public void testAutoQueueSizingWithMax() throws Exception {
        ThreadContext context = new ThreadContext(Settings.EMPTY);
        ResizableBlockingQueue<Runnable> queue =
                new ResizableBlockingQueue<>(ConcurrentCollections.<Runnable>newBlockingQueue(),
                        5000);

        int threads = randomIntBetween(1, 5);
        int measureWindow = randomIntBetween(10, 100);
        int max = randomIntBetween(5010, 5024);
        logger.info(""--> auto-queue with a measurement window of {} tasks"", measureWindow);
        QueueResizingEsThreadPoolExecutor executor =
                new QueueResizingEsThreadPoolExecutor(
                        ""test-threadpool"", threads, threads, 1000,
                        TimeUnit.MILLISECONDS, queue, 10, max, fastWrapper(), measureWindow, TimeValue.timeValueMillis(1),
                        EsExecutors.daemonThreadFactory(""queuetest""), new EsAbortPolicy(), context);
        executor.prestartAllCoreThreads();
        logger.info(""--> executor: {}"", executor);

        // Execute a task multiple times that takes 1ms
        executeTask(executor, measureWindow * 3);

        // The queue capacity should increase, but no higher than the maximum
        assertBusy(() -> {
            assertThat(queue.capacity(), equalTo(max));
        });
        executor.shutdown();
        executor.awaitTermination(10, TimeUnit.SECONDS);
        context.close();
    }
"
"    @TestLogging(""_root:DEBUG,org.elasticsearch.index.shard.IndexShard:TRACE,org.elasticsearch.action.search:TRACE"")
    public void testAutoGenerateIdNoDuplicates() throws Exception {
        int numberOfIterations = scaledRandomIntBetween(10, 50);
        for (int i = 0; i < numberOfIterations; i++) {
            Exception firstError = null;
            createIndex(""test"");
            int numOfDocs = randomIntBetween(10, 100);
            logger.info(""indexing [{}] docs"", numOfDocs);
            List<IndexRequestBuilder> builders = new ArrayList<>(numOfDocs);
            for (int j = 0; j < numOfDocs; j++) {
                builders.add(client().prepareIndex(""test"", ""type"").setSource(""field"", ""value_"" + j));
            }
            indexRandom(true, builders);
            logger.info(""verifying indexed content"");
            int numOfChecks = randomIntBetween(8, 12);
            for (int j = 0; j < numOfChecks; j++) {
                try {
                    logger.debug(""running search with all types"");
                    SearchResponse response = client().prepareSearch(""test"").get();
                    if (response.getHits().getTotalHits() != numOfDocs) {
                        final String message = ""Count is "" + response.getHits().getTotalHits() + "" but "" + numOfDocs + "" was expected. ""
                            + ElasticsearchAssertions.formatShardStatus(response);
                        logger.error(""{}. search response: \n{}"", message, response);
                        fail(message);
                    }
                } catch (Exception e) {
                    logger.error(""search for all docs types failed"", e);
                    if (firstError == null) {
                        firstError = e;
                    }
                }
                try {
                    logger.debug(""running search with a specific type"");
                    SearchResponse response = client().prepareSearch(""test"").setTypes(""type"").get();
                    if (response.getHits().getTotalHits() != numOfDocs) {
                        final String message = ""Count is "" + response.getHits().getTotalHits() + "" but "" + numOfDocs + "" was expected. ""
                            + ElasticsearchAssertions.formatShardStatus(response);
                        logger.error(""{}. search response: \n{}"", message, response);
                        fail(message);
                    }
                } catch (Exception e) {
                    logger.error(""search for all docs of a specific type failed"", e);
                    if (firstError == null) {
                        firstError = e;
                    }
                }
            }
            if (firstError != null) {
                fail(firstError.getMessage());
            }
            internalCluster().wipeIndices(""test"");
        }
    }
"
"    @TestLogging(""_root:DEBUG"")
    public void testMasterFailoverDuringIndexingWithMappingChanges() throws Throwable {
        logger.info(""--> start 4 nodes, 3 master, 1 data"");

        final Settings sharedSettings = Settings.builder()
                .put(FaultDetection.PING_TIMEOUT_SETTING.getKey(), ""1s"") // for hitting simulated network failures quickly
                .put(FaultDetection.PING_RETRIES_SETTING.getKey(), ""1"") // for hitting simulated network failures quickly
                .put(""discovery.zen.join_timeout"", ""10s"")  // still long to induce failures but to long so test won't time out
                .put(DiscoverySettings.PUBLISH_TIMEOUT_SETTING.getKey(), ""1s"") // <-- for hitting simulated network failures quickly
                .put(ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES_SETTING.getKey(), 2)
                .build();

        internalCluster().startMasterOnlyNodes(3, sharedSettings);

        String dataNode = internalCluster().startDataOnlyNode(sharedSettings);

        logger.info(""--> wait for all nodes to join the cluster"");
        ensureStableCluster(4);

        // We index data with mapping changes into cluster and have master failover at same time
        client().admin().indices().prepareCreate(""myindex"")
                .setSettings(Settings.builder().put(""index.number_of_shards"", 1).put(""index.number_of_replicas"", 0))
                .get();
        ensureGreen(""myindex"");

        final CyclicBarrier barrier = new CyclicBarrier(2);

        Thread indexingThread = new Thread(new Runnable() {
            @Override
            public void run() {
                try {
                    barrier.await();
                } catch (InterruptedException e) {
                    logger.warn(""Barrier interrupted"", e);
                    return;
                } catch (BrokenBarrierException e) {
                    logger.warn(""Broken barrier"", e);
                    return;
                }
                for (int i = 0; i < 10; i++) {
                    // index data with mapping changes
                    IndexResponse response = client(dataNode).prepareIndex(""myindex"", ""mytype"").setSource(""field_"" + i, ""val"").get();
                    assertEquals(DocWriteResponse.Result.CREATED, response.getResult());
                }
            }
"
"@TestLogging(""_root:DEBUG,org.elasticsearch.action.admin.indices.shards:TRACE,org.elasticsearch.cluster.service:TRACE"")
    public void testEmpty() {
        ensureGreen();
        IndicesShardStoresResponse rsp = client().admin().indices().prepareShardStores().get();
        assertThat(rsp.getStoreStatuses().size(), equalTo(0));
    }
"
"    @TestLogging(""_root:DEBUG, org.elasticsearch.transport:TRACE"")
    public void testCloseWhileConcurrentlyConnecting() throws IOException, InterruptedException, BrokenBarrierException {
        List<DiscoveryNode> knownNodes = new CopyOnWriteArrayList<>();
        try (MockTransportService seedTransport = startTransport(""seed_node"", knownNodes, Version.CURRENT);
             MockTransportService seedTransport1 = startTransport(""seed_node_1"", knownNodes, Version.CURRENT);
             MockTransportService discoverableTransport = startTransport(""discoverable_node"", knownNodes, Version.CURRENT)) {
            DiscoveryNode seedNode = seedTransport.getLocalDiscoNode();
            DiscoveryNode seedNode1 = seedTransport1.getLocalDiscoNode();
            knownNodes.add(seedTransport.getLocalDiscoNode());
            knownNodes.add(discoverableTransport.getLocalDiscoNode());
            knownNodes.add(seedTransport1.getLocalDiscoNode());
            Collections.shuffle(knownNodes, random());
            List<Supplier<DiscoveryNode>> seedNodes = Arrays.asList(() -> seedNode1, () -> seedNode);
            Collections.shuffle(seedNodes, random());

            try (MockTransportService service = MockTransportService.createNewService(Settings.EMPTY, Version.CURRENT, threadPool, null)) {
                service.start();
                service.acceptIncomingRequests();
                try (RemoteClusterConnection connection = new RemoteClusterConnection(Settings.EMPTY, ""test-cluster"",
                    seedNodes, service, service.getConnectionManager(), Integer.MAX_VALUE, n -> true)) {
                    int numThreads = randomIntBetween(4, 10);
                    Thread[] threads = new Thread[numThreads];
                    CyclicBarrier barrier = new CyclicBarrier(numThreads + 1);
                    for (int i = 0; i < threads.length; i++) {
                        final int numConnectionAttempts = randomIntBetween(10, 100);
                        threads[i] = new Thread() {
                            @Override
                            public void run() {
                                try {
                                    barrier.await();
                                    CountDownLatch latch = new CountDownLatch(numConnectionAttempts);
                                    for (int i = 0; i < numConnectionAttempts; i++) {
                                        AtomicReference<Exception> executed = new AtomicReference<>();
                                        ActionListener<Void> listener = ActionListener.wrap(
                                            x -> {
                                                if (executed.compareAndSet(null, new RuntimeException())) {
                                                    latch.countDown();
                                                } else {
                                                    throw new AssertionError(""shit's been called twice"", executed.get());
                                                }
                                            },
                                            x -> {
                                                if (executed.compareAndSet(null, x)) {
                                                    latch.countDown();
                                                } else {
                                                    final String message = x.getMessage();
                                                    if ((executed.get().getClass() == x.getClass()
                                                        && ""operation was cancelled reason [connect handler is closed]"".equals(message)
                                                        && message.equals(executed.get().getMessage())) == false) {
                                                        // we do cancel the operation and that means that if timing allows it, the caller
                                                        // of a blocking call as well as the handler will get the exception from the
                                                        // ExecutionCancelledException concurrently. unless that is the case we fail
                                                        // if we get called more than once!
                                                        AssertionError assertionError = new AssertionError(""shit's been called twice"", x);
                                                        assertionError.addSuppressed(executed.get());
                                                        throw assertionError;
                                                    }
                                                }
                                                if (x instanceof RejectedExecutionException || x instanceof AlreadyClosedException
                                                    || x instanceof CancellableThreads.ExecutionCancelledException) {
                                                    // that's fine
                                                } else {
                                                    throw new AssertionError(x);
                                                }
                                            });
                                        try {
                                            connection.updateSeedNodes(null, seedNodes, listener);
                                        } catch (Exception e) {
                                            // it's ok if we're shutting down
                                            assertThat(e.getMessage(), containsString(""threadcontext is already closed""));
                                            latch.countDown();
                                        }
                                    }
                                    latch.await();
                                } catch (Exception ex) {
                                    throw new AssertionError(ex);
                                }
                            }
"
"    @TestLogging(""_root:DEBUG,org.elasticsearch.action.bulk:TRACE,org.elasticsearch.index.shard:TRACE,org.elasticsearch.cluster.service:TRACE"")
    public void testPrimaryRelocationWhileIndexing() throws Exception {
        internalCluster().ensureAtLeastNumDataNodes(randomIntBetween(2, 3));
        client().admin().indices().prepareCreate(""test"")
            .setSettings(Settings.builder().put(""index.number_of_shards"", 1).put(""index.number_of_replicas"", 0))
            .addMapping(""type"", ""field"", ""type=text"")
            .get();
        ensureGreen(""test"");
        AtomicInteger numAutoGenDocs = new AtomicInteger();
        final AtomicBoolean finished = new AtomicBoolean(false);
        Thread indexingThread = new Thread() {
            @Override
            public void run() {
                while (finished.get() == false) {
                    IndexResponse indexResponse = client().prepareIndex(""test"", ""type"", ""id"").setSource(""field"", ""value"").get();
                    assertEquals(DocWriteResponse.Result.CREATED, indexResponse.getResult());
                    DeleteResponse deleteResponse = client().prepareDelete(""test"", ""type"", ""id"").get();
                    assertEquals(DocWriteResponse.Result.DELETED, deleteResponse.getResult());
                    client().prepareIndex(""test"", ""type"").setSource(""auto"", true).get();
                    numAutoGenDocs.incrementAndGet();
                }
            }
"
"    @TestLogging(
    public void testRerouteRecovery() throws Exception {
        logger.info(""--> start node A"");
        final String nodeA = internalCluster().startNode();

        logger.info(""--> create index on node: {}"", nodeA);
        ByteSizeValue shardSize = createAndPopulateIndex(INDEX_NAME, 1, SHARD_COUNT, REPLICA_COUNT).getShards()[0].getStats().getStore().size();

        logger.info(""--> start node B"");
        final String nodeB = internalCluster().startNode();

        ensureGreen();

        logger.info(""--> slowing down recoveries"");
        slowDownRecovery(shardSize);

        logger.info(""--> move shard from: {} to: {}"", nodeA, nodeB);
        client().admin().cluster().prepareReroute()
                .add(new MoveAllocationCommand(INDEX_NAME, 0, nodeA, nodeB))
                .execute().actionGet().getState();

        logger.info(""--> waiting for recovery to start both on source and target"");
        final Index index = resolveIndex(INDEX_NAME);
        assertBusy(() -> {
            IndicesService indicesService = internalCluster().getInstance(IndicesService.class, nodeA);
            assertThat(indicesService.indexServiceSafe(index).getShard(0).recoveryStats().currentAsSource(),
                    equalTo(1));
            indicesService = internalCluster().getInstance(IndicesService.class, nodeB);
            assertThat(indicesService.indexServiceSafe(index).getShard(0).recoveryStats().currentAsTarget(),
                    equalTo(1));
        });

        logger.info(""--> request recoveries"");
        RecoveryResponse response = client().admin().indices().prepareRecoveries(INDEX_NAME).execute().actionGet();

        List<RecoveryState> recoveryStates = response.shardRecoveryStates().get(INDEX_NAME);
        List<RecoveryState> nodeARecoveryStates = findRecoveriesForTargetNode(nodeA, recoveryStates);
        assertThat(nodeARecoveryStates.size(), equalTo(1));
        List<RecoveryState> nodeBRecoveryStates = findRecoveriesForTargetNode(nodeB, recoveryStates);
        assertThat(nodeBRecoveryStates.size(), equalTo(1));

        assertRecoveryState(nodeARecoveryStates.get(0), 0, RecoverySource.EmptyStoreRecoverySource.INSTANCE, true, Stage.DONE, null, nodeA);
        validateIndexRecoveryState(nodeARecoveryStates.get(0).getIndex());

        assertOnGoingRecoveryState(nodeBRecoveryStates.get(0), 0, PeerRecoverySource.INSTANCE, true, nodeA, nodeB);
        validateIndexRecoveryState(nodeBRecoveryStates.get(0).getIndex());

        logger.info(""--> request node recovery stats"");
        NodesStatsResponse statsResponse = client().admin().cluster().prepareNodesStats().clear().setIndices(new CommonStatsFlags(CommonStatsFlags.Flag.Recovery)).get();
        long nodeAThrottling = Long.MAX_VALUE;
        long nodeBThrottling = Long.MAX_VALUE;
        for (NodeStats nodeStats : statsResponse.getNodes()) {
            final RecoveryStats recoveryStats = nodeStats.getIndices().getRecoveryStats();
            if (nodeStats.getNode().getName().equals(nodeA)) {
                assertThat(""node A should have ongoing recovery as source"", recoveryStats.currentAsSource(), equalTo(1));
                assertThat(""node A should not have ongoing recovery as target"", recoveryStats.currentAsTarget(), equalTo(0));
                nodeAThrottling = recoveryStats.throttleTime().millis();
            }
            if (nodeStats.getNode().getName().equals(nodeB)) {
                assertThat(""node B should not have ongoing recovery as source"", recoveryStats.currentAsSource(), equalTo(0));
                assertThat(""node B should have ongoing recovery as target"", recoveryStats.currentAsTarget(), equalTo(1));
                nodeBThrottling = recoveryStats.throttleTime().millis();
            }
        }

        logger.info(""--> checking throttling increases"");
        final long finalNodeAThrottling = nodeAThrottling;
        final long finalNodeBThrottling = nodeBThrottling;
        assertBusy(() -> {
            NodesStatsResponse statsResponse1 = client().admin().cluster().prepareNodesStats().clear().setIndices(new CommonStatsFlags(CommonStatsFlags.Flag.Recovery)).get();
            assertThat(statsResponse1.getNodes(), hasSize(2));
            for (NodeStats nodeStats : statsResponse1.getNodes()) {
                final RecoveryStats recoveryStats = nodeStats.getIndices().getRecoveryStats();
                if (nodeStats.getNode().getName().equals(nodeA)) {
                    assertThat(""node A throttling should increase"", recoveryStats.throttleTime().millis(), greaterThan(finalNodeAThrottling));
                }
                if (nodeStats.getNode().getName().equals(nodeB)) {
                    assertThat(""node B throttling should increase"", recoveryStats.throttleTime().millis(), greaterThan(finalNodeBThrottling));
                }
            }
        });


        logger.info(""--> speeding up recoveries"");
        restoreRecoverySpeed();

        // wait for it to be finished
        ensureGreen();

        response = client().admin().indices().prepareRecoveries(INDEX_NAME).execute().actionGet();

        recoveryStates = response.shardRecoveryStates().get(INDEX_NAME);
        assertThat(recoveryStates.size(), equalTo(1));

        assertRecoveryState(recoveryStates.get(0), 0, PeerRecoverySource.INSTANCE, true, Stage.DONE, nodeA, nodeB);
        validateIndexRecoveryState(recoveryStates.get(0).getIndex());
        Consumer<String> assertNodeHasThrottleTimeAndNoRecoveries = nodeName ->  {
            NodesStatsResponse nodesStatsResponse = client().admin().cluster().prepareNodesStats().setNodesIds(nodeName)
                .clear().setIndices(new CommonStatsFlags(CommonStatsFlags.Flag.Recovery)).get();
            assertThat(nodesStatsResponse.getNodes(), hasSize(1));
            NodeStats nodeStats = nodesStatsResponse.getNodes().get(0);
            final RecoveryStats recoveryStats = nodeStats.getIndices().getRecoveryStats();
            assertThat(recoveryStats.currentAsSource(), equalTo(0));
            assertThat(recoveryStats.currentAsTarget(), equalTo(0));
            assertThat(nodeName + "" throttling should be >0"", recoveryStats.throttleTime().millis(), greaterThan(0L));
        };
        // we have to use assertBusy as recovery counters are decremented only when the last reference to the RecoveryTarget
        // is decremented, which may happen after the recovery was done.
        assertBusy(() -> assertNodeHasThrottleTimeAndNoRecoveries.accept(nodeA));
        assertBusy(() -> assertNodeHasThrottleTimeAndNoRecoveries.accept(nodeB));

        logger.info(""--> bump replica count"");
        client().admin().indices().prepareUpdateSettings(INDEX_NAME)
                .setSettings(Settings.builder().put(""number_of_replicas"", 1)).execute().actionGet();
        ensureGreen();

        assertBusy(() -> assertNodeHasThrottleTimeAndNoRecoveries.accept(nodeA));
        assertBusy(() -> assertNodeHasThrottleTimeAndNoRecoveries.accept(nodeB));

        logger.info(""--> start node C"");
        String nodeC = internalCluster().startNode();
        assertFalse(client().admin().cluster().prepareHealth().setWaitForNodes(""3"").get().isTimedOut());

        logger.info(""--> slowing down recoveries"");
        slowDownRecovery(shardSize);

        logger.info(""--> move replica shard from: {} to: {}"", nodeA, nodeC);
        client().admin().cluster().prepareReroute()
                .add(new MoveAllocationCommand(INDEX_NAME, 0, nodeA, nodeC))
                .execute().actionGet().getState();

        response = client().admin().indices().prepareRecoveries(INDEX_NAME).execute().actionGet();
        recoveryStates = response.shardRecoveryStates().get(INDEX_NAME);

        nodeARecoveryStates = findRecoveriesForTargetNode(nodeA, recoveryStates);
        assertThat(nodeARecoveryStates.size(), equalTo(1));
        nodeBRecoveryStates = findRecoveriesForTargetNode(nodeB, recoveryStates);
        assertThat(nodeBRecoveryStates.size(), equalTo(1));
        List<RecoveryState> nodeCRecoveryStates = findRecoveriesForTargetNode(nodeC, recoveryStates);
        assertThat(nodeCRecoveryStates.size(), equalTo(1));

        assertRecoveryState(nodeARecoveryStates.get(0), 0, PeerRecoverySource.INSTANCE, false, Stage.DONE, nodeB, nodeA);
        validateIndexRecoveryState(nodeARecoveryStates.get(0).getIndex());

        assertRecoveryState(nodeBRecoveryStates.get(0), 0, PeerRecoverySource.INSTANCE, true, Stage.DONE, nodeA, nodeB);
        validateIndexRecoveryState(nodeBRecoveryStates.get(0).getIndex());

        // relocations of replicas are marked as REPLICA and the source node is the node holding the primary (B)
        assertOnGoingRecoveryState(nodeCRecoveryStates.get(0), 0, PeerRecoverySource.INSTANCE, false, nodeB, nodeC);
        validateIndexRecoveryState(nodeCRecoveryStates.get(0).getIndex());

        if (randomBoolean()) {
            // shutdown node with relocation source of replica shard and check if recovery continues
            internalCluster().stopRandomNode(InternalTestCluster.nameFilter(nodeA));
            ensureStableCluster(2);

            response = client().admin().indices().prepareRecoveries(INDEX_NAME).execute().actionGet();
            recoveryStates = response.shardRecoveryStates().get(INDEX_NAME);

            nodeARecoveryStates = findRecoveriesForTargetNode(nodeA, recoveryStates);
            assertThat(nodeARecoveryStates.size(), equalTo(0));
            nodeBRecoveryStates = findRecoveriesForTargetNode(nodeB, recoveryStates);
            assertThat(nodeBRecoveryStates.size(), equalTo(1));
            nodeCRecoveryStates = findRecoveriesForTargetNode(nodeC, recoveryStates);
            assertThat(nodeCRecoveryStates.size(), equalTo(1));

            assertRecoveryState(nodeBRecoveryStates.get(0), 0, PeerRecoverySource.INSTANCE, true, Stage.DONE, nodeA, nodeB);
            validateIndexRecoveryState(nodeBRecoveryStates.get(0).getIndex());

            assertOnGoingRecoveryState(nodeCRecoveryStates.get(0), 0, PeerRecoverySource.INSTANCE, false, nodeB, nodeC);
            validateIndexRecoveryState(nodeCRecoveryStates.get(0).getIndex());
        }

        logger.info(""--> speeding up recoveries"");
        restoreRecoverySpeed();
        ensureGreen();

        response = client().admin().indices().prepareRecoveries(INDEX_NAME).execute().actionGet();
        recoveryStates = response.shardRecoveryStates().get(INDEX_NAME);

        nodeARecoveryStates = findRecoveriesForTargetNode(nodeA, recoveryStates);
        assertThat(nodeARecoveryStates.size(), equalTo(0));
        nodeBRecoveryStates = findRecoveriesForTargetNode(nodeB, recoveryStates);
        assertThat(nodeBRecoveryStates.size(), equalTo(1));
        nodeCRecoveryStates = findRecoveriesForTargetNode(nodeC, recoveryStates);
        assertThat(nodeCRecoveryStates.size(), equalTo(1));

        assertRecoveryState(nodeBRecoveryStates.get(0), 0, PeerRecoverySource.INSTANCE, true, Stage.DONE, nodeA, nodeB);
        validateIndexRecoveryState(nodeBRecoveryStates.get(0).getIndex());

        // relocations of replicas are marked as REPLICA and the source node is the node holding the primary (B)
        assertRecoveryState(nodeCRecoveryStates.get(0), 0, PeerRecoverySource.INSTANCE, false, Stage.DONE, nodeB, nodeC);
        validateIndexRecoveryState(nodeCRecoveryStates.get(0).getIndex());
    }
"
"    @TestLogging(""_root:DEBUG,org.elasticsearch.indices.recovery:TRACE"")
    public void testDisconnectsDuringRecovery() throws Exception {
        boolean primaryRelocation = randomBoolean();
        final String indexName = ""test"";
        final Settings nodeSettings = Settings.builder()
            .put(RecoverySettings.INDICES_RECOVERY_RETRY_DELAY_NETWORK_SETTING.getKey(), TimeValue.timeValueMillis(randomIntBetween(0, 100)))
            .build();
        TimeValue disconnectAfterDelay = TimeValue.timeValueMillis(randomIntBetween(0, 100));
        // start a master node
        String masterNodeName = internalCluster().startMasterOnlyNode(nodeSettings);

        final String blueNodeName = internalCluster().startNode(Settings.builder().put(""node.attr.color"", ""blue"").put(nodeSettings).build());
        final String redNodeName = internalCluster().startNode(Settings.builder().put(""node.attr.color"", ""red"").put(nodeSettings).build());

        client().admin().indices().prepareCreate(indexName)
            .setSettings(
                Settings.builder()
                    .put(IndexMetaData.INDEX_ROUTING_INCLUDE_GROUP_SETTING.getKey() + ""color"", ""blue"")
                    .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1)
                    .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 0)
            ).get();

        List<IndexRequestBuilder> requests = new ArrayList<>();
        int numDocs = scaledRandomIntBetween(25, 250);
        for (int i = 0; i < numDocs; i++) {
            requests.add(client().prepareIndex(indexName, ""type"").setSource(""{}"", XContentType.JSON));
        }
        indexRandom(true, requests);
        ensureSearchable(indexName);
        assertHitCount(client().prepareSearch(indexName).get(), numDocs);

        MockTransportService masterTransportService = (MockTransportService) internalCluster().getInstance(TransportService.class, masterNodeName);
        MockTransportService blueMockTransportService = (MockTransportService) internalCluster().getInstance(TransportService.class, blueNodeName);
        MockTransportService redMockTransportService = (MockTransportService) internalCluster().getInstance(TransportService.class, redNodeName);

        redMockTransportService.addSendBehavior(blueMockTransportService, new StubbableTransport.SendRequestBehavior() {
            private final AtomicInteger count = new AtomicInteger();

            @Override
            public void sendRequest(Transport.Connection connection, long requestId, String action, TransportRequest request,
                                    TransportRequestOptions options) throws IOException {
                logger.info(""--> sending request {} on {}"", action, connection.getNode());
                if (PeerRecoverySourceService.Actions.START_RECOVERY.equals(action) && count.incrementAndGet() == 1) {
                    // ensures that it's considered as valid recovery attempt by source
                    try {
                        awaitBusy(() -> client(blueNodeName).admin().cluster().prepareState().setLocal(true).get()
                            .getState().getRoutingTable().index(""test"").shard(0).getAllInitializingShards().isEmpty() == false);
                    } catch (InterruptedException e) {
                        throw new RuntimeException(e);
                    }
                    connection.sendRequest(requestId, action, request, options);
                    try {
                        Thread.sleep(disconnectAfterDelay.millis());
                    } catch (InterruptedException e) {
                        throw new RuntimeException(e);
                    }
                    throw new ConnectTransportException(connection.getNode(), ""DISCONNECT: simulation disconnect after successfully sending "" + action + "" request"");
                } else {
                    connection.sendRequest(requestId, action, request, options);
                }
            }
"
"@TestLogging(""_root:DEBUG"")
    public void testAssignmentWithJustAddedNodes() throws Exception {
        internalCluster().startNode();
        final String index = ""index"";
        prepareCreate(index).setSettings(Settings.builder().put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1)
            .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 0)).get();
        ensureGreen(index);

        // close to have some unassigned started shards shards..
        client().admin().indices().prepareClose(index).get();


        final String masterName = internalCluster().getMasterName();
        final ClusterService clusterService = internalCluster().clusterService(masterName);
        final AllocationService allocationService = internalCluster().getInstance(AllocationService.class, masterName);
        clusterService.submitStateUpdateTask(""test-inject-node-and-reroute"", new ClusterStateUpdateTask() {
            @Override
            public ClusterState execute(ClusterState currentState) throws Exception {
                // inject a node
                ClusterState.Builder builder = ClusterState.builder(currentState);
                builder.nodes(DiscoveryNodes.builder(currentState.nodes()).add(new DiscoveryNode(""_non_existent"",
                        buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT)));

                // open index
                final IndexMetaData indexMetaData = IndexMetaData.builder(currentState.metaData().index(index)).state(IndexMetaData.State.OPEN).build();

                builder.metaData(MetaData.builder(currentState.metaData()).put(indexMetaData, true));
                builder.blocks(ClusterBlocks.builder().blocks(currentState.blocks()).removeIndexBlocks(index));
                ClusterState updatedState = builder.build();

                RoutingTable.Builder routingTable = RoutingTable.builder(updatedState.routingTable());
                routingTable.addAsRecovery(updatedState.metaData().index(index));
                updatedState = ClusterState.builder(updatedState).routingTable(routingTable.build()).build();

                return allocationService.reroute(updatedState, ""reroute"");

            }
"
"@TestLogging(""org.elasticsearch.index.reindex:TRACE,org.elasticsearch.action.bulk:TRACE,org.elasticsearch.search.SearchService:TRACE"")
    public void testBasics() throws Exception {
        indexRandom(true, client().prepareIndex(""test"", ""test"", ""1"").setSource(""foo"", ""a""),
                client().prepareIndex(""test"", ""test"", ""2"").setSource(""foo"", ""a""),
                client().prepareIndex(""test"", ""test"", ""3"").setSource(""foo"", ""b""),
                client().prepareIndex(""test"", ""test"", ""4"").setSource(""foo"", ""c""));
        assertHitCount(client().prepareSearch(""test"").setTypes(""test"").setSize(0).get(), 4);
        assertEquals(1, client().prepareGet(""test"", ""test"", ""1"").get().getVersion());
        assertEquals(1, client().prepareGet(""test"", ""test"", ""4"").get().getVersion());

        // Reindex all the docs
        assertThat(updateByQuery().source(""test"").refresh(true).get(), matcher().updated(4));
        assertEquals(2, client().prepareGet(""test"", ""test"", ""1"").get().getVersion());
        assertEquals(2, client().prepareGet(""test"", ""test"", ""4"").get().getVersion());

        // Now none of them
        assertThat(updateByQuery().source(""test"").filter(termQuery(""foo"", ""no_match"")).refresh(true).get(), matcher().updated(0));
        assertEquals(2, client().prepareGet(""test"", ""test"", ""1"").get().getVersion());
        assertEquals(2, client().prepareGet(""test"", ""test"", ""4"").get().getVersion());

        // Now half of them
        assertThat(updateByQuery().source(""test"").filter(termQuery(""foo"", ""a"")).refresh(true).get(), matcher().updated(2));
        assertEquals(3, client().prepareGet(""test"", ""test"", ""1"").get().getVersion());
        assertEquals(3, client().prepareGet(""test"", ""test"", ""2"").get().getVersion());
        assertEquals(2, client().prepareGet(""test"", ""test"", ""3"").get().getVersion());
        assertEquals(2, client().prepareGet(""test"", ""test"", ""4"").get().getVersion());

        // Limit with size
        UpdateByQueryRequestBuilder request = updateByQuery().source(""test"").size(3).refresh(true);
        request.source().addSort(""foo.keyword"", SortOrder.ASC);
        assertThat(request.get(), matcher().updated(3));
        // Only the first three documents are updated because of sort
        assertEquals(4, client().prepareGet(""test"", ""test"", ""1"").get().getVersion());
        assertEquals(4, client().prepareGet(""test"", ""test"", ""2"").get().getVersion());
        assertEquals(3, client().prepareGet(""test"", ""test"", ""3"").get().getVersion());
        assertEquals(2, client().prepareGet(""test"", ""test"", ""4"").get().getVersion());
    }
"
"@TestLogging(""_root:DEBUG"")
    public void testFailuresCauseAbortDefault() throws Exception {
        /*
         * Create the destination index such that the copy will cause a mapping
         * conflict on every request.
         */
        indexRandom(true,
                client().prepareIndex(""dest"", ""test"", ""test"").setSource(""test"", 10) /* Its a string in the source! */);

        indexDocs(100);

        ReindexRequestBuilder copy = reindex().source(""source"").destination(""dest"");
        /*
         * Set the search size to something very small to cause there to be
         * multiple batches for this request so we can assert that we abort on
         * the first batch.
         */
        copy.source().setSize(1);

        BulkByScrollResponse response = copy.get();
        assertThat(response, matcher()
                .batches(1)
                .failures(both(greaterThan(0)).and(lessThanOrEqualTo(maximumNumberOfShards()))));
        for (Failure failure: response.getBulkFailures()) {
            assertThat(failure.getMessage(), containsString(""IllegalArgumentException[For input string: \""words words\""]""));
        }
    }
"
"@TestLogging(""org.elasticsearch.index.reindex:DEBUG,org.elasticsearch.action.bulk:DEBUG"")
    public void clearAllowedOperations() {
        ALLOWED_OPERATIONS.drainPermits();
    }
"
"  @Test
  public void testNERStanfordDependencies() throws Exception{
    String sentence = ""John lives in Washington."";
    Properties props = new Properties();
    props.setProperty(""annotators"",""tokenize, ssplit, pos, lemma, ner, parse"");
    props.setProperty(""parse.originalDependencies"", ""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    Annotation doc = new Annotation(sentence);
    pipeline.annotate(doc);
    CoreMap sent = doc.get(CoreAnnotations.SentencesAnnotation.class).get(0);
    SemanticGraph graph = sent.get(SemanticGraphCoreAnnotations.CollapsedCCProcessedDependenciesAnnotation.class);
    graph.prettyPrint();
    String patStr = ""({word:/lives/} >/prep_in/ {word:/\\QCalifornia\\E|\\QWashington\\E/} >nsubj {ner:PERSON})"";
    SemgrexPattern pat = SemgrexPattern.compile(patStr);
    SemgrexMatcher mat = pat.matcher(graph, true);
    assertTrue(mat.find());
  }
"
"  @Test
  public void testNERUniversalDependencies() throws Exception{
    String sentence = ""John lives in Washington."";
    Properties props = new Properties();
    props.setProperty(""annotators"",""tokenize, ssplit, pos, lemma, ner, parse"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    props.setProperty(""parse.originalDependencies"", ""false"");
    Annotation doc = new Annotation(sentence);
    pipeline.annotate(doc);
    CoreMap sent = doc.get(CoreAnnotations.SentencesAnnotation.class).get(0);
    SemanticGraph graph = sent.get(SemanticGraphCoreAnnotations.CollapsedCCProcessedDependenciesAnnotation.class);
    graph.prettyPrint();
    String patStr = ""({word:/lives/} >/obl:in/ {word:/\\QCalifornia\\E|\\QWashington\\E/} >nsubj {ner:PERSON})"";
    SemgrexPattern pat = SemgrexPattern.compile(patStr);
    SemgrexMatcher mat = pat.matcher(graph, true);
    assertTrue(mat.find());
  }
"
"  @Test
  public void testWorking() throws SUTimeSimpleParser.SUTimeParsingError {
    String[] inputs = { ""1972"", ""1972-07-05"", ""Jan 12, 1975 5:30"", ""7:12"", ""0712"", ""1972-04"" };
    String[] outputs = { ""1972-XX-XX"", ""1972-07-05"", ""1975-01-12T05:30"", ""T07:12"", ""712-XX-XX"", ""1972-04"" };
    // todo: second last case is totally bad, but it's what it does at present. But I guess 1930 is ambiguous....
    assertEquals(inputs.length, outputs.length);

    for (int i = 0; i < inputs.length; i++) {
      // System.err.println(""String: "" + inputs[i]);
      SUTime.Temporal timeExpression = parse(inputs[i]);
      // System.err.println(""Parsed: "" + timeExpression);
      assertEquals(outputs[i], timeExpression.toString());
    }
  }
"
"  @Test
  public void runHeidelTimeEnglish() throws Exception {
    String text = ""On Monday, some cataclysmic news about a a release last Christmas was released."";

    Annotation ann = new Annotation(text);
    String date = ""2017-07-07"";
    ann.set(CoreAnnotations.DocDateAnnotation.class, date);

    String heideltimeEnv = System.getenv(""HEIDELTIME_PATH"");
    if (heideltimeEnv == null) {
      heideltimeEnv = DEFAULT_HEIDELTIME_LOCATION;
    }

    Properties defaultProps = new Properties();
    defaultProps.load(IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(""edu/stanford/nlp/pipeline/StanfordCoreNLP.properties""));

    Properties props = new Properties(defaultProps);
    props.setProperty(""customAnnotatorClass.heideltime"", ""edu.stanford.nlp.time.HeidelTimeAnnotator"");
    props.setProperty(HeidelTimeAnnotator.HEIDELTIME_PATH_PROPERTY, heideltimeEnv);
    props.setProperty(HeidelTimeAnnotator.HEIDELTIME_LANGUAGE_PROPERTY, ""english"");
    props.setProperty(""annotators"", ""tokenize,ssplit,heideltime"");

    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    pipeline.annotate(ann);

    List<CoreMap> outputs = ann.get(TimeAnnotations.TimexAnnotations.class);
    Assert.assertEquals(2, outputs.size());

    Assert.assertEquals(""Monday"", outputs.get(0).get(TimeAnnotations.TimexAnnotation.class).text());
    Assert.assertEquals(""2017-07-03"", outputs.get(0).get(TimeAnnotations.TimexAnnotation.class).value());

    Assert.assertEquals(""Christmas"", outputs.get(1).get(TimeAnnotations.TimexAnnotation.class).text());
    Assert.assertEquals(""2016-12-25"", outputs.get(1).get(TimeAnnotations.TimexAnnotation.class).value());
  }
"
"  @Test
  public void runHeidelTimeSpanish() throws Exception {
    String text = ""El lunes, algunas noticias cataclÃ­smicas sobre un lanzamiento de la Navidad pasada fueron liberadas."";

    Annotation ann = new Annotation(text);
    String date = ""2017-07-07"";
    ann.set(CoreAnnotations.DocDateAnnotation.class, date);

    String heideltimeEnv = System.getenv(""HEIDELTIME_PATH"");
    if (heideltimeEnv == null) {
      heideltimeEnv = DEFAULT_HEIDELTIME_LOCATION;
    }

    Properties defaultProps = new Properties();
    defaultProps.load(IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(""edu/stanford/nlp/pipeline/StanfordCoreNLP-spanish.properties""));

    Properties props = new Properties(defaultProps);
    props.setProperty(""customAnnotatorClass.heideltime"", ""edu.stanford.nlp.time.HeidelTimeAnnotator"");
    props.setProperty(HeidelTimeAnnotator.HEIDELTIME_PATH_PROPERTY, heideltimeEnv);
    props.setProperty(HeidelTimeAnnotator.HEIDELTIME_LANGUAGE_PROPERTY, ""spanish"");
    props.setProperty(""annotators"", ""tokenize,ssplit,heideltime"");

    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    pipeline.annotate(ann);

    List<CoreMap> outputs = ann.get(TimeAnnotations.TimexAnnotations.class);
    Assert.assertEquals(1, outputs.size()); // Unfortunately, HeidelTime doesn't get Navidad :-(

    Assert.assertEquals(""El lunes"", outputs.get(0).get(TimeAnnotations.TimexAnnotation.class).text());
    Assert.assertEquals(""2017-07-03"", outputs.get(0).get(TimeAnnotations.TimexAnnotation.class).value());

    //Assert.assertEquals(""Navidad"", outputs.get(1).get(TimeAnnotations.TimexAnnotation.class).text());
    //Assert.assertEquals(""2016-12-25"", outputs.get(1).get(TimeAnnotations.TimexAnnotation.class).value());
  }
"
"  @Test
  public void testCoref() throws IOException {
    String doc = IOUtils.slurpFile(""edu/stanford/nlp/dcoref/STILLALONEWOLF_20050102.1100.eng.LDC2005E83.sgm"");
    Annotation annotation = pipeline.process(doc);
    Map<Integer, CorefChain> chains = annotation.get(CorefCoreAnnotations.CorefChainAnnotation.class);
    Map<Integer, List<ExpectedMention>> expected = loadExpectedResults(""edu/stanford/nlp/dcoref/STILLALONEWOLF_20050102.1100.eng.LDC2005E83.expectedcoref"");
    compareResults(expected, chains);
  }
"
"  @Test
  public void testDcoref() throws Exception {
    Counter<String> results = getCorefResults(runCorefTest(true));

    Counter<String> lowResults = new ClassicCounter<>();
    Counter<String> highResults = new ClassicCounter<>();
    Counter<String> expectedResults = new ClassicCounter<>();

    setLowHighExpected(lowResults, highResults, expectedResults, MENTION_TP, 12400, 12410, 12405);
    setLowHighExpected(lowResults, highResults, expectedResults, MENTION_F1, 50.4, 50.45, 50.42);

    setLowHighExpected(lowResults, highResults, expectedResults, MUC_TP, 6245, 6255, 6250);
    setLowHighExpected(lowResults, highResults, expectedResults, MUC_F1, 60.65, 60.7, 60.66);

    setLowHighExpected(lowResults, highResults, expectedResults, BCUBED_TP, 12440, 12452.25, 12452.25);
    setLowHighExpected(lowResults, highResults, expectedResults, BCUBED_F1, 70.75, 70.85, 70.80);

    setLowHighExpected(lowResults, highResults, expectedResults, CEAFM_TP, 10915, 10930, 10920);
    setLowHighExpected(lowResults, highResults, expectedResults, CEAFM_F1, 59.4, 59.5, 59.42);

    setLowHighExpected(lowResults, highResults, expectedResults, CEAFE_TP, 3830, 3840, 3831.36);
    setLowHighExpected(lowResults, highResults, expectedResults, CEAFE_F1, 47.4, 47.5, 47.45);

    setLowHighExpected(lowResults, highResults, expectedResults, BLANC_F1, 75.35, 75.44, 75.38);

    setLowHighExpected(lowResults, highResults, expectedResults, CONLL_SCORE, 59.6, 59.7, 59.64);

    BenchmarkingHelper.benchmarkResults(results, lowResults, highResults, expectedResults);
  }
"
"  @Test
  public void testRebuildingMWTText() throws IOException {
    // set up French properties
    Properties frenchProperties = LanguageInfo.getLanguageProperties(""french"");
    frenchProperties.setProperty(""annotators"", ""tokenize,ssplit,mwt"");
    StanfordCoreNLP frenchPipeline = new StanfordCoreNLP(frenchProperties);
    String frenchText = ""Le but des bandes de roulement est d'augmenter la traction."";
    CoreDocument frenchDoc = new CoreDocument(frenchPipeline.process(frenchText));
    String rebuiltFrenchText = SentenceUtils.listToOriginalTextString(frenchDoc.tokens());
    assertTrue(frenchText.equals(rebuiltFrenchText));
  }
"
"  @Test
  public void testRebuildingText() {
    // set up basic English pipeline
    Properties basicProperties = new Properties();
    basicProperties.setProperty(""annotators"", ""tokenize,ssplit"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(basicProperties);
    String text = ""Let's hope this doesn't not work properly.  Especially across sentences. "";
    CoreDocument doc = new CoreDocument(pipeline.process(text));
    String rebuiltText = SentenceUtils.listToOriginalTextString(doc.tokens());
    assertTrue(text.equals(rebuiltText));
  }
"
"  @Test
  public void testTokenSequenceMatcherValue() throws IOException {
    CoreMap doc = createDocument(testText);

    // Test simple sequence with value
    TokenSequencePattern p = TokenSequencePattern.compile(getOrPatternExpr(
            new Pair<String,Object>(""one"", 1), new Pair<String,Object>(""two"", null), new Pair<String,Object>(""fifty"", 50)));
    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));

    boolean match = m.find();
    assertTrue(match);
    assertEquals(""one"", m.group());
    assertEquals(1, m.groupValue());

    match = m.find();
    assertTrue(match);
    assertEquals(""two"", m.group());
    assertNull(m.groupValue());

    match = m.find();
    assertTrue(match);
    assertEquals(""fifty"", m.group());
    assertEquals(50, m.groupValue());

    match = m.find();
    assertFalse(match);
  }
"
"  @Test
  public void testTokenSequenceMatcherBeginEnd() throws IOException {
    CoreMap doc = createDocument(testText);

    // Test simple sequence with begin sequence matching
    TokenSequencePattern p = TokenSequencePattern.compile(""^ [] []"");
    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));

    boolean match = m.find();
    assertTrue(match);
    assertEquals(""the number"", m.group());

    match = m.find();
    assertFalse(match);

    // Test simple sequence with end sequence matching
    p = TokenSequencePattern.compile(""[] [] $"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));

    match = m.find();
    assertTrue(match);
    assertEquals(""fifty."", m.group());

    match = m.find();
    assertFalse(match);

    // Test simple sequence with begin and end sequence matching
    p = TokenSequencePattern.compile(""^ [] [] $"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));

    match = m.find();
    assertFalse(match);

    // Test simple sequence with ^$ in a string regular expression
    p = TokenSequencePattern.compile(""/^number$/"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));

    match = m.find();
    assertTrue(match);
    assertEquals(""number"", m.group());

    match = m.find();
    assertFalse(match);
  }
"
"  @Test
  public void testTokenSequenceMatcher1() throws IOException {
    CoreMap doc = createDocument(testText1);

    // Test simple sequence
    TokenSequencePattern p = TokenSequencePattern.compile(getSequencePatternExpr(""Archbishop"", ""of"", ""Canterbury""));
    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    boolean match = m.find();
    assertTrue(match);
    assertEquals(""Archbishop of Canterbury"", m.group());
    match = m.find();
    assertFalse(match);

    m.reset();
    match = m.find();
    assertTrue(match);
    assertEquals(""Archbishop of Canterbury"", m.group());

    m.reset();
    match = m.matches();
    assertFalse(match);

    // Test sequence with or
    p = TokenSequencePattern.compile(
            new SequencePattern.OrPatternExpr(
                    getSequencePatternExpr(""Archbishop"", ""of"", ""Canterbury""),
                    getSequencePatternExpr(""Bishop"", ""of"", ""London"")
            ));
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""Bishop of London"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""Archbishop of Canterbury"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""Bishop of London"", m.group());
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile(
              new SequencePattern.SequencePatternExpr(
                    SequencePattern.SEQ_BEGIN_PATTERN_EXPR,
                    getSequencePatternExpr(""Archbishop"", ""of"", ""Canterbury"")
            ));
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile(
            new SequencePattern.SequencePatternExpr(
                    SequencePattern.SEQ_BEGIN_PATTERN_EXPR,
                    getSequencePatternExpr(""Mellitus"", ""was"", ""the"")
            ));
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""Mellitus was the"", m.group());
    match = m.find();
    assertFalse(match);


    p = TokenSequencePattern.compile(
            new SequencePattern.SequencePatternExpr(
                    getSequencePatternExpr(""Archbishop"", ""of"", ""Canterbury""),
                    SequencePattern.SEQ_END_PATTERN_EXPR
                    ));
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile(
            new SequencePattern.SequencePatternExpr(
                    getSequencePatternExpr(""London"", ""in"", ""604"", "".""),
                    SequencePattern.SEQ_END_PATTERN_EXPR
                    ));
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""London in 604."", m.group());
    match = m.find();
    assertFalse(match);
  }
"
"  @Test
  public void testTokenSequenceMatcher2() throws IOException {
    CoreMap doc = createDocument(testText1);
    TokenSequencePattern p = TokenSequencePattern.compile(
                    getSequencePatternExpr("".*"", "".*"", ""of"", "".*""));

    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    boolean match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""first Bishop of London"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""third Archbishop of Canterbury"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""a member of the"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""as Bishop of London"", m.group());
    match = m.find();
    assertFalse(match);

    // Test sequence with groups
    p = TokenSequencePattern.compile(
                    new SequencePattern.SequencePatternExpr(
                      new SequencePattern.GroupPatternExpr(
                            getSequencePatternExpr("".*"", "".*"")),
                      getNodePatternExpr(""of""),
                      new SequencePattern.GroupPatternExpr(
                            getSequencePatternExpr("".*""))));

    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""first Bishop of London"", m.group());
    assertEquals(""first Bishop"", m.group(1));
    assertEquals(""London"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""third Archbishop of Canterbury"", m.group());
    assertEquals(""third Archbishop"", m.group(1));
    assertEquals(""Canterbury"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""a member of the"", m.group());
    assertEquals(""a member"", m.group(1));
    assertEquals(""the"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""as Bishop of London"", m.group());
    assertEquals(""as Bishop"", m.group(1));
    assertEquals(""London"", m.group(2));
    match = m.find();
    assertFalse(match);

  }
"
"  @Test
  public void testTokenSequenceMatcher3() throws IOException {
    CoreMap doc = createDocument(testText1);

    // Test sequence with groups
    TokenSequencePattern p = TokenSequencePattern.compile(
        new SequencePattern.SequencePatternExpr(
            new SequencePattern.GroupPatternExpr(
                new SequencePattern.RepeatPatternExpr(
                    getSequencePatternExpr(""[A-Za-z]+""), 1, 2)),
            getNodePatternExpr(""of""),
            new SequencePattern.GroupPatternExpr(
                new SequencePattern.RepeatPatternExpr(
                    getSequencePatternExpr(""[A-Za-z]+""), 1, 3))));

    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    boolean match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""first Bishop of London"", m.group());
    assertEquals(""first Bishop"", m.group(1));
    assertEquals(""London"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""third Archbishop of Canterbury"", m.group());
    assertEquals(""third Archbishop"", m.group(1));
    assertEquals(""Canterbury"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""a member of the Gregorian mission"", m.group());
    assertEquals(""a member"", m.group(1));
    assertEquals(""the Gregorian mission"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""as Bishop of London in"", m.group());
    assertEquals(""as Bishop"", m.group(1));
    assertEquals(""London in"", m.group(2));
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile(
        new SequencePattern.SequencePatternExpr(
            new SequencePattern.GroupPatternExpr(
                new SequencePattern.RepeatPatternExpr(
                    getNodePatternExpr(""[A-Za-z]+""), 2, 2)),
            getNodePatternExpr(""of""),
            new SequencePattern.GroupPatternExpr(
                new SequencePattern.RepeatPatternExpr(
                    getNodePatternExpr(""[A-Za-z]+""), 1, 3, false))));

    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""first Bishop of London"", m.group());
    assertEquals(""first Bishop"", m.group(1));
    assertEquals(""London"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""third Archbishop of Canterbury"", m.group());
    assertEquals(""third Archbishop"", m.group(1));
    assertEquals(""Canterbury"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""a member of the"", m.group());
    assertEquals(""a member"", m.group(1));
    assertEquals(""the"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""as Bishop of London"", m.group());
    assertEquals(""as Bishop"", m.group(1));
    assertEquals(""London"", m.group(2));
    match = m.find();
    assertFalse(match);
  }
"
"  @Test
  public void testTokenSequenceMatcherConj() throws IOException {
    CoreMap doc = createDocument(testText1);
    TokenSequencePattern p = TokenSequencePattern.compile(
                  new SequencePattern.AndPatternExpr(
                    new SequencePattern.SequencePatternExpr(
                      new SequencePattern.GroupPatternExpr(
                            new SequencePattern.RepeatPatternExpr(
                                    getNodePatternExpr(""[A-Za-z]+""), 2, 2)),
                      getNodePatternExpr(""of""),
                      new SequencePattern.GroupPatternExpr(
                            new SequencePattern.RepeatPatternExpr(
                                    getNodePatternExpr(""[A-Za-z]+""), 1, 3, false))),
                    new SequencePattern.SequencePatternExpr(
                      new SequencePattern.GroupPatternExpr(
                        new SequencePattern.RepeatPatternExpr(
                            getNodePatternExpr("".*""), 0, -1)),
                      getNodePatternExpr(""Bishop""),
                      new SequencePattern.RepeatPatternExpr(
                            getNodePatternExpr("".*""), 0, -1)
                    )));

    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    boolean match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    assertEquals(""first Bishop of London"", m.group());
    assertEquals(""first Bishop"", m.group(1));
    assertEquals(""London"", m.group(2));
    assertEquals(""first"", m.group(3));
    match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    // TODO: This conjunction has both a greedy and nongreedy pattern
    //  - the greedy will try to match as much as possible
    //  - while the non greedy will try to match less
    //  - currently the greedy overrides the nongreedy so we get an additional in...
    assertEquals(""as Bishop of London in"", m.group());
    assertEquals(""as Bishop"", m.group(1));
    assertEquals(""London in"", m.group(2));
    assertEquals(""as"", m.group(3));
    match = m.find();
    assertFalse(match);


    // Same as before, but both non-greedy now...
    p = TokenSequencePattern.compile(
                  new SequencePattern.AndPatternExpr(
                    new SequencePattern.SequencePatternExpr(
                      new SequencePattern.GroupPatternExpr(
                            new SequencePattern.RepeatPatternExpr(
                                    getNodePatternExpr(""[A-Za-z]+""), 2, 2)),
                      getNodePatternExpr(""of""),
                      new SequencePattern.GroupPatternExpr(
                            new SequencePattern.RepeatPatternExpr(
                                    getNodePatternExpr(""[A-Za-z]+""), 1, 3, false))),
                    new SequencePattern.SequencePatternExpr(
                      new SequencePattern.GroupPatternExpr(
                        new SequencePattern.RepeatPatternExpr(
                            getNodePatternExpr("".*""), 0, -1)),
                      getNodePatternExpr(""Bishop""),
                      new SequencePattern.RepeatPatternExpr(
                            getNodePatternExpr("".*""), 0, -1, false)
                    )));

    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    assertEquals(""first Bishop of London"", m.group());
    assertEquals(""first Bishop"", m.group(1));
    assertEquals(""London"", m.group(2));
    assertEquals(""first"", m.group(3));
    match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    assertEquals(""as Bishop of London"", m.group());
    assertEquals(""as Bishop"", m.group(1));
    assertEquals(""London"", m.group(2));
    assertEquals(""as"", m.group(3));
    match = m.find();
    assertFalse(match);


    // Same as before, but compiled from string
    p = TokenSequencePattern.compile(
            ""(?: (/[A-Za-z]+/{2,2}) /of/ (/[A-Za-z]+/{1,3}?) ) & (?: (/.*/*) /Bishop/ /.*/*? )"");

    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    assertEquals(""first Bishop of London"", m.group());
    assertEquals(""first Bishop"", m.group(1));
    assertEquals(""London"", m.group(2));
    assertEquals(""first"", m.group(3));
    match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    assertEquals(""as Bishop of London"", m.group());
    assertEquals(""as Bishop"", m.group(1));
    assertEquals(""London"", m.group(2));
    assertEquals(""as"", m.group(3));
    match = m.find();
    assertFalse(match);
  }
"
"  @Test
  public void testTokenSequenceMatcherConj2() throws IOException {
    String content = ""The cat is sleeping on the floor."";
    String greedyPattern = ""(?: ([]* cat []*) & ([]* sleeping []*))"";

    TokenizerFactory tf = PTBTokenizer.factory(new CoreLabelTokenFactory(), """");
    List<CoreLabel> tokens = tf.getTokenizer(new StringReader(content)).tokenize();
    TokenSequencePattern seqPattern = TokenSequencePattern.compile(greedyPattern);
    TokenSequenceMatcher matcher = seqPattern.getMatcher(tokens);

    boolean entireMatch = matcher.matches();
    assertTrue(entireMatch);

    boolean match = matcher.find();
    assertTrue(match);
    assertEquals(""The cat is sleeping on the floor."", matcher.group());

    String reluctantPattern = ""(?: ([]*? cat []*?) & ([]*? sleeping []*?))"";
    TokenSequencePattern seqPattern2 = TokenSequencePattern.compile(reluctantPattern);
    TokenSequenceMatcher matcher2 = seqPattern2.getMatcher(tokens);

    match = matcher2.find();
    assertTrue(match);
    assertEquals(""The cat is sleeping"", matcher2.group());
  }
"
"  @Test
  public void testTokenSequenceMatcherConjAll() throws IOException {
    CoreMap doc = createDocument(testText1);
    TokenSequencePattern p = TokenSequencePattern.compile(
            ""(?: (/[A-Za-z]+/{1,2}) /of/ (/[A-Za-z]+/{1,3}?) ) & (?: (/.*/*) /Bishop/ /.*/*? )"");

    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    m.setFindType(SequenceMatcher.FindType.FIND_ALL);
    // Test finding of ALL matching sequences with conjunctions
    // todo: Not all sequences are found for some reason - missing sequences starting with just Bishop....
    boolean match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    assertEquals(""first Bishop of London"", m.group());
    assertEquals(""first Bishop"", m.group(1));
    assertEquals(""London"", m.group(2));
    assertEquals(""first"", m.group(3));
    match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    assertEquals(""Bishop of London"", m.group());
    assertEquals(""Bishop"", m.group(1));
    assertEquals(""London"", m.group(2));
    assertEquals("""", m.group(3));
    match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    assertEquals(""as Bishop of London"", m.group());
    assertEquals(""as Bishop"", m.group(1));
    assertEquals(""London"", m.group(2));
    assertEquals(""as"", m.group(3));
    match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    assertEquals(""as Bishop of London in"", m.group());
    assertEquals(""as Bishop"", m.group(1));
    assertEquals(""London in"", m.group(2));
    assertEquals(""as"", m.group(3));
    match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    assertEquals(""Bishop of London"", m.group());
    assertEquals(""Bishop"", m.group(1));
    assertEquals(""London"", m.group(2));
    assertEquals("""", m.group(3));
    match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    assertEquals(""Bishop of London in"", m.group());
    assertEquals(""Bishop"", m.group(1));
    assertEquals(""London in"", m.group(2));
    assertEquals("""", m.group(3));
    match = m.find();
    assertFalse(match);
  }
"
"  @Test
  public void testTokenSequenceMatcherAll() throws IOException {
    CoreMap doc = createDocument(testText1);
    TokenSequencePattern p = TokenSequencePattern.compile(
            ""(/[A-Za-z]+/{1,2}) /of/ (/[A-Za-z]+/{1,3}?) "");

    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    m.setFindType(SequenceMatcher.FindType.FIND_ALL);
    // Test finding of ALL matching sequences
    // NOTE: when using FIND_ALL greedy/reluctant modifiers are not enforced
    //       perhaps should add syntax where some of them are enforced...
    boolean match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""first Bishop of London"", m.group());
    assertEquals(""first Bishop"", m.group(1));
    assertEquals(""London"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""Bishop of London"", m.group());
    assertEquals(""Bishop"", m.group(1));
    assertEquals(""London"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""third Archbishop of Canterbury"", m.group());
    assertEquals(""third Archbishop"", m.group(1));
    assertEquals(""Canterbury"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""Archbishop of Canterbury"", m.group());
    assertEquals(""Archbishop"", m.group(1));
    assertEquals(""Canterbury"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""a member of the"", m.group());
    assertEquals(""a member"", m.group(1));
    assertEquals(""the"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""a member of the Gregorian"", m.group());
    assertEquals(""a member"", m.group(1));
    assertEquals(""the Gregorian"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""a member of the Gregorian mission"", m.group());
    assertEquals(""a member"", m.group(1));
    assertEquals(""the Gregorian mission"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""member of the"", m.group());
    assertEquals(""member"", m.group(1));
    assertEquals(""the"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""member of the Gregorian"", m.group());
    assertEquals(""member"", m.group(1));
    assertEquals(""the Gregorian"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""member of the Gregorian mission"", m.group());
    assertEquals(""member"", m.group(1));
    assertEquals(""the Gregorian mission"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""as Bishop of London"", m.group());
    assertEquals(""as Bishop"", m.group(1));
    assertEquals(""London"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""as Bishop of London in"", m.group());
    assertEquals(""as Bishop"", m.group(1));
    assertEquals(""London in"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""Bishop of London"", m.group());
    assertEquals(""Bishop"", m.group(1));
    assertEquals(""London"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""Bishop of London in"", m.group());
    assertEquals(""Bishop"", m.group(1));
    assertEquals(""London in"", m.group(2));
    match = m.find();
    assertFalse(match);
  }
"
"  @Test
  public void testTokenSequenceMatcherAll2() throws IOException {
    String text = ""DATE1 PROD1 PRICE1 PROD2 PRICE2 PROD3 PRICE3 DATE2 PROD4 PRICE4 PROD5 PRICE5 PROD6 PRICE6"";
    CoreMap doc = createDocument(text);
    TokenSequencePattern p = TokenSequencePattern.compile(
        ""(/DATE.*/) (?: /PROD.*/ /PRICE.*/)* (/PROD.*/) (/PRICE.*/)"");

    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    m.setFindType(SequenceMatcher.FindType.FIND_ALL);
    // Test finding of ALL matching sequences
    boolean match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    assertEquals(""DATE1"", m.group(1));
    assertEquals(""PROD3"", m.group(2));
    assertEquals(""PRICE3"", m.group(3));
    match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    assertEquals(""DATE1"", m.group(1));
    assertEquals(""PROD2"", m.group(2));
    assertEquals(""PRICE2"", m.group(3));
    match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    assertEquals(""DATE1"", m.group(1));
    assertEquals(""PROD1"", m.group(2));
    assertEquals(""PRICE1"", m.group(3));
    match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    assertEquals(""DATE2"", m.group(1));
    assertEquals(""PROD6"", m.group(2));
    assertEquals(""PRICE6"", m.group(3));
    match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    assertEquals(""DATE2"", m.group(1));
    assertEquals(""PROD5"", m.group(2));
    assertEquals(""PRICE5"", m.group(3));
    match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    assertEquals(""DATE2"", m.group(1));
    assertEquals(""PROD4"", m.group(2));
    assertEquals(""PRICE4"", m.group(3));
    match = m.find();
    assertFalse(match);
  }
"
"  @Test
  public void testTokenSequenceMatcherNonOverlapping() throws IOException {
    String text = ""DATE1 PROD1 PRICE1 PROD2 PRICE2 PROD3 PRICE3 DATE2 PROD4 PRICE4 PROD5 PRICE5 PROD6 PRICE6"";
    CoreMap doc = createDocument(text);
    TokenSequencePattern p = TokenSequencePattern.compile(
        ""(/DATE.*/) ((/PROD.*/ /PRICE.*/)+)"");

    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    boolean match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    assertEquals(""DATE1"", m.group(1));
    assertEquals(""PROD1 PRICE1 PROD2 PRICE2 PROD3 PRICE3"", m.group(2));
    assertEquals(""PROD3 PRICE3"", m.group(3));
    match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    assertEquals(""DATE2"", m.group(1));
    assertEquals(""PROD4 PRICE4 PROD5 PRICE5 PROD6 PRICE6"", m.group(2));
    assertEquals(""PROD6 PRICE6"", m.group(3));
    match = m.find();
    assertFalse(match);
  }
"
"  @Test
  public void testTokenSequenceMatcher4() throws IOException {
    CoreMap doc = createDocument(testText1);

    // Test sequence with groups
    TokenSequencePattern p = TokenSequencePattern.compile(
                      new SequencePattern.RepeatPatternExpr(
                                    getSequencePatternExpr(""[A-Za-z]+""), 1, -1));

    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    boolean match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""Mellitus was the first Bishop of London"", m.group());

    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""the third Archbishop of Canterbury"", m.group());

    p = TokenSequencePattern.compile(
            new SequencePattern.SequencePatternExpr(
                      new SequencePattern.RepeatPatternExpr(
                              getSequencePatternExpr(""[A-Za-z]+""), 0, -1),
                      getSequencePatternExpr(""Mellitus"", ""was"")));

    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""Mellitus was"", m.group());
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile(
            new SequencePattern.SequencePatternExpr(
                      new SequencePattern.RepeatPatternExpr(
                              getSequencePatternExpr(""[A-Za-z]+""), 1, -1),
                      getSequencePatternExpr(""Mellitus"", ""was"")));

    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertFalse(match);

  }
"
"  @Test
  public void testTokenSequenceMatcher5() throws IOException {
    CoreMap doc = createDocument(testText1);

    // Test simple sequence
    TokenSequencePattern p = TokenSequencePattern.compile("" [ { word:\""Archbishop\"" } ]  [ { word:\""of\"" } ]  [ { word:\""Canterbury\"" } ]"");
    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    boolean match = m.find();
    assertTrue(match);
    assertEquals(""Archbishop of Canterbury"", m.group());
    match = m.find();
    assertFalse(match);

    m.reset();
    match = m.find();
    assertTrue(match);
    assertEquals(""Archbishop of Canterbury"", m.group());

    m.reset();
    match = m.matches();
    assertFalse(match);


    p = TokenSequencePattern.compile("" [ \""Archbishop\"" ]  [ \""of\""  ]  [ \""Canterbury\""  ]"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(""Archbishop of Canterbury"", m.group());
    match = m.find();
    assertFalse(match);

    m.reset();
    match = m.find();
    assertTrue(match);
    assertEquals(""Archbishop of Canterbury"", m.group());

    m.reset();
    match = m.matches();
    assertFalse(match);

    // Test sequence with or
    p = TokenSequencePattern.compile("" [ \""Archbishop\""] [\""of\""] [\""Canterbury\""] |  [ \""Bishop\"" ] [ \""of\"" ]  [ \""London\"" ] "");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""Bishop of London"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""Archbishop of Canterbury"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""Bishop of London"", m.group());
    match = m.find();
    assertFalse(match);

  }
"
"  @Test
  public void testTokenSequenceMatcher6() throws IOException {
    CoreMap doc = createDocument(testText1);
    TokenSequencePattern p = TokenSequencePattern.compile(""[ /.*/ ] [ /.*/ ] [/of/] [/.*/]"");
    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    boolean match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""first Bishop of London"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""third Archbishop of Canterbury"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""a member of the"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""as Bishop of London"", m.group());
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile(""([ /.*/ ] [ /.*/ ]) [/of/] ([/.*/])"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""first Bishop of London"", m.group());
    assertEquals(""first Bishop"", m.group(1));
    assertEquals(""London"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""third Archbishop of Canterbury"", m.group());
    assertEquals(""third Archbishop"", m.group(1));
    assertEquals(""Canterbury"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""a member of the"", m.group());
    assertEquals(""a member"", m.group(1));
    assertEquals(""the"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""as Bishop of London"", m.group());
    assertEquals(""as Bishop"", m.group(1));
    assertEquals(""London"", m.group(2));
    match = m.find();
    assertFalse(match);

  }
"
"  @Test
  public void testTokenSequenceMatcher7() throws IOException {
    CoreMap doc = createDocument(testText1);

    // Test sequence with groups
    TokenSequencePattern p = TokenSequencePattern.compile("" ( [ /[A-Za-z]+/ ]{1,2} )  [ /of/ ] ( [ /[A-Za-z]+/ ]{1,3} )"");
    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    boolean match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""first Bishop of London"", m.group());
    assertEquals(""first Bishop"", m.group(1));
    assertEquals(""London"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""third Archbishop of Canterbury"", m.group());
    assertEquals(""third Archbishop"", m.group(1));
    assertEquals(""Canterbury"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""a member of the Gregorian mission"", m.group());
    assertEquals(""a member"", m.group(1));
    assertEquals(""the Gregorian mission"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""as Bishop of London in"", m.group());
    assertEquals(""as Bishop"", m.group(1));
    assertEquals(""London in"", m.group(2));
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile( "" ( [ /[A-Za-z]+/ ]{2,2} )  [ /of/ ] ( [ /[A-Za-z]+/ ]{1,3}? )"");

    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""first Bishop of London"", m.group());
    assertEquals(""first Bishop"", m.group(1));
    assertEquals(""London"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""third Archbishop of Canterbury"", m.group());
    assertEquals(""third Archbishop"", m.group(1));
    assertEquals(""Canterbury"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""a member of the"", m.group());
    assertEquals(""a member"", m.group(1));
    assertEquals(""the"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""as Bishop of London"", m.group());
    assertEquals(""as Bishop"", m.group(1));
    assertEquals(""London"", m.group(2));
    match = m.find();
    assertFalse(match);
  }
"
"  @Test
  public void testTokenSequenceMatcher8() throws IOException {
    CoreMap doc = createDocument(testText1);

    // Test sequence with groups
    TokenSequencePattern p = TokenSequencePattern.compile( ""[ /[A-Za-z]+/ ]*"");

    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    boolean match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""Mellitus was the first Bishop of London"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""the third Archbishop of Canterbury"", m.group());

    p = TokenSequencePattern.compile( ""[ /[A-Za-z]+/ ]*  [\""Mellitus\""] [ \""was\""]"");

    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""Mellitus was"", m.group());
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile( ""[ /[A-Za-z]+/ ]+  [\""Mellitus\""] [ \""was\""]"");

    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertFalse(match);

  }
"
"  @Test
  public void testTokenSequenceMatcher9() throws IOException {
    CoreMap doc = createDocument(testText1);

    // Test sequence with groups
//    TokenSequencePattern p = TokenSequencePattern.compile( ""(?$contextprev /.*/) (?$treat [{{treat}} & /.*/]) (?$contextnext [/.*/])"");
    TokenSequencePattern p = TokenSequencePattern.compile(""(?$contextprev /.*/) (?$test [{tag:NNP} & /.*/]) (?$contextnext [/.*/])"");

    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    boolean match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    assertEquals(""first Bishop of"", m.group());

    assertEquals(""first"", m.group(1));
    assertEquals(""Bishop"", m.group(2));
    assertEquals(""of"", m.group(3));
    assertEquals(""first"", m.group(""$contextprev""));
    assertEquals(""Bishop"", m.group(""$test""));
    assertEquals(""of"", m.group(""$contextnext""));
    assertEquals(""first"", m.group("" $contextprev""));
    assertEquals(""Bishop"", m.group(""$test ""));
    assertEquals(null, m.group(""$contex tnext""));

    assertEquals(3, m.start(""$contextprev""));
    assertEquals(4, m.end(""$contextprev""));
    assertEquals(4, m.start(""$test""));
    assertEquals(5, m.end(""$test""));
    assertEquals(5, m.start(""$contextnext""));
    assertEquals(6, m.end(""$contextnext""));
  }
"
"  @Test
  public void testTokenSequenceMatcher10() throws IOException {
    CoreMap doc = createDocument(""the number is five or 5 or 5.0 or but not 5x or -5 or 5L."");

    // Test simplified pattern with number
    TokenSequencePattern p = TokenSequencePattern.compile( ""(five|5|5x|5.0|-5|5L)"");

    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    boolean match = m.find();
    assertTrue(match);
    assertEquals(1, m.groupCount());
    assertEquals(""five"", m.group(1));

    match = m.find();
    assertTrue(match);
    assertEquals(1, m.groupCount());
    assertEquals(""5"", m.group(1));

    match = m.find();
    assertTrue(match);
    assertEquals(1, m.groupCount());
    assertEquals(""5.0"", m.group(1));

    match = m.find();
    assertTrue(match);
    assertEquals(1, m.groupCount());
    assertEquals(""5x"", m.group(1));

    match = m.find();
    assertTrue(match);
    assertEquals(1, m.groupCount());
    assertEquals(""-5"", m.group(1));

    match = m.find();
    assertTrue(match);
    assertEquals(1, m.groupCount());
    assertEquals(""5L"", m.group(1));

    match = m.find();
    assertFalse(match);
  }
"
"  @Test
  public void testTokenSequenceOptimizeOrString() throws IOException {
    CoreMap doc = createDocument(""atropine we need to have many many words here but we don't sweating"");

    // Test simplified pattern with number
    TokenSequencePattern p = TokenSequencePattern.compile( ""(?$dt \""atropine\"") []{0,15} "" +
            ""(?$se  \""social\"" \""avoidant\"" \""behaviour\""|\""dysuria\""|\""hyperglycaemia\""| \""mental\"" \""disorder\""|\""vertigo\""|\""flutter\""| \""chest\"" \""pain\""| \""elevated\"" \""blood\"" \""pressure\""|\""mania\""| \""rash\"" \""erythematous\""|\""manic\""| \""papular\"" \""rash\""|\""death\""| \""atrial\"" \""arrhythmia\""| \""dry\"" \""eyes\""| \""loss\"" \""of\"" \""libido\""| \""rash\"" \""papular\""|\""hypersensitivity\""| \""blood\"" \""pressure\"" \""increased\""|\""dyspepsia\""| \""accommodation\"" \""disorder\""| \""reflexes\"" \""increased\""|\""lesions\""|\""asthenia\""| \""gastrointestinal\"" \""pain\""|\""excitement\""| \""breast\"" \""feeding\""|\""hypokalaemia\""| \""cerebellar\"" \""syndrome\""|\""nervousness\""| \""pulmonary\"" \""oedema\""| \""inspiratory\"" \""stridor\""| \""taste\"" \""altered\""|\""paranoia\""| \""psychotic\"" \""disorder\""| \""open\"" \""angle\"" \""glaucoma\""|\""photophobia\""| \""dry\"" \""eye\""|\""osteoarthritis\""| \""keratoconjunctivitis\"" \""sicca\""| \""haemoglobin\"" \""increased\""| \""ventricular\"" \""extrasystoles\""|\""hallucinations\""|\""conjunctivitis\""|\""paralysis\""| \""qrs\"" \""complex\""|\""anxiety\""| \""conjunctival\"" \""disorder\""|\""coma\""|\""strabismus\""|\""thirst\""|\""para\""| \""sicca\"" \""syndrome\""| \""atrioventricular\"" \""dissociation\""|\""desquamation\""|\""crusting\""| \""abdominal\"" \""distension\""|\""blindness\""|\""hypotension\""|\""dermatitis\""| \""sinus\"" \""tachycardia\""| \""abdominal\"" \""distention\""| \""lacrimation\"" \""decreased\""|\""sicca\""| \""paralytic\"" \""ileus\""| \""urinary\"" \""hesitation\""|\""withdrawn\""| \""erectile\"" \""dysfunction\""|\""keratoconjunctivitis\""|\""anaphylaxis\""| \""psychiatric\"" \""disorders\""| \""altered\"" \""taste\""|\""somnolence\""|\""extrasystoles\""|\""ageusia\""| \""intraocular\"" \""pressure\"" \""increased\""| \""left\"" \""ventricular\"" \""failure\""|\""impotence\""|\""drowsiness\""|\""conjunctiva\""| \""delayed\"" \""gastric\"" \""emptying\""| \""gastrointestinal\"" \""sounds\"" \""abnormal\""| \""qt\"" \""prolonged\""| \""supraventricular\"" \""tachycardia\""|\""weakness\""|\""hypertonia\""| \""confusional\"" \""state\""|\""anhidrosis\""|\""myopia\""|\""dyspnoea\""| \""speech\"" \""impairment\"" \""nos\""| \""rash\"" \""maculo\"" \""papular\""|\""petechiae\""|\""tachypnea\""| \""acute\"" \""angle\"" \""closure\"" \""glaucoma\""| \""gastrooesophageal\"" \""reflux\"" \""disease\""|\""hypokalemia\""| \""left\"" \""heart\"" \""failure\""| \""myocardial\"" \""infarction\""| \""site\"" \""reaction\""| \""ventricular\"" \""fibrillation\""|\""fibrillation\""| \""maculopapular\"" \""rash\""| \""impaired\"" \""gastric\"" \""emptying\""|\""amnesia\""| \""labored\"" \""respirations\""| \""decreased\"" \""lacrimation\""|\""mydriasis\""|\""headache\""| \""dry\"" \""mouth\""|\""scab\""| \""cardiac\"" \""syncope\""| \""visual\"" \""acuity\"" \""reduced\""|\""tension\""| \""blurred\"" \""vision\""| \""bloated\"" \""feeling\""| \""labored\"" \""breathing\""| \""stridor\"" \""inspiratory\""| \""skin\"" \""exfoliation\""| \""memory\"" \""loss\""|\""syncope\""| \""rash\"" \""scarlatiniform\""|\""hyperpyrexia\""| \""cardiac\"" \""flutter\""|\""heartburn\""| \""bowel\"" \""sounds\"" \""decreased\""|\""blepharitis\""|\""tachycardia\""| \""excessive\"" \""thirst\""|\""confusion\""| \""rash\"" \""macular\""| \""taste\"" \""loss\""| \""respiratory\"" \""failure\""|\""hesitancy\""|\""dysmetria\""|\""disorientation\""| \""decreased\"" \""hemoglobin\""| \""atrial\"" \""fibrillation\""| \""urinary\"" \""retention\""| \""dry\"" \""skin\""|\""dehydration\""|\""hyponatraemia\""|\""dysgeusia\""|\""disorder\""| \""increased\"" \""intraocular\"" \""pressure\""| \""speech\"" \""disorder\""| \""feeling\"" \""abnormal\""|\""pain\""| \""anaphylactic\"" \""shock\""|\""hallucination\""| \""abdominal\"" \""pain\""| \""junctional\"" \""tachycardia\""| \""bun\"" \""increased\""| \""ventricular\"" \""flutter\""| \""scarlatiniform\"" \""rash\""|\""agitation\""| \""feeling\"" \""hot\""|\""hyponatremia\""| \""decreased\"" \""bowel\"" \""sounds\""|\""cyanosis\""|\""dysarthria\""| \""heat\"" \""intolerance\""|\""hyperglycemia\""|\""reflux\""| \""angle\"" \""closure\"" \""glaucoma\""| \""electrocardiogram\"" \""qt\"" \""prolonged\""| \""vision\"" \""blurred\""| \""blood\"" \""urea\"" \""increased\""|\""dizziness\""|\""arrhythmia\""|\""erythema\""|\""vomiting\""| \""difficulty\"" \""in\"" \""micturition\""|\""infarction\""|\""laryngospasm\""|\""hypoglycaemia\""|\""hypoglycemia\""| \""elevated\"" \""hemoglobin\""| \""skin\"" \""warm\""| \""ventricular\"" \""arrhythmia\""|\""dissociation\""| \""warm\"" \""skin\""| \""follicular\"" \""conjunctivitis\""|\""urticaria\""|\""fatigue\""| \""cardiac\"" \""fibrillation\""| \""decreased\"" \""sweating\""| \""decreased\"" \""visual\"" \""acuity\""|\""lethargy\""| \""acute\"" \""angle\"" \""closure\"" \""glaucoma\""| \""nodal\"" \""rhythm\""|\""borborygmi\""|\""hyperreflexia\""| \""respiratory\"" \""depression\""|\""diarrhea\""|\""leukocytosis\""| \""speech\"" \""disturbance\""|\""ataxia\""|\""cycloplegia\""|\""tachypnoea\""|\""eczema\""| \""supraventricular\"" \""extrasystoles\""|\""ileus\""| \""cardiac\"" \""arrest\""| \""ventricular\"" \""tachycardia\""|\""laryngitis\""|\""delirium\""|\""lactation\""|\""glaucoma\""|\""obstruction\""|\""hypohidrosis\""|\""parity\""|\""palpitations\""| \""temperature\"" \""intolerance\""|\""constipation\""|\""cyclophoria\""| \""acute\"" \""coronary\"" \""syndrome\""| \""arrhythmia\"" \""supraventricular\""|\""arrest\""|\""lesion\""|\""nausea\""| \""sweating\"" \""decreased\""|\""keratitis\""|\""dyskinesia\""| \""pulmonary\"" \""function\"" \""test\"" \""decreased\""|\""stridor\""|\""swelling\""|\""dysphagia\""| \""haemoglobin\"" \""decreased\""|\""diarrhoea\""| \""ileus\"" \""paralytic\""|\""clonus\""|\""insomnia\""| \""electrocardiogram\"" \""qrs\"" \""complex\""| \""nasal\"" \""congestion\""| \""nasal\"" \""dryness\""|\""sweating\""|\""rash\""| \""nodal\"" \""arrhythmia\""|\""irritability\""|\""hyperhidrosis\""| \""ventricular\"" \""failure\"")"");

    Timing timing = new Timing();
    timing.start();
    for (int i = 0; i < 100; i++) {
      TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
      boolean match = m.find();
      assertTrue(match);
      assertEquals(""atropine we need to have many many words here but we don't sweating"", m.group(0));

      match = m.find();
      assertFalse(match);
    }
    timing.stop(""testTokenSequenceOptimizeOrString matched"");


    CoreMap docNoMatch = createDocument(""atropine we need to have many many words here but we don't, many many many words but still no match"");
    timing.start();
    for (int i = 0; i < 100; i++) {
      TokenSequenceMatcher m = p.getMatcher(docNoMatch.get(CoreAnnotations.TokensAnnotation.class));
      boolean match = m.find();
      assertFalse(match);
    }
    timing.stop(""testTokenSequenceOptimizeOrString no match"");
  }
"
"  @Test
  public void testMultiplePatterns() throws IOException {
    TokenSequencePattern p1 = TokenSequencePattern.compile(""(?$dt \""atropine\"") []{0,15} "" +
        ""(?$se  \""social\"" \""avoidant\"" \""behaviour\""|\""dysuria\""|\""hyperglycaemia\""| \""mental\"" \""disorder\""|\""vertigo\""|\""flutter\""| \""chest\"" \""pain\""| \""elevated\"" \""blood\"" \""pressure\""|\""mania\""| \""rash\"" \""erythematous\""|\""manic\""| \""papular\"" \""rash\""|\""death\""| \""atrial\"" \""arrhythmia\""| \""dry\"" \""eyes\""| \""loss\"" \""of\"" \""libido\""| \""rash\"" \""papular\""|\""hypersensitivity\""| \""blood\"" \""pressure\"" \""increased\""|\""dyspepsia\""| \""accommodation\"" \""disorder\""| \""reflexes\"" \""increased\""|\""lesions\""|\""asthenia\""| \""gastrointestinal\"" \""pain\""|\""excitement\""| \""breast\"" \""feeding\""|\""hypokalaemia\""| \""cerebellar\"" \""syndrome\""|\""nervousness\""| \""pulmonary\"" \""oedema\""| \""inspiratory\"" \""stridor\""| \""taste\"" \""altered\""|\""paranoia\""| \""psychotic\"" \""disorder\""| \""open\"" \""angle\"" \""glaucoma\""|\""photophobia\""| \""dry\"" \""eye\""|\""osteoarthritis\""| \""keratoconjunctivitis\"" \""sicca\""| \""haemoglobin\"" \""increased\""| \""ventricular\"" \""extrasystoles\""|\""hallucinations\""|\""conjunctivitis\""|\""paralysis\""| \""qrs\"" \""complex\""|\""anxiety\""| \""conjunctival\"" \""disorder\""|\""coma\""|\""strabismus\""|\""thirst\""|\""para\""| \""sicca\"" \""syndrome\""| \""atrioventricular\"" \""dissociation\""|\""desquamation\""|\""crusting\""| \""abdominal\"" \""distension\""|\""blindness\""|\""hypotension\""|\""dermatitis\""| \""sinus\"" \""tachycardia\""| \""abdominal\"" \""distention\""| \""lacrimation\"" \""decreased\""|\""sicca\""| \""paralytic\"" \""ileus\""| \""urinary\"" \""hesitation\""|\""withdrawn\""| \""erectile\"" \""dysfunction\""|\""keratoconjunctivitis\""|\""anaphylaxis\""| \""psychiatric\"" \""disorders\""| \""altered\"" \""taste\""|\""somnolence\""|\""extrasystoles\""|\""ageusia\""| \""intraocular\"" \""pressure\"" \""increased\""| \""left\"" \""ventricular\"" \""failure\""|\""impotence\""|\""drowsiness\""|\""conjunctiva\""| \""delayed\"" \""gastric\"" \""emptying\""| \""gastrointestinal\"" \""sounds\"" \""abnormal\""| \""qt\"" \""prolonged\""| \""supraventricular\"" \""tachycardia\""|\""weakness\""|\""hypertonia\""| \""confusional\"" \""state\""|\""anhidrosis\""|\""myopia\""|\""dyspnoea\""| \""speech\"" \""impairment\"" \""nos\""| \""rash\"" \""maculo\"" \""papular\""|\""petechiae\""|\""tachypnea\""| \""acute\"" \""angle\"" \""closure\"" \""glaucoma\""| \""gastrooesophageal\"" \""reflux\"" \""disease\""|\""hypokalemia\""| \""left\"" \""heart\"" \""failure\""| \""myocardial\"" \""infarction\""| \""site\"" \""reaction\""| \""ventricular\"" \""fibrillation\""|\""fibrillation\""| \""maculopapular\"" \""rash\""| \""impaired\"" \""gastric\"" \""emptying\""|\""amnesia\""| \""labored\"" \""respirations\""| \""decreased\"" \""lacrimation\""|\""mydriasis\""|\""headache\""| \""dry\"" \""mouth\""|\""scab\""| \""cardiac\"" \""syncope\""| \""visual\"" \""acuity\"" \""reduced\""|\""tension\""| \""blurred\"" \""vision\""| \""bloated\"" \""feeling\""| \""labored\"" \""breathing\""| \""stridor\"" \""inspiratory\""| \""skin\"" \""exfoliation\""| \""memory\"" \""loss\""|\""syncope\""| \""rash\"" \""scarlatiniform\""|\""hyperpyrexia\""| \""cardiac\"" \""flutter\""|\""heartburn\""| \""bowel\"" \""sounds\"" \""decreased\""|\""blepharitis\""|\""tachycardia\""| \""excessive\"" \""thirst\""|\""confusion\""| \""rash\"" \""macular\""| \""taste\"" \""loss\""| \""respiratory\"" \""failure\""|\""hesitancy\""|\""dysmetria\""|\""disorientation\""| \""decreased\"" \""hemoglobin\""| \""atrial\"" \""fibrillation\""| \""urinary\"" \""retention\""| \""dry\"" \""skin\""|\""dehydration\""|\""hyponatraemia\""|\""dysgeusia\""|\""disorder\""| \""increased\"" \""intraocular\"" \""pressure\""| \""speech\"" \""disorder\""| \""feeling\"" \""abnormal\""|\""pain\""| \""anaphylactic\"" \""shock\""|\""hallucination\""| \""abdominal\"" \""pain\""| \""junctional\"" \""tachycardia\""| \""bun\"" \""increased\""| \""ventricular\"" \""flutter\""| \""scarlatiniform\"" \""rash\""|\""agitation\""| \""feeling\"" \""hot\""|\""hyponatremia\""| \""decreased\"" \""bowel\"" \""sounds\""|\""cyanosis\""|\""dysarthria\""| \""heat\"" \""intolerance\""|\""hyperglycemia\""|\""reflux\""| \""angle\"" \""closure\"" \""glaucoma\""| \""electrocardiogram\"" \""qt\"" \""prolonged\""| \""vision\"" \""blurred\""| \""blood\"" \""urea\"" \""increased\""|\""dizziness\""|\""arrhythmia\""|\""erythema\""|\""vomiting\""| \""difficulty\"" \""in\"" \""micturition\""|\""infarction\""|\""laryngospasm\""|\""hypoglycaemia\""|\""hypoglycemia\""| \""elevated\"" \""hemoglobin\""| \""skin\"" \""warm\""| \""ventricular\"" \""arrhythmia\""|\""dissociation\""| \""warm\"" \""skin\""| \""follicular\"" \""conjunctivitis\""|\""urticaria\""|\""fatigue\""| \""cardiac\"" \""fibrillation\""| \""decreased\"" \""sweating\""| \""decreased\"" \""visual\"" \""acuity\""|\""lethargy\""| \""acute\"" \""angle\"" \""closure\"" \""glaucoma\""| \""nodal\"" \""rhythm\""|\""borborygmi\""|\""hyperreflexia\""| \""respiratory\"" \""depression\""|\""diarrhea\""|\""leukocytosis\""| \""speech\"" \""disturbance\""|\""ataxia\""|\""cycloplegia\""|\""tachypnoea\""|\""eczema\""| \""supraventricular\"" \""extrasystoles\""|\""ileus\""| \""cardiac\"" \""arrest\""| \""ventricular\"" \""tachycardia\""|\""laryngitis\""|\""delirium\""|\""lactation\""|\""glaucoma\""|\""obstruction\""|\""hypohidrosis\""|\""parity\""|\""palpitations\""| \""temperature\"" \""intolerance\""|\""constipation\""|\""cyclophoria\""| \""acute\"" \""coronary\"" \""syndrome\""| \""arrhythmia\"" \""supraventricular\""|\""arrest\""|\""lesion\""|\""nausea\""| \""sweating\"" \""decreased\""|\""keratitis\""|\""dyskinesia\""| \""pulmonary\"" \""function\"" \""test\"" \""decreased\""|\""stridor\""|\""swelling\""|\""dysphagia\""| \""haemoglobin\"" \""decreased\""|\""diarrhoea\""| \""ileus\"" \""paralytic\""|\""clonus\""|\""insomnia\""| \""electrocardiogram\"" \""qrs\"" \""complex\""| \""nasal\"" \""congestion\""| \""nasal\"" \""dryness\""|\""sweating\""|\""rash\""| \""nodal\"" \""arrhythmia\""|\""irritability\""|\""hyperhidrosis\""| \""ventricular\"" \""failure\"")"");
    TokenSequencePattern p2 = TokenSequencePattern.compile( ""(?$dt \""disease\"") []{0,15} "" +
            ""(?$se  \""social\"" \""avoidant\"" \""behaviour\""|\""dysuria\""|\""hyperglycaemia\""| \""mental\"" \""disorder\""|\""vertigo\""|\""flutter\""| \""chest\"" \""pain\""| \""elevated\"" \""blood\"" \""pressure\""|\""mania\""| \""rash\"" \""erythematous\""|\""manic\""| \""papular\"" \""rash\""|\""death\""| \""atrial\"" \""arrhythmia\""| \""dry\"" \""eyes\""| \""loss\"" \""of\"" \""libido\""| \""rash\"" \""papular\""|\""hypersensitivity\""| \""blood\"" \""pressure\"" \""increased\""|\""dyspepsia\""| \""accommodation\"" \""disorder\""| \""reflexes\"" \""increased\""|\""lesions\""|\""asthenia\""| \""gastrointestinal\"" \""pain\""|\""excitement\""| \""breast\"" \""feeding\""|\""hypokalaemia\""| \""cerebellar\"" \""syndrome\""|\""nervousness\""| \""pulmonary\"" \""oedema\""| \""inspiratory\"" \""stridor\""| \""taste\"" \""altered\""|\""paranoia\""| \""psychotic\"" \""disorder\""| \""open\"" \""angle\"" \""glaucoma\""|\""photophobia\""| \""dry\"" \""eye\""|\""osteoarthritis\""| \""keratoconjunctivitis\"" \""sicca\""| \""haemoglobin\"" \""increased\""| \""ventricular\"" \""extrasystoles\""|\""hallucinations\""|\""conjunctivitis\""|\""paralysis\""| \""qrs\"" \""complex\""|\""anxiety\""| \""conjunctival\"" \""disorder\""|\""coma\""|\""strabismus\""|\""thirst\""|\""para\""| \""sicca\"" \""syndrome\""| \""atrioventricular\"" \""dissociation\""|\""desquamation\""|\""crusting\""| \""abdominal\"" \""distension\""|\""blindness\""|\""hypotension\""|\""dermatitis\""| \""sinus\"" \""tachycardia\""| \""abdominal\"" \""distention\""| \""lacrimation\"" \""decreased\""|\""sicca\""| \""paralytic\"" \""ileus\""| \""urinary\"" \""hesitation\""|\""withdrawn\""| \""erectile\"" \""dysfunction\""|\""keratoconjunctivitis\""|\""anaphylaxis\""| \""psychiatric\"" \""disorders\""| \""altered\"" \""taste\""|\""somnolence\""|\""extrasystoles\""|\""ageusia\""| \""intraocular\"" \""pressure\"" \""increased\""| \""left\"" \""ventricular\"" \""failure\""|\""impotence\""|\""drowsiness\""|\""conjunctiva\""| \""delayed\"" \""gastric\"" \""emptying\""| \""gastrointestinal\"" \""sounds\"" \""abnormal\""| \""qt\"" \""prolonged\""| \""supraventricular\"" \""tachycardia\""|\""weakness\""|\""hypertonia\""| \""confusional\"" \""state\""|\""anhidrosis\""|\""myopia\""|\""dyspnoea\""| \""speech\"" \""impairment\"" \""nos\""| \""rash\"" \""maculo\"" \""papular\""|\""petechiae\""|\""tachypnea\""| \""acute\"" \""angle\"" \""closure\"" \""glaucoma\""| \""gastrooesophageal\"" \""reflux\"" \""disease\""|\""hypokalemia\""| \""left\"" \""heart\"" \""failure\""| \""myocardial\"" \""infarction\""| \""site\"" \""reaction\""| \""ventricular\"" \""fibrillation\""|\""fibrillation\""| \""maculopapular\"" \""rash\""| \""impaired\"" \""gastric\"" \""emptying\""|\""amnesia\""| \""labored\"" \""respirations\""| \""decreased\"" \""lacrimation\""|\""mydriasis\""|\""headache\""| \""dry\"" \""mouth\""|\""scab\""| \""cardiac\"" \""syncope\""| \""visual\"" \""acuity\"" \""reduced\""|\""tension\""| \""blurred\"" \""vision\""| \""bloated\"" \""feeling\""| \""labored\"" \""breathing\""| \""stridor\"" \""inspiratory\""| \""skin\"" \""exfoliation\""| \""memory\"" \""loss\""|\""syncope\""| \""rash\"" \""scarlatiniform\""|\""hyperpyrexia\""| \""cardiac\"" \""flutter\""|\""heartburn\""| \""bowel\"" \""sounds\"" \""decreased\""|\""blepharitis\""|\""tachycardia\""| \""excessive\"" \""thirst\""|\""confusion\""| \""rash\"" \""macular\""| \""taste\"" \""loss\""| \""respiratory\"" \""failure\""|\""hesitancy\""|\""dysmetria\""|\""disorientation\""| \""decreased\"" \""hemoglobin\""| \""atrial\"" \""fibrillation\""| \""urinary\"" \""retention\""| \""dry\"" \""skin\""|\""dehydration\""|\""hyponatraemia\""|\""dysgeusia\""|\""disorder\""| \""increased\"" \""intraocular\"" \""pressure\""| \""speech\"" \""disorder\""| \""feeling\"" \""abnormal\""|\""pain\""| \""anaphylactic\"" \""shock\""|\""hallucination\""| \""abdominal\"" \""pain\""| \""junctional\"" \""tachycardia\""| \""bun\"" \""increased\""| \""ventricular\"" \""flutter\""| \""scarlatiniform\"" \""rash\""|\""agitation\""| \""feeling\"" \""hot\""|\""hyponatremia\""| \""decreased\"" \""bowel\"" \""sounds\""|\""cyanosis\""|\""dysarthria\""| \""heat\"" \""intolerance\""|\""hyperglycemia\""|\""reflux\""| \""angle\"" \""closure\"" \""glaucoma\""| \""electrocardiogram\"" \""qt\"" \""prolonged\""| \""vision\"" \""blurred\""| \""blood\"" \""urea\"" \""increased\""|\""dizziness\""|\""arrhythmia\""|\""erythema\""|\""vomiting\""| \""difficulty\"" \""in\"" \""micturition\""|\""infarction\""|\""laryngospasm\""|\""hypoglycaemia\""|\""hypoglycemia\""| \""elevated\"" \""hemoglobin\""| \""skin\"" \""warm\""| \""ventricular\"" \""arrhythmia\""|\""dissociation\""| \""warm\"" \""skin\""| \""follicular\"" \""conjunctivitis\""|\""urticaria\""|\""fatigue\""| \""cardiac\"" \""fibrillation\""| \""decreased\"" \""sweating\""| \""decreased\"" \""visual\"" \""acuity\""|\""lethargy\""| \""acute\"" \""angle\"" \""closure\"" \""glaucoma\""| \""nodal\"" \""rhythm\""|\""borborygmi\""|\""hyperreflexia\""| \""respiratory\"" \""depression\""|\""diarrhea\""|\""leukocytosis\""| \""speech\"" \""disturbance\""|\""ataxia\""|\""cycloplegia\""|\""tachypnoea\""|\""eczema\""| \""supraventricular\"" \""extrasystoles\""|\""ileus\""| \""cardiac\"" \""arrest\""| \""ventricular\"" \""tachycardia\""|\""laryngitis\""|\""delirium\""|\""lactation\""|\""glaucoma\""|\""obstruction\""|\""hypohidrosis\""|\""parity\""|\""palpitations\""| \""temperature\"" \""intolerance\""|\""constipation\""|\""cyclophoria\""| \""acute\"" \""coronary\"" \""syndrome\""| \""arrhythmia\"" \""supraventricular\""|\""arrest\""|\""lesion\""|\""nausea\""| \""sweating\"" \""decreased\""|\""keratitis\""|\""dyskinesia\""| \""pulmonary\"" \""function\"" \""test\"" \""decreased\""|\""stridor\""|\""swelling\""|\""dysphagia\""| \""haemoglobin\"" \""decreased\""|\""diarrhoea\""| \""ileus\"" \""paralytic\""|\""clonus\""|\""insomnia\""| \""electrocardiogram\"" \""qrs\"" \""complex\""| \""nasal\"" \""congestion\""| \""nasal\"" \""dryness\""|\""sweating\""|\""rash\""| \""nodal\"" \""arrhythmia\""|\""irritability\""|\""hyperhidrosis\""| \""ventricular\"" \""failure\"")"");
    CoreMap doc = createDocument(""atropine we need to have many many words here but we don't sweating"");
    MultiPatternMatcher<CoreMap> multiPatternMatcher = TokenSequencePattern.getMultiPatternMatcher(p1, p2);
    List<String> expected = new ArrayList<String>();
    expected.add(""atropine we need to have many many words here but we don't sweating"");
    Iterator<String> expectedIter = expected.iterator();

    Iterable<SequenceMatchResult<CoreMap>> matches =
            multiPatternMatcher.findAllNonOverlappingMatchesPerPattern(doc.get(CoreAnnotations.TokensAnnotation.class));
    for (SequenceMatchResult<CoreMap> match:matches) {
     assertEquals(expectedIter.next(), match.group());
    }
    assertFalse(expectedIter.hasNext());
  }
"
"  @Test
  public void testTokenSequenceMatcherPosNNP() throws IOException {
    CoreMap doc = createDocument(testText1);

    // Test sequence with groups
    TokenSequencePattern p = TokenSequencePattern.compile( ""[ { tag:\""NNP\"" } ]+"");
    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    boolean match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""Mellitus"", m.group());

    p = TokenSequencePattern.compile( ""[ { tag:\""NNP\"" } ] [ /is|was/ ] []*? [ { tag:\""NNP\"" } ]+ "");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""Mellitus was the first Bishop"", m.group());

    TokenSequencePattern nnpPattern = TokenSequencePattern.compile( ""[ { tag:\""NNP\"" } ]"" );
    Env env = TokenSequencePattern.getNewEnv();
    env.bind(""$NNP"", nnpPattern);
    p = TokenSequencePattern.compile(env, "" $NNP [ /is|was/ ] []*? $NNP+ [ \""of\"" ] $NNP+ "");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""Mellitus was the first Bishop of London"", m.group());

    p = TokenSequencePattern.compile(env, "" ($NNP) /is|was/ []*? ($NNP)+ \""of\"" ($NNP)+ "");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    assertEquals(""Mellitus was the first Bishop of London"", m.group());
    assertEquals(""Mellitus"", m.group(1));
    assertEquals(""Bishop"", m.group(2));
    assertEquals(""London"", m.group(3));


    nnpPattern = TokenSequencePattern.compile( "" ( [ { tag:\""NNP\"" } ] )"" );
    env.bind(""$NNP"", nnpPattern);
    p = TokenSequencePattern.compile(env, "" $NNP /is|was/ []*? $NNP+ \""of\"" $NNP+ "");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    assertEquals(""Mellitus was the first Bishop of London"", m.group());
    assertEquals(""Mellitus"", m.group(1));
    assertEquals(""Bishop"", m.group(2));
    assertEquals(""London"", m.group(3));


    // Same as above but without extra ""{}""
    nnpPattern = TokenSequencePattern.compile( "" ( [ tag:\""NNP\"" ] )"" );
    env.bind(""$NNP"", nnpPattern);
    p = TokenSequencePattern.compile(env, "" $NNP /is|was/ []*? $NNP+ \""of\"" $NNP+ "");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    assertEquals(""Mellitus was the first Bishop of London"", m.group());
    assertEquals(""Mellitus"", m.group(1));
    assertEquals(""Bishop"", m.group(2));
    assertEquals(""London"", m.group(3));

    // Same as above but using ""pos""
    nnpPattern = TokenSequencePattern.compile( "" ( [ pos:\""NNP\"" ] )"" );
    env.bind(""$NNP"", nnpPattern);
    p = TokenSequencePattern.compile(env, "" $NNP /is|was/ []*? $NNP+ \""of\"" $NNP+ "");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    assertEquals(""Mellitus was the first Bishop of London"", m.group());
    assertEquals(""Mellitus"", m.group(1));
    assertEquals(""Bishop"", m.group(2));
    assertEquals(""London"", m.group(3));
  }
"
"  @Test
  public void testTokenSequenceMatcherNumber() throws IOException {
    CoreMap doc = createDocument(""It happened on January 3, 2002"");

    // Test sequence with groups
    TokenSequencePattern p = TokenSequencePattern.compile( ""[ { word::IS_NUM } ]+"");
    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    boolean match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""3"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""2002"", m.group());
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile( ""[ { word>=2002 } ]+"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""2002"", m.group());
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile( ""[ { word>2002 } ]+"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertFalse(match);

    // Check no {} with or
    p = TokenSequencePattern.compile( ""[ word > 2002 | word==2002 ]+"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""2002"", m.group());
    match = m.find();
    assertFalse(match);

    // Check no {} with and
    p = TokenSequencePattern.compile( ""[ word>2002 & word==2002 ]+"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile( ""[ { word>2000 } ]+"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""2002"", m.group());
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile( ""[ { word<=2002 } ]+"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""3"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""2002"", m.group());
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile( ""[ { word<2002 } ]+"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""3"", m.group());
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile( ""[ { word==2002 } ]+"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""2002"", m.group());
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile( ""[ { ner:DATE } ]+"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""January 3, 2002"", m.group());
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile( ""[ { ner::NOT_NIL } ]+"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""January 3, 2002"", m.group());
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile( ""[ { ner::IS_NIL } ]+"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""It happened on"", m.group());
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile( ""[ {{ word=~/2002/ }} ]+"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""2002"", m.group());
    match = m.find();
    assertFalse(match);
  }
"
"  @Test
  public void testTokenSequenceMatcherNested() throws IOException {
    CoreMap doc = createDocument(""A A A B B B B B B C C"");

    // Test sequence with groups
    TokenSequencePattern p = TokenSequencePattern.compile( ""( /B/+ )+"");
    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    boolean match = m.find();
    assertTrue(match);
    assertEquals(1, m.groupCount());
    assertEquals(""B B B B B B"", m.group());
    assertEquals(""B B B B B B"", m.group(1));
    match = m.find();
    assertFalse(match);
  }
"
"  @Test
  public void testTokenSequenceMatcherAAs() throws IOException {
    StringBuilder s = new StringBuilder();
 //   Timing timing = new Timing();
    for (int i = 1; i <= 10; i++) {
      s.append(""A "");
      CoreMap doc = createDocument(s.toString());
      TokenSequencePattern p = TokenSequencePattern.compile(""(A?)"" + ""{"" + i + ""} "" + ""A"" + ""{"" + i + ""}"");
//      TokenSequencePattern p = TokenSequencePattern.compile( ""(A?)"" + ""{"" + i + ""}"");
      TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
//      timing.start();
      boolean match = m.matches();
      assertTrue(match);
//      timing.stop(""matched: "" + match + "" "" + i);
    }
  }
"
"  @Test
  public void testTokenSequenceFindsWildcard() throws IOException {
    CoreMap doc = createDocument(""word1 word2"");

    // Test sequence with groups
    TokenSequencePattern p = TokenSequencePattern.compile( ""[]{2}|[]"");
    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    boolean match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""word1 word2"", m.group());
    match = m.find();
    assertFalse(match);

    // Reverse order
    p = TokenSequencePattern.compile( ""[]|[]{2}"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""word1 word2"", m.group());
    match = m.find();
    assertFalse(match);

    // Using {1,2}
    p = TokenSequencePattern.compile( ""[]{2}"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""word1 word2"", m.group());
    match = m.find();
    assertFalse(match);
  }
"
"  @Test
  public void testTokenSequenceMatchesWildcard() throws IOException {
    CoreMap doc = createDocument(""word1 word2"");

    // Test sequence with groups
    TokenSequencePattern p = TokenSequencePattern.compile( ""[]{2}|[]"");
    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    boolean matches = m.matches();
    assertTrue(matches);

    // Reverse order
    p = TokenSequencePattern.compile( ""[]|[]{2}"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    matches = m.matches();
    assertTrue(matches);

    // Using {1,2}
    p = TokenSequencePattern.compile( ""[]{1,2}"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    matches = m.matches();
    assertTrue(matches);
  }
"
"  @Test
  public void testTokenSequenceMatcherABs() throws IOException {
    CoreMap doc = createDocument(""A A A A A A A B A A B A C A E A A A A A A A A A A A B A A A"");

    // Test sequence with groups
    TokenSequencePattern p = TokenSequencePattern.compile( ""/A/+ B"");
    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    boolean match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""A A A A A A A B"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""A A B"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""A A A A A A A A A A A B"", m.group());
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile( ""(/A/+ B)+"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(1, m.groupCount());
    assertEquals(""A A A A A A A B A A B"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(1, m.groupCount());
    assertEquals(""A A A A A A A A A A A B"", m.group());
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile( ""( A+ ( /B/+ )? )*"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""A A A A A A A B A A B A"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""A"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""A A A A A A A A A A A B A A A"", m.group());
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile( ""(/A/+ /B/+ )+"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(1, m.groupCount());
    assertEquals(""A A A A A A A B A A B"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(1, m.groupCount());
    assertEquals(""A A A A A A A A A A A B"", m.group());
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile( ""(/A/+ /C/? /A/* )+"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(1, m.groupCount());
    assertEquals(""A A A A A A A"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(1, m.groupCount());
    assertEquals(""A A"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(1, m.groupCount());
    assertEquals(""A C A"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(1, m.groupCount());
    assertEquals(""A A A A A A A A A A A"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(1, m.groupCount());
    assertEquals(""A A A"", m.group());
    match = m.find();
    assertFalse(match);
  }
"
"  @Test
  public void testTokenSequenceMatcherMultiNodePattern() throws IOException {
    CoreMap doc = createDocument(""blah four-years blah blah four - years"");

    // Test sequence with groups
    CoreMapNodePattern nodePattern  = CoreMapNodePattern.valueOf(""four\\s*-?\\s*years"");
    SequencePattern.MultiNodePatternExpr expr = new SequencePattern.MultiNodePatternExpr(
            new MultiCoreMapNodePattern(nodePattern));
    TokenSequencePattern p = TokenSequencePattern.compile(expr);
    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    boolean match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""four-years"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""four - years"", m.group());
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile(""(?m) /four\\s*-?\\s*years/"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""four-years"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""four - years"", m.group());
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile(""(?m){2,3} /four\\s*-?\\s*years/"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""four - years"", m.group());
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile( ""(?m){1,2} /four\\s*-?\\s*years/"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""four-years"", m.group());
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile(""(?m){1,3} /four\\s*-?\\s*years/ ==> &annotate( { ner=YEAR } )"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""four-years"", m.group());
    p.getAction().apply(m, 0);
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""four - years"", m.group());
    SequenceMatchResult<CoreMap> res = p.getAction().apply(m, 0);
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile( ""[ { ner:YEAR } ]+"");
    m = p.getMatcher(res.elements());
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""four-years"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""four - years"", m.group());
    match = m.find();
    assertFalse(match);
  }
"
"  @Test
  public void testTokenSequenceMatcherMultiNodePattern2() throws IOException {
    CoreMap doc = createDocument(""Replace the lamp with model wss.32dc55c3e945384dbc5e533ab711fd24"");

    // Greedy
    TokenSequencePattern p = TokenSequencePattern.compile(""/model/ ((?m){1,4}/\\w+\\.\\w+/)"");
    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    boolean match = m.find();
    assertTrue(match);
    assertEquals(1, m.groupCount());
    assertEquals(""model wss.32dc55c3e945384dbc5e533ab711fd24"", m.group());
    assertEquals(""wss.32dc55c3e945384dbc5e533ab711fd24"", m.group(1));
    match = m.find();
    assertFalse(match);

    // Reluctant
    p = TokenSequencePattern.compile(""/model/ ((?m){1,4}?/\\w+\\.\\w+/)"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(1, m.groupCount());
    assertEquals(""model wss.32"", m.group());
    assertEquals(""wss.32"", m.group(1));
    match = m.find();
    assertFalse(match);
  }
"
"  @Test
  public void testTokenSequenceMatcherBackRef() throws IOException {
    CoreMap doc = createDocument(""A A A A A A A B A A B A C A E A A A A A A A A A A A B A A A"");

    // Test sequence with groups
    TokenSequencePattern p = TokenSequencePattern.compile( ""(/A/+) B \\1"");
    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    boolean match = m.find();
    assertTrue(match);
    assertEquals(1, m.groupCount());
    assertEquals(""A A B A A"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(1, m.groupCount());
    assertEquals(""A A A B A A A"", m.group());
    match = m.find();
    assertFalse(match);

  }
"
"  @Test
  public void testMultiPatternMatcher() throws IOException {
    CoreMap doc = createDocument(testText1);

    // Test simple sequence
    TokenSequencePattern p1 = TokenSequencePattern.compile(""/Archbishop/ /of/ /Canterbury/"");
    p1.setPriority(1);
    TokenSequencePattern p2 = TokenSequencePattern.compile(""/[a-zA-Z]+/{1,2}  /of/ /[a-zA-Z]+/+"");
    MultiPatternMatcher<CoreMap> m = new MultiPatternMatcher<CoreMap>(p2,p1);
    List<SequenceMatchResult<CoreMap>> matched = m.findNonOverlapping(doc.get(CoreAnnotations.TokensAnnotation.class));
    assertEquals(4, matched.size());
    assertEquals(""first Bishop of London"", matched.get(0).group());
    assertEquals(""Archbishop of Canterbury"", matched.get(1).group());
    assertEquals(""a member of the Gregorian mission sent to England to convert the"", matched.get(2).group());
    assertEquals(""as Bishop of London in"", matched.get(3).group());
  }
"
"  @Test
  public void testStringPatternMatchCaseInsensitive() throws IOException {
    CoreMap doc = createDocument(testText1);

    // Test simple sequence
    Env env = TokenSequencePattern.getNewEnv();
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE);
    TokenSequencePattern p = TokenSequencePattern.compile(env, ""/archbishop/ /of/ /canterbury/"");
    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    assertTrue(m.find());
    assertEquals(""Archbishop of Canterbury"", m.group());
    assertFalse(m.find());

    p = TokenSequencePattern.compile(env, ""/ARCHBISHOP/ /OF/ /CANTERBURY/"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    assertTrue(m.find());
    assertEquals(""Archbishop of Canterbury"", m.group());
    assertFalse(m.find());
  }
"
"  @Test
  public void testStringMatchCaseInsensitive() throws IOException {
    CoreMap doc = createDocument(testText1);

    // Test simple sequence
    Env env = TokenSequencePattern.getNewEnv();
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE);
    TokenSequencePattern p = TokenSequencePattern.compile(env, ""archbishop of canterbury"");
    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    assertTrue(m.find());
    assertEquals(""Archbishop of Canterbury"", m.group());
    assertFalse(m.find());

    p = TokenSequencePattern.compile(env, ""ARCHBISHOP OF CANTERBURY"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    assertTrue(m.find());
    assertEquals(""Archbishop of Canterbury"", m.group());
    assertFalse(m.find());
  }
"
"  @Test
  public void testCompile() {
    String s = ""(?$se \""matching\"" \""this\""|\""don't\"")"";
    CoreMap doc = createDocument(""does this do matching this"");
    TokenSequencePattern p = TokenSequencePattern.compile(s);
    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    boolean match = m.find();
    assertTrue(match);
    //assertEquals(m.group(), ""matching this"");
  }
"
"  @Test
  public void testBindingCompile(){
    Env env = TokenSequencePattern.getNewEnv();
    env.bind(""wordname"",CoreAnnotations.TextAnnotation.class);
    String s = ""[wordname:\""name\""]{1,2}"";
    TokenSequencePattern p = TokenSequencePattern.compile(env, s);
  }
"
"//  @Test
//  public void testNoBindingCompile(){
//    Env env = TokenSequencePattern.getNewEnv();
//    String s = ""["" + CoreAnnotations.TextAnnotation.class.getName()+"":\""name\""]{1,2}"";
//    TokenSequencePattern p = TokenSequencePattern.compile(env, s);
//  }
"
"  @Test
  public void testCaseInsensitive1(){
    Env env = TokenSequencePattern.getNewEnv();
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE);
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE);
    String s = ""for /President/"";
    CoreMap doc = createDocument(""for president"");
    TokenSequencePattern p = TokenSequencePattern.compile(env, s);
    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    boolean match = m.find();
    assertTrue(match);
  }
"
"  @Test
  public void testCaseInsensitive2(){
    Env env = TokenSequencePattern.getNewEnv();
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE);
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE);

    String s = ""for president"";
    CoreMap doc = createDocument(""for President"");

    TokenSequencePattern p = TokenSequencePattern.compile(env, s);
    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    boolean match = m.find();
    assertTrue(match);
  }
"
"  // @Test
  public void testOnlyGloss() {
    List<List<String>> entries = Collections.singletonList(
            Arrays.asList(""124"", ""docid1"", ""1"", ""This is a test document.""));

    TSVSentenceIterator it = new TSVSentenceIterator(entries.iterator(),
            Arrays.asList(SentenceField.ID, SentenceField.DOC_ID, SentenceField.SENTENCE_INDEX, SentenceField.GLOSS));
    Sentence sentence = it.next();
    Assert.assertEquals(1, sentence.sentenceIndex());
    Assert.assertEquals(""This is a test document.""  , sentence.text());
    Assert.assertEquals(""docid1"", sentence.asCoreMap().get(CoreAnnotations.DocIDAnnotation.class));
    Assert.assertEquals(""124"", sentence.asCoreMap().get(CoreAnnotations.SentenceIDAnnotation.class));
  }
"
"  @Test
  public void testFullTokens() {
    List<List<String>> entries = Collections.singletonList(
            Arrays.asList(
                    ""3424"",
                    ""d2-s1-a1"",
                    ""0"",
                    ""{Chess,is,not,a,predominantly,physical,sport,\""\"",\""\"",yet,neither,are,shooting,and,curling,-LRB-,which,\""\"",\""\"",in,fact,\""\"",\""\"",has,been,nicknamed,``,chess,on,ice,'',5,),.}"",
                    ""{chess,be,not,a,predominantly,physical,sport,\""\"",\""\"",yet,neither,be,shooting,and,curling,-lrb-,which,\""\"",\""\"",in,fact,\""\"",\""\"",have,be,nickname,``,chess,on,ice,'',5,),.}"",
                    ""{NN,VBZ,RB,DT,RB,JJ,NN,\""\"",\""\"",RB,DT,VBP,JJ,CC,NN,-LRB-,WDT,\""\"",\""\"",IN,NN,\""\"",\""\"",VBZ,VBN,VBN,``,NN,IN,NN,'',LS,-RRB-,.}"",
                    ""{O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,NUMBER,O,O}"",
                    ""{0,6,9,13,15,29,38,43,45,49,57,61,70,74,82,83,88,90,93,97,99,103,108,118,119,125,128,131,132,133,134}"",
                    ""{5,8,12,14,28,37,43,44,48,56,60,69,73,81,83,88,89,92,97,98,102,107,117,119,124,127,131,132,133,134,135}""	,
                    //""[{\""\""dependent\""\"": 7, \""\""dep\""\"": \""\""ROOT\""\"", \""\""governorGloss\""\"": \""\""ROOT\""\"", \""\""governor\""\"": 0, \""\""dependentGloss\""\"": \""\""sport\""\""}, {\""\""dependent\""\"": 1, \""\""dep\""\"": \""\""nsubj\""\"", \""\""governorGloss\""\"": \""\""sport\""\"", \""\""governor\""\"": 7, \""\""dependentGloss\""\"": \""\""Chess\""\""}, {\""\""dependent\""\"": 2, \""\""dep\""\"": \""\""cop\""\"", \""\""governorGloss\""\"": \""\""sport\""\"", \""\""governor\""\"": 7, \""\""dependentGloss\""\"": \""\""is\""\""}, {\""\""dependent\""\"": 3, \""\""dep\""\"": \""\""neg\""\"", \""\""governorGloss\""\"": \""\""sport\""\"", \""\""governor\""\"": 7, \""\""dependentGloss\""\"": \""\""not\""\""}, {\""\""dependent\""\"": 4, \""\""dep\""\"": \""\""det\""\"", \""\""governorGloss\""\"": \""\""sport\""\"", \""\""governor\""\"": 7, \""\""dependentGloss\""\"": \""\""a\""\""}, {\""\""dependent\""\"": 5, \""\""dep\""\"": \""\""advmod\""\"", \""\""governorGloss\""\"": \""\""physical\""\"", \""\""governor\""\"": 6, \""\""dependentGloss\""\"": \""\""predominantly\""\""}, {\""\""dependent\""\"": 6, \""\""dep\""\"": \""\""amod\""\"", \""\""governorGloss\""\"": \""\""sport\""\"", \""\""governor\""\"": 7, \""\""dependentGloss\""\"": \""\""physical\""\""}, {\""\""dependent\""\"": 9, \""\""dep\""\"": \""\""advmod\""\"", \""\""governorGloss\""\"": \""\""sport\""\"", \""\""governor\""\"": 7, \""\""dependentGloss\""\"": \""\""yet\""\""}, {\""\""dependent\""\"": 10, \""\""dep\""\"": \""\""nsubj\""\"", \""\""governorGloss\""\"": \""\""shooting\""\"", \""\""governor\""\"": 12, \""\""dependentGloss\""\"": \""\""neither\""\""}, {\""\""dependent\""\"": 11, \""\""dep\""\"": \""\""cop\""\"", \""\""governorGloss\""\"": \""\""shooting\""\"", \""\""governor\""\"": 12, \""\""dependentGloss\""\"": \""\""are\""\""}, {\""\""dependent\""\"": 12, \""\""dep\""\"": \""\""parataxis\""\"", \""\""governorGloss\""\"": \""\""sport\""\"", \""\""governor\""\"": 7, \""\""dependentGloss\""\"": \""\""shooting\""\""}, {\""\""dependent\""\"": 13, \""\""dep\""\"": \""\""cc\""\"", \""\""governorGloss\""\"": \""\""shooting\""\"", \""\""governor\""\"": 12, \""\""dependentGloss\""\"": \""\""and\""\""}, {\""\""dependent\""\"": 14, \""\""dep\""\"": \""\""parataxis\""\"", \""\""governorGloss\""\"": \""\""sport\""\"", \""\""governor\""\"": 7, \""\""dependentGloss\""\"": \""\""curling\""\""}, {\""\""dependent\""\"": 14, \""\""dep\""\"": \""\""conj:and\""\"", \""\""governorGloss\""\"": \""\""shooting\""\"", \""\""governor\""\"": 12, \""\""dependentGloss\""\"": \""\""curling\""\""}, {\""\""dependent\""\"": 16, \""\""dep\""\"": \""\""nsubjpass\""\"", \""\""governorGloss\""\"": \""\""nicknamed\""\"", \""\""governor\""\"": 23, \""\""dependentGloss\""\"": \""\""which\""\""}, {\""\""dependent\""\"": 18, \""\""dep\""\"": \""\""case\""\"", \""\""governorGloss\""\"": \""\""fact\""\"", \""\""governor\""\"": 19, \""\""dependentGloss\""\"": \""\""in\""\""}, {\""\""dependent\""\"": 19, \""\""dep\""\"": \""\""nmod:in\""\"", \""\""governorGloss\""\"": \""\""nicknamed\""\"", \""\""governor\""\"": 23, \""\""dependentGloss\""\"": \""\""fact\""\""}, {\""\""dependent\""\"": 21, \""\""dep\""\"": \""\""aux\""\"", \""\""governorGloss\""\"": \""\""nicknamed\""\"", \""\""governor\""\"": 23, \""\""dependentGloss\""\"": \""\""has\""\""}, {\""\""dependent\""\"": 22, \""\""dep\""\"": \""\""auxpass\""\"", \""\""governorGloss\""\"": \""\""nicknamed\""\"", \""\""governor\""\"": 23, \""\""dependentGloss\""\"": \""\""been\""\""}, {\""\""dependent\""\"": 23, \""\""dep\""\"": \""\""dep\""\"", \""\""governorGloss\""\"": \""\""shooting\""\"", \""\""governor\""\"": 12, \""\""dependentGloss\""\"": \""\""nicknamed\""\""}, {\""\""dependent\""\"": 25, \""\""dep\""\"": \""\""dobj\""\"", \""\""governorGloss\""\"": \""\""nicknamed\""\"", \""\""governor\""\"": 23, \""\""dependentGloss\""\"": \""\""chess\""\""}, {\""\""dependent\""\"": 26, \""\""dep\""\"": \""\""case\""\"", \""\""governorGloss\""\"": \""\""ice\""\"", \""\""governor\""\"": 27, \""\""dependentGloss\""\"": \""\""on\""\""}, {\""\""dependent\""\"": 27, \""\""dep\""\"": \""\""nmod:on\""\"", \""\""governorGloss\""\"": \""\""chess\""\"", \""\""governor\""\"": 25, \""\""dependentGloss\""\"": \""\""ice\""\""}, {\""\""dependent\""\"": 29, \""\""dep\""\"": \""\""amod\""\"", \""\""governorGloss\""\"": \""\""chess\""\"", \""\""governor\""\"": 25, \""\""dependentGloss\""\"": \""\""5\""\""}]"",
                    ""Chess is not a predominantly physical sport, yet neither are shooting and curling (which, in fact, has been nicknamed âchess on iceâ5).""
            ));

    TSVSentenceIterator it = new TSVSentenceIterator(entries.iterator(), Arrays.asList(
            SentenceField.ID,
            SentenceField.DOC_ID,
            SentenceField.SENTENCE_INDEX,
            SentenceField.WORDS,
            SentenceField.LEMMAS,
            SentenceField.POS_TAGS,
            SentenceField.NER_TAGS,
            SentenceField.DOC_CHAR_BEGIN,
            SentenceField.DOC_CHAR_END,
            SentenceField.GLOSS
    ));

    Sentence sentence = it.next();
    Assert.assertEquals(""3424"", sentence.sentenceid().orElse(""-1""));
    Assert.assertEquals(""d2-s1-a1"", sentence.document.docid().orElse(""???""));
    Assert.assertEquals(0, sentence.sentenceIndex());
    Assert.assertEquals(""Chess is not a predominantly physical sport, yet neither are shooting and curling (which, in fact, has been nicknamed âchess on iceâ5)."" , sentence.text());
    Assert.assertArrayEquals(new String[]{
            ""Chess"",""is"",""not"",""a"",""predominantly"",""physical"",""sport"","","",""yet"",""neither"",""are"",""shooting"",""and"",""curling"",""-LRB-"",""which"","","",""in"",""fact"","","",""has"",""been"",""nicknamed"",""``"",""chess"",""on"",""ice"",""''"",""5"","")"","".""
    }, sentence.words().toArray());
    Assert.assertArrayEquals(new String[]{
            ""chess"",""be"",""not"",""a"",""predominantly"",""physical"",""sport"","","",""yet"",""neither"",""be"",""shooting"",""and"",""curling"",""-lrb-"",""which"","","",""in"",""fact"","","",""have"",""be"",""nickname"",""``"",""chess"",""on"",""ice"",""''"",""5"","")"","".""
    }, sentence.lemmas().toArray());
    Assert.assertArrayEquals(new String[]{
            ""NN"",""VBZ"",""RB"",""DT"",""RB"",""JJ"",""NN"","","",""RB"",""DT"",""VBP"",""JJ"",""CC"",""NN"",""-LRB-"",""WDT"","","",""IN"",""NN"","","",""VBZ"",""VBN"",""VBN"",""``"",""NN"",""IN"",""NN"",""''"",""LS"",""-RRB-"","".""
    }, sentence.posTags().toArray());
    Assert.assertArrayEquals(new String[]{
            ""O"",""O"",""O"",""O"",""O"",""O"",""O"",""O"",""O"",""O"",""O"",""O"",""O"",""O"",""O"",""O"",""O"",""O"",""O"",""O"",""O"",""O"",""O"",""O"",""O"",""O"",""O"",""O"",""NUMBER"",""O"",""O""
    }, sentence.nerTags().toArray());
    Assert.assertArrayEquals(new Integer[]{
            0,6,9,13,15,29,38,43,45,49,57,61,70,74,82,83,88,90,93,97,99,103,108,118,119,125,128,131,132,133,134
    }, sentence.characterOffsetBegin().toArray());
    Assert.assertArrayEquals(new Integer[]{
            5,8,12,14,28,37,43,44,48,56,60,69,73,81,83,88,89,92,97,98,102,107,117,119,124,127,131,132,133,134,135
    }, sentence.characterOffsetEnd().toArray());
  }
"
"  @Test
  public void testParseArray() {
    String in = ""{Chess,is,not,a,predominantly,physical,sport,\""\"",\""\"",yet,neither,are,shooting,and,curling,-LRB-,which,\""\"",\""\"",in,fact,\""\"",\""\"",has,been,nicknamed,``,chess,on,ice,'',5,-RRB-,.}"";
    String[] out = {""Chess"",""is"",""not"",""a"",""predominantly"",""physical"",""sport"","","",""yet"",""neither"",""are"",""shooting"",""and"",""curling"",""-LRB-"",""which"","","",""in"",""fact"","","",""has"",""been"",""nicknamed"",""``"",""chess"",""on"",""ice"",""''"",""5"",""-RRB-"","".""};

    // System.err.println(in);
    // System.err.println(Arrays.asList(out));
    // System.err.println(Arrays.asList(TSVUtils.parseArray(in).toArray()));
    Assert.assertArrayEquals(out, TSVUtils.parseArray(in).toArray());

    String in2 = ""{Chess,is,not,a,predominantly,physical,sport,\""\"",\""\"",yet,neither,are,shooting,and,curling,(,which,\""\"",\""\"",in,fact,\""\"",\""\"",has,been,nicknamed,``,chess,on,ice,'',5,),.}"";
    String[] out2 = {""Chess"",""is"",""not"",""a"",""predominantly"",""physical"",""sport"","","",""yet"",""neither"",""are"",""shooting"",""and"",""curling"",""("",""which"","","",""in"",""fact"","","",""has"",""been"",""nicknamed"",""``"",""chess"",""on"",""ice"",""''"",""5"","")"","".""};

    Assert.assertArrayEquals(out2, TSVUtils.parseArray(in2).toArray());
  }
"
"  @Test
  public void testParseTrees() {
    List<List<String>> entries = Collections.singletonList(
            Arrays.asList(
                    ""3424"",
                    ""d2-s1-a1"",
                    ""0"",
                    ""{Chess,is,not,a,predominantly,physical,sport,\""\"",\""\"",yet,neither,are,shooting,and,curling,-LRB-,which,\""\"",\""\"",in,fact,\""\"",\""\"",has,been,nicknamed,``,chess,on,ice,'',5,-RRB-,.}"",
                    ""{chess,be,not,a,predominantly,physical,sport,\""\"",\""\"",yet,neither,be,shooting,and,curling,-lrb-,which,\""\"",\""\"",in,fact,\""\"",\""\"",have,be,nickname,``,chess,on,ice,'',5,-rrb-,.}"",
                    ""{NN,VBZ,RB,DT,RB,JJ,NN,\""\"",\""\"",RB,DT,VBP,JJ,CC,NN,-LRB-,WDT,\""\"",\""\"",IN,NN,\""\"",\""\"",VBZ,VBN,VBN,``,NN,IN,NN,'',LS,-RRB-,.}"",
                    ""{O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,NUMBER,O,O}"",
                    ""{0,6,9,13,15,29,38,43,45,49,57,61,70,74,82,83,88,90,93,97,99,103,108,118,119,125,128,131,132,133,134}"",
                    ""{5,8,12,14,28,37,43,44,48,56,60,69,73,81,83,88,89,92,97,98,102,107,117,119,124,127,131,132,133,134,135}""	,
                    ""[{\""\""dependent\""\"": 7, \""\""dep\""\"": \""\""ROOT\""\"", \""\""governorGloss\""\"": \""\""ROOT\""\"", \""\""governor\""\"": 0, \""\""dependentGloss\""\"": \""\""sport\""\""}, {\""\""dependent\""\"": 1, \""\""dep\""\"": \""\""nsubj\""\"", \""\""governorGloss\""\"": \""\""sport\""\"", \""\""governor\""\"": 7, \""\""dependentGloss\""\"": \""\""Chess\""\""}, {\""\""dependent\""\"": 2, \""\""dep\""\"": \""\""cop\""\"", \""\""governorGloss\""\"": \""\""sport\""\"", \""\""governor\""\"": 7, \""\""dependentGloss\""\"": \""\""is\""\""}, {\""\""dependent\""\"": 3, \""\""dep\""\"": \""\""neg\""\"", \""\""governorGloss\""\"": \""\""sport\""\"", \""\""governor\""\"": 7, \""\""dependentGloss\""\"": \""\""not\""\""}, {\""\""dependent\""\"": 4, \""\""dep\""\"": \""\""det\""\"", \""\""governorGloss\""\"": \""\""sport\""\"", \""\""governor\""\"": 7, \""\""dependentGloss\""\"": \""\""a\""\""}, {\""\""dependent\""\"": 5, \""\""dep\""\"": \""\""advmod\""\"", \""\""governorGloss\""\"": \""\""physical\""\"", \""\""governor\""\"": 6, \""\""dependentGloss\""\"": \""\""predominantly\""\""}, {\""\""dependent\""\"": 6, \""\""dep\""\"": \""\""amod\""\"", \""\""governorGloss\""\"": \""\""sport\""\"", \""\""governor\""\"": 7, \""\""dependentGloss\""\"": \""\""physical\""\""}, {\""\""dependent\""\"": 9, \""\""dep\""\"": \""\""advmod\""\"", \""\""governorGloss\""\"": \""\""sport\""\"", \""\""governor\""\"": 7, \""\""dependentGloss\""\"": \""\""yet\""\""}, {\""\""dependent\""\"": 10, \""\""dep\""\"": \""\""nsubj\""\"", \""\""governorGloss\""\"": \""\""shooting\""\"", \""\""governor\""\"": 12, \""\""dependentGloss\""\"": \""\""neither\""\""}, {\""\""dependent\""\"": 11, \""\""dep\""\"": \""\""cop\""\"", \""\""governorGloss\""\"": \""\""shooting\""\"", \""\""governor\""\"": 12, \""\""dependentGloss\""\"": \""\""are\""\""}, {\""\""dependent\""\"": 12, \""\""dep\""\"": \""\""parataxis\""\"", \""\""governorGloss\""\"": \""\""sport\""\"", \""\""governor\""\"": 7, \""\""dependentGloss\""\"": \""\""shooting\""\""}, {\""\""dependent\""\"": 13, \""\""dep\""\"": \""\""cc\""\"", \""\""governorGloss\""\"": \""\""shooting\""\"", \""\""governor\""\"": 12, \""\""dependentGloss\""\"": \""\""and\""\""}, {\""\""dependent\""\"": 14, \""\""dep\""\"": \""\""parataxis\""\"", \""\""governorGloss\""\"": \""\""sport\""\"", \""\""governor\""\"": 7, \""\""dependentGloss\""\"": \""\""curling\""\""}, {\""\""dependent\""\"": 14, \""\""dep\""\"": \""\""conj:and\""\"", \""\""governorGloss\""\"": \""\""shooting\""\"", \""\""governor\""\"": 12, \""\""dependentGloss\""\"": \""\""curling\""\""}, {\""\""dependent\""\"": 16, \""\""dep\""\"": \""\""nsubjpass\""\"", \""\""governorGloss\""\"": \""\""nicknamed\""\"", \""\""governor\""\"": 23, \""\""dependentGloss\""\"": \""\""which\""\""}, {\""\""dependent\""\"": 18, \""\""dep\""\"": \""\""case\""\"", \""\""governorGloss\""\"": \""\""fact\""\"", \""\""governor\""\"": 19, \""\""dependentGloss\""\"": \""\""in\""\""}, {\""\""dependent\""\"": 19, \""\""dep\""\"": \""\""nmod:in\""\"", \""\""governorGloss\""\"": \""\""nicknamed\""\"", \""\""governor\""\"": 23, \""\""dependentGloss\""\"": \""\""fact\""\""}, {\""\""dependent\""\"": 21, \""\""dep\""\"": \""\""aux\""\"", \""\""governorGloss\""\"": \""\""nicknamed\""\"", \""\""governor\""\"": 23, \""\""dependentGloss\""\"": \""\""has\""\""}, {\""\""dependent\""\"": 22, \""\""dep\""\"": \""\""auxpass\""\"", \""\""governorGloss\""\"": \""\""nicknamed\""\"", \""\""governor\""\"": 23, \""\""dependentGloss\""\"": \""\""been\""\""}, {\""\""dependent\""\"": 23, \""\""dep\""\"": \""\""dep\""\"", \""\""governorGloss\""\"": \""\""shooting\""\"", \""\""governor\""\"": 12, \""\""dependentGloss\""\"": \""\""nicknamed\""\""}, {\""\""dependent\""\"": 25, \""\""dep\""\"": \""\""dobj\""\"", \""\""governorGloss\""\"": \""\""nicknamed\""\"", \""\""governor\""\"": 23, \""\""dependentGloss\""\"": \""\""chess\""\""}, {\""\""dependent\""\"": 26, \""\""dep\""\"": \""\""case\""\"", \""\""governorGloss\""\"": \""\""ice\""\"", \""\""governor\""\"": 27, \""\""dependentGloss\""\"": \""\""on\""\""}, {\""\""dependent\""\"": 27, \""\""dep\""\"": \""\""nmod:on\""\"", \""\""governorGloss\""\"": \""\""chess\""\"", \""\""governor\""\"": 25, \""\""dependentGloss\""\"": \""\""ice\""\""}, {\""\""dependent\""\"": 29, \""\""dep\""\"": \""\""amod\""\"", \""\""governorGloss\""\"": \""\""chess\""\"", \""\""governor\""\"": 25, \""\""dependentGloss\""\"": \""\""5\""\""}]"",
                    ""Chess is not a predominantly physical sport, yet neither are shooting and curling (which, in fact, has been nicknamed âchess on iceâ5)."")
    );

    TSVSentenceIterator it = new TSVSentenceIterator(entries.iterator(), Arrays.asList(
            SentenceField.ID,
            SentenceField.DOC_ID,
            SentenceField.SENTENCE_INDEX,
            SentenceField.WORDS,
            SentenceField.LEMMAS,
            SentenceField.POS_TAGS,
            SentenceField.NER_TAGS,
            SentenceField.DOC_CHAR_BEGIN,
            SentenceField.DOC_CHAR_END,
            SentenceField.DEPENDENCIES_BASIC,
            SentenceField.GLOSS
    ));
    Sentence sentence = it.next();
    sentence.dependencyGraph();
    sentence.openieTriples();
  }
"
"  @Test
  public void testMorphaAnnotator() {
    Annotation document = new Annotation(text);
    fullPipeline.annotate(document);
    checkResult(document.get(CoreAnnotations.TokensAnnotation.class));
  }
"
"  @Test
  public void testSentencesAnnotation() {
    List<CoreLabel> words = getTestWords();

    CoreMap sentence = new ArrayCoreMap();
    sentence.set(CoreAnnotations.TokensAnnotation.class, words);
    List<CoreMap> sentences = new ArrayList<>();
    sentences.add(sentence);
    Annotation document = new Annotation(text);
    document.set(CoreAnnotations.SentencesAnnotation.class, sentences);

    shortPipeline.annotate(document);
    checkResult(words);
  }
"
"    @Test
    public void testInvalidOutputter() throws IOException {
        try {
            Annotation ann = new Annotation(""CoNLL-U is neat. Better than XML."");
            pipeline.annotate(ann);
            String actual = new CoNLLUOutputter(""this should fail"").print(ann);
            throw new AssertionError(""This should have failed"");
        } catch (IllegalArgumentException e) {
            // yay
        }
    }
"
"    @Test
    public void testSimpleSentence() throws IOException {
        Annotation ann = new Annotation(""CoNLL-U is neat. Better than XML."");
        pipeline.annotate(ann);
        String actual = new CoNLLUOutputter(""enhanced"").print(ann);
        String expected = ""1\tCoNLL\tconll\tNOUN\tNN\tNumber=Sing\t3\tcompound\t3:compound\t_\n"" +
            ""2\t-\t-\tPUNCT\tHYPH\t_\t3\tpunct\t3:punct\t_\n"" +
            ""3\tU\tu\tNOUN\tNN\tNumber=Sing\t5\tnsubj\t5:nsubj\t_\n"" +
            ""4\tis\tbe\tVERB\tVBZ\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\t5\tcop\t5:cop\t_\n"" +
            ""5\tneat\tneat\tADJ\tJJ\tDegree=Pos\t0\troot\t0:root\t_\n"" +
            ""6\t.\t.\tPUNCT\t.\t_\t5\tpunct\t5:punct\t_\n"" +
            ""\n"" +
            ""1\tBetter\tbetter\tADJ\tJJR\tDegree=Cmp\t0\troot\t0:root\t_\n"" +
            ""2\tthan\tthan\tADP\tIN\t_\t3\tcase\t3:case\t_\n"" +
            ""3\tXML\txml\tNOUN\tNN\tNumber=Sing\t1\tobl\t1:obl:than\t_\n"" +
            ""4\t.\t.\tPUNCT\t.\t_\t1\tpunct\t1:punct\t_\n\n"";
        assertEquals(expected, actual);
    }
"
"  @Test
  public void testChineseSerialization() {

    try {
      AnnotationSerializer serializer = new ProtobufAnnotationSerializer();
      // write Chinese doc
      String sampleChineseDocument = ""å·´æåÂ·å¥¥å·´é©¬æ¯ç¾å½æ»ç»ãä»å¨2008å¹´å½é"";
      Properties chineseProperties = StringUtils.argsToProperties(""-props"",
              ""StanfordCoreNLP-chinese.properties"");
      Annotation doc = new StanfordCoreNLP(chineseProperties).process(sampleChineseDocument);

      // fake having a section in the annotation so the test passes.
      // todo [2017] clean up the status of sections.
      doc.set(CoreAnnotations.SectionsAnnotation.class, new ArrayList<CoreMap>());

      ByteArrayOutputStream ks = new ByteArrayOutputStream();
      serializer.write(doc, ks).close();
      // read
      InputStream kis = new ByteArrayInputStream(ks.toByteArray());
      Pair<Annotation, InputStream> pair = serializer.read(kis);
      pair.second.close();
      Annotation readDoc = pair.first;
      kis.close();
      // check characters are equal
      List<CoreLabel> docChars = doc.get(SegmenterCoreAnnotations.CharactersAnnotation.class);
      List<CoreLabel> readDocChars = doc.get(SegmenterCoreAnnotations.CharactersAnnotation.class);
      assertEquals(docChars.size(),readDocChars.size());
      int numChars = docChars.size();
      int currChar = 0;
      while (currChar < numChars) {
        assertEquals(docChars.get(currChar),readDocChars.get(currChar));
        currChar++;
      }
      // check that sentences are equal
      /*int sentenceCount = 0;
      while (sentenceCount < doc.get(CoreAnnotations.SentencesAnnotation.class).size()) {
        assertEquals(doc.get(CoreAnnotations.SentencesAnnotation.class).get(sentenceCount),
                readDoc.get(CoreAnnotations.SentencesAnnotation.class).get(sentenceCount));
        sentenceCount++;
      }*/
      // check JSON output is same
      String docJSON = JSONOutputter.jsonPrint(doc);
      String readDocJSON = JSONOutputter.jsonPrint(readDoc);
      assertEquals(docJSON,readDocJSON);
    } catch (Exception e) { throw new RuntimeException(e); }
  }
"
"  @Test
  public void testTokensRegexSyntax() throws Exception {
    String[][] regexes =
      new String[][]{
        new String[]{""( /University/ /of/ [ {ner:LOCATION} ] )"", ""SCHOOL""}
        // TODO: TokensRegex literal string patterns ignores ignoreCase settings
        //new String[]{""( University of [ {ner:LOCATION} ] )"", ""SCHOOL""}
    };
    Annotator annotatorCased = getTokensRegexNerAnnotator(regexes, false);

    String str = ""University of Alaska is located in Alaska."";
    Annotation document = createDocument(str);
    annotatorCased.annotate(document);
    List<CoreLabel> tokens = document.get(CoreAnnotations.TokensAnnotation.class);

    checkNerTags(tokens,
      ""ORGANIZATION"", ""ORGANIZATION"", ""ORGANIZATION"", ""O"", ""O"", ""O"", ""LOCATION"", ""O"");

    reannotate(tokens, CoreAnnotations.NamedEntityTagAnnotation.class,
            ""O"", ""O"", ""LOCATION"", ""O"", ""O"", ""O"", ""LOCATION"", ""O"");
    annotatorCased.annotate(document);

    checkNerTags(tokens,
      ""SCHOOL"", ""SCHOOL"", ""SCHOOL"", ""O"", ""O"", ""O"", ""LOCATION"", ""O"");

    // Try lowercase
    Annotator annotatorCaseless = getTokensRegexNerAnnotator(regexes, true);

    str = ""university of alaska is located in alaska."";
    document = createDocument(str);
    tokens = document.get(CoreAnnotations.TokensAnnotation.class);
    checkNerTags(tokens,
      ""O"", ""O"", ""LOCATION"", ""O"", ""O"", ""O"", ""LOCATION"", ""O"");
    annotatorCased.annotate(document);
    checkNerTags(tokens,
      ""O"", ""O"", ""LOCATION"", ""O"", ""O"", ""O"", ""LOCATION"", ""O"");
    annotatorCaseless.annotate(document);
    checkNerTags(tokens,
      ""SCHOOL"", ""SCHOOL"", ""SCHOOL"", ""O"", ""O"", ""O"", ""LOCATION"", ""O"");
  }
"
"  @Test
  public void testTokensRegexMatchGroup() throws Exception {
    String[][] regexes =
      new String[][]{
        new String[]{""( /the/? /movie/ (/[A-Z].*/+) )"", ""MOVIE"", """", ""0"", ""1""}
      };
    Annotator annotatorCased = getTokensRegexNerAnnotator(regexes, false);

    String str = ""the movie Mud was very muddy"";
    Annotation document = createDocument(str);
    annotatorCased.annotate(document);
    List<CoreLabel> tokens = document.get(CoreAnnotations.TokensAnnotation.class);

    checkNerTags(tokens,
      ""O"", ""O"", ""MOVIE"", ""O"", ""O"", ""O"");

  }
"
"  @Test
  public void testTokensRegexNormalizedAnnotate() throws Exception {
    Properties props = new Properties();
    props.setProperty(REGEX_ANNOTATOR_NAME + "".mapping.header"", ""pattern,ner,normalized,overwrite,priority,group"");

    String[][] regexes =
      new String[][]{
        new String[]{""blue"",  ""COLOR"", ""B"", """", ""0""},
        new String[]{""red"",   ""COLOR"", ""R"", """", ""0""},
        new String[]{""green"", ""COLOR"", ""G"", """", ""0""}
      };
    Annotator annotatorCased = getTokensRegexNerAnnotator(props, regexes, false);

    String str = ""These are all colors: blue, red, and green."";
    Annotation document = createDocument(str);
    annotatorCased.annotate(document);
    List<CoreLabel> tokens = document.get(CoreAnnotations.TokensAnnotation.class);

    checkTags(tokens, CoreAnnotations.TextAnnotation.class, ""These"", ""are"", ""all"", ""colors"", "":"", ""blue"", "","", ""red"", "","", ""and"", ""green"", ""."");
    checkTags(tokens, CoreAnnotations.NamedEntityTagAnnotation.class,  ""O"", ""O"", ""O"", ""O"", ""O"", ""COLOR"", ""O"", ""COLOR"", ""O"", ""O"", ""COLOR"", ""O"");
    checkTags(tokens, CoreAnnotations.NormalizedNamedEntityTagAnnotation.class,  null, null, null, null, null, ""B"", null, ""R"", null, null, ""G"", null);
  }
"
"  @Test
  public void testTokensRegexCustomAnnotate() throws Exception {

    Properties props = new Properties();
    props.setProperty(REGEX_ANNOTATOR_NAME + "".mapping.header"", ""pattern,test,overwrite,priority,group"");
    props.setProperty(REGEX_ANNOTATOR_NAME + "".mapping.field.test"", ""edu.stanford.nlp.pipeline.TokensRegexNERAnnotatorITest$TestAnnotation"");
    String[][] regexes =
      new String[][]{
        new String[]{""test"", ""TEST"", """", ""0""}
      };
    Annotator annotatorCased = getTokensRegexNerAnnotator(props, regexes, true);

    String str = ""Marking all test as test"";
    Annotation document = createDocument(str);
    annotatorCased.annotate(document);
    List<CoreLabel> tokens = document.get(CoreAnnotations.TokensAnnotation.class);

    checkTags(tokens, CoreAnnotations.TextAnnotation.class, ""Marking"", ""all"", ""test"", ""as"", ""test"");
    checkTags(tokens, TestAnnotation.class, null, null, ""TEST"", null, ""TEST"");
  }
"
"  @Test
  public void testBasicMatching() {
    String str = ""President Barack Obama lives in Chicago , Illinois , "" +
    ""and is a practicing Christian ."";
    Annotation document = createDocument(str);
    annotator.annotate(document);
    List<CoreLabel> tokens = document.get(CoreAnnotations.TokensAnnotation.class);

    checkNerTags(tokens,
      ""TITLE"", ""PERSON"", ""PERSON"", ""O"", ""O"", ""LOCATION"", ""O"", ""STATE_OR_PROVINCE"",
      ""O"", ""O"", ""O"", ""O"", ""O"", ""IDEOLOGY"", ""O"");

  }
"
"  @Test
  public void testOverwrite() {
    String str = ""I like Ontario Bank and Ontario Lake , and I like the Native American Church , too ."";
    Annotation document = createDocument(str);
    annotator.annotate(document);
    List<CoreLabel> tokens = document.get(CoreAnnotations.TokensAnnotation.class);

    checkNerTags(tokens, ""O"", ""O"", ""ORGANIZATION"", ""ORGANIZATION"", ""O"", ""STATE_OR_PROVINCE"", ""LOCATION"", ""O"", ""O"", ""O"", ""O"", ""O"", ""RELIGION"",
      ""RELIGION"", ""RELIGION"", ""O"", ""O"", ""O"");

  }
"
"  @Test
  public void testPriority() {
    String str = ""Christianity is of higher regex priority than Early Christianity . "";
    Annotation document = createDocument(str);
    annotator.annotate(document);
    List<CoreLabel> tokens = document.get(CoreAnnotations.TokensAnnotation.class);
    checkNerTags(tokens, ""RELIGION"", ""O"", ""O"", ""O"", ""O"", ""O"", ""O"", ""O"", ""RELIGION"", ""O"");
  }
"
"  @Test
  public void testEmptyAnnotation() {
    try {
      annotator.annotate(new Annotation(""""));
    } catch(RuntimeException e) {
      return;
    }
    Assert.fail(""Never expected to get this far... the annotator should have thrown an exception by now"");
  }
"
"  @Test
  public void testQuantifiableEntityNormalizingAnnotator() {
    Annotation document = new Annotation(text);
    pipeline.annotate(document);

    int i = 0;
    for (CoreMap sentence: document.get(CoreAnnotations.SentencesAnnotation.class)) {
        List<CoreLabel> tokens = sentence.get(CoreAnnotations.TokensAnnotation.class);
        for (CoreLabel token : tokens) {
          System.out.println(token.get(CoreAnnotations.TextAnnotation.class) + "": "" + token.get(CoreAnnotations.NamedEntityTagAnnotation.class) + "", "" + token.get(CoreAnnotations.NormalizedNamedEntityTagAnnotation.class));
        }
      for (CoreLabel token : tokens) {
        String normalization = token.get(CoreAnnotations.NormalizedNamedEntityTagAnnotation.class);
        if (normalization != null) {
          Assert.assertEquals(answer_text[i], token.get(CoreAnnotations.OriginalTextAnnotation.class));
          Assert.assertEquals(answer_time[i], normalization);
          i++;
        }
      }
    }
    Assert.assertEquals(answer_text.length, i);
    Assert.assertEquals(answer_time.length, i);
  }
"
"  @Test
  public void testDefaultPipeline() {
    testAnnotatorSequence(Arrays.asList(""tokenize"", ""ssplit"", ""pos"", ""lemma"", ""ner"", ""gender"", ""parse"", ""coref""));
  }
"
"  @Test
  public void testDepparsePipeline() {
    testAnnotatorSequence(Arrays.asList(""tokenize"", ""ssplit"", ""pos"", ""depparse""));
  }
"
"  @Test
  public void testQuotePipeline() {
    testAnnotatorSequence(Arrays.asList(""tokenize"",""ssplit"",""pos"",""lemma"",""ner"",""depparse"",""coref"",""quote""));
  }
"
"   @Test
   public void testTrueCasePipeline() {
     testAnnotatorSequence(Arrays.asList(""tokenize"",""ssplit"",""pos"",""lemma"",""truecase""));
   }
"
"  @Test
  public void testOpenIEPipeline() {
    testAnnotatorSequence(Arrays.asList(""tokenize"",""ssplit"",""pos"",""lemma"",""depparse"",""natlog"",""openie""));
  }
"
"  @Test
  public void testMentionRegression() {
    testAnnotatorSequence(Arrays.asList());
  }
"
"  @Test
  public void testPipeline() throws Exception {
    // create pipeline
    AnnotationPipeline pipeline = new AnnotationPipeline();
    pipeline.addAnnotator(new TokenizerAnnotator(false, ""en""));
    pipeline.addAnnotator(new WordsToSentencesAnnotator(false));
    pipeline.addAnnotator(new POSTaggerAnnotator(false));
    pipeline.addAnnotator(new MorphaAnnotator(false));
    pipeline.addAnnotator(new NERCombinerAnnotator(false));
    pipeline.addAnnotator(new ParserAnnotator(false, -1));
    //pipeline.addAnnotator(new CorefAnnotator(null, null, null, false));
    //pipeline.addAnnotator(new SRLAnnotator(false));

    // create annotation with text
    String text = ""Dan Ramage is working for\nMicrosoft. He's in Seattle! \n"";
    Annotation document = new Annotation(text);
    Assert.assertEquals(text, document.toString());
    Assert.assertEquals(text, document.get(CoreAnnotations.TextAnnotation.class));

    // annotate text with pipeline
    pipeline.annotate(document);

    // demonstrate typical usage
    for (CoreMap sentence: document.get(CoreAnnotations.SentencesAnnotation.class)) {

      // get the tree for the sentence
      Tree tree = sentence.get(TreeCoreAnnotations.TreeAnnotation.class);

      // get the tokens for the sentence and iterate over them
      for (CoreLabel token: sentence.get(CoreAnnotations.TokensAnnotation.class)) {

        // get token attributes
        String tokenText = token.get(CoreAnnotations.TextAnnotation.class);
        String tokenPOS = token.get(CoreAnnotations.PartOfSpeechAnnotation.class);
        String tokenLemma = token.get(CoreAnnotations.LemmaAnnotation.class);
        String tokenNE = token.get(CoreAnnotations.NamedEntityTagAnnotation.class);

        // text, pos, lemma and name entity tag should be defined
        Assert.assertNotNull(tokenText);
        Assert.assertNotNull(tokenPOS);
        Assert.assertNotNull(tokenLemma);
        Assert.assertNotNull(tokenNE);
      }
      // tree should be defined
      Assert.assertNotNull(tree);
    }

    // get tokens
    List<CoreLabel> tokens = document.get(CoreAnnotations.TokensAnnotation.class);
    String tokensText = ""Dan Ramage is working for Microsoft . He 's in Seattle !"";
    Assert.assertNotNull(tokens);
    Assert.assertEquals(12, tokens.size());
    Assert.assertEquals(tokensText, join(tokens));
    Assert.assertEquals(0, (int)tokens.get(0).get(CoreAnnotations.CharacterOffsetBeginAnnotation.class));
    Assert.assertEquals(3, (int)tokens.get(0).get(CoreAnnotations.CharacterOffsetEndAnnotation.class));
    Assert.assertEquals(""NNP"", tokens.get(0).get(CoreAnnotations.PartOfSpeechAnnotation.class));
    Assert.assertEquals(""VBZ"", tokens.get(2).get(CoreAnnotations.PartOfSpeechAnnotation.class));
    Assert.assertEquals(""."", tokens.get(11).get(CoreAnnotations.PartOfSpeechAnnotation.class));
    Assert.assertEquals(""Ramage"", tokens.get(1).get(CoreAnnotations.LemmaAnnotation.class));
    Assert.assertEquals(""be"", tokens.get(2).get(CoreAnnotations.LemmaAnnotation.class));
    Assert.assertEquals(""PERSON"", tokens.get(0).get(CoreAnnotations.NamedEntityTagAnnotation.class));
    Assert.assertEquals(""PERSON"", tokens.get(1).get(CoreAnnotations.NamedEntityTagAnnotation.class));
    Assert.assertEquals(""CITY"", tokens.get(10).get(CoreAnnotations.NamedEntityTagAnnotation.class));

    // get sentences
    List<CoreMap> sentences = document.get(CoreAnnotations.SentencesAnnotation.class);
    Assert.assertNotNull(sentences);
    Assert.assertEquals(2, sentences.size());

    // sentence 1
    String text1 = ""Dan Ramage is working for\nMicrosoft."";
    CoreMap sentence1 = sentences.get(0);
    Assert.assertEquals(text1, sentence1.toString());
    Assert.assertEquals(text1, sentence1.get(CoreAnnotations.TextAnnotation.class));
    Assert.assertEquals(0, (int)sentence1.get(CoreAnnotations.CharacterOffsetBeginAnnotation.class));
    Assert.assertEquals(36, (int)sentence1.get(CoreAnnotations.CharacterOffsetEndAnnotation.class));
    Assert.assertEquals(0, (int)sentence1.get(CoreAnnotations.TokenBeginAnnotation.class));
    Assert.assertEquals(7, (int)sentence1.get(CoreAnnotations.TokenEndAnnotation.class));

    // sentence 1 tree
    Tree tree1 = Tree.valueOf(""(ROOT (S (NP (NNP Dan) (NNP Ramage)) (VP (VBZ is) "" +
        ""(VP (VBG working) (PP (IN for) (NP (NNP Microsoft))))) (. .)))"");
    Assert.assertEquals(tree1, sentence1.get(TreeCoreAnnotations.TreeAnnotation.class));

    // sentence 1 tokens
    String tokenText1 = ""Dan Ramage is working for Microsoft ."";
    List<CoreLabel> tokens1 = sentence1.get(CoreAnnotations.TokensAnnotation.class);
    Assert.assertNotNull(tokens1);
    Assert.assertEquals(7, tokens1.size());
    Assert.assertEquals(tokenText1, join(tokens1));
    Assert.assertEquals(4, (int)tokens1.get(1).get(CoreAnnotations.CharacterOffsetBeginAnnotation.class));
    Assert.assertEquals(10, (int)tokens1.get(1).get(CoreAnnotations.CharacterOffsetEndAnnotation.class));
    Assert.assertEquals(""IN"", tokens1.get(4).get(CoreAnnotations.PartOfSpeechAnnotation.class));
    Assert.assertEquals(""NNP"", tokens1.get(5).get(CoreAnnotations.PartOfSpeechAnnotation.class));
    Assert.assertEquals(""work"", tokens1.get(3).get(CoreAnnotations.LemmaAnnotation.class));
    Assert.assertEquals(""."", tokens1.get(6).get(CoreAnnotations.LemmaAnnotation.class));
    Assert.assertEquals(""ORGANIZATION"", tokens1.get(5).get(CoreAnnotations.NamedEntityTagAnnotation.class));

    // sentence 2
    String text2 = ""He's in Seattle!"";
    CoreMap sentence2 = sentences.get(1);
    Assert.assertEquals(text2, sentence2.toString());
    Assert.assertEquals(text2, sentence2.get(CoreAnnotations.TextAnnotation.class));
    Assert.assertEquals(37, (int)sentence2.get(CoreAnnotations.CharacterOffsetBeginAnnotation.class));
    Assert.assertEquals(53, (int)sentence2.get(CoreAnnotations.CharacterOffsetEndAnnotation.class));
    Assert.assertEquals(7, (int)sentence2.get(CoreAnnotations.TokenBeginAnnotation.class));
    Assert.assertEquals(12, (int)sentence2.get(CoreAnnotations.TokenEndAnnotation.class));

    // sentence 2 tree (note error on Seattle, caused by part of speech tagger)
    Tree tree2 = Tree.valueOf(""(ROOT (S (NP (PRP He)) (VP (VBZ 's) (PP (IN in) "" +
        ""(NP (NNP Seattle)))) (. !)))"");
    Assert.assertEquals(tree2, sentence2.get(TreeCoreAnnotations.TreeAnnotation.class));

    // sentence 2 tokens
    String tokenText2 = ""He 's in Seattle !"";
    List<CoreLabel> tokens2 = sentence2.get(CoreAnnotations.TokensAnnotation.class);
    Assert.assertNotNull(tokens2);
    Assert.assertEquals(5, tokens2.size());
    Assert.assertEquals(tokenText2, join(tokens2));
    Assert.assertEquals(39, (int)tokens2.get(1).get(CoreAnnotations.CharacterOffsetBeginAnnotation.class));
    Assert.assertEquals(41, (int)tokens2.get(1).get(CoreAnnotations.CharacterOffsetEndAnnotation.class));
    Assert.assertEquals(""VBZ"", tokens2.get(1).get(CoreAnnotations.PartOfSpeechAnnotation.class));
    Assert.assertEquals(""be"", tokens2.get(1).get(CoreAnnotations.LemmaAnnotation.class));
    Assert.assertEquals(""CITY"", tokens2.get(3).get(CoreAnnotations.NamedEntityTagAnnotation.class));
  }
"
"  @Test
  public void buildFrenchPipeline() {
    // expected output
    List<String> expectedTokens = Arrays.asList(""Emmanuel"", ""Macron"", ""est"", ""le"", ""prÃ©sident"", ""de"", ""la"", ""France"", ""."");
    List<String> expectedTags = Arrays.asList(""PROPN"", ""PROPN"", ""AUX"", ""DET"", ""NOUN"", ""ADP"", ""DET"", ""PROPN"", ""PUNCT"");
    List<String> expectedNER = Arrays.asList(""I-PER"", ""I-PER"", ""O"", ""O"", ""O"", ""O"", ""I-LOC"", ""I-LOC"", ""O"");
    String expectedDependencyParse = ""root(ROOT-0, prÃ©sident-5)\n"" +
        ""nsubj(prÃ©sident-5, Emmanuel-1)\n"" +
        ""flat:name(Emmanuel-1, Macron-2)\n"" +
        ""cop(prÃ©sident-5, est-3)\n"" +
        ""det(prÃ©sident-5, le-4)\n"" +
        ""case(France-8, de-6)\n"" +
        ""det(France-8, la-7)\n"" +
        ""nmod:de(prÃ©sident-5, France-8)\n"" +
        ""punct(prÃ©sident-5, .-9)\n"";
    // build doc
    CoreDocument doc = new CoreDocument(""Emmanuel Macron est le prÃ©sident de la France."");
    // build pipeline with language name
    StanfordCoreNLP frenchPipeline = new StanfordCoreNLP(""french"");
    // annotate
    frenchPipeline.annotate(doc);
    // compare results
    assertEquals(expectedTokens, doc.tokens().stream().map(w -> w.word()).collect(Collectors.toList()));
    assertEquals(expectedTags, doc.tokens().stream().map(w -> w.tag()).collect(Collectors.toList()));
    assertEquals(expectedNER, doc.tokens().stream().map(w -> w.ner()).collect(Collectors.toList()));
    assertEquals(expectedDependencyParse, doc.sentences().get(0).dependencyParse().toList());
  }
"
"  @Test
  public void testCoreQuote() {
    // make the core document
    CoreDocument testDoc = new CoreDocument(testDocText);
    // annotate
    pipeline.annotate(testDoc);
    // test canonical entity mention is correct
    // ""Joe Smith"" should be first entity mention
    CoreMap canonicalEntityMention =
        testDoc.annotation().get(CoreAnnotations.MentionsAnnotation.class).get(1);
    // test canonical mention is correct
    assertEquals(""Joe Smith"", canonicalEntityMention.get(CoreAnnotations.TextAnnotation.class));
    assertEquals(14,
        canonicalEntityMention.get(CoreAnnotations.CharacterOffsetBeginAnnotation.class).intValue());
    assertEquals(23,
        canonicalEntityMention.get(CoreAnnotations.CharacterOffsetEndAnnotation.class).intValue());
    // test the CoreQuote has the correct entity mention for the canonical speaker
    assertEquals(canonicalEntityMention, testDoc.quotes().get(0).canonicalSpeakerEntityMention().get().coreMap());
  }
"
"  @Test
  public void testSimple() throws IOException {
    Annotation annotation = new Annotation(""This is a test"");
    fullPipeline.annotate(annotation);
    runTest(annotation);
  }
"
"  @Test
  public void testCollapsedGraphs() throws IOException {
    Annotation annotation = new Annotation(""I bought a bone for my dog."");
    fullPipeline.annotate(annotation);
    runTest(annotation);
  }
"
"  @Test
  public void testTwoSentences() throws IOException {
    Annotation annotation = new Annotation(""I bought a bone for my dog.  He chews it every day."");
    fullPipeline.annotate(annotation);
    runTest(annotation);
  }
"
"  @Test
  public void testCopyWordGraphs() throws IOException {
    Annotation annotation = new Annotation(""I went over the river and through the woods"");
    fullPipeline.annotate(annotation);
    runTest(annotation);
  }
"
"  @Test
  public void testLive() {
    String result = IOUtils.slurpURLNoExceptions(""http://localhost:"" + port + ""/live"");
    Assert.assertNotNull(result);
    Assert.assertEquals(""live"", result.trim());
  }
"
"  @Test
  public void testReady() {
    String result = IOUtils.slurpURLNoExceptions(""http://localhost:"" + port + ""/ready"");
    Assert.assertNotNull(result);
    Assert.assertEquals(""ready"", result.trim());
  }
"
"  @Test
  public void testClient() {
    String query = ""The dog ate a fish"";
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,parse"");
    StanfordCoreNLPClient client = new StanfordCoreNLPClient(props, ""http://localhost"", port);
    // if something goes wrong, we don't want the unittest waiting forever for a response
    client.setTimeoutMilliseconds(30 * 1000);
    Annotation annotation = client.process(query);
    Throwable t = annotation.get(CoreAnnotations.ExceptionAnnotation.class);
    Assert.assertNull(t);

    List<CoreMap> sentences = annotation.get(CoreAnnotations.SentencesAnnotation.class);
    Assert.assertEquals(1, sentences.size());
  }
"
"  @Test
  public void testClientFailure() {
    String query = ""The dog ate a fish"";
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,parse"");
    //StanfordCoreNLPClient client = new StanfordCoreNLPClient(props, ""http://localhost"", port);
    StanfordCoreNLPClient client = new StanfordCoreNLPClient(props, ""localhost"", port);
    client.setTimeoutMilliseconds(1000);
    Annotation annotation = client.process(query);
    Throwable t = annotation.get(CoreAnnotations.ExceptionAnnotation.class);
    Assert.assertNotNull(t);
  }
"
"  @Test
  public void testTregexJson() throws IOException {

    String expected=""{\""sentences\"":[{\""0\"":{\""sentIndex\"":0,\""characterOffsetBegin\"":4,\""characterOffsetEnd\"":7,\""match\"":\""(NNdog)\\n\"",\""spanString\"":\""dog\"",\""namedNodes\"":[]},\""1\"":{\""sentIndex\"":0,\""characterOffsetBegin\"":14,\""characterOffsetEnd\"":18,\""match\"":\""(NNfish)\\n\"",\""spanString\"":\""fish\"",\""namedNodes\"":[]}}]}"".replaceAll("" "", """");

    String query = ""The dog ate a fish"";
    byte[] message = query.getBytes(""utf-8"");
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,parse"");
    String queryParams = String.format(""pattern=NN&properties=%s"",
                                       URLEncoder.encode(PropertiesUtils.propsAsJsonString(props), ""utf-8""));
    URL serverURL = new URL(""http"", ""localhost"", port, ""/tregex?"" + queryParams);
    String response = slurpURL(serverURL, message);

    Assert.assertEquals(expected, response.replaceAll("" "", """").replaceAll(""\n"", """"));
  }
"
"  @Test
  public void testSemgrexJson() throws IOException {
    String expected=""{ \""sentences\"": [ { \""0\"": { \""text\"": \""ate\"", \""begin\"": 2, \""end\"": 3, \""$obj\"": { \""text\"": \""fish\"", \""begin\"": 4, \""end\"": 5 }, \""$verb\"": { \""text\"": \""ate\"", \""begin\"": 2, \""end\"": 3 } }, \""length\"": 1 }  ]}"".replaceAll("" "", """");

    String query = ""The dog ate a fish"";
    byte[] message = query.getBytes(""utf-8"");
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,parse"");
    String queryParams = String.format(""pattern=%s&properties=%s"",
                                       URLEncoder.encode(""{}=verb >obj {}=obj"", ""utf-8""),
                                       URLEncoder.encode(PropertiesUtils.propsAsJsonString(props), ""utf-8""));
    URL serverURL = new URL(""http"", ""localhost"", port, ""/semgrex?"" + queryParams);
    String response = slurpURL(serverURL, message);

    Assert.assertEquals(expected, response.replaceAll("" "", """").replaceAll(""\n"", """"));
  }
"
"  @Test
  public void testSemgrexAnnotation() throws IOException {
    String expected = ""result { result { match { matchIndex: 3 node { name: \""obj\"" matchIndex: 5 } node { name: \""verb\"" matchIndex: 3 } } }}"".replaceAll("" "", """");
    String query = ""The dog ate a fish"";
    byte[] message = query.getBytes(""utf-8"");
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,parse"");
    String queryParams = String.format(""pattern=%s&properties=%s&outputFormat=serialized"",
                                       URLEncoder.encode(""{}=verb >obj {}=obj"", ""utf-8""),
                                       URLEncoder.encode(PropertiesUtils.propsAsJsonString(props), ""utf-8""));
    URL serverURL = new URL(""http"", ""localhost"", port, ""/semgrex?"" + queryParams);
    InputStream is = postURL(serverURL, message);
    CoreNLPProtos.SemgrexResponse response = CoreNLPProtos.SemgrexResponse.parseFrom(is);

    Assert.assertEquals(expected, response.toString().replaceAll("" "", """").replaceAll(""\n"", """"));
  }
"
"  @Test
  public void testSemgrexFilter() throws IOException {
    String expected=""{ \""sentences\"": [ true, false ]}"".replaceAll("" "", """");

    String query = ""The dog ate a fish.  He went home."";
    byte[] message = query.getBytes(""utf-8"");
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,parse"");
    String queryParams = String.format(""pattern=%s&properties=%s&filter=true"",
                                       URLEncoder.encode(""{}=verb >obj {}=obj"", ""utf-8""),
                                       URLEncoder.encode(PropertiesUtils.propsAsJsonString(props), ""utf-8""));
    URL serverURL = new URL(""http"", ""localhost"", port, ""/semgrex?"" + queryParams);
    String response = slurpURL(serverURL, message);

    Assert.assertEquals(expected, response.replaceAll("" "", """").replaceAll(""\n"", """"));
  }
"
"  @Test
  public void testBasicExample() throws ClassNotFoundException, IOException {
    // set up document
    CoreDocument sampleDocument = new CoreDocument(sampleText);
    // annotate
    pipeline.annotate(sampleDocument);
    // serialize
    ByteArrayOutputStream ks = new ByteArrayOutputStream();
    serializer.writeCoreDocument(sampleDocument, ks).close();
    // Read
    InputStream kis = new ByteArrayInputStream(ks.toByteArray());
    Pair<Annotation, InputStream> pair = serializer.read(kis);
    pair.second.close();
    Annotation readAnnotation = pair.first;
    kis.close();
    ProtobufAnnotationSerializerSlowITest.sameAsRead(sampleDocument.annotation(), readAnnotation);
  }
"
"  @Test
  public void testMentions() {
    testAnnotators(""tokenize,ssplit,pos,lemma,ner,entitymentions"");
  }
"
"  @Test
  public void testSentiment() {
    testAnnotators(""tokenize,ssplit,pos,parse,sentiment"");
  }
"
"  @Test
  public void testOpenie() {
    testAnnotators(""tokenize,ssplit,pos,lemma,depparse,natlog,openie"");
  }
"
"  @Test
  public void testQuote() {
    testAnnotators(""quote"");
    testAnnotators(""tokenize,quote"");
    testAnnotators(""tokenize,ssplit,quote"");
    testAnnotators(""tokenize,ssplit,quote"");
    testAnnotators(""tokenize,ssplit,pos,lemma,ner,depparse,coref,quote"");
  }
"
"  @Test
  public void testGetPossibleAnnotators() {
    assertNotNull(possibleAnnotators());
    assertNotEquals(0, possibleAnnotators().length);
  }
"
"  @Test
  public void testSave() throws IOException {
    ByteArrayOutputStream os = new ByteArrayOutputStream();
    new ProtobufAnnotationSerializer().write(mkAnnotation(), os).close();
    String json = new String(os.toByteArray(), ""UTF-8"").trim();
    assertNotNull(json);
  }
"
"  @Test
  public void testSaveLarge() throws IOException {
    ByteArrayOutputStream os = new ByteArrayOutputStream();
    new ProtobufAnnotationSerializer().write(mkLargeAnnotation(), os).close();
    String json = new String(os.toByteArray(), ""UTF-8"");
    assertNotNull(json);
  }
"
"  @Test
  public void testSaveSize() throws IOException {
    // Annotate
    ByteArrayOutputStream os = new ByteArrayOutputStream();
    ByteArrayOutputStream compressedImpl = new ByteArrayOutputStream();
    GZIPOutputStream compressed = new GZIPOutputStream(compressedImpl);
    new ProtobufAnnotationSerializer().write(mkLargeAnnotation(), os).close();
    new ProtobufAnnotationSerializer().write(mkLargeAnnotation(), compressed).close();
    byte[] uncompressedProto = os.toByteArray();
    byte[] compressedProto = compressedImpl.toByteArray();
    assertNotNull(uncompressedProto);
    assertNotNull(compressedProto);

    // Check size
    assertTrue(""Length too long: "" + compressedProto.length, compressedProto.length < 460000);
    assertTrue(""Length too long: "" + uncompressedProto.length, uncompressedProto.length < 2800000);
  }
"
"  @Test
  public void testCanWriteRead() {
    try {
      AnnotationSerializer serializer = new ProtobufAnnotationSerializer();
      // Write
      StanfordCoreNLP pipe = new StanfordCoreNLP(new Properties());
      Annotation doc = pipe.process(prideAndPrejudiceFirstBit);
      ByteArrayOutputStream ks = new ByteArrayOutputStream();
      serializer.write(doc, ks).close();

      // Read
      InputStream kis = new ByteArrayInputStream(ks.toByteArray());
      Pair<Annotation, InputStream> pair = serializer.read(kis);
      pair.second.close();
      Annotation readDoc = pair.first;
      kis.close();

      sameAsRead(doc, readDoc);
    } catch (Exception e) { throw new RuntimeException(e); }
  }
"
"  @Test
  public void testCanWriteReadCleanXML() {
    try {
      AnnotationSerializer serializer = new ProtobufAnnotationSerializer();
      // Write
      Properties props = new Properties();
      props.setProperty(""annotators"", ""tokenize,cleanxml"");
      StanfordCoreNLP pipe = new StanfordCoreNLP(props);
      Annotation doc = pipe.process(prideAndPrejudiceFirstBit);
      ByteArrayOutputStream ks = new ByteArrayOutputStream();
      serializer.write(doc, ks).close();

      // Read
      InputStream kis = new ByteArrayInputStream(ks.toByteArray());
      Pair<Annotation, InputStream> pair = serializer.read(kis);
      pair.second.close();
      Annotation readDoc = pair.first;
      kis.close();

      sameAsRead(doc, readDoc);
    } catch (Exception e) {
      throw new RuntimeException(e);
    }
  }
"
"  @Test
  public void testCanWriteReadWriteReadLargeFile() {
    try {
      AnnotationSerializer serializer = new ProtobufAnnotationSerializer();
      // Write
      StanfordCoreNLP pipe = new StanfordCoreNLP(new Properties());
      Annotation doc = pipe.process(prideAndPrejudiceChapters1to5);

      ByteArrayOutputStream ks = new ByteArrayOutputStream();
      serializer.write(doc, ks).close();

      // Read
      InputStream kis = new ByteArrayInputStream(ks.toByteArray());
      Pair<Annotation, InputStream> pair1 = serializer.read(kis);
      pair1.second.close();
      Annotation readDoc = pair1.first;
      kis.close();

      for (int i = 0 ; i < doc.get(CoreAnnotations.MentionsAnnotation.class).size() ; i++) {
        CoreMap cm1 = doc.get(CoreAnnotations.MentionsAnnotation.class).get(i);
        CoreMap cm2 = readDoc.get(CoreAnnotations.MentionsAnnotation.class).get(i);
        diffCoreMaps(i,cm1,cm2);
      }

      sameAsRead(doc, readDoc);

      // Write 2
      ByteArrayOutputStream ks2 = new ByteArrayOutputStream();
      serializer.write(readDoc, ks2).close();

      // Read 2
      InputStream kis2 = new ByteArrayInputStream(ks2.toByteArray());
      Pair<Annotation, InputStream> pair = serializer.read(kis2);
      pair.second.close();
      Annotation readDoc2 = pair.first;
      kis2.close();

      sameAsRead(readDoc, readDoc2);
      sameAsRead(doc, readDoc2);
    } catch (Exception e) {
      throw new RuntimeException(e);
    }
  }
"
"  @Test
  public void testSerializeLanguage() {
    testAnnotators(""tokenize,ssplit,parse"");
    testAnnotators(""tokenize,ssplit,pos,depparse"");
  }
"
"  @Test
  public void testRelation() {
    testAnnotators(""tokenize,ssplit,pos,lemma,ner,parse,relation"");
  }
"
"    @BeforeEach
    public void setup() {
        this.unirestInstance = Unirest.spawnInstance();
        this.unirestInstance.config().interceptor(interceptor);
    }
"
"    @Test
        public String writeValue(Object value) {
            return ""derp"";
        }
"
"    @Test
    public void expectAnyPath(){
        client.expect(HttpMethod.GET)
                .thenReturn(""woh"");

        Unirest.get(path).asEmpty();

        client.verifyAll();
    }
"
"    @Test
    public void setTimeoutsAndCustomClient() {
        try {
            Unirest.config().connectTimeout(1000).socketTimeout(2000);
        } catch (Exception e) {
            fail();
        }

        try {
            Unirest.config().asyncClient(HttpAsyncClientBuilder.create().build());
        } catch (Exception e) {
            fail();
        }

        try {
            Unirest.config().asyncClient(HttpAsyncClientBuilder.create().build());
            Unirest.config().connectTimeout(1000).socketTimeout(2000);
            fail();
        } catch (Exception e) {
            // Ok
        }

        try {
            Unirest.config().httpClient(HttpClientBuilder.create().build());
            Unirest.config().connectTimeout(1000).socketTimeout(2000);
            fail();
        } catch (Exception e) {
            // Ok
        }
    }
"
"    @Test
        public void process(org.apache.http.HttpRequest httpRequest, org.apache.http.protocol.HttpContext httpContext) throws HttpException, IOException {
            httpRequest.addHeader(""x-custom"", ""foo"");
        }
"
"    @BeforeEach
    public void setUp() {
        super.setUp();
    }
"
"    @AfterEach
    public void tearDown() {
        super.tearDown();
        requestConfigUsed = false;
    }
"
"    @Test
        public Object getClient() {
            return null;
        }
"
"    @Test
    public void testMangler_encoding() {
        assertLinkSurvives(""http://localhost/test%2Fthis"");
    }
"
"    @Test
    public void testMangler_fragment() {
        assertLinkSurvives(""http://localhost/test?a=b#fragment"");
    }
"
"    @Test
    public void basicBoringUri() {
        assertLinkSurvives(""http://localhost/test?a=b"");
    }
"
"    @Test
    public void semicolonsAsParam() {
        assertLinkSurvives(""http://localhost/test?a=b;foo=bar"");
    }
"
"    @Test
    public void utf8Chars(){
        assertLinkSurvives(""http://localhost/test?foo=ããã«ã¡ã¯"");
    }
"
"    @AfterEach
    public void before(){
        Unirest.shutDown(true);
    }
"
"    @Test
        public String toString(){
            return ""Hello World"";
        }
"
"    @Test
        public Date getDate() {
            return date;
        }
"
"    @AfterEach
    public void tearDown() {
        super.tearDown();
        asyncDone = false;
        status = 0;
        File file = test.toFile();
        if(file.exists()){
            file.delete();
        }
    }
"
"    @Test
        public String writeValue(Object value) {
            writeWasCalled = true;
            return new Gson().toJson(value);
        }
"
"    @Override @BeforeEach
    public void setUp() {
        super.setUp();
        clearUnirestHooks();
    }
"
"    @Test
        public T getSomeTees() {
            return someTees;
        }
"
"    @Test @Disabled
                        public void completed(HttpResponse<JsonNode> response) {
                            throw new UnirestException(""Failure!"");
                        }
"
"    @Test @Disabled
                    public void completed(HttpResponse<JsonNode> response) {
                        throw new UnirestException(""Failure!"");
                    }
"
"    @BeforeEach
    public void setUp() {
        super.setUp();
        this.monitor = new TestMonitor();
    }
"
"    @AfterEach
    public void tearDown() {
        super.tearDown();
        captured = null;
    }
"
"    @Test
        public void accept(HttpResponse<?> httpResponse) {

            this.httpResponse = httpResponse;
        }
"
"    @Override @AfterEach
    public void tearDown() {
        try {
            Files.delete(test);
        } catch (Exception ignored) { }
    }
"
"    @Test
        public void invalidate() {
            regular.invalidateAll();
            async.invalidateAll();
        }
"
"    @AfterEach
    public void tearDown() {
        super.tearDown();
        Unirest.shutDown(true);
        JankyProxy.shutdown();
    }
"
"    @BeforeEach
    public void setUp() {
        super.setUp();
        interceptor = new UniInterceptor(""x-custom"", ""foo"");
    }
"
"    @Test
            public void onRequest(HttpRequest<?> request, Config config) {
                request.getBody().ifPresent(b ->
                        b.multiParts().forEach(part ->
                                values.add(part.toString())));
            }
"
"    @BeforeEach
    public void setUp() {
        super.setUp();
        customOm = Mockito.spy(JsonObjectMapper.class);
    }
"
"    @Override @BeforeEach
    public void setUp() {
        super.setUp();
        this.monitor = new TestMonitor();
    }
"
"    @Test
    public void simpleSequence() throws Exception
    {
        Distribution dist = OptionDistribution.get(""seq(1..10)"").get();
        assertTrue(dist instanceof DistributionSequence);

        assertEquals(1, dist.minValue());
        assertEquals(10, dist.maxValue());
        assertEquals(5, dist.average());

        assertEquals(1, dist.inverseCumProb(0d));
        assertEquals(10, dist.inverseCumProb(1d));

        long min = dist.next();
        assertEquals(1,min);

        long last = min;
        for (int i=0; i<9; i++)
        {
            long next = dist.next();
            assertEquals(next, last+1); //increase by one each step
            last = next;
        }

        assertEquals(1, dist.next()); // wrapping
    }
"
"    @Test
    public void negValueSequence() throws Exception
    {
        Distribution dist = OptionDistribution.get(""seq(-1000..-10)"").get();
        assertTrue(dist instanceof DistributionSequence);

        assertEquals(-1000, dist.minValue());
        assertEquals( -10, dist.maxValue());
        assertEquals(-504, dist.average());

        assertEquals(-1000, dist.inverseCumProb(0d));
        assertEquals(-10, dist.inverseCumProb(1d));

        long min = dist.next();
        assertEquals(-1000, min);

        long last = min;
        long next = dist.next();
        while (last<next)
        {
            assertEquals(next, last+1); //increase by one each step
            last = next;
            next = dist.next();
        }

        assertEquals(-10, last); // wrapping
        assertEquals(-1000, next); // wrapping
    }
"
"    @Test
    public void bigSequence() throws Exception
    {
        Distribution dist = OptionDistribution.get(String.format(""seq(1..%d)"", Long.MAX_VALUE)).get();
        assertTrue(dist instanceof DistributionSequence);

        assertEquals(1, dist.minValue());
        assertEquals(Long.MAX_VALUE, dist.maxValue());

        assertEquals(1, dist.inverseCumProb(0d));
        assertEquals(Long.MAX_VALUE, dist.inverseCumProb(1d));

    }
"
"    @Test
    public void setSeed() throws Exception
    {
        Distribution dist = OptionDistribution.get(""seq(1..10)"").get();
        assertTrue(dist instanceof DistributionSequence);

        for (int seed=1; seed<500; seed+=seed)
        {
            dist.setSeed(seed);
            assertEquals(1, dist.minValue());
            assertEquals(10, dist.maxValue());
            assertEquals(5, dist.average());

            assertEquals(1, dist.inverseCumProb(0d));
            assertEquals(10, dist.inverseCumProb(1d));

            long last = dist.next();
            for (int i = 0; i < 9; i++)
            {
                long next = dist.next();
                if (next>1)
                {
                    assertEquals(next, last + 1); //increase by one each step
                }else{
                    assertEquals(last, 10); //wrap after the end
                }
                last = next;
            }
        }
    }
"
"    @Test
    public void simpleGaussian()
    {
        Distribution dist = OptionDistribution.get(""gaussian(1..10)"").get();
        assertTrue(dist instanceof DistributionBoundApache);

        assertEquals(1, dist.minValue());
        assertEquals(10, dist.maxValue());
        assertEquals(5, dist.average());

        assertEquals(1, dist.inverseCumProb(0d));
        assertEquals(10, dist.inverseCumProb(1d));

        int testCount = 100000;
        int[] results = new int[11];
        for (int i = 0; i < testCount; i++)
        {
            int val = toIntExact(dist.next());
            results[val]++;
        }

        // Increasing for the first half
        for (int i = toIntExact(dist.minValue()); i < dist.average(); i++)
        {
            assertTrue(results[i] < results[i + 1]);
        }

        // Decreasing for the second half
        for (int i = toIntExact(dist.average()) + 1; i < dist.maxValue(); i++)
        {
            assertTrue(results[i] > results[i + 1]);
        }
    }
"
"    @Test
    public void negValueGaussian()
    {
        Distribution dist = OptionDistribution.get(""gaussian(-1000..-10)"").get();
        assertTrue(dist instanceof DistributionBoundApache);

        assertEquals(-1000, dist.minValue());
        assertEquals( -10, dist.maxValue());
        assertEquals(-504, dist.average());

        assertEquals(-1000, dist.inverseCumProb(0d));
        assertEquals(-10, dist.inverseCumProb(1d));
    }
"
"    @Test
    public void delegatesToInitialPrintStream() throws Exception
    {
        ByteArrayOutputStream output = new ByteArrayOutputStream();
        PrintStream printStream = new PrintStream(output, true);
        MultiResultLogger underTest = new MultiResultLogger(printStream);

        underTest.println(""Very important result"");

        assertEquals(""Very important result\n"", output.toString());
    }
"
"    @Test
    public void printingExceptions() throws Exception
    {
        ByteArrayOutputStream output = new ByteArrayOutputStream();
        PrintStream printStream = new PrintStream(output, true);
        MultiResultLogger underTest = new MultiResultLogger(printStream);

        underTest.printException(new RuntimeException(""Bad things""));

        String stackTrace = output.toString();
        assertTrue(""Expected strack trace to be printed but got: "" + stackTrace, stackTrace.startsWith(""java.lang.RuntimeException: Bad things\n"" +
                                                ""\tat org.apache.cassandra.stress.util.MultiResultLoggerTest.printingExceptions""));
    }
"
"    @Test
    public void delegatesToAdditionalPrintStreams() throws Exception
    {
        ByteArrayOutputStream output = new ByteArrayOutputStream();
        PrintStream additionalPrintStream = new PrintStream(output, true);
        MultiResultLogger underTest = new MultiResultLogger(new PrintStream(NOOP));

        underTest.addStream(additionalPrintStream);
        underTest.println(""Very important result"");

        assertEquals(""Very important result\n"", output.toString());
    }
"
"    @Test
    public void delegatesPrintfToAdditionalPrintStreams() throws Exception
    {
        ByteArrayOutputStream output = new ByteArrayOutputStream();
        PrintStream additionalPrintStream = new PrintStream(output, true);
        MultiResultLogger underTest = new MultiResultLogger(new PrintStream(NOOP));

        underTest.addStream(additionalPrintStream);
        underTest.printf(""%s %s %s"", ""one"", ""two"", ""three"");

        assertEquals(""one two three"", output.toString());
    }
"
"    @Test
    public void delegatesPrintlnToAdditionalPrintStreams() throws Exception
    {
        ByteArrayOutputStream output = new ByteArrayOutputStream();
        PrintStream additionalPrintStream = new PrintStream(output, true);
        MultiResultLogger underTest = new MultiResultLogger(new PrintStream(NOOP));

        underTest.addStream(additionalPrintStream);
        underTest.println();

        assertEquals(""\n"", output.toString());
    }
"
"    @Test
    public void testDefaults() throws Exception
    {
        SettingsNode settingsNode = new SettingsNode(new SettingsNode.Options());
        assertEquals(null, settingsNode.datacenter);
    }
"
"    @Test
    public void testOveridingDataCenter() throws Exception
    {
        SettingsNode.Options options = new SettingsNode.Options();
        options.accept(""datacenter=dc1"");
        SettingsNode settingsNode = new SettingsNode(options);
        assertEquals(""dc1"", settingsNode.datacenter);
    }
"
"    @Test
    public void versionTriggersSpecialOption() throws Exception
    {
        assertTrue(SettingsMisc.maybeDoSpecial(ImmutableMap.of(""version"", new String[] {})));
    }
"
"    @Test
    public void noSpecialOptions() throws Exception
    {
        assertFalse(SettingsMisc.maybeDoSpecial(Collections.emptyMap()));
    }
"
"    @Test
    public void parsesVersionMatch() throws Exception
    {
        String versionString = SettingsMisc.parseVersionFile(""CassandraVersion=TheBestVersion\n"");
        assertEquals(""Version: TheBestVersion"", versionString);
    }
"
"    @Test
    public void parsesVersionNoMatch() throws Exception
    {
        String versionString = SettingsMisc.parseVersionFile(""VersionFileChangedFormat :("");
        assertEquals(""Unable to find version information"", versionString);
    }
"
"    @Test
    public void defaultsToReplicationFactorOfOne() throws Exception
    {
        OptionReplication defaults = new OptionReplication();
        assertEquals(ImmutableMap.of(""replication_factor"", ""1""), defaults.getOptions());
    }
"
"    @Test
    public void isSerializable() throws Exception
    {
        Map<String, String[]> args = new HashMap<>();
        args.put(""write"", new String[] {});
        StressSettings settings = StressSettings.get(args);
        // Will throw if not all settings are Serializable
        new ObjectOutputStream(new ByteArrayOutputStream()).writeObject(settings);
    }
"
"    @Test
    public void testOrderedReplay() throws IOException
    {
        File f = generateQueries(100, true);
        int queryCount = 0;
        try (ChronicleQueue queue = ChronicleQueueBuilder.single(f).build();
             FQLQueryIterator iter = new FQLQueryIterator(queue.createTailer(), 101))
        {
            long last = -1;
            while (iter.hasNext())
            {
                FQLQuery q = iter.next();
                assertTrue(q.queryStartTime >= last);
                last = q.queryStartTime;
                queryCount++;
            }
        }
        assertEquals(100, queryCount);
    }
"
"    @Test
    public void testQueryIterator() throws IOException
    {
        File f = generateQueries(100, false);
        int queryCount = 0;
        try (ChronicleQueue queue = ChronicleQueueBuilder.single(f).build();
             FQLQueryIterator iter = new FQLQueryIterator(queue.createTailer(), 1))
        {
            long last = -1;
            while (iter.hasNext())
            {
                FQLQuery q = iter.next();
                assertTrue(q.queryStartTime >= last);
                last = q.queryStartTime;
                queryCount++;
            }
        }
        assertEquals(100, queryCount);
    }
"
"    @Test
    public void testMergingIterator() throws IOException
    {
        File f = generateQueries(100, false);
        File f2 = generateQueries(100, false);
        int queryCount = 0;
        try (ChronicleQueue queue = ChronicleQueueBuilder.single(f).build();
             ChronicleQueue queue2 = ChronicleQueueBuilder.single(f2).build();
             FQLQueryIterator iter = new FQLQueryIterator(queue.createTailer(), 101);
             FQLQueryIterator iter2 = new FQLQueryIterator(queue2.createTailer(), 101);
             MergeIterator<FQLQuery, List<FQLQuery>> merger = MergeIterator.get(Lists.newArrayList(iter, iter2), FQLQuery::compareTo, new Replay.Reducer()))
        {
            long last = -1;

            while (merger.hasNext())
            {
                List<FQLQuery> qs = merger.next();
                assertEquals(2, qs.size());
                assertEquals(0, qs.get(0).compareTo(qs.get(1)));
                assertTrue(qs.get(0).queryStartTime >= last);
                last = qs.get(0).queryStartTime;
                queryCount++;
            }
        }
        assertEquals(100, queryCount);
    }
"
"    @Test
    public void testFQLQueryReader() throws IOException
    {
        FQLQueryReader reader = new FQLQueryReader();

        try (ChronicleQueue queue = ChronicleQueueBuilder.single(generateQueries(1000, true)).build())
        {
            ExcerptTailer tailer = queue.createTailer();
            int queryCount = 0;
            while (tailer.readDocument(reader))
            {
                assertNotNull(reader.getQuery());
                if (reader.getQuery() instanceof FQLQuery.Single)
                {
                    assertTrue(reader.getQuery().keyspace() == null || reader.getQuery().keyspace().equals(""querykeyspace""));
                }
                else
                {
                    assertEquals(""someks"", reader.getQuery().keyspace());
                }
                queryCount++;
            }
            assertEquals(1000, queryCount);
        }
    }
"
"    @Test
    public void testStoringResults() throws Throwable
    {
        File tmpDir = Files.createTempDirectory(""results"").toFile();
        File queryDir = Files.createTempDirectory(""queries"").toFile();

        ResultHandler.ComparableResultSet res = createResultSet(10, 10, true);
        ResultStore rs = new ResultStore(Collections.singletonList(tmpDir), queryDir);
        FQLQuery query = new FQLQuery.Single(""abc"", QueryOptions.DEFAULT.getProtocolVersion().asInt(), QueryOptions.DEFAULT, 12345, 11111, 22, ""select * from abc"", Collections.emptyList());
        try
        {
            rs.storeColumnDefinitions(query, Collections.singletonList(res.getColumnDefinitions()));
            Iterator<ResultHandler.ComparableRow> it = res.iterator();
            while (it.hasNext())
            {
                List<ResultHandler.ComparableRow> row = Collections.singletonList(it.next());
                rs.storeRows(row);
            }
            // this marks the end of the result set:
            rs.storeRows(Collections.singletonList(null));
        }
        finally
        {
            rs.close();
        }

        compareResults(Collections.singletonList(Pair.create(query, res)),
                       readResultFile(tmpDir, queryDir));

    }
"
"    @Test
    public void testCompareColumnDefinitions()
    {
        ResultHandler.ComparableResultSet res = createResultSet(10, 10, false);
        ResultComparator rc = new ResultComparator();

        List<ResultHandler.ComparableColumnDefinitions> colDefs = new ArrayList<>(100);
        List<String> targetHosts = new ArrayList<>(100);
        for (int i = 0; i < 100; i++)
        {
            targetHosts.add(""host""+i);
            colDefs.add(res.getColumnDefinitions());
        }
        assertTrue(rc.compareColumnDefinitions(targetHosts, null, colDefs));
        colDefs.set(50, createResultSet(9, 9, false).getColumnDefinitions());
        assertFalse(rc.compareColumnDefinitions(targetHosts, null, colDefs));
    }
"
"    @Test
    public void testCompareEqualRows()
    {
        ResultComparator rc = new ResultComparator();

        ResultHandler.ComparableResultSet res = createResultSet(10, 10, false);
        ResultHandler.ComparableResultSet res2 = createResultSet(10, 10, false);
        List<ResultHandler.ComparableResultSet> toCompare = Lists.newArrayList(res, res2);
        List<Iterator<ResultHandler.ComparableRow>> iters = toCompare.stream().map(Iterable::iterator).collect(Collectors.toList());

        while (true)
        {
            List<ResultHandler.ComparableRow> rows = ResultHandler.rows(iters);
            assertTrue(rc.compareRows(Lists.newArrayList(""eq1"", ""eq2""), null, rows));
            if (rows.stream().allMatch(Objects::isNull))
                break;
        }
    }
"
"    @Test
    public void testCompareRowsDifferentCount()
    {
        ResultComparator rc = new ResultComparator();
        ResultHandler.ComparableResultSet res = createResultSet(10, 10, false);
        ResultHandler.ComparableResultSet res2 = createResultSet(10, 10, false);
        List<ResultHandler.ComparableResultSet> toCompare = Lists.newArrayList(res, res2, createResultSet(10, 11, false));
        List<Iterator<ResultHandler.ComparableRow>> iters = toCompare.stream().map(Iterable::iterator).collect(Collectors.toList());
        boolean foundMismatch = false;
        while (true)
        {
            List<ResultHandler.ComparableRow> rows = ResultHandler.rows(iters);
            if (rows.stream().allMatch(Objects::isNull))
                break;
            if (!rc.compareRows(Lists.newArrayList(""eq1"", ""eq2"", ""diff""), null, rows))
            {
                foundMismatch = true;
            }
        }
        assertTrue(foundMismatch);
    }
"
"    @Test
    public void testCompareRowsDifferentContent()
    {
        ResultComparator rc = new ResultComparator();
        ResultHandler.ComparableResultSet res = createResultSet(10, 10, false);
        ResultHandler.ComparableResultSet res2 = createResultSet(10, 10, false);
        List<ResultHandler.ComparableResultSet> toCompare = Lists.newArrayList(res, res2, createResultSet(10, 10, true));
        List<Iterator<ResultHandler.ComparableRow>> iters = toCompare.stream().map(Iterable::iterator).collect(Collectors.toList());
        while (true)
        {
            List<ResultHandler.ComparableRow> rows = ResultHandler.rows(iters);
            if (rows.stream().allMatch(Objects::isNull))
                break;
            assertFalse(rows.toString(), rc.compareRows(Lists.newArrayList(""eq1"", ""eq2"", ""diff""), null, rows));
        }
    }
"
"    @Test
    public void testCompareRowsDifferentColumnCount()
    {
        ResultComparator rc = new ResultComparator();
        ResultHandler.ComparableResultSet res = createResultSet(10, 10, false);
        ResultHandler.ComparableResultSet res2 = createResultSet(10, 10, false);
        List<ResultHandler.ComparableResultSet> toCompare = Lists.newArrayList(res, res2, createResultSet(11, 10, false));
        List<Iterator<ResultHandler.ComparableRow>> iters = toCompare.stream().map(Iterable::iterator).collect(Collectors.toList());
        while (true)
        {
            List<ResultHandler.ComparableRow> rows = ResultHandler.rows(iters);
            if (rows.stream().allMatch(Objects::isNull))
                break;
            assertFalse(rows.toString(), rc.compareRows(Lists.newArrayList(""eq1"", ""eq2"", ""diff""), null, rows));
        }
    }
"
"    @Test
    public void testResultHandler() throws IOException
    {
        List<String> targetHosts = Lists.newArrayList(""hosta"", ""hostb"", ""hostc"");
        File tmpDir = Files.createTempDirectory(""testresulthandler"").toFile();
        File queryDir = Files.createTempDirectory(""queries"").toFile();
        List<File> resultPaths = new ArrayList<>();
        targetHosts.forEach(host -> { File f = new File(tmpDir, host); f.mkdir(); resultPaths.add(f);});

        ResultHandler.ComparableResultSet res = createResultSet(10, 10, false);
        ResultHandler.ComparableResultSet res2 = createResultSet(10, 10, false);
        ResultHandler.ComparableResultSet res3 = createResultSet(10, 10, false);
        List<ResultHandler.ComparableResultSet> toCompare = Lists.newArrayList(res, res2, res3);
        FQLQuery query = new FQLQuery.Single(""abcabc"", QueryOptions.DEFAULT.getProtocolVersion().asInt(), QueryOptions.DEFAULT, 1111, 2222, 3333, ""select * from xyz"", Collections.emptyList());
        try (ResultHandler rh = new ResultHandler(targetHosts, resultPaths, queryDir))
        {
            rh.handleResults(query, toCompare);
        }
        List<Pair<FQLQuery, ResultHandler.ComparableResultSet>> results1 = readResultFile(resultPaths.get(0), queryDir);
        List<Pair<FQLQuery, ResultHandler.ComparableResultSet>> results2 = readResultFile(resultPaths.get(1), queryDir);
        List<Pair<FQLQuery, ResultHandler.ComparableResultSet>> results3 = readResultFile(resultPaths.get(2), queryDir);
        compareResults(results1, results2);
        compareResults(results1, results3);
        compareResults(results3, Collections.singletonList(Pair.create(query, res)));
    }
"
"    @Test
    public void testResultHandlerWithDifference() throws IOException
    {
        List<String> targetHosts = Lists.newArrayList(""hosta"", ""hostb"", ""hostc"");
        File tmpDir = Files.createTempDirectory(""testresulthandler"").toFile();
        File queryDir = Files.createTempDirectory(""queries"").toFile();
        List<File> resultPaths = new ArrayList<>();
        targetHosts.forEach(host -> { File f = new File(tmpDir, host); f.mkdir(); resultPaths.add(f);});

        ResultHandler.ComparableResultSet res = createResultSet(10, 10, false);
        ResultHandler.ComparableResultSet res2 = createResultSet(10, 5, false);
        ResultHandler.ComparableResultSet res3 = createResultSet(10, 10, false);
        List<ResultHandler.ComparableResultSet> toCompare = Lists.newArrayList(res, res2, res3);
        FQLQuery query = new FQLQuery.Single(""aaa"", QueryOptions.DEFAULT.getProtocolVersion().asInt(), QueryOptions.DEFAULT, 123123, 11111, 22222, ""select * from abcabc"", Collections.emptyList());
        try (ResultHandler rh = new ResultHandler(targetHosts, resultPaths, queryDir))
        {
            rh.handleResults(query, toCompare);
        }
        List<Pair<FQLQuery, ResultHandler.ComparableResultSet>> results1 = readResultFile(resultPaths.get(0), queryDir);
        List<Pair<FQLQuery, ResultHandler.ComparableResultSet>> results2 = readResultFile(resultPaths.get(1), queryDir);
        List<Pair<FQLQuery, ResultHandler.ComparableResultSet>> results3 = readResultFile(resultPaths.get(2), queryDir);
        compareResults(results1, results3);
        compareResults(results2, Collections.singletonList(Pair.create(query, res2)));
    }
"
"    @Test
    public void testResultHandlerMultipleResultSets() throws IOException
    {
        List<String> targetHosts = Lists.newArrayList(""hosta"", ""hostb"", ""hostc"");
        File tmpDir = Files.createTempDirectory(""testresulthandler"").toFile();
        File queryDir = Files.createTempDirectory(""queries"").toFile();
        List<File> resultPaths = new ArrayList<>();
        targetHosts.forEach(host -> { File f = new File(tmpDir, host); f.mkdir(); resultPaths.add(f);});
        List<Pair<FQLQuery, List<ResultHandler.ComparableResultSet>>> resultSets = new ArrayList<>();
        Random random = new Random();
        for (int i = 0; i < 10; i++)
        {
            List<ResultHandler.ComparableResultSet> results = new ArrayList<>();
            List<ByteBuffer> values = Collections.singletonList(ByteBufferUtil.bytes(i * 50));
            for (int jj = 0; jj < targetHosts.size(); jj++)
            {
                results.add(createResultSet(5, 1 + random.nextInt(10), true));
            }
            FQLQuery q = i % 2 == 0
                         ? new FQLQuery.Single(""abc""+i,
                                             3,
                                             QueryOptions.forInternalCalls(values),
                                             i * 1000,
                                             12345,
                                             54321,
                                             ""select * from xyz where id = ""+i,
                                             values)
                         : new FQLQuery.Batch(""abc""+i,
                                              3,
                                              QueryOptions.forInternalCalls(values),
                                              i * 1000,
                                              i * 54321,
                                              i * 12345,
                                              com.datastax.driver.core.BatchStatement.Type.UNLOGGED,
                                              Lists.newArrayList(""select * from aaaa""),
                                              Collections.singletonList(values));

            resultSets.add(Pair.create(q, results));
        }
        try (ResultHandler rh = new ResultHandler(targetHosts, resultPaths, queryDir))
        {
            for (int i = 0; i < resultSets.size(); i++)
                rh.handleResults(resultSets.get(i).left, resultSets.get(i).right);
        }

        for (int i = 0; i < targetHosts.size(); i++)
            compareWithFile(resultPaths, queryDir, resultSets, i);
    }
"
"    @Test
    public void testResultHandlerFailedQuery() throws IOException
    {
        List<String> targetHosts = Lists.newArrayList(""hosta"", ""hostb"", ""hostc"", ""hostd"");
        File tmpDir = Files.createTempDirectory(""testresulthandler"").toFile();
        File queryDir = Files.createTempDirectory(""queries"").toFile();
        List<File> resultPaths = new ArrayList<>();
        targetHosts.forEach(host -> { File f = new File(tmpDir, host); f.mkdir(); resultPaths.add(f);});

        List<Pair<FQLQuery, List<ResultHandler.ComparableResultSet>>> resultSets = new ArrayList<>();
        Random random = new Random();
        for (int i = 0; i < 10; i++)
        {
            List<ResultHandler.ComparableResultSet> results = new ArrayList<>();
            List<ByteBuffer> values = Collections.singletonList(ByteBufferUtil.bytes(i * 50));
            for (int jj = 0; jj < targetHosts.size(); jj++)
            {
                results.add(createResultSet(5, 1 + random.nextInt(10), true));
            }
            results.set(0, StoredResultSet.failed(""testing abc""));
            results.set(3, StoredResultSet.failed(""testing abc""));
            FQLQuery q = new FQLQuery.Single(""abc""+i,
                                             3,
                                             QueryOptions.forInternalCalls(values),
                                             i * 1000,
                                             i * 12345,
                                             i * 54321,
                                             ""select * from xyz where id = ""+i,
                                             values);
            resultSets.add(Pair.create(q, results));
        }
        try (ResultHandler rh = new ResultHandler(targetHosts, resultPaths, queryDir))
        {
            for (int i = 0; i < resultSets.size(); i++)
                rh.handleResults(resultSets.get(i).left, resultSets.get(i).right);
        }
        for (int i = 0; i < targetHosts.size(); i++)
            compareWithFile(resultPaths, queryDir, resultSets, i);
    }
"
"    @Test
    public void testCompare()
    {
        FQLQuery q1 = new FQLQuery.Single(""abc"", 0, QueryOptions.DEFAULT, 123, 111, 222, ""aaaa"", Collections.emptyList());
        FQLQuery q2 = new FQLQuery.Single(""abc"", 0, QueryOptions.DEFAULT, 123, 111, 222,""aaaa"", Collections.emptyList());

        assertEquals(0, q1.compareTo(q2));
        assertEquals(0, q2.compareTo(q1));

        FQLQuery q3 = new FQLQuery.Batch(""abc"", 0, QueryOptions.DEFAULT, 123, 111, 222, com.datastax.driver.core.BatchStatement.Type.UNLOGGED, Collections.emptyList(), Collections.emptyList());
        // single queries before batch queries
        assertTrue(q1.compareTo(q3) < 0);
        assertTrue(q3.compareTo(q1) > 0);

        // check that smaller query time
        FQLQuery q4 = new FQLQuery.Single(""abc"", 0, QueryOptions.DEFAULT, 124, 111, 222, ""aaaa"", Collections.emptyList());
        assertTrue(q1.compareTo(q4) < 0);
        assertTrue(q4.compareTo(q1) > 0);

        FQLQuery q5 = new FQLQuery.Batch(""abc"", 0, QueryOptions.DEFAULT, 124, 111, 222, com.datastax.driver.core.BatchStatement.Type.UNLOGGED, Collections.emptyList(), Collections.emptyList());
        assertTrue(q1.compareTo(q5) < 0);
        assertTrue(q5.compareTo(q1) > 0);

        FQLQuery q6 = new FQLQuery.Single(""abc"", 0, QueryOptions.DEFAULT, 123, 111, 222, ""aaaa"", Collections.singletonList(ByteBufferUtil.bytes(10)));
        FQLQuery q7 = new FQLQuery.Single(""abc"", 0, QueryOptions.DEFAULT, 123, 111, 222, ""aaaa"", Collections.emptyList());
        assertTrue(q6.compareTo(q7) > 0);
        assertTrue(q7.compareTo(q6) < 0);

        FQLQuery q8 = new FQLQuery.Single(""abc"", 0, QueryOptions.DEFAULT, 123, 111, 222, ""aaaa"", Collections.singletonList(ByteBufferUtil.bytes(""a"")));
        FQLQuery q9 = new FQLQuery.Single(""abc"", 0, QueryOptions.DEFAULT, 123, 111, 222, ""aaaa"", Collections.singletonList(ByteBufferUtil.bytes(""b"")));
        assertTrue(q8.compareTo(q9) < 0);
        assertTrue(q9.compareTo(q8) > 0);
    }
"
"    @Test
    public void testFQLQuerySingleToStatement()
    {
        List<ByteBuffer> values = new ArrayList<>();
        for (int i = 0; i < 10; i++)
            values.add(ByteBufferUtil.bytes(i));
        FQLQuery.Single single = new FQLQuery.Single(""xyz"",
                                                     QueryOptions.DEFAULT.getProtocolVersion().asInt(),
                                                     QueryOptions.forInternalCalls(values),
                                                     1234,
                                                     12345,
                                                     54321,
                                                     ""select * from aaa"",
                                                     values);
        Statement stmt = single.toStatement();
        assertEquals(stmt.getDefaultTimestamp(), 12345);
        assertTrue(stmt instanceof SimpleStatement);
        SimpleStatement simpleStmt = (SimpleStatement)stmt;
        assertEquals(""select * from aaa"",simpleStmt.getQueryString(CodecRegistry.DEFAULT_INSTANCE));
        assertArrayEquals(values.toArray(), simpleStmt.getValues(com.datastax.driver.core.ProtocolVersion.fromInt(QueryOptions.DEFAULT.getProtocolVersion().asInt()), CodecRegistry.DEFAULT_INSTANCE));
    }
"
"    @Test
    public void testFQLQueryBatchToStatement()
    {
        List<List<ByteBuffer>> values = new ArrayList<>();
        List<String> queries = new ArrayList<>();
        for (int bqCount = 0; bqCount < 10; bqCount++)
        {
            queries.add(""select * from asdf where x = ? and y = "" + bqCount);
            List<ByteBuffer> queryValues = new ArrayList<>();
            for (int i = 0; i < 10; i++)
                queryValues.add(ByteBufferUtil.bytes(i + "":"" + bqCount));
            values.add(queryValues);
        }

        FQLQuery.Batch batch = new FQLQuery.Batch(""xyz"",
                                                   QueryOptions.DEFAULT.getProtocolVersion().asInt(),
                                                   QueryOptions.DEFAULT,
                                                   1234,
                                                   12345,
                                                   54321,
                                                   com.datastax.driver.core.BatchStatement.Type.UNLOGGED,
                                                   queries,
                                                   values);
        Statement stmt = batch.toStatement();
        assertEquals(stmt.getDefaultTimestamp(), 12345);
        assertTrue(stmt instanceof com.datastax.driver.core.BatchStatement);
        com.datastax.driver.core.BatchStatement batchStmt = (com.datastax.driver.core.BatchStatement)stmt;
        List<Statement> statements = Lists.newArrayList(batchStmt.getStatements());
        List<Statement> fromFQLQueries = batch.queries.stream().map(FQLQuery.Single::toStatement).collect(Collectors.toList());
        assertEquals(statements.size(), fromFQLQueries.size());
        assertEquals(12345, batchStmt.getDefaultTimestamp());
        for (int i = 0; i < statements.size(); i++)
            compareStatements(statements.get(i), fromFQLQueries.get(i));
    }
"
"    @Test
    public void testParser() {
        QueryReplayer.ParsedTargetHost pth;
        pth = fromString(""127.0.0.1"");
        assertEquals(""127.0.0.1"", pth.host);
        assertEquals(9042, pth.port );
        assertNull(pth.user);
        assertNull(pth.password);

        pth = fromString(""127.0.0.1:3333"");
        assertEquals(""127.0.0.1"", pth.host);
        assertEquals(3333, pth.port );
        assertNull(pth.user);
        assertNull(pth.password);

        pth = fromString(""aaa:bbb@127.0.0.1:3333"");
        assertEquals(""127.0.0.1"", pth.host);
        assertEquals(3333, pth.port );
        assertEquals(""aaa"", pth.user);
        assertEquals(""bbb"", pth.password);

        pth = fromString(""aaa:bbb@127.0.0.1"");
        assertEquals(""127.0.0.1"", pth.host);
        assertEquals(9042, pth.port );
        assertEquals(""aaa"", pth.user);
        assertEquals(""bbb"", pth.password);
    }
"
"    @Test(expected = RuntimeException.class)
    public void testNoPass()
    {
        fromString(""blabla@abc.com:1234"");
    }
"
"    @Test(expected = RuntimeException.class)
    public void testBadPort()
    {
        fromString(""aaa:bbb@abc.com:xyz"");
    }
"
"    @Test (expected = IORuntimeException.class)
    public void testFutureVersion() throws Exception
    {
        FQLQueryReader reader = new FQLQueryReader();
        File dir = Files.createTempDirectory(""chronicle"").toFile();
        try (ChronicleQueue queue = ChronicleQueueBuilder.single(dir).build())
        {
            ExcerptAppender appender = queue.acquireAppender();
            appender.writeDocument(new BinLog.ReleaseableWriteMarshallable() {
                protected long version()
                {
                    return 999;
                }

                protected String type()
                {
                    return FullQueryLogger.SINGLE_QUERY;
                }

                public void writeMarshallablePayload(WireOut wire)
                {
                    wire.write(""future-field"").text(""future_value"");
                }
"
"    @Test (expected = IORuntimeException.class)
    public void testUnknownRecord() throws Exception
    {
        FQLQueryReader reader = new FQLQueryReader();
        File dir = Files.createTempDirectory(""chronicle"").toFile();
        try (ChronicleQueue queue = ChronicleQueueBuilder.single(dir).build())
        {
            ExcerptAppender appender = queue.acquireAppender();
            appender.writeDocument(new BinLog.ReleaseableWriteMarshallable() {
                protected long version()
                {
                    return FullQueryLogger.CURRENT_VERSION;
                }

                protected String type()
                {
                    return ""unknown-type"";
                }

                public void writeMarshallablePayload(WireOut wire)
                {
                    wire.write(""unknown-field"").text(""unknown_value"");
                }
"
"    @Test
    public void endToEnd() throws IOException
    {
        List<String> targetHosts = Lists.newArrayList(""hosta"", ""hostb"");
        File tmpDir = Files.createTempDirectory(""testresulthandler"").toFile();
        File queryDir = Files.createTempDirectory(""queries"").toFile();
        List<File> resultPaths = generateResultSets(targetHosts, tmpDir, queryDir, true, false);
        Compare.compare(queryDir.toString(), resultPaths.stream().map(File::toString).collect(Collectors.toList()));
    }
"
"    @Test
    public void endToEndQueryFailures() throws IOException
    {
        List<String> targetHosts = Lists.newArrayList(""hosta"", ""hostb"");
        File tmpDir = Files.createTempDirectory(""testresulthandler"").toFile();
        File queryDir = Files.createTempDirectory(""queries"").toFile();
        List<File> resultPaths = generateResultSets(targetHosts, tmpDir, queryDir, true,true);
        Compare.compare(queryDir.toString(), resultPaths.stream().map(File::toString).collect(Collectors.toList()));
    }
"
"    @Test
    public void compareEqual() throws IOException
    {
        List<String> targetHosts = Lists.newArrayList(""hosta"", ""hostb"");
        File tmpDir = Files.createTempDirectory(""testresulthandler"").toFile();
        File queryDir = Files.createTempDirectory(""queries"").toFile();
        List<File> resultPaths = generateResultSets(targetHosts, tmpDir, queryDir, false,false);

        ResultComparator comparator = new ResultComparator();
        List<ChronicleQueue> readQueues = null;
        try
        {
            readQueues = resultPaths.stream().map(s -> ChronicleQueueBuilder.single(s).readOnly(true).build()).collect(Collectors.toList());
            List<Iterator<ResultHandler.ComparableResultSet>> its = readQueues.stream().map(q -> new Compare.StoredResultSetIterator(q.createTailer())).collect(Collectors.toList());
            List<ResultHandler.ComparableResultSet> resultSets = Compare.resultSets(its);
            while(resultSets.stream().allMatch(Objects::nonNull))
            {
                assertTrue(comparator.compareColumnDefinitions(targetHosts, query(), resultSets.stream().map(ResultHandler.ComparableResultSet::getColumnDefinitions).collect(Collectors.toList())));
                List<Iterator<ResultHandler.ComparableRow>> rows = resultSets.stream().map(Iterable::iterator).collect(Collectors.toList());

                List<ResultHandler.ComparableRow> toCompare = ResultHandler.rows(rows);

                while (toCompare.stream().allMatch(Objects::nonNull))
                {
                    assertTrue(comparator.compareRows(targetHosts, query(), ResultHandler.rows(rows)));
                    toCompare = ResultHandler.rows(rows);
                }
                resultSets = Compare.resultSets(its);
            }
        }
        finally
        {
            if (readQueues != null)
                readQueues.forEach(Closeable::close);
        }
    }
"
"//    @Test
//    public void isOpen()
//    {
//        Assert.assertTrue(inputPlus.isOpen());
//        inputPlus.requestClosure();
//        Assert.assertFalse(inputPlus.isOpen());
//    }
"
"    @Test
    public void append_closed()
    {
        inputPlus = new AsyncStreamingInputPlus(channel);
        inputPlus.requestClosure();
        inputPlus.close();
        buf = channel.alloc().buffer(4);
        assertFalse(inputPlus.append(buf));
    }
"
"    @Test
    public void append_normal()
    {
        inputPlus = new AsyncStreamingInputPlus(channel);
        int size = 4;
        buf = channel.alloc().buffer(size);
        buf.writerIndex(size);
        inputPlus.append(buf);
        Assert.assertEquals(buf.readableBytes(), inputPlus.unsafeAvailable());
    }
"
"    @Test
    public void read() throws IOException
    {
        inputPlus = new AsyncStreamingInputPlus(channel);
        // put two buffers of 8 bytes each into the queue.
        // then read an int, then a long. the latter tests offset into the inputPlus, as well as spanning across queued buffers.
        // the values of those int/long will both be '42', but spread across both queue buffers.
        ByteBuf buf = channel.alloc().buffer(8);
        buf.writeInt(42);
        buf.writerIndex(8);
        inputPlus.append(buf);
        buf = channel.alloc().buffer(8);
        buf.writeInt(42);
        buf.writerIndex(8);
        inputPlus.append(buf);
        Assert.assertEquals(16, inputPlus.unsafeAvailable());

//        ByteBuffer out = ByteBuffer.allocate(4);
//        int readCount = inputPlus.read(out);
//        Assert.assertEquals(4, readCount);
//        out.flip();
//        Assert.assertEquals(42, out.getInt());
//        Assert.assertEquals(12, inputPlus.unsafeAvailable());

//        out = ByteBuffer.allocate(8);
//        readCount = inputPlus.read(out);
//        Assert.assertEquals(8, readCount);
//        out.flip();
//        Assert.assertEquals(42, out.getLong());
//        Assert.assertEquals(4, inputPlus.unsafeAvailable());
    }
"
"//    @Test (expected = EOFException.class)
//    public void read_closed() throws IOException
//    {
//        inputPlus.requestClosure();
//        ByteBuffer buf = ByteBuffer.allocate(1);
//        inputPlus.read(buf);
//    }
"
"    @Test
    public void available_closed()
    {
        inputPlus = new AsyncStreamingInputPlus(channel);
        inputPlus.requestClosure();
        inputPlus.unsafeAvailable();
    }
"
"    @Test
    public void available_HappyPath()
    {
        inputPlus = new AsyncStreamingInputPlus(channel);
        int size = 4;
        buf = channel.alloc().heapBuffer(size);
        buf.writerIndex(size);
        inputPlus.append(buf);
        Assert.assertEquals(size, inputPlus.unsafeAvailable());
    }
"
"    @Test
    public void available_ClosedButWithBytes()
    {
        inputPlus = new AsyncStreamingInputPlus(channel);
        int size = 4;
        buf = channel.alloc().heapBuffer(size);
        buf.writerIndex(size);
        inputPlus.append(buf);
        inputPlus.requestClosure();
        Assert.assertEquals(size, inputPlus.unsafeAvailable());
    }
"
"    @Test
    public void consumeUntil_SingleBuffer_Partial_HappyPath() throws IOException
    {
        consumeUntilTestCycle(1, 8, 0, 4);
    }
"
"    @Test
    public void consumeUntil_SingleBuffer_AllBytes_HappyPath() throws IOException
    {
        consumeUntilTestCycle(1, 8, 0, 8);
    }
"
"    @Test
    public void consumeUntil_MultipleBufferr_Partial_HappyPath() throws IOException
    {
        consumeUntilTestCycle(2, 8, 0, 13);
    }
"
"    @Test
    public void consumeUntil_MultipleBuffer_AllBytes_HappyPath() throws IOException
    {
        consumeUntilTestCycle(2, 8, 0, 16);
    }
"
"    @Test(expected = EOFException.class)
    public void consumeUntil_SingleBuffer_Fails() throws IOException
    {
        consumeUntilTestCycle(1, 8, 0, 9);
    }
"
"    @Test(expected = EOFException.class)
    public void consumeUntil_MultipleBuffer_Fails() throws IOException
    {
        consumeUntilTestCycle(2, 8, 0, 17);
    }
"
"    @Test
    public void rebufferTimeout() throws IOException
    {
        long timeoutMillis = 1000;
        inputPlus = new AsyncStreamingInputPlus(channel, timeoutMillis, TimeUnit.MILLISECONDS);

        long startNanos = System.nanoTime();
        try
        {
            inputPlus.readInt();
            Assert.fail(""should not have been able to read from the queue"");
        }
        catch (InputTimeoutException e)
        {
            // this is the success case, and is expected. any other exception is a failure.
        }

        long durationNanos = System.nanoTime() - startNanos;
        Assert.assertTrue(TimeUnit.MILLISECONDS.toNanos(timeoutMillis) <= durationNanos);
    }
"
"    @Test(expected = IllegalArgumentException.class)
    public void testAcceptsNoLessThanThreeArguments() throws Exception
    {
        new RateBasedBackPressure(ImmutableMap.of(HIGH_RATIO, ""1""), new TestTimeSource(), 10);
    }
"
"    @Test(expected = IllegalArgumentException.class)
    public void testHighRatioMustBeBiggerThanZero() throws Exception
    {
        new RateBasedBackPressure(ImmutableMap.of(HIGH_RATIO, ""0"", FACTOR, ""2"", FLOW, ""FAST""), new TestTimeSource(), 10);
    }
"
"    @Test(expected = IllegalArgumentException.class)
    public void testHighRatioMustBeSmallerEqualThanOne() throws Exception
    {
        new RateBasedBackPressure(ImmutableMap.of(HIGH_RATIO, ""2"", FACTOR, ""2"", FLOW, ""FAST""), new TestTimeSource(), 10);
    }
"
"    @Test(expected = IllegalArgumentException.class)
    public void testFactorMustBeBiggerEqualThanOne() throws Exception
    {
        new RateBasedBackPressure(ImmutableMap.of(HIGH_RATIO, ""0.9"", FACTOR, ""0"", FLOW, ""FAST""), new TestTimeSource(), 10);
    }
"
"    @Test(expected = IllegalArgumentException.class)
    public void testWindowSizeMustBeBiggerEqualThanTen() throws Exception
    {
        new RateBasedBackPressure(ImmutableMap.of(HIGH_RATIO, ""0.9"", FACTOR, ""5"", FLOW, ""FAST""), new TestTimeSource(), 1);
    }
"
"    @Test
    public void testFlowMustBeEitherFASTorSLOW() throws Exception
    {
        new RateBasedBackPressure(ImmutableMap.of(HIGH_RATIO, ""0.9"", FACTOR, ""1"", FLOW, ""FAST""), new TestTimeSource(), 10);
        new RateBasedBackPressure(ImmutableMap.of(HIGH_RATIO, ""0.9"", FACTOR, ""1"", FLOW, ""SLOW""), new TestTimeSource(), 10);
        try
        {
            new RateBasedBackPressure(ImmutableMap.of(HIGH_RATIO, ""0.9"", FACTOR, ""1"", FLOW, ""WRONG""), new TestTimeSource(), 10);
            fail(""Expected to fail with wrong flow type."");
        }
        catch (Exception ex)
        {
        }
    }
"
"    @Test
    public void testBackPressureStateUpdates()
    {
        long windowSize = 6000;
        TestTimeSource timeSource = new TestTimeSource();
        RateBasedBackPressure strategy = new RateBasedBackPressure(ImmutableMap.of(HIGH_RATIO, ""0.9"", FACTOR, ""10"", FLOW, ""FAST""), timeSource, windowSize);

        RateBasedBackPressureState state = strategy.newState(InetAddressAndPort.getLoopbackAddress());
        state.onMessageSent(null);
        assertEquals(0, state.incomingRate.size());
        assertEquals(0, state.outgoingRate.size());

        state = strategy.newState(InetAddressAndPort.getLoopbackAddress());
        state.onResponseReceived();
        assertEquals(1, state.incomingRate.size());
        assertEquals(1, state.outgoingRate.size());

        state = strategy.newState(InetAddressAndPort.getLoopbackAddress());
        state.onResponseTimeout();
        assertEquals(0, state.incomingRate.size());
        assertEquals(1, state.outgoingRate.size());
    }
"
"    @Test
    public void testBackPressureIsNotUpdatedBeyondInfinity() throws Exception
    {
        long windowSize = 6000;
        TestTimeSource timeSource = new TestTimeSource();
        RateBasedBackPressure strategy = new RateBasedBackPressure(ImmutableMap.of(HIGH_RATIO, ""0.9"", FACTOR, ""10"", FLOW, ""FAST""), timeSource, windowSize);
        RateBasedBackPressureState state = strategy.newState(InetAddressAndPort.getLoopbackAddress());

        // Get initial rate:
        double initialRate = state.rateLimiter.getRate();
        assertEquals(Double.POSITIVE_INFINITY, initialRate, 0.0);

        // Update incoming and outgoing rate equally:
        state.incomingRate.update(1);
        state.outgoingRate.update(1);

        // Move time ahead:
        timeSource.sleep(windowSize, TimeUnit.MILLISECONDS);

        // Verify the rate doesn't change because already at infinity:
        strategy.apply(Sets.newHashSet(state), 1, TimeUnit.SECONDS);
        assertEquals(initialRate, state.rateLimiter.getRate(), 0.0);
    }
"
"    @Test
    public void testBackPressureIsUpdatedOncePerWindowSize() throws Exception
    {
        long windowSize = 6000;
        TestTimeSource timeSource = new TestTimeSource();
        RateBasedBackPressure strategy = new RateBasedBackPressure(ImmutableMap.of(HIGH_RATIO, ""0.9"", FACTOR, ""10"", FLOW, ""FAST""), timeSource, windowSize);
        RateBasedBackPressureState state = strategy.newState(InetAddressAndPort.getLoopbackAddress());

        // Get initial time:
        long current = state.getLastIntervalAcquire();
        assertEquals(0, current);

        // Update incoming and outgoing rate:
        state.incomingRate.update(1);
        state.outgoingRate.update(1);

        // Move time ahead by window size:
        timeSource.sleep(windowSize, TimeUnit.MILLISECONDS);

        // Verify the timestamp changed:
        strategy.apply(Sets.newHashSet(state), 1, TimeUnit.SECONDS);
        current = state.getLastIntervalAcquire();
        assertEquals(timeSource.currentTimeMillis(), current);

        // Move time ahead by less than interval:
        long previous = current;
        timeSource.sleep(windowSize / 2, TimeUnit.MILLISECONDS);

        // Verify the last timestamp didn't change because below the window size:
        strategy.apply(Sets.newHashSet(state), 1, TimeUnit.SECONDS);
        current = state.getLastIntervalAcquire();
        assertEquals(previous, current);
    }
"
"    @Test
    public void testBackPressureWhenBelowHighRatio() throws Exception
    {
        long windowSize = 6000;
        TestTimeSource timeSource = new TestTimeSource();
        RateBasedBackPressure strategy = new RateBasedBackPressure(ImmutableMap.of(HIGH_RATIO, ""0.9"", FACTOR, ""10"", FLOW, ""FAST""), timeSource, windowSize);
        RateBasedBackPressureState state = strategy.newState(InetAddressAndPort.getLoopbackAddress());

        // Update incoming and outgoing rate so that the ratio is 0.5:
        state.incomingRate.update(50);
        state.outgoingRate.update(100);

        // Move time ahead:
        timeSource.sleep(windowSize, TimeUnit.MILLISECONDS);

        // Verify the rate is decreased by factor:
        strategy.apply(Sets.newHashSet(state), 1, TimeUnit.SECONDS);
        assertEquals(7.4, state.rateLimiter.getRate(), 0.1);
    }
"
"    @Test
    public void testBackPressureRateLimiterIsIncreasedAfterGoingAgainAboveHighRatio() throws Exception
    {
        long windowSize = 6000;
        TestTimeSource timeSource = new TestTimeSource();
        RateBasedBackPressure strategy = new RateBasedBackPressure(ImmutableMap.of(HIGH_RATIO, ""0.9"", FACTOR, ""10"", FLOW, ""FAST""), timeSource, windowSize);
        RateBasedBackPressureState state = strategy.newState(InetAddressAndPort.getLoopbackAddress());

        // Update incoming and outgoing rate so that the ratio is 0.5:
        state.incomingRate.update(50);
        state.outgoingRate.update(100);

        // Move time ahead:
        timeSource.sleep(windowSize, TimeUnit.MILLISECONDS);

        // Verify the rate decreased:
        strategy.apply(Sets.newHashSet(state), 1, TimeUnit.SECONDS);
        assertEquals(7.4, state.rateLimiter.getRate(), 0.1);

        // Update incoming and outgoing rate back above high rate:
        state.incomingRate.update(50);
        state.outgoingRate.update(50);

        // Move time ahead:
        timeSource.sleep(windowSize, TimeUnit.MILLISECONDS);

        // Verify rate limiter is increased by factor:
        strategy.apply(Sets.newHashSet(state), 1, TimeUnit.SECONDS);
        assertEquals(8.25, state.rateLimiter.getRate(), 0.1);

        // Update incoming and outgoing rate to keep it below the limiter rate:
        state.incomingRate.update(1);
        state.outgoingRate.update(1);

        // Move time ahead:
        timeSource.sleep(windowSize, TimeUnit.MILLISECONDS);

        // Verify rate limiter is not increased as already higher than the actual rate:
        strategy.apply(Sets.newHashSet(state), 1, TimeUnit.SECONDS);
        assertEquals(8.25, state.rateLimiter.getRate(), 0.1);
    }
"
"    @Test
    public void testBackPressureFastFlow() throws Exception
    {
        long windowSize = 6000;
        TestTimeSource timeSource = new TestTimeSource();
        TestableBackPressure strategy = new TestableBackPressure(ImmutableMap.of(HIGH_RATIO, ""0.9"", FACTOR, ""10"", FLOW, ""FAST""), timeSource, windowSize);
        RateBasedBackPressureState state1 = strategy.newState(InetAddressAndPort.getByName(""127.0.0.1""));
        RateBasedBackPressureState state2 = strategy.newState(InetAddressAndPort.getByName(""127.0.0.2""));
        RateBasedBackPressureState state3 = strategy.newState(InetAddressAndPort.getByName(""127.0.0.3""));

        // Update incoming and outgoing rates:
        state1.incomingRate.update(50);
        state1.outgoingRate.update(100);
        state2.incomingRate.update(80); // fast
        state2.outgoingRate.update(100);
        state3.incomingRate.update(20);
        state3.outgoingRate.update(100);

        // Move time ahead:
        timeSource.sleep(windowSize, TimeUnit.MILLISECONDS);

        // Verify the fast replica rate limiting has been applied:
        Set<RateBasedBackPressureState> replicaGroup = Sets.newHashSet(state1, state2, state3);
        strategy.apply(replicaGroup, 1, TimeUnit.SECONDS);
        assertTrue(strategy.checkAcquired());
        assertTrue(strategy.checkApplied());
        assertEquals(12.0, strategy.getRateLimiterForReplicaGroup(replicaGroup).getRate(), 0.1);
    }
"
"    @Test
    public void testBackPressureSlowFlow() throws Exception
    {
        long windowSize = 6000;
        TestTimeSource timeSource = new TestTimeSource();
        TestableBackPressure strategy = new TestableBackPressure(ImmutableMap.of(HIGH_RATIO, ""0.9"", FACTOR, ""10"", FLOW, ""SLOW""), timeSource, windowSize);
        RateBasedBackPressureState state1 = strategy.newState(InetAddressAndPort.getByName(""127.0.0.1""));
        RateBasedBackPressureState state2 = strategy.newState(InetAddressAndPort.getByName(""127.0.0.2""));
        RateBasedBackPressureState state3 = strategy.newState(InetAddressAndPort.getByName(""127.0.0.3""));

        // Update incoming and outgoing rates:
        state1.incomingRate.update(50);
        state1.outgoingRate.update(100);
        state2.incomingRate.update(100);
        state2.outgoingRate.update(100);
        state3.incomingRate.update(20); // slow
        state3.outgoingRate.update(100);

        // Move time ahead:
        timeSource.sleep(windowSize, TimeUnit.MILLISECONDS);

        // Verify the slow replica rate limiting has been applied:
        Set<RateBasedBackPressureState> replicaGroup = Sets.newHashSet(state1, state2, state3);
        strategy.apply(replicaGroup, 1, TimeUnit.SECONDS);
        assertTrue(strategy.checkAcquired());
        assertTrue(strategy.checkApplied());
        assertEquals(3.0, strategy.getRateLimiterForReplicaGroup(replicaGroup).getRate(), 0.1);
    }
"
"    @Test
    public void testBackPressureWithDifferentGroups() throws Exception
    {
        long windowSize = 6000;
        TestTimeSource timeSource = new TestTimeSource();
        TestableBackPressure strategy = new TestableBackPressure(ImmutableMap.of(HIGH_RATIO, ""0.9"", FACTOR, ""10"", FLOW, ""SLOW""), timeSource, windowSize);
        RateBasedBackPressureState state1 = strategy.newState(InetAddressAndPort.getByName(""127.0.0.1""));
        RateBasedBackPressureState state2 = strategy.newState(InetAddressAndPort.getByName(""127.0.0.2""));
        RateBasedBackPressureState state3 = strategy.newState(InetAddressAndPort.getByName(""127.0.0.3""));
        RateBasedBackPressureState state4 = strategy.newState(InetAddressAndPort.getByName(""127.0.0.4""));

        // Update incoming and outgoing rates:
        state1.incomingRate.update(50); // this
        state1.outgoingRate.update(100);
        state2.incomingRate.update(100);
        state2.outgoingRate.update(100);
        state3.incomingRate.update(20); // this
        state3.outgoingRate.update(100);
        state4.incomingRate.update(80);
        state4.outgoingRate.update(100);

        // Move time ahead:
        timeSource.sleep(windowSize, TimeUnit.MILLISECONDS);

        // Verify the first group:
        Set<RateBasedBackPressureState> replicaGroup = Sets.newHashSet(state1, state2);
        strategy.apply(replicaGroup, 1, TimeUnit.SECONDS);
        assertTrue(strategy.checkAcquired());
        assertTrue(strategy.checkApplied());
        assertEquals(7.4, strategy.getRateLimiterForReplicaGroup(replicaGroup).getRate(), 0.1);

        // Verify the second group:
        replicaGroup = Sets.newHashSet(state3, state4);
        strategy.apply(replicaGroup, 1, TimeUnit.SECONDS);
        assertTrue(strategy.checkAcquired());
        assertTrue(strategy.checkApplied());
        assertEquals(3.0, strategy.getRateLimiterForReplicaGroup(replicaGroup).getRate(), 0.1);
    }
"
"    @Test
    public void testBackPressurePastTimeout() throws Exception
    {
        long windowSize = 10000;
        TestTimeSource timeSource = new TestTimeSource();
        TestableBackPressure strategy = new TestableBackPressure(ImmutableMap.of(HIGH_RATIO, ""0.9"", FACTOR, ""10"", FLOW, ""SLOW""), timeSource, windowSize);
        RateBasedBackPressureState state1 = strategy.newState(InetAddressAndPort.getByName(""127.0.0.1""));
        RateBasedBackPressureState state2 = strategy.newState(InetAddressAndPort.getByName(""127.0.0.2""));
        RateBasedBackPressureState state3 = strategy.newState(InetAddressAndPort.getByName(""127.0.0.3""));

        // Update incoming and outgoing rates:
        state1.incomingRate.update(5); // slow
        state1.outgoingRate.update(100);
        state2.incomingRate.update(100);
        state2.outgoingRate.update(100);
        state3.incomingRate.update(100);
        state3.outgoingRate.update(100);

        // Move time ahead:
        timeSource.sleep(windowSize, TimeUnit.MILLISECONDS);

        // Verify the slow replica rate limiting has been applied:
        Set<RateBasedBackPressureState> replicaGroup = Sets.newHashSet(state1, state2, state3);
        strategy.apply(replicaGroup, 4, TimeUnit.SECONDS);
        assertTrue(strategy.checkAcquired());
        assertTrue(strategy.checkApplied());
        assertEquals(0.5, strategy.getRateLimiterForReplicaGroup(replicaGroup).getRate(), 0.1);

        // Make one more apply call to saturate the rate limit timeout (0.5 requests per second means 2 requests span
        // 4 seconds, but we can only make one as we have to subtract the incoming response time):
        strategy.apply(replicaGroup, 4, TimeUnit.SECONDS);

        // Now verify another call to apply doesn't acquire the rate limit because of the max timeout of 4 seconds minus
        // 2 seconds of response time, so the time source itself sleeps two second:
        long start = timeSource.currentTimeMillis();
        strategy.apply(replicaGroup, 4, TimeUnit.SECONDS);
        assertFalse(strategy.checkAcquired());
        assertTrue(strategy.checkApplied());
        assertEquals(TimeUnit.NANOSECONDS.convert(2, TimeUnit.SECONDS),
                     strategy.timeout);
        assertEquals(strategy.timeout,
                     TimeUnit.NANOSECONDS.convert(timeSource.currentTimeMillis() - start, TimeUnit.MILLISECONDS));
    }
"
"    @Test
    public void testDroppedMessages()
    {
        Verb verb = Verb.READ_REQ;

        for (int i = 1; i <= 5000; i++)
            messagingService.metrics.recordDroppedMessage(verb, i, MILLISECONDS, i % 2 == 0);

        List<String> logs = new ArrayList<>();
        messagingService.metrics.resetAndConsumeDroppedErrors(logs::add);
        assertEquals(1, logs.size());
        Pattern regexp = Pattern.compile(""READ_REQ messages were dropped in last 5000 ms: (\\d+) internal and (\\d+) cross node. Mean internal dropped latency: (\\d+) ms and Mean cross-node dropped latency: (\\d+) ms"");
        Matcher matcher = regexp.matcher(logs.get(0));
        assertTrue(matcher.find());
        assertEquals(2500, Integer.parseInt(matcher.group(1)));
        assertEquals(2500, Integer.parseInt(matcher.group(2)));
        assertTrue(Integer.parseInt(matcher.group(3)) > 0);
        assertTrue(Integer.parseInt(matcher.group(4)) > 0);
        assertEquals(5000, (int) messagingService.metrics.getDroppedMessages().get(verb.toString()));

        logs.clear();
        messagingService.metrics.resetAndConsumeDroppedErrors(logs::add);
        assertEquals(0, logs.size());

        for (int i = 0; i < 2500; i++)
            messagingService.metrics.recordDroppedMessage(verb, i, MILLISECONDS, i % 2 == 0);

        logs.clear();
        messagingService.metrics.resetAndConsumeDroppedErrors(logs::add);
        assertEquals(1, logs.size());
        matcher = regexp.matcher(logs.get(0));
        assertTrue(matcher.find());
        assertEquals(1250, Integer.parseInt(matcher.group(1)));
        assertEquals(1250, Integer.parseInt(matcher.group(2)));
        assertTrue(Integer.parseInt(matcher.group(3)) > 0);
        assertTrue(Integer.parseInt(matcher.group(4)) > 0);
        assertEquals(7500, (int) messagingService.metrics.getDroppedMessages().get(verb.toString()));
    }
"
"    @Test
    public void testDCLatency() throws Exception
    {
        int latency = 100;
        ConcurrentHashMap<String, MessagingMetrics.DCLatencyRecorder> dcLatency = MessagingService.instance().metrics.dcLatency;
        dcLatency.clear();

        long now = System.currentTimeMillis();
        long sentAt = now - latency;
        assertNull(dcLatency.get(""datacenter1""));
        addDCLatency(sentAt, now);
        assertNotNull(dcLatency.get(""datacenter1""));
        assertEquals(1, dcLatency.get(""datacenter1"").dcLatency.getCount());
        long expectedBucket = bucketOffsets[Math.abs(Arrays.binarySearch(bucketOffsets, MILLISECONDS.toNanos(latency))) - 1];
        assertEquals(expectedBucket, dcLatency.get(""datacenter1"").dcLatency.getSnapshot().getMax());
    }
"
"    @Test
    public void testNegativeDCLatency()
    {
        MessagingMetrics.DCLatencyRecorder updater = MessagingService.instance().metrics.internodeLatencyRecorder(InetAddressAndPort.getLocalHost());

        // if clocks are off should just not track anything
        int latency = -100;

        long now = System.currentTimeMillis();
        long sentAt = now - latency;

        long count = updater.dcLatency.getCount();
        updater.accept(now - sentAt, MILLISECONDS);
        // negative value shoudln't be recorded
        assertEquals(count, updater.dcLatency.getCount());
    }
"
"    @Test
    public void testQueueWaitLatency()
    {
        int latency = 100;
        Verb verb = Verb.MUTATION_REQ;

        Map<Verb, Timer> queueWaitLatency = MessagingService.instance().metrics.internalLatency;
        MessagingService.instance().metrics.recordInternalLatency(verb, latency, MILLISECONDS);
        assertEquals(1, queueWaitLatency.get(verb).getCount());
        long expectedBucket = bucketOffsets[Math.abs(Arrays.binarySearch(bucketOffsets, MILLISECONDS.toNanos(latency))) - 1];
        assertEquals(expectedBucket, queueWaitLatency.get(verb).getSnapshot().getMax());
    }
"
"    @Test
    public void testNegativeQueueWaitLatency() throws Exception
    {
        int latency = -100;
        Verb verb = Verb.MUTATION_REQ;

        Map<Verb, Timer> queueWaitLatency = MessagingService.instance().metrics.internalLatency;
        queueWaitLatency.clear();

        assertNull(queueWaitLatency.get(verb));
        MessagingService.instance().metrics.recordInternalLatency(verb, latency, MILLISECONDS);
        assertNull(queueWaitLatency.get(verb));
    }
"
"    @Test
    public void testUpdatesBackPressureOnSendWhenEnabledAndWithSupportedCallback() throws UnknownHostException
    {
        MockBackPressureStrategy.MockBackPressureState backPressureState = (MockBackPressureStrategy.MockBackPressureState) messagingService.getBackPressureState(InetAddressAndPort.getByName(""127.0.0.2""));
        RequestCallback bpCallback = new BackPressureCallback();
        RequestCallback noCallback = new NoBackPressureCallback();
        Message<?> ignored = null;

        DatabaseDescriptor.setBackPressureEnabled(true);
        messagingService.updateBackPressureOnSend(InetAddressAndPort.getByName(""127.0.0.2""), noCallback, ignored);
        assertFalse(backPressureState.onSend);

        DatabaseDescriptor.setBackPressureEnabled(false);
        messagingService.updateBackPressureOnSend(InetAddressAndPort.getByName(""127.0.0.2""), bpCallback, ignored);
        assertFalse(backPressureState.onSend);

        DatabaseDescriptor.setBackPressureEnabled(true);
        messagingService.updateBackPressureOnSend(InetAddressAndPort.getByName(""127.0.0.2""), bpCallback, ignored);
        assertTrue(backPressureState.onSend);
    }
"
"    @Test
    public void testUpdatesBackPressureOnReceiveWhenEnabledAndWithSupportedCallback() throws UnknownHostException
    {
        MockBackPressureStrategy.MockBackPressureState backPressureState = (MockBackPressureStrategy.MockBackPressureState) messagingService.getBackPressureState(InetAddressAndPort.getByName(""127.0.0.2""));
        RequestCallback bpCallback = new BackPressureCallback();
        RequestCallback noCallback = new NoBackPressureCallback();
        boolean timeout = false;

        DatabaseDescriptor.setBackPressureEnabled(true);
        messagingService.updateBackPressureOnReceive(InetAddressAndPort.getByName(""127.0.0.2""), noCallback, timeout);
        assertFalse(backPressureState.onReceive);
        assertFalse(backPressureState.onTimeout);

        DatabaseDescriptor.setBackPressureEnabled(false);
        messagingService.updateBackPressureOnReceive(InetAddressAndPort.getByName(""127.0.0.2""), bpCallback, timeout);
        assertFalse(backPressureState.onReceive);
        assertFalse(backPressureState.onTimeout);

        DatabaseDescriptor.setBackPressureEnabled(true);
        messagingService.updateBackPressureOnReceive(InetAddressAndPort.getByName(""127.0.0.2""), bpCallback, timeout);
        assertTrue(backPressureState.onReceive);
        assertFalse(backPressureState.onTimeout);
    }
"
"    @Test
    public void testUpdatesBackPressureOnTimeoutWhenEnabledAndWithSupportedCallback() throws UnknownHostException
    {
        MockBackPressureStrategy.MockBackPressureState backPressureState = (MockBackPressureStrategy.MockBackPressureState) messagingService.getBackPressureState(InetAddressAndPort.getByName(""127.0.0.2""));
        RequestCallback bpCallback = new BackPressureCallback();
        RequestCallback noCallback = new NoBackPressureCallback();
        boolean timeout = true;

        DatabaseDescriptor.setBackPressureEnabled(true);
        messagingService.updateBackPressureOnReceive(InetAddressAndPort.getByName(""127.0.0.2""), noCallback, timeout);
        assertFalse(backPressureState.onReceive);
        assertFalse(backPressureState.onTimeout);

        DatabaseDescriptor.setBackPressureEnabled(false);
        messagingService.updateBackPressureOnReceive(InetAddressAndPort.getByName(""127.0.0.2""), bpCallback, timeout);
        assertFalse(backPressureState.onReceive);
        assertFalse(backPressureState.onTimeout);

        DatabaseDescriptor.setBackPressureEnabled(true);
        messagingService.updateBackPressureOnReceive(InetAddressAndPort.getByName(""127.0.0.2""), bpCallback, timeout);
        assertFalse(backPressureState.onReceive);
        assertTrue(backPressureState.onTimeout);
    }
"
"    @Test
    public void testAppliesBackPressureWhenEnabled() throws UnknownHostException
    {
        DatabaseDescriptor.setBackPressureEnabled(false);
        messagingService.applyBackPressure(Arrays.asList(InetAddressAndPort.getByName(""127.0.0.2"")), ONE_SECOND);
        assertFalse(MockBackPressureStrategy.applied);

        DatabaseDescriptor.setBackPressureEnabled(true);
        messagingService.applyBackPressure(Arrays.asList(InetAddressAndPort.getByName(""127.0.0.2"")), ONE_SECOND);
        assertTrue(MockBackPressureStrategy.applied);
    }
"
"    @Test
    public void testDoesntApplyBackPressureToBroadcastAddress() throws UnknownHostException
    {
        DatabaseDescriptor.setBackPressureEnabled(true);
        messagingService.applyBackPressure(Arrays.asList(InetAddressAndPort.getByName(""127.0.0.1"")), ONE_SECOND);
        assertFalse(MockBackPressureStrategy.applied);
    }
"
"    @Test
    public void testFailedInternodeAuth() throws Exception
    {
        MessagingService ms = MessagingService.instance();
        DatabaseDescriptor.setInternodeAuthenticator(ALLOW_NOTHING_AUTHENTICATOR);
        InetAddressAndPort address = InetAddressAndPort.getByName(""127.0.0.250"");

        //Should return null
        Message messageOut = Message.out(Verb.ECHO_REQ, NoPayload.noPayload);
        assertFalse(ms.isConnected(address, messageOut));

        //Should tolerate null
        ms.closeOutbound(address);
        ms.send(messageOut, address);
    }
"
"//    @Test
//    public void reconnectWithNewIp() throws Exception
//    {
//        InetAddressAndPort publicIp = InetAddressAndPort.getByName(""127.0.0.2"");
//        InetAddressAndPort privateIp = InetAddressAndPort.getByName(""127.0.0.3"");
//
//        // reset the preferred IP value, for good test hygene
//        SystemKeyspace.updatePreferredIP(publicIp, publicIp);
//
//        // create pool/conn with public addr
//        Assert.assertEquals(publicIp, messagingService.getCurrentEndpoint(publicIp));
//        messagingService.maybeReconnectWithNewIp(publicIp, privateIp).await(1L, TimeUnit.SECONDS);
//        Assert.assertEquals(privateIp, messagingService.getCurrentEndpoint(publicIp));
//
//        messagingService.closeOutbound(publicIp);
//
//        // recreate the pool/conn, and make sure the preferred ip addr is used
//        Assert.assertEquals(privateIp, messagingService.getCurrentEndpoint(publicIp));
//    }
"
"    @Test
    public void listenPlainConnection() throws InterruptedException
    {
        ServerEncryptionOptions serverEncryptionOptions = new ServerEncryptionOptions()
                                                          .withInternodeEncryption(ServerEncryptionOptions.InternodeEncryption.none);
        listen(serverEncryptionOptions, false);
    }
"
"    @Test
    public void listenPlainConnectionWithBroadcastAddr() throws InterruptedException
    {
        ServerEncryptionOptions serverEncryptionOptions = new ServerEncryptionOptions()
                                                          .withInternodeEncryption(ServerEncryptionOptions.InternodeEncryption.none);
        listen(serverEncryptionOptions, true);
    }
"
"    @Test
    public void listenRequiredSecureConnection() throws InterruptedException
    {
        ServerEncryptionOptions serverEncryptionOptions = new ServerEncryptionOptions()
                                                          .withOptional(false)
                                                          .withInternodeEncryption(ServerEncryptionOptions.InternodeEncryption.all)
                                                          .withLegacySslStoragePort(false);
        listen(serverEncryptionOptions, false);
    }
"
"    @Test
    public void listenRequiredSecureConnectionWithBroadcastAddr() throws InterruptedException
    {
        ServerEncryptionOptions serverEncryptionOptions = new ServerEncryptionOptions()
                                                          .withOptional(false)
                                                          .withInternodeEncryption(ServerEncryptionOptions.InternodeEncryption.all)
                                                          .withLegacySslStoragePort(false);
        listen(serverEncryptionOptions, true);
    }
"
"    @Test
    public void listenRequiredSecureConnectionWithLegacyPort() throws InterruptedException
    {
        ServerEncryptionOptions serverEncryptionOptions = new ServerEncryptionOptions()
                                                          .withInternodeEncryption(ServerEncryptionOptions.InternodeEncryption.all)
                                                          .withOptional(false)
                                                          .withLegacySslStoragePort(true);
        listen(serverEncryptionOptions, false);
    }
"
"    @Test
    public void listenRequiredSecureConnectionWithBroadcastAddrAndLegacyPort() throws InterruptedException
    {
        ServerEncryptionOptions serverEncryptionOptions = new ServerEncryptionOptions()
                                                          .withInternodeEncryption(ServerEncryptionOptions.InternodeEncryption.all)
                                                          .withOptional(false)
                                                          .withLegacySslStoragePort(true);
        listen(serverEncryptionOptions, true);
    }
"
"    @Test
    public void listenOptionalSecureConnection() throws InterruptedException
    {
        ServerEncryptionOptions serverEncryptionOptions = new ServerEncryptionOptions()
                                                          .withOptional(true);
        listen(serverEncryptionOptions, false);
    }
"
"    @Test
    public void listenOptionalSecureConnectionWithBroadcastAddr() throws InterruptedException
    {
        ServerEncryptionOptions serverEncryptionOptions = new ServerEncryptionOptions()
                                                          .withOptional(true);
        listen(serverEncryptionOptions, true);
    }
"
"//    @Test
//    public void getPreferredRemoteAddrUsesPrivateIp() throws UnknownHostException
//    {
//        MessagingService ms = MessagingService.instance();
//        InetAddressAndPort remote = InetAddressAndPort.getByNameOverrideDefaults(""127.0.0.151"", 7000);
//        InetAddressAndPort privateIp = InetAddressAndPort.getByName(""127.0.0.6"");
//
//        OutboundConnectionSettings template = new OutboundConnectionSettings(remote)
//                                              .withConnectTo(privateIp)
//                                              .withAuthenticator(ALLOW_NOTHING_AUTHENTICATOR);
//        OutboundConnections pool = new OutboundConnections(template, new MockBackPressureStrategy(null).newState(remote));
//        ms.channelManagers.put(remote, pool);
//
//        Assert.assertEquals(privateIp, ms.getPreferredRemoteAddr(remote));
//    }
"
"//    @Test
//    public void getPreferredRemoteAddrUsesPreferredIp() throws UnknownHostException
//    {
//        MessagingService ms = MessagingService.instance();
//        InetAddressAndPort remote = InetAddressAndPort.getByNameOverrideDefaults(""127.0.0.115"", 7000);
//
//        InetAddressAndPort preferredIp = InetAddressAndPort.getByName(""127.0.0.16"");
//        SystemKeyspace.updatePreferredIP(remote, preferredIp);
//
//        Assert.assertEquals(preferredIp, ms.getPreferredRemoteAddr(remote));
//    }
"
"//    @Test
//    public void getPreferredRemoteAddrUsesPrivateIpOverridesPreferredIp() throws UnknownHostException
//    {
//        MessagingService ms = MessagingService.instance();
//        InetAddressAndPort local = InetAddressAndPort.getByNameOverrideDefaults(""127.0.0.4"", 7000);
//        InetAddressAndPort remote = InetAddressAndPort.getByNameOverrideDefaults(""127.0.0.105"", 7000);
//        InetAddressAndPort privateIp = InetAddressAndPort.getByName(""127.0.0.6"");
//
//        OutboundConnectionSettings template = new OutboundConnectionSettings(remote)
//                                              .withConnectTo(privateIp)
//                                              .withAuthenticator(ALLOW_NOTHING_AUTHENTICATOR);
//
//        OutboundConnections pool = new OutboundConnections(template, new MockBackPressureStrategy(null).newState(remote));
//        ms.channelManagers.put(remote, pool);
//
//        InetAddressAndPort preferredIp = InetAddressAndPort.getByName(""127.0.0.16"");
//        SystemKeyspace.updatePreferredIP(remote, preferredIp);
//
//        Assert.assertEquals(privateIp, ms.getPreferredRemoteAddr(remote));
//    }
"
"    @Test (expected = IllegalArgumentException.class)
    public void build_SmallSendSize()
    {
        test(settings -> settings.withSocketSendBufferSizeInBytes(999));
    }
"
"    @Test (expected = IllegalArgumentException.class)
    public void build_SendSizeLessThanZero()
    {
        test(settings -> settings.withSocketSendBufferSizeInBytes(-1));
    }
"
"    @Test (expected = IllegalArgumentException.class)
    public void build_TcpConnectTimeoutLessThanZero()
    {
        test(settings -> settings.withTcpConnectTimeoutInMS(-1));
    }
"
"	@Test
	public void contextLoads() {
		// The remote config was bad so there is no bootstrap
		assertThat(this.environment.getPropertySources().contains(""bootstrap"")).isFalse();
	}
"
"	@Test
	public void contextLoads() {
		Map res = new TestRestTemplate().getForObject(
				""http://localhost:"" + this.port + BASE_PATH + ""/env/info.foo"", Map.class);
		assertThat(res).containsKey(""propertySources"");
		Map<String, Object> property = (Map<String, Object>) res.get(""property"");
		assertThat(property).containsEntry(""value"", ""bar"");
	}
"
"	@Test
	public void contextLoads() {
		Map res = new TestRestTemplate().getForObject(
				""http://localhost:"" + this.port + BASE_PATH + ""/env/info.foo"", Map.class);
		assertThat(res).containsKey(""propertySources"");
		Map<String, Object> property = (Map<String, Object>) res.get(""property"");
		assertThat(property).containsEntry(""value"", ""bar"");
	}
"
"	@Test
	public void contextFails() {
		try {
			new SpringApplicationBuilder().sources(Application.class).run(
					""--server.port=0"", ""--spring.cloud.config.enabled=true"",
					""--spring.cloud.config.fail-fast=true"",
					""--spring.cloud.config.uri=http://server-host-doesnt-exist:1234"");
			fail(""failFast option did not produce an exception"");
		}
		catch (Exception e) {
			assertThat(e.getMessage().contains(""fail fast""))
					.as(""Exception not caused by fail fast"").isTrue();
		}
	}
"
"	@Test
	public void stateChangedWorks() {
		ConfigClientWatch watch = new ConfigClientWatch(null);
		assertThat(watch.stateChanged(null, ""1"")).isTrue();
		assertThat(watch.stateChanged(""1"", ""2"")).isTrue();
		assertThat(watch.stateChanged(""1"", null)).isTrue();
		assertThat(watch.stateChanged(""1"", ""1"")).isFalse();
		watch.close();
	}
"
"	@Test
	public void overrideConfigServicePropertySourceLocatorWhenBeanIsProvided() {
		TestPropertyValues.of(""spring.cloud.config.enabled=true"").applyTo(this.context);
		this.context.register(ConfigServicePropertySourceLocatorOverrideConfig.class);
		this.context.register(ConfigServiceBootstrapConfiguration.class);
		this.context.refresh();

		ConfigServicePropertySourceLocator locator = this.context
				.getBean(ConfigServicePropertySourceLocator.class);

		Field restTemplateField = ReflectionUtils
				.findField(ConfigServicePropertySourceLocator.class, ""restTemplate"");
		restTemplateField.setAccessible(true);

		RestTemplate restTemplate = (RestTemplate) ReflectionUtils
				.getField(restTemplateField, locator);

		assertThat(restTemplate).isNotNull();
	}
"
"	@Test
	public void sunnyDay() {
		AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(
				ConfigClientAutoConfiguration.class);
		assertThat(BeanFactoryUtils.beanNamesForTypeIncludingAncestors(context,
				ConfigClientProperties.class).length).isEqualTo(1);
		context.close();
	}
"
"	@Test
	public void withParent() {
		ConfigurableApplicationContext context = new SpringApplicationBuilder(
				ConfigClientAutoConfiguration.class).child(Object.class)
						.web(WebApplicationType.NONE).run();
		assertThat(BeanFactoryUtils.beanNamesForTypeIncludingAncestors(context,
				ConfigClientProperties.class).length).isEqualTo(1);
		context.close();
	}
"
"	@Test
	public void shouldFailWithExceptionGetConfigServerInstanceFromDiscoveryClient()
			throws Exception {
		givenDiscoveryClientReturnsNoInfo();

		expectNoInstancesOfConfigServerException();

		setup(""spring.cloud.config.discovery.enabled=true"",
				""spring.cloud.config.fail-fast=true"");
	}
"
"	@Test
	public void shouldFailWithMessageGetConfigServerInstanceFromDiscoveryClient()
			throws Exception {
		givenDiscoveryClientReturnsNoInfo();

		setup(""spring.cloud.config.discovery.enabled=true"",
				""spring.cloud.config.fail-fast=false"");

		expectDiscoveryClientConfigServiceBootstrapConfigurationIsSetup();
		expectConfigClientPropertiesHasDefaultConfiguration();
		verifyDiscoveryClientCalledOnce();
	}
"
"	@Test
	public void shouldSucceedGetConfigServerInstanceFromDiscoveryClient()
			throws Exception {
		givenDiscoveryClientReturnsInfo();

		setup(""spring.cloud.config.discovery.enabled=true"",
				""spring.cloud.config.fail-fast=true"");

		expectDiscoveryClientConfigServiceBootstrapConfigurationIsSetup();
		expectConfigClientPropertiesHasConfigurationFromEureka();
		verifyDiscoveryClientCalledOnce();
	}
"
"	@Test
	public void offByDefault() throws Exception {
		this.context = new AnnotationConfigApplicationContext(
				DiscoveryClientConfigServiceBootstrapConfiguration.class);

		assertThat(this.context.getBeanNamesForType(DiscoveryClient.class).length)
				.isEqualTo(0);
		assertThat(this.context.getBeanNamesForType(
				DiscoveryClientConfigServiceBootstrapConfiguration.class).length)
						.isEqualTo(0);
	}
"
"	@Test
	public void onWhenRequested() throws Exception {
		givenDiscoveryClientReturnsInfo();

		setup(""spring.cloud.config.discovery.enabled=true"");

		expectDiscoveryClientConfigServiceBootstrapConfigurationIsSetup();
		verifyDiscoveryClientCalledOnce();
		expectConfigClientPropertiesHasConfigurationFromEureka();
	}
"
"	@Test
	public void onWhenHeartbeat() throws Exception {
		setup(""spring.cloud.config.discovery.enabled=true"");

		expectDiscoveryClientConfigServiceBootstrapConfigurationIsSetup();

		givenDiscoveryClientReturnsInfo();
		verifyDiscoveryClientCalledOnce();

		this.context.publishEvent(new HeartbeatEvent(this.context, ""new""));

		expectConfigClientPropertiesHasConfigurationFromEureka();
	}
"
"	@Test
	public void secureWhenRequested() throws Exception {
		this.info = new DefaultServiceInstance(""app"", ""foo"", 443, true);
		givenDiscoveryClientReturnsInfo();

		setup(""spring.cloud.config.discovery.enabled=true"");

		expectDiscoveryClientConfigServiceBootstrapConfigurationIsSetup();

		verifyDiscoveryClientCalledOnce();
		expectConfigClientPropertiesHasConfiguration(""https://foo:443/"");
	}
"
"	@Test
	public void multipleInstancesReturnedFromDiscovery() {
		ServiceInstance info1 = new DefaultServiceInstance(""app"", ""localhost"", 8888,
				true);
		ServiceInstance info2 = new DefaultServiceInstance(""app"", ""localhost1"", 8888,
				false);
		givenDiscoveryClientReturnsInfoForMultipleInstances(info1, info2);

		setup(""spring.cloud.config.discovery.enabled=true"");

		expectDiscoveryClientConfigServiceBootstrapConfigurationIsSetup();

		verifyDiscoveryClientCalledOnce();
		expectConfigClientPropertiesHasMultipleUris(""https://localhost:8888/"",
				""http://localhost1:8888/"");

	}
"
"	@Test
	public void setsPasssword() throws Exception {
		this.info.getMetadata().put(""password"", ""bar"");
		givenDiscoveryClientReturnsInfo();

		setup(""spring.cloud.config.discovery.enabled=true"");

		ConfigClientProperties locator = this.context
				.getBean(ConfigClientProperties.class);
		Credentials credentials = locator.getCredentials(0);
		assertThat(credentials.getUri()).isEqualTo(""http://foo:8877/"");
		assertThat(credentials.getPassword()).isEqualTo(""bar"");
		assertThat(credentials.getUsername()).isEqualTo(""user"");
	}
"
"	@Test
	public void setsPath() throws Exception {
		this.info.getMetadata().put(""configPath"", ""/bar"");
		givenDiscoveryClientReturnsInfo();

		setup(""spring.cloud.config.discovery.enabled=true"");

		expectConfigClientPropertiesHasConfiguration(""http://foo:8877/bar"");
	}
"
"	@Test
	public void shouldFailGetConfigServerInstanceFromDiscoveryClient() throws Exception {
		givenDiscoveryClientReturnsNoInfo();

		setup(""spring.cloud.config.discovery.enabled=true"");

		expectDiscoveryClientConfigServiceBootstrapConfigurationIsSetup();
		verifyDiscoveryClientCalledOnce();
		expectConfigClientPropertiesHasDefaultConfiguration();
	}
"
"	@Test
	public void shouldRetryAndSucceedGetConfigServerInstanceFromDiscoveryClient()
			throws Exception {
		givenDiscoveryClientReturnsInfoOnThirdTry();

		setup(""spring.cloud.config.discovery.enabled=true"",
				""spring.cloud.config.retry.maxAttempts=3"",
				""spring.cloud.config.retry.initialInterval=10"",
				""spring.cloud.config.fail-fast=true"");

		expectDiscoveryClientConfigServiceBootstrapConfigurationIsSetup();
		verifyDiscoveryClientCalledThreeTimes();

		this.context.publishEvent(new HeartbeatEvent(this.context, ""new""));

		expectConfigClientPropertiesHasConfigurationFromEureka();
	}
"
"	@Test
	public void shouldNotRetryIfNotFailFastPropertySet() throws Exception {
		givenDiscoveryClientReturnsInfoOnThirdTry();

		setup(""spring.cloud.config.discovery.enabled=true"",
				""spring.cloud.config.retry.maxAttempts=3"",
				""spring.cloud.config.retry.initialInterval=10"");

		expectDiscoveryClientConfigServiceBootstrapConfigurationIsSetup();
		verifyDiscoveryClientCalledOnce();
		expectConfigClientPropertiesHasDefaultConfiguration();
	}
"
"	@Test
	public void shouldRetryAndFailWithExceptionGetConfigServerInstanceFromDiscoveryClient()
			throws Exception {
		givenDiscoveryClientReturnsNoInfo();

		expectNoInstancesOfConfigServerException();

		setup(""spring.cloud.config.discovery.enabled=true"",
				""spring.cloud.config.retry.maxAttempts=3"",
				""spring.cloud.config.retry.initialInterval=10"",
				""spring.cloud.config.fail-fast=true"");
	}
"
"	@Test
	public void shouldRetryAndFailWithMessageGetConfigServerInstanceFromDiscoveryClient()
			throws Exception {
		givenDiscoveryClientReturnsNoInfo();

		setup(""spring.cloud.config.discovery.enabled=true"",
				""spring.cloud.config.retry.maxAttempts=3"",
				""spring.cloud.config.retry.initialInterval=10"",
				""spring.cloud.config.fail-fast=false"");

		expectDiscoveryClientConfigServiceBootstrapConfigurationIsSetup();
		expectConfigClientPropertiesHasDefaultConfiguration();
	}
"
"	@Test
	public void testDefaultStatus() {
		// UNKNOWN is better than DOWN since it doesn't stop the app from working
		assertThat(this.indicator.health().getStatus()).isEqualTo(Status.UNKNOWN);
	}
"
"	@Test
	public void testExceptionStatus() {
		doThrow(new IllegalStateException()).when(this.locator)
				.locate(any(Environment.class));
		assertThat(this.indicator.health().getStatus()).isEqualTo(Status.DOWN);
		verify(this.locator, times(1)).locate(any(Environment.class));
	}
"
"	@Test
	public void testServerUp() {
		PropertySource<?> source = new MapPropertySource(""foo"",
				Collections.<String, Object>emptyMap());
		doReturn(source).when(this.locator).locate(any(Environment.class));
		assertThat(this.indicator.health().getStatus()).isEqualTo(Status.UP);
		verify(this.locator, times(1)).locate(any(Environment.class));
	}
"
"	@Test
	public void healthIsCached() {
		PropertySource<?> source = new MapPropertySource(""foo"",
				Collections.<String, Object>emptyMap());
		doReturn(source).when(this.locator).locate(any(Environment.class));

		// not cached
		assertThat(this.indicator.health().getStatus()).isEqualTo(Status.UP);

		// cached
		assertThat(this.indicator.health().getStatus()).isEqualTo(Status.UP);

		verify(this.locator, times(1)).locate(any(Environment.class));
	}
"
"	@Test
	public void sunnyDay() {
		Environment body = new Environment(""app"", ""master"");
		mockRequestResponseWithoutLabel(new ResponseEntity<>(body, HttpStatus.OK));
		this.locator.setRestTemplate(this.restTemplate);

		ArgumentCaptor<HttpEntity> argumentCaptor = ArgumentCaptor
				.forClass(HttpEntity.class);

		assertThat(this.locator.locate(this.environment)).isNotNull();

		Mockito.verify(this.restTemplate).exchange(anyString(), any(HttpMethod.class),
				argumentCaptor.capture(), any(Class.class), anyString(), anyString());

		HttpEntity httpEntity = argumentCaptor.getValue();
		assertThat(httpEntity.getHeaders().getAccept())
				.containsExactly(MediaType.APPLICATION_JSON);
	}
"
"	@Test
	public void sunnyDayWithLabel() {
		Environment body = new Environment(""app"", ""master"");
		mockRequestResponseWithLabel(new ResponseEntity<>(body, HttpStatus.OK), ""v1.0.0"");
		this.locator.setRestTemplate(this.restTemplate);
		TestPropertyValues.of(""spring.cloud.config.label:v1.0.0"")
				.applyTo(this.environment);
		assertThat(this.locator.locate(this.environment)).isNotNull();
	}
"
"	@Test
	public void sunnyDayWithLabelThatContainsASlash() {
		Environment body = new Environment(""app"", ""master"");
		mockRequestResponseWithLabel(new ResponseEntity<>(body, HttpStatus.OK),
				""release(_)v1.0.0"");
		this.locator.setRestTemplate(this.restTemplate);
		TestPropertyValues.of(""spring.cloud.config.label:release/v1.0.0"")
				.applyTo(this.environment);
		assertThat(this.locator.locate(this.environment)).isNotNull();
	}
"
"	@Test
	public void sunnyDayWithNoSuchLabel() {
		mockRequestResponseWithLabel(
				new ResponseEntity<Void>((Void) null, HttpStatus.NOT_FOUND),
				""nosuchlabel"");
		this.locator.setRestTemplate(this.restTemplate);
		assertThat(this.locator.locate(this.environment)).isNull();
	}
"
"	@Test
	public void failsQuietly() {
		mockRequestResponseWithoutLabel(
				new ResponseEntity<>(""Wah!"", HttpStatus.INTERNAL_SERVER_ERROR));
		this.locator.setRestTemplate(this.restTemplate);
		assertThat(this.locator.locate(this.environment)).isNull();
	}
"
"	@Test
	public void failFast() throws Exception {
		ClientHttpRequestFactory requestFactory = Mockito
				.mock(ClientHttpRequestFactory.class);
		ClientHttpRequest request = Mockito.mock(ClientHttpRequest.class);
		ClientHttpResponse response = Mockito.mock(ClientHttpResponse.class);
		Mockito.when(requestFactory.createRequest(Mockito.any(URI.class),
				Mockito.any(HttpMethod.class))).thenReturn(request);
		RestTemplate restTemplate = new RestTemplate(requestFactory);
		ConfigClientProperties defaults = new ConfigClientProperties(this.environment);
		defaults.setFailFast(true);
		this.locator = new ConfigServicePropertySourceLocator(defaults);
		Mockito.when(request.getHeaders()).thenReturn(new HttpHeaders());
		Mockito.when(request.execute()).thenReturn(response);
		HttpHeaders headers = new HttpHeaders();
		headers.setContentType(MediaType.APPLICATION_JSON);
		Mockito.when(response.getHeaders()).thenReturn(headers);
		Mockito.when(response.getStatusCode())
				.thenReturn(HttpStatus.INTERNAL_SERVER_ERROR);
		Mockito.when(response.getBody())
				.thenReturn(new ByteArrayInputStream(""{}"".getBytes()));
		this.locator.setRestTemplate(restTemplate);
		this.expected
				.expectCause(IsInstanceOf.instanceOf(IllegalArgumentException.class));
		this.expected.expectMessage(""fail fast property is set"");
		this.locator.locate(this.environment);
	}
"
"	@Test
	public void failFastWhenNotFound() throws Exception {
		ClientHttpRequestFactory requestFactory = Mockito
				.mock(ClientHttpRequestFactory.class);
		ClientHttpRequest request = Mockito.mock(ClientHttpRequest.class);
		ClientHttpResponse response = Mockito.mock(ClientHttpResponse.class);
		Mockito.when(requestFactory.createRequest(Mockito.any(URI.class),
				Mockito.any(HttpMethod.class))).thenReturn(request);
		RestTemplate restTemplate = new RestTemplate(requestFactory);
		ConfigClientProperties defaults = new ConfigClientProperties(this.environment);
		defaults.setFailFast(true);
		this.locator = new ConfigServicePropertySourceLocator(defaults);
		Mockito.when(request.getHeaders()).thenReturn(new HttpHeaders());
		Mockito.when(request.execute()).thenReturn(response);
		HttpHeaders headers = new HttpHeaders();
		headers.setContentType(MediaType.APPLICATION_JSON);
		Mockito.when(response.getHeaders()).thenReturn(headers);
		Mockito.when(response.getStatusCode()).thenReturn(HttpStatus.NOT_FOUND);
		Mockito.when(response.getBody())
				.thenReturn(new ByteArrayInputStream("""".getBytes()));
		this.locator.setRestTemplate(restTemplate);
		this.expected
				.expectCause(IsInstanceOf.instanceOf(IllegalArgumentException.class));
		this.expected.expectMessage(""fail fast property is set"");
		this.locator.locate(this.environment);
	}
"
"	@Test
	public void failFastWhenBothPasswordAndAuthorizationPropertiesSet() throws Exception {
		ClientHttpRequestFactory requestFactory = Mockito
				.mock(ClientHttpRequestFactory.class);
		ClientHttpRequest request = Mockito.mock(ClientHttpRequest.class);
		Mockito.when(requestFactory.createRequest(Mockito.any(URI.class),
				Mockito.any(HttpMethod.class))).thenReturn(request);
		ConfigClientProperties defaults = new ConfigClientProperties(this.environment);
		defaults.setFailFast(true);
		defaults.setUsername(""username"");
		defaults.setPassword(""password"");
		defaults.getHeaders().put(AUTHORIZATION, ""Basic dXNlcm5hbWU6cGFzc3dvcmQNCg=="");
		this.locator = new ConfigServicePropertySourceLocator(defaults);
		this.expected.expect(IllegalStateException.class);
		this.expected.expectMessage(
				""Could not locate PropertySource and the fail fast property is set, failing"");
		this.locator.locate(this.environment);
	}
"
"	@Test
	public void interceptorShouldAddHeadersWhenHeadersPropertySet() throws Exception {
		MockClientHttpRequest request = new MockClientHttpRequest();
		ClientHttpRequestExecution execution = Mockito
				.mock(ClientHttpRequestExecution.class);
		byte[] body = new byte[] {};
		Map<String, String> headers = new HashMap<>();
		headers.put(""X-Example-Version"", ""2.1"");
		new ConfigServicePropertySourceLocator.GenericRequestHeaderInterceptor(headers)
				.intercept(request, body, execution);
		Mockito.verify(execution).execute(request, body);
		assertThat(request.getHeaders().getFirst(""X-Example-Version"")).isEqualTo(""2.1"");
	}
"
"	@Test
	public void shouldAddAuthorizationHeaderWhenPasswordSet() {
		HttpHeaders headers = new HttpHeaders();
		ConfigClientProperties defaults = new ConfigClientProperties(this.environment);
		this.locator = new ConfigServicePropertySourceLocator(defaults);
		String username = ""user"";
		String password = ""pass"";
		ReflectionTestUtils.invokeMethod(this.locator, ""addAuthorizationToken"", defaults,
				headers, username, password);
		assertThat(headers).hasSize(1);
	}
"
"	@Test
	public void shouldAddAuthorizationHeaderWhenAuthorizationSet() {
		HttpHeaders headers = new HttpHeaders();
		ConfigClientProperties defaults = new ConfigClientProperties(this.environment);
		defaults.getHeaders().put(AUTHORIZATION, ""Basic dXNlcm5hbWU6cGFzc3dvcmQNCg=="");
		this.locator = new ConfigServicePropertySourceLocator(defaults);
		String username = ""user"";
		String password = null;
		ReflectionTestUtils.invokeMethod(this.locator, ""addAuthorizationToken"", defaults,
				headers, username, password);
		assertThat(headers).hasSize(1);
	}
"
"	@Test
	public void shouldThrowExceptionWhenPasswordAndAuthorizationBothSet() {
		HttpHeaders headers = new HttpHeaders();
		ConfigClientProperties defaults = new ConfigClientProperties(this.environment);
		defaults.getHeaders().put(AUTHORIZATION, ""Basic dXNlcm5hbWU6cGFzc3dvcmQNCg=="");
		this.locator = new ConfigServicePropertySourceLocator(defaults);
		String username = ""user"";
		String password = ""pass"";
		this.expected.expect(IllegalStateException.class);
		this.expected.expectMessage(""You must set either 'password' or 'authorization'"");
		ReflectionTestUtils.invokeMethod(this.locator, ""addAuthorizationToken"", defaults,
				headers, username, password);
	}
"
"	@Test
	public void shouldThrowExceptionWhenNegativeReadTimeoutSet() {
		ConfigClientProperties defaults = new ConfigClientProperties(this.environment);
		defaults.setRequestReadTimeout(-1);
		this.locator = new ConfigServicePropertySourceLocator(defaults);
		this.expected.expect(IllegalStateException.class);
		this.expected.expectMessage(""Invalid Value for Read Timeout set."");
		ReflectionTestUtils.invokeMethod(this.locator, ""getSecureRestTemplate"", defaults);
	}
"
"	@Test
	public void shouldThrowExceptionWhenNegativeConnectTimeoutSet() {
		ConfigClientProperties defaults = new ConfigClientProperties(this.environment);
		defaults.setRequestConnectTimeout(-1);
		this.locator = new ConfigServicePropertySourceLocator(defaults);
		this.expected.expect(IllegalStateException.class);
		this.expected.expectMessage(""Invalid Value for Connect Timeout set."");
		ReflectionTestUtils.invokeMethod(this.locator, ""getSecureRestTemplate"", defaults);
	}
"
"	@Test
	public void checkInterceptorHasNoAuthorizationHeaderPresent() {
		ConfigClientProperties defaults = new ConfigClientProperties(this.environment);
		defaults.getHeaders().put(AUTHORIZATION, ""Basic dXNlcm5hbWU6cGFzc3dvcmQNCg=="");
		defaults.getHeaders().put(""key"", ""value"");
		this.locator = new ConfigServicePropertySourceLocator(defaults);
		RestTemplate restTemplate = ReflectionTestUtils.invokeMethod(this.locator,
				""getSecureRestTemplate"", defaults);
		Iterator<ClientHttpRequestInterceptor> iterator = restTemplate.getInterceptors()
				.iterator();
		while (iterator.hasNext()) {
			GenericRequestHeaderInterceptor genericRequestHeaderInterceptor = (GenericRequestHeaderInterceptor) iterator
					.next();
			assertThat(genericRequestHeaderInterceptor.getHeaders().get(AUTHORIZATION))
					.isEqualTo(null);
		}
	}
"
"	@Test
	public void vanilla() {
		this.locator.setUri(new String[] { ""http://localhost:9999"" });
		this.locator.setPassword(""secret"");
		Credentials credentials = this.locator.getCredentials(0);
		assertThat(credentials.getUri()).isEqualTo(""http://localhost:9999"");
		assertThat(credentials.getUsername()).isEqualTo(""user"");
		assertThat(credentials.getPassword()).isEqualTo(""secret"");
	}
"
"	@Test
	public void uriCreds() {
		this.locator.setUri(new String[] { ""http://foo:bar@localhost:9999"" });
		Credentials credentials = this.locator.getCredentials(0);
		assertThat(credentials.getUri()).isEqualTo(""http://localhost:9999"");
		assertThat(credentials.getUsername()).isEqualTo(""foo"");
		assertThat(credentials.getPassword()).isEqualTo(""bar"");
	}
"
"	@Test
	public void explicitPassword() {
		this.locator.setUri(new String[] { ""http://foo:bar@localhost:9999"" });
		this.locator.setPassword(""secret"");
		Credentials credentials = this.locator.getCredentials(0);
		assertThat(credentials.getUri()).isEqualTo(""http://localhost:9999"");
		assertThat(credentials.getUsername()).isEqualTo(""foo"");
		assertThat(credentials.getPassword()).isEqualTo(""secret"");
	}
"
"	@Test
	public void testIfNoColonPresentInUriCreds() {
		this.locator.setUri(new String[] { ""http://foobar@localhost:9999"" });
		this.locator.setPassword(""secret"");
		Credentials credentials = this.locator.getCredentials(0);
		assertThat(credentials.getUri()).isEqualTo(""http://localhost:9999"");
		assertThat(credentials.getUsername()).isEqualTo(""foobar"");
		assertThat(credentials.getPassword()).isEqualTo(""secret"");
	}
"
"	@Test
	public void testIfColonPresentAtTheEndInUriCreds() {
		this.locator.setUri(new String[] { ""http://foobar:@localhost:9999"" });
		this.locator.setPassword(""secret"");
		Credentials credentials = this.locator.getCredentials(0);
		assertThat(credentials.getUri()).isEqualTo(""http://localhost:9999"");
		assertThat(credentials.getUsername()).isEqualTo(""foobar"");
		assertThat(credentials.getPassword()).isEqualTo(""secret"");
	}
"
"	@Test
	public void testIfColonPresentAtTheStartInUriCreds() {
		this.locator.setUri(new String[] { ""http://:foobar@localhost:9999"" });
		Credentials credentials = this.locator.getCredentials(0);
		assertThat(credentials.getUri()).isEqualTo(""http://localhost:9999"");
		assertThat(credentials.getUsername()).isEqualTo("""");
		assertThat(credentials.getPassword()).isEqualTo(""foobar"");
	}
"
"	@Test
	public void testIfColonPresentAtTheStartAndEndInUriCreds() {
		this.locator.setUri(new String[] { ""http://:foobar:@localhost:9999"" });
		Credentials credentials = this.locator.getCredentials(0);
		assertThat(credentials.getUri()).isEqualTo(""http://localhost:9999"");
		assertThat(credentials.getUsername()).isEqualTo("""");
		assertThat(credentials.getPassword()).isEqualTo(""foobar:"");
	}
"
"	@Test
	public void testIfSpacePresentAsUriCreds() {
		this.locator.setUri(new String[] { ""http://  @localhost:9999"" });
		this.locator.setPassword(""secret"");
		Credentials credentials = this.locator.getCredentials(0);
		assertThat(credentials.getUri()).isEqualTo(""http://localhost:9999"");
		assertThat(credentials.getUsername()).isEqualTo(""  "");
		assertThat(credentials.getPassword()).isEqualTo(""secret"");
	}
"
"	@Test
	public void changeNameInOverride() {
		this.locator.setName(""one"");
		ConfigurableEnvironment environment = new StandardEnvironment();
		TestPropertyValues.of(""spring.application.name:two"").applyTo(environment);
		ConfigClientProperties override = this.locator.override(environment);
		assertThat(override.getName()).isEqualTo(""two"");
	}
"
"	@Test
	public void testThatExplicitUsernamePasswordTakePrecedence() {
		ConfigClientProperties properties = new ConfigClientProperties(
				new MockEnvironment());

		properties.setUri(
				new String[] { ""https://userInfoName:userInfoPW@localhost:8888/"" });
		properties.setUsername(""explicitName"");
		properties.setPassword(""explicitPW"");
		Credentials credentials = properties.getCredentials(0);
		assertThat(credentials.getPassword()).isEqualTo(""explicitPW"");
		assertThat(credentials.getUsername()).isEqualTo(""explicitName"");
	}
"
"	@Test
	public void checkIfExceptionThrownForNegativeIndex() {
		this.locator.setUri(
				new String[] { ""http://localhost:8888"", ""http://localhost:8889"" });
		this.expected.expect(IllegalStateException.class);
		this.expected.expectMessage(""Trying to access an invalid array index"");
		Credentials credentials = this.locator.getCredentials(-1);
	}
"
"	@Test
	public void checkIfExceptionThrownForPositiveInvalidIndex() {
		this.locator.setUri(
				new String[] { ""http://localhost:8888"", ""http://localhost:8889"" });
		this.expected.expect(IllegalStateException.class);
		this.expected.expectMessage(""Trying to access an invalid array index"");
		Credentials credentials = this.locator.getCredentials(3);
	}
"
"	@Test
	public void checkIfExceptionThrownForIndexEqualToLength() {
		this.locator.setUri(
				new String[] { ""http://localhost:8888"", ""http://localhost:8889"" });
		this.expected.expect(IllegalStateException.class);
		this.expected.expectMessage(""Trying to access an invalid array index"");
		Credentials credentials = this.locator.getCredentials(2);
	}
"
"	@Test
	public void withHealthIndicator() {
		ConfigurableApplicationContext context = new SpringApplicationBuilder(
				PropertySourceBootstrapConfiguration.class,
				ConfigServiceBootstrapConfiguration.class)
						.child(ConfigClientAutoConfiguration.class)
						.web(WebApplicationType.NONE).run();
		assertThat(BeanFactoryUtils.beanNamesForTypeIncludingAncestors(context,
				ConfigClientProperties.class).length).isEqualTo(1);
		assertThat(BeanFactoryUtils.beanNamesForTypeIncludingAncestors(context,
				ConfigServerHealthIndicator.class).length).isEqualTo(1);
		context.close();
	}
"
"	@Test(expected = EncryptionTooWeakException.class)
	public void cannotDecryptWithoutKey() {
		this.controller.decrypt(""foo"", MediaType.TEXT_PLAIN);
	}
"
"	@Test(expected = EncryptionTooWeakException.class)
	public void cannotDecryptWithNoopEncryptor() {
		this.controller.decrypt(""foo"", MediaType.TEXT_PLAIN);
	}
"
"	@Test(expected = InvalidCipherException.class)
	public void shouldThrowExceptionOnDecryptInvalidData() {
		this.controller = new EncryptionController(
				new SingleTextEncryptorLocator(new RsaSecretEncryptor()));
		this.controller.decrypt(""foo"", MediaType.TEXT_PLAIN);
	}
"
"	@Test(expected = InvalidCipherException.class)
	public void shouldThrowExceptionOnDecryptWrongKey() {
		RsaSecretEncryptor encryptor = new RsaSecretEncryptor();
		this.controller = new EncryptionController(
				new SingleTextEncryptorLocator(new RsaSecretEncryptor()));
		this.controller.decrypt(encryptor.encrypt(""foo""), MediaType.TEXT_PLAIN);
	}
"
"	@Test
	public void sunnyDayRsaKey() {
		this.controller = new EncryptionController(
				new SingleTextEncryptorLocator(new RsaSecretEncryptor()));
		String cipher = this.controller.encrypt(""foo"", MediaType.TEXT_PLAIN);
		assertThat(this.controller.decrypt(cipher, MediaType.TEXT_PLAIN))
				.isEqualTo(""foo"");
	}
"
"	@Test
	public void publicKey() {
		this.controller = new EncryptionController(
				new SingleTextEncryptorLocator(new RsaSecretEncryptor()));
		String key = this.controller.getPublicKey();
		assertThat(key.startsWith(""ssh-rsa"")).as(""Wrong key format: "" + key).isTrue();
	}
"
"	@Test
	public void appAndProfile() {
		this.controller = new EncryptionController(
				new SingleTextEncryptorLocator(new RsaSecretEncryptor()));
		// Add space to input
		String cipher = this.controller.encrypt(""app"", ""default"", ""foo bar"",
				MediaType.TEXT_PLAIN);
		String decrypt = this.controller.decrypt(""app"", ""default"", cipher,
				MediaType.TEXT_PLAIN);
		assertThat(decrypt).as(""Wrong decrypted plaintext: "" + decrypt)
				.isEqualTo(""foo bar"");
	}
"
"	@Test
	public void formDataIn() {
		this.controller = new EncryptionController(
				new SingleTextEncryptorLocator(new RsaSecretEncryptor()));
		// Add space to input
		String cipher = this.controller.encrypt(""foo bar="",
				MediaType.APPLICATION_FORM_URLENCODED);
		String decrypt = this.controller.decrypt(cipher + ""="",
				MediaType.APPLICATION_FORM_URLENCODED);
		assertThat(decrypt).as(""Wrong decrypted plaintext: "" + decrypt)
				.isEqualTo(""foo bar"");
	}
"
"	@Test
	public void formDataInWithPrefix() {
		this.controller = new EncryptionController(
				new SingleTextEncryptorLocator(new RsaSecretEncryptor()));
		// Add space to input
		String cipher = this.controller.encrypt(""{key:test}foo bar="",
				MediaType.APPLICATION_FORM_URLENCODED);
		String decrypt = this.controller.decrypt(cipher + ""="",
				MediaType.APPLICATION_FORM_URLENCODED);
		assertThat(decrypt).as(""Wrong decrypted plaintext: "" + decrypt)
				.isEqualTo(""foo bar"");
	}
"
"	@Test
	public void prefixStrippedBeforeEncrypt() {
		TextEncryptor encryptor = mock(TextEncryptor.class);
		when(encryptor.encrypt(anyString())).thenReturn(""myEncryptedValue"");

		this.controller = new EncryptionController(
				new SingleTextEncryptorLocator(encryptor));
		this.controller.encrypt(""{key:test}foo"", MediaType.TEXT_PLAIN);

		ArgumentCaptor<String> captor = ArgumentCaptor.forClass(String.class);
		verify(encryptor, atLeastOnce()).encrypt(captor.capture());
		assertThat(captor.getValue()).doesNotContain(""{key:test}"")
				.as(""Prefix must be stripped prior to encrypt"");
	}
"
"	@Test
	public void encryptDecyptTextWithCurlyBrace() {
		this.controller = new EncryptionController(
				new SingleTextEncryptorLocator(new RsaSecretEncryptor()));

		String plain = ""textwith}brace"";
"
"	@Test
	public void addEnvironment() {
		TextEncryptorLocator locator = new TextEncryptorLocator() {

			private RsaSecretEncryptor encryptor = new RsaSecretEncryptor();

			@Override
			public TextEncryptor locate(Map<String, String> keys) {
				return this.encryptor;
			}
"
"	@Test
	public void shouldDecryptEnvironment() {
		// given
		String secret = randomUUID().toString();

		// when
		Environment environment = new Environment(""name"", ""profile"", ""label"");
		environment.add(new PropertySource(""a"", Collections.<Object, Object>singletonMap(
				environment.getName(), ""{cipher}"" + this.textEncryptor.encrypt(secret))));

		// then
		assertThat(this.encryptor.decrypt(environment).getPropertySources().get(0)
				.getSource().get(environment.getName())).isEqualTo(secret);
	}
"
"	@Test
	public void shouldDecryptEnvironmentWithKey() {
		// given
		String secret = randomUUID().toString();

		// when
		Environment environment = new Environment(""name"", ""profile"", ""label"");
		environment.add(new PropertySource(""a"",
				Collections.<Object, Object>singletonMap(environment.getName(),
						""{cipher}{key:test}"" + this.textEncryptor.encrypt(secret))));

		// then
		assertThat(this.encryptor.decrypt(environment).getPropertySources().get(0)
				.getSource().get(environment.getName())).isEqualTo(secret);
	}
"
"	@Test
	public void shouldBeAbleToUseNullAsPropertyValue() {

		// when
		Environment environment = new Environment(""name"", ""profile"", ""label"");
		environment.add(new PropertySource(""a"",
				Collections.<Object, Object>singletonMap(environment.getName(), null)));

		// then
		assertThat(this.encryptor.decrypt(environment).getPropertySources().get(0)
				.getSource().get(environment.getName())).isEqualTo(null);
	}
"
"		@Test
		public void symmetricEncryptionEnabled() throws Exception {
			ResponseEntity<String> entity = this.testRestTemplate
					.getForEntity(""/encrypt/status"", String.class);
			assertThat(entity.getStatusCode()).isEqualTo(HttpStatus.OK);
		}
"
"		@Test
		public void symmetricEncryptionBootstrapConfig() throws Exception {
			ResponseEntity<String> entity = this.testRestTemplate
					.getForEntity(""/encrypt/status"", String.class);
			assertThat(entity.getStatusCode()).isEqualTo(HttpStatus.OK);
		}
"
"		@Test
		public void keystoreBootstrapConfig() throws Exception {
			ResponseEntity<String> entity = this.testRestTemplate
					.getForEntity(""/encrypt/status"", String.class);
			assertThat(entity.getStatusCode()).isEqualTo(HttpStatus.OK);
		}
"
"	@Test
	public void testAddPrefix() {
		assertThat(this.helper.addPrefix(Collections.singletonMap(""bar"", ""spam""), ""foo""))
				.isEqualTo(""{bar:spam}foo"");
	}
"
"	@Test
	public void testAddNoPrefix() {
		assertThat(this.helper.addPrefix(Collections.<String, String>emptyMap(), ""foo""))
				.isEqualTo(""foo"");
	}
"
"	@Test
	public void testStripNoPrefix() {
		assertThat(this.helper.stripPrefix(""foo"")).isEqualTo(""foo"");
	}
"
"	@Test
	public void testStripPrefix() {
		assertThat(this.helper.stripPrefix(""{key:foo}foo"")).isEqualTo(""foo"");
	}
"
"	@Test
	public void testStripPrefixWithEscape() {
		assertThat(this.helper.stripPrefix(""{plain}{key:foo}foo""))
				.isEqualTo(""{key:foo}foo"");
	}
"
"	@Test
	public void testKeysDefaults() {
		Map<String, String> keys = this.helper.getEncryptorKeys(""foo"", ""bar"", ""spam"");
		assertThat(keys.get(""name"")).isEqualTo(""foo"");
		assertThat(keys.get(""profiles"")).isEqualTo(""bar"");
	}
"
"	@Test
	public void testKeysWithPrefix() {
		Map<String, String> keys = this.helper.getEncryptorKeys(""foo"", ""bar"",
				""{key:mykey}foo"");
		assertThat(keys.size()).isEqualTo(3);
		assertThat(keys.get(""key"")).isEqualTo(""mykey"");
	}
"
"	@Test
	public void testKeysWithPrefixAndEscape() {
		Map<String, String> keys = this.helper.getEncryptorKeys(""foo"", ""bar"",
				""{key:mykey}{plain}{foo:bar}foo"");
		assertThat(keys.size()).isEqualTo(3);
		assertThat(keys.get(""key"")).isEqualTo(""mykey"");
	}
"
"	@Test
	public void testTextWithCurlyBracesNoPrefix() {
		assertThat(this.helper.stripPrefix(""textwith}brac{es""))
				.isEqualTo(""textwith}brac{es"");
	}
"
"	@Test
	public void testTextWithCurlyBracesPrefix() {
		assertThat(
				this.helper.stripPrefix(""{key:foo}{name:bar}textwith}brac{es{and}prefix""))
						.isEqualTo(""textwith}brac{es{and}prefix"");
	}
"
"	@Test
	public void shouldEncryptUsingApplicationAndProfiles() {

		this.controller = new EncryptionController(
				new SingleTextEncryptorLocator(Encryptors.text(""application"", ""11"")));

		// when
		String encrypted = this.controller.encrypt(this.application, this.profiles,
				this.data, TEXT_PLAIN);

		// then
		assertThat(this.controller.decrypt(this.application, this.profiles, encrypted,
				TEXT_PLAIN)).isEqualTo(this.data);
	}
"
"	@Test(expected = EncryptionTooWeakException.class)
	public void shouldNotEncryptUsingNoOp() {
		// given
		String application = ""unknown"";

		// when
		this.controller.encrypt(application, this.profiles, this.data, TEXT_PLAIN);

		// then exception is thrown
	}
"
"	@Test(expected = EncryptionTooWeakException.class)
	public void shouldNotDecryptUsingNoOp() {
		// given
		String application = ""unknown"";

		// when
		this.controller.decrypt(application, this.profiles, this.data, TEXT_PLAIN);

		// then exception is thrown
	}
"
"	@Test
	public void testDefaults() {
		TextEncryptor encryptor = this.locator
				.locate(Collections.<String, String>emptyMap());
		assertThat(encryptor.decrypt(encryptor.encrypt(""foo""))).isEqualTo(""foo"");
	}
"
"	@Test
	public void testDifferentKeyDefaultSecret() {
		this.locator.setSecretLocator(new SecretLocator() {

			@Override
			public char[] locate(String secret) {
				assertThat(secret).isEqualTo(""changeme"");
				// The actual secret for ""mykey"" is the same as the keystore password
				return ""letmein"".toCharArray();
			}
"
"	@Test
	public void testDifferentKeyAndSecret() {
		Map<String, String> map = new HashMap<String, String>();
		map.put(""key"", ""mytestkey"");
		map.put(""secret"", ""changeme"");
		TextEncryptor encryptor = this.locator.locate(map);
		assertThat(encryptor.decrypt(encryptor.encrypt(""foo""))).isEqualTo(""foo"");
	}
"
"	@Test
	public void testEscapedPlaceholdersRemoved() {
		assertThat(resolvePlaceholders(this.env, ""\\${abc}"")).isEqualTo(""${abc}"");
		// JSON generated from jackson will be double escaped
		assertThat(resolvePlaceholders(this.env, ""\\\\${abc}"")).isEqualTo(""${abc}"");
	}
"
"	@Test
	public void testCanHandle() {
		assertThat(GitSkipSslValidationCredentialsProvider
				.canHandle(""https://github.com/org/repo"")).as(
						""GitSkipSslValidationCredentialsProvider only handles HTTPS uris"")
						.isTrue();
		assertThat(GitSkipSslValidationCredentialsProvider
				.canHandle(""git@github.com:org/repo"")).as(
						""GitSkipSslValidationCredentialsProvider only handles HTTPS uris"")
						.isFalse();
	}
"
"	@Test
	public void testIsInteractive() {
		assertThat(this.skipSslValidationCredentialsProvider.isInteractive()).as(
				""GitSkipSslValidationCredentialsProvider with no delegate requires no user interaction"")
				.isFalse();
	}
"
"	@Test
	public void testIsInteractiveWithDelegate() {
		this.skipSslValidationCredentialsProvider = new GitSkipSslValidationCredentialsProvider(
				this.mockDelegateCredentialsProvider);

		when(this.mockDelegateCredentialsProvider.isInteractive()).thenReturn(true);

		assertThat(this.skipSslValidationCredentialsProvider.isInteractive()).as(
				""With a delegate provider, isInteractive value depends on the delegate"")
				.isTrue();
	}
"
"	@Test
	public void testSupportsSslFailureInformationalMessage() {
		CredentialItem informationalMessage = new CredentialItem.InformationalMessage(
				""text "" + JGitText.get().sslFailureTrustExplanation + "" more text"");
		assertThat(this.skipSslValidationCredentialsProvider
				.supports(informationalMessage)).as(
						""GitSkipSslValidationCredentialsProvider should always support SSL failure InformationalMessage"")
						.isTrue();

		informationalMessage = new CredentialItem.InformationalMessage(""unrelated"");
		assertThat(this.skipSslValidationCredentialsProvider
				.supports(informationalMessage)).as(
						""GitSkipSslValidationCredentialsProvider should not support unrelated InformationalMessage items"")
						.isFalse();
	}
"
"	@Test
	public void testSupportsSslFailureInformationalMessageWithDelegate() {
		this.skipSslValidationCredentialsProvider = new GitSkipSslValidationCredentialsProvider(
				this.mockDelegateCredentialsProvider);

		testSupportsSslFailureInformationalMessage();
	}
"
"	@Test
	public void testSupportsSslValidationYesNoTypes() {
		CredentialItem yesNoType = new CredentialItem.YesNoType(
				JGitText.get().sslTrustNow);
		assertThat(this.skipSslValidationCredentialsProvider.supports(yesNoType)).as(
				""GitSkipSslValidationCredentialsProvider should always support the trust now YesNoType item"")
				.isTrue();

		yesNoType = new CredentialItem.YesNoType(
				MessageFormat.format(JGitText.get().sslTrustForRepo, ""/a/path.git""));
		assertThat(this.skipSslValidationCredentialsProvider.supports(yesNoType)).as(
				""GitSkipSslValidationCredentialsProvider should always support the trust repo YesNoType item"")
				.isTrue();

		yesNoType = new CredentialItem.YesNoType(JGitText.get().sslTrustAlways);
		assertThat(this.skipSslValidationCredentialsProvider.supports(yesNoType)).as(
				""GitSkipSslValidationCredentialsProvider should always support the trust always YesNoType item"")
				.isTrue();

		yesNoType = new CredentialItem.YesNoType(""unrelated"");
		assertThat(this.skipSslValidationCredentialsProvider.supports(yesNoType)).as(
				""GitSkipSslValidationCredentialsProvider should not support unrelated YesNoType items"")
				.isFalse();
	}
"
"	@Test
	public void testSupportsYesNoTypeWithDelegate() {
		this.skipSslValidationCredentialsProvider = new GitSkipSslValidationCredentialsProvider(
				this.mockDelegateCredentialsProvider);

		testSupportsSslValidationYesNoTypes();
	}
"
"	@Test
	public void testSupportsUnrelatedCredentialItemTypes() {
		CredentialItem usernameCredentialItem = new CredentialItem.Username();

		boolean supportsItems = this.skipSslValidationCredentialsProvider
				.supports(usernameCredentialItem);

		assertThat(supportsItems).as(
				""Credential item types not related to SSL validation skipping should not be supported"")
				.isFalse();
	}
"
"	@Test
	public void testSupportsUnrelatedCredentialItemTypesWithDelegate() {
		this.skipSslValidationCredentialsProvider = new GitSkipSslValidationCredentialsProvider(
				this.mockDelegateCredentialsProvider);
		CredentialItem usernameCredentialItem = new CredentialItem.Username();

		when(this.mockDelegateCredentialsProvider.supports(usernameCredentialItem))
				.thenReturn(true);

		boolean supportsItems = this.skipSslValidationCredentialsProvider
				.supports(usernameCredentialItem);

		assertThat(supportsItems).as(
				""GitSkipSslValidationCredentialsProvider must support the types supported by its delegate CredentialsProvider"")
				.isTrue();
	}
"
"  @Test
  public void testQueries()
      throws Exception {
    String sql = ""SELECT SUM(AirTime), SUM(ArrDelay) FROM mytable"";
    testSqlQuery(sql, Collections.singletonList(sql));
    sql = ""SELECT SUM(AirTime), DaysSinceEpoch FROM mytable GROUP BY DaysSinceEpoch ORDER BY SUM(AirTime) DESC"";
    testSqlQuery(sql, Collections.singletonList(sql));
    sql = ""SELECT Origin, SUM(ArrDelay) FROM mytable WHERE Carrier = 'AA' GROUP BY Origin ORDER BY Origin"";
    testSqlQuery(sql, Collections.singletonList(sql));
  }
"
"  @Test
  public void testPartitionMetadata() {
    int[] numSegmentsForPartition = new int[2];
    String realtimeTableName = TableNameBuilder.REALTIME.tableNameWithType(getTableName());
    List<SegmentZKMetadata> segmentsZKMetadata = _helixResourceManager.getSegmentsZKMetadata(realtimeTableName);
    for (SegmentZKMetadata segmentZKMetadata : segmentsZKMetadata) {
      SegmentPartitionMetadata segmentPartitionMetadata = segmentZKMetadata.getPartitionMetadata();
      assertNotNull(segmentPartitionMetadata);
      Map<String, ColumnPartitionMetadata> columnPartitionMetadataMap =
          segmentPartitionMetadata.getColumnPartitionMap();
      assertEquals(columnPartitionMetadataMap.size(), 1);
      ColumnPartitionMetadata columnPartitionMetadata = columnPartitionMetadataMap.get(PARTITION_COLUMN);
      assertNotNull(columnPartitionMetadata);
      assertTrue(columnPartitionMetadata.getFunctionName().equalsIgnoreCase(""murmur""));
      assertEquals(columnPartitionMetadata.getNumPartitions(), 2);
      int partitionGroupId = new LLCSegmentName(segmentZKMetadata.getSegmentName()).getPartitionGroupId();
      assertEquals(columnPartitionMetadata.getPartitions(), Collections.singleton(partitionGroupId));
      numSegmentsForPartition[partitionGroupId]++;
    }

    // There should be 2 segments for partition 0, 2 segments for partition 1
    assertEquals(numSegmentsForPartition[0], 2);
    assertEquals(numSegmentsForPartition[1], 2);
  }
"
"  @Test(dependsOnMethods = ""testPartitionMetadata"")
  public void testPartitionRouting()
      throws Exception {
    // Query partition 0
    {
      String query = ""SELECT COUNT(*) FROM mytable WHERE DestState = 'CA'"";
      JsonNode response = postQuery(query);

      String queryToCompare = ""SELECT COUNT(*) FROM mytable WHERE DestState BETWEEN 'CA' AND 'CA'"";
      JsonNode responseToCompare = postQuery(queryToCompare);

      // Should only query the segments for partition 0
      assertEquals(response.get(MetadataKey.NUM_SEGMENTS_QUERIED.getName()).asInt(), 2);
      assertEquals(responseToCompare.get(MetadataKey.NUM_SEGMENTS_QUERIED.getName()).asInt(), 4);

      assertEquals(response.get(""aggregationResults"").get(0).get(""value"").asInt(),
          responseToCompare.get(""aggregationResults"").get(0).get(""value"").asInt());
    }

    // Query partition 1
    {
      String query = ""SELECT COUNT(*) FROM mytable WHERE DestState = 'FL'"";
      JsonNode response = postQuery(query);

      String queryToCompare = ""SELECT COUNT(*) FROM mytable WHERE DestState BETWEEN 'FL' AND 'FL'"";
      JsonNode responseToCompare = postQuery(queryToCompare);

      // Should only query the segments for partition 1
      assertEquals(response.get(MetadataKey.NUM_SEGMENTS_QUERIED.getName()).asInt(), 2);
      assertEquals(responseToCompare.get(MetadataKey.NUM_SEGMENTS_QUERIED.getName()).asInt(), 4);

      assertEquals(response.get(""aggregationResults"").get(0).get(""value"").asInt(),
          responseToCompare.get(""aggregationResults"").get(0).get(""value"").asInt());
    }
  }
"
"  @Test(dependsOnMethods = ""testPartitionRouting"")
  public void testNonPartitionedStream()
      throws Exception {
    // Push the second Avro file into Kafka without partitioning
    _partitionColumn = null;
    pushAvroIntoKafka(Collections.singletonList(_avroFiles.get(1)));

    // Wait for all documents loaded
    _countStarResult += NUM_DOCS_IN_SECOND_AVRO_FILE;
    waitForAllDocsLoaded(600_000L);

    // Check partition metadata
    int[] numSegmentsForPartition = new int[2];
    String realtimeTableName = TableNameBuilder.REALTIME.tableNameWithType(getTableName());
    List<SegmentZKMetadata> segmentsZKMetadata = _helixResourceManager.getSegmentsZKMetadata(realtimeTableName);
    for (SegmentZKMetadata segmentZKMetadata : segmentsZKMetadata) {
      SegmentPartitionMetadata segmentPartitionMetadata = segmentZKMetadata.getPartitionMetadata();
      assertNotNull(segmentPartitionMetadata);
      Map<String, ColumnPartitionMetadata> columnPartitionMetadataMap =
          segmentPartitionMetadata.getColumnPartitionMap();
      assertEquals(columnPartitionMetadataMap.size(), 1);
      ColumnPartitionMetadata columnPartitionMetadata = columnPartitionMetadataMap.get(PARTITION_COLUMN);
      assertNotNull(columnPartitionMetadata);
      assertTrue(columnPartitionMetadata.getFunctionName().equalsIgnoreCase(""murmur""));
      assertEquals(columnPartitionMetadata.getNumPartitions(), 2);
      int partitionGroupId = new LLCSegmentName(segmentZKMetadata.getSegmentName()).getPartitionGroupId();
      numSegmentsForPartition[partitionGroupId]++;

      if (segmentZKMetadata.getStatus() == Status.IN_PROGRESS) {
        // For consuming segment, the partition metadata should only contain the stream partition
        assertEquals(columnPartitionMetadata.getPartitions(), Collections.singleton(partitionGroupId));
      } else {
        LLCSegmentName llcSegmentName = new LLCSegmentName(segmentZKMetadata.getSegmentName());
        int sequenceNumber = llcSegmentName.getSequenceNumber();
        if (sequenceNumber == 0) {
          // The partition metadata for the first completed segment should only contain the stream partition
          assertEquals(columnPartitionMetadata.getPartitions(), Collections.singleton(partitionGroupId));
        } else {
          // The partition metadata for the new completed segments should contain both partitions
          assertEquals(columnPartitionMetadata.getPartitions(), new HashSet<>(Arrays.asList(0, 1)));
        }
      }
    }

    // There should be 4 segments for partition 0, 4 segments for partition 1
    assertEquals(numSegmentsForPartition[0], 4);
    assertEquals(numSegmentsForPartition[1], 4);

    // Check partition routing
    int numSegments = segmentsZKMetadata.size();

    // Query partition 0
    {
      String query = ""SELECT COUNT(*) FROM mytable WHERE DestState = 'CA'"";
      JsonNode response = postQuery(query);

      String queryToCompare = ""SELECT COUNT(*) FROM mytable WHERE DestState BETWEEN 'CA' AND 'CA'"";
      JsonNode responseToCompare = postQuery(queryToCompare);

      // Should skip the first completed segments and the consuming segment for partition 1
      assertEquals(response.get(MetadataKey.NUM_SEGMENTS_QUERIED.getName()).asInt(), numSegments - 2);
      assertEquals(responseToCompare.get(MetadataKey.NUM_SEGMENTS_QUERIED.getName()).asInt(), numSegments);

      // The result won't match because the consuming segment for partition 1 is pruned out
    }

    // Query partition 1
    {
      String query = ""SELECT COUNT(*) FROM mytable WHERE DestState = 'FL'"";
      JsonNode response = postQuery(query);

      String queryToCompare = ""SELECT COUNT(*) FROM mytable WHERE DestState BETWEEN 'FL' AND 'FL'"";
      JsonNode responseToCompare = postQuery(queryToCompare);

      // Should skip the first completed segments and the consuming segment for partition 0
      assertEquals(response.get(MetadataKey.NUM_SEGMENTS_QUERIED.getName()).asInt(), numSegments - 2);
      assertEquals(responseToCompare.get(MetadataKey.NUM_SEGMENTS_QUERIED.getName()).asInt(), numSegments);

      // The result won't match because the consuming segment for partition 0 is pruned out
    }

    // Push the third Avro file into Kafka with partitioning
    _partitionColumn = PARTITION_COLUMN;
    pushAvroIntoKafka(Collections.singletonList(_avroFiles.get(2)));

    // Wait for all documents loaded
    _countStarResult += NUM_DOCS_IN_THIRD_AVRO_FILE;
    waitForAllDocsLoaded(600_000L);

    // Check partition metadata
    numSegmentsForPartition = new int[2];
    segmentsZKMetadata = _helixResourceManager.getSegmentsZKMetadata(realtimeTableName);
    for (SegmentZKMetadata segmentZKMetadata : segmentsZKMetadata) {
      SegmentPartitionMetadata segmentPartitionMetadata = segmentZKMetadata.getPartitionMetadata();
      assertNotNull(segmentPartitionMetadata);
      Map<String, ColumnPartitionMetadata> columnPartitionMetadataMap =
          segmentPartitionMetadata.getColumnPartitionMap();
      assertEquals(columnPartitionMetadataMap.size(), 1);
      ColumnPartitionMetadata columnPartitionMetadata = columnPartitionMetadataMap.get(PARTITION_COLUMN);
      assertNotNull(columnPartitionMetadata);
      assertTrue(columnPartitionMetadata.getFunctionName().equalsIgnoreCase(""murmur""));
      assertEquals(columnPartitionMetadata.getNumPartitions(), 2);
      int partitionGroupId = new LLCSegmentName(segmentZKMetadata.getSegmentName()).getPartitionGroupId();
      numSegmentsForPartition[partitionGroupId]++;

      if (segmentZKMetadata.getStatus() == Status.IN_PROGRESS) {
        // For consuming segment, the partition metadata should only contain the stream partition
        assertEquals(columnPartitionMetadata.getPartitions(), Collections.singleton(partitionGroupId));
      } else {
        // The partition metadata for the new completed segments should only contain the stream partition
        LLCSegmentName llcSegmentName = new LLCSegmentName(segmentZKMetadata.getSegmentName());
        int sequenceNumber = llcSegmentName.getSequenceNumber();
        if (sequenceNumber == 0 || sequenceNumber >= 4) {
          // The partition metadata for the first and new completed segments should only contain the stream partition
          assertEquals(columnPartitionMetadata.getPartitions(), Collections.singleton(partitionGroupId));
        } else {
          // The partition metadata for the completed segments containing records from the second Avro file should
          // contain both partitions
          assertEquals(columnPartitionMetadata.getPartitions(), new HashSet<>(Arrays.asList(0, 1)));
        }
      }
    }

    // There should be 6 segments for partition 0, 6 segments for partition 1
    assertEquals(numSegmentsForPartition[0], 6);
    assertEquals(numSegmentsForPartition[1], 6);

    // Check partition routing
    numSegments = segmentsZKMetadata.size();

    // Query partition 0
    {
      String query = ""SELECT COUNT(*) FROM mytable WHERE DestState = 'CA'"";
      JsonNode response = postQuery(query);

      String queryToCompare = ""SELECT COUNT(*) FROM mytable WHERE DestState BETWEEN 'CA' AND 'CA'"";
      JsonNode responseToCompare = postQuery(queryToCompare);

      // Should skip 2 completed segments and the consuming segment for partition 1
      assertEquals(response.get(MetadataKey.NUM_SEGMENTS_QUERIED.getName()).asInt(), numSegments - 3);
      assertEquals(responseToCompare.get(MetadataKey.NUM_SEGMENTS_QUERIED.getName()).asInt(), numSegments);

      // The result should match again after all the segments with the non-partitioning records are committed
      assertEquals(response.get(""aggregationResults"").get(0).get(""value"").asInt(),
          responseToCompare.get(""aggregationResults"").get(0).get(""value"").asInt());
    }

    // Query partition 1
    {
      String query = ""SELECT COUNT(*) FROM mytable WHERE DestState = 'FL'"";
      JsonNode response = postQuery(query);

      String queryToCompare = ""SELECT COUNT(*) FROM mytable WHERE DestState BETWEEN 'FL' AND 'FL'"";
      JsonNode responseToCompare = postQuery(queryToCompare);

      // Should skip 2 completed segments and the consuming segment for partition 0
      assertEquals(response.get(MetadataKey.NUM_SEGMENTS_QUERIED.getName()).asInt(), numSegments - 3);
      assertEquals(responseToCompare.get(MetadataKey.NUM_SEGMENTS_QUERIED.getName()).asInt(), numSegments);

      // The result should match again after all the segments with the non-partitioning records are committed
      assertEquals(response.get(""aggregationResults"").get(0).get(""value"").asInt(),
          responseToCompare.get(""aggregationResults"").get(0).get(""value"").asInt());
    }
  }
"
"  @Test
  public void testTotalCount()
      throws Exception {
    String query = ""SELECT count(*) FROM "" + getTableName();
    testQuery(query, Collections.singletonList(query));
  }
"
"  @Test
  public void testCountWithNullDescription()
      throws Exception {
    String query = ""SELECT count(*) FROM "" + getTableName() + "" where description IS NOT NULL"";
    testQuery(query, Collections.singletonList(query));
  }
"
"  @Test
  public void testCountWithNullDescriptionAndSalary()
      throws Exception {
    String query = ""SELECT count(*) FROM "" + getTableName() + "" where description IS NOT NULL AND salary IS NOT NULL"";
    testQuery(query, Collections.singletonList(query));
  }
"
"  @Test(enabled = false)
  public void testShortZookeeperFreeze()
      throws Exception {
    testFreezeZookeeper(10000L);
  }
"
"  @Test(enabled = false)
  public void testLongZookeeperFreeze()
      throws Exception {
    testFreezeZookeeper(60000L);
  }
"
"  @Test
  public void testQueriesFromQueryFile()
      throws Exception {
    super.testQueriesFromQueryFile();
  }
"
"  @Test
  public void testGeneratedQueriesWithMultiValues()
      throws Exception {
    super.testGeneratedQueriesWithMultiValues();
  }
"
"  @Test
  public void testDictionaryBasedQueries()
      throws Exception {

    // Dictionary columns
    // int
    testDictionaryBasedFunctions(""NASDelay"");

    // long
    testDictionaryBasedFunctions(""AirlineID"");

    // double
    testDictionaryBasedFunctions(""ArrDelayMinutes"");

    // float
    testDictionaryBasedFunctions(""DepDelayMinutes"");

    // Non Dictionary columns
    // int
    testDictionaryBasedFunctions(""ActualElapsedTime"");

    // double
    testDictionaryBasedFunctions(""DepDelay"");

    // float
    testDictionaryBasedFunctions(""ArrDelay"");
  }
"
"  @Test
  public void testQueryExceptions()
      throws Exception {
    super.testQueryExceptions();
  }
"
"  @Test
  public void testInstanceShutdown()
      throws Exception {
    super.testInstanceShutdown();
  }
"
"  @Test
  public void testHardcodedSqlQueries()
      throws Exception {
    super.testHardcodedSqlQueries();
  }
"
"  @Test
  public void testSqlQueriesFromQueryFile()
      throws Exception {
    super.testSqlQueriesFromQueryFile();
  }
"
"  @Test
  public void testSingleLevelConcat()
      throws Exception {
    // The original segments are time partitioned by month:
    // segmentName (totalDocs)
    // myTable1_16071_16101_3 (9746)
    // myTable1_16102_16129_4 (8690)
    // myTable1_16130_16159_5 (9621)
    // myTable1_16160_16189_6 (9454)
    // myTable1_16190_16220_7 (10329)
    // myTable1_16221_16250_8 (10468)
    // myTable1_16251_16281_9 (10499)
    // myTable1_16282_16312_10 (10196)
    // myTable1_16313_16342_11 (9136)
    // myTable1_16343_16373_0 (9292)
    // myTable1_16374_16404_1 (8736)
    // myTable1_16405_16435_2 (9378)

    // Expected merge tasks and result segments:
    // 1.
    //    {myTable1_16071_16101_3}
    //      -> {merged_100days_T1_0_myTable1_16071_16099_0, merged_100days_T1_0_myTable1_16100_16101_1}
    // 2.
    //    {merged_100days_T1_0_myTable1_16100_16101_1, myTable1_16102_16129_4, myTable1_16130_16159_5}
    //      -> {merged_100days_T2_0_myTable1_16100_???_0(15000), merged_100days_T2_0_myTable1_???_16159_1}
    //    {myTable1_16160_16189_6, myTable1_16190_16220_7}
    //      -> {merged_100days_T2_1_myTable1_16160_16199_0, merged_100days_T2_1_myTable1_16200_16220_1}
    // 3.
    //    {merged_100days_T2_1_myTable1_16200_16220_1, myTable1_16221_16250_8}
    //      -> {merged_100days_T3_0_myTable1_16200_???_0(15000), merged_100days_T3_0_myTable1_???_16250_1}
    //    {myTable1_16251_16281_9, myTable1_16282_16312_10}
    //      -> {merged_100days_T3_1_myTable1_16251_???_0(15000), merged_100days_T3_1_myTable1_???_16299_1,
    //      merged_100days_T3_1_myTable1_16300_16312_2}
    // 4.
    //    {merged_100days_T3_1_myTable1_16300_16312_2, myTable1_16313_16342_11, myTable1_16343_16373_0}
    //      -> {merged_100days_T4_0_myTable1_16300_???_0(15000), merged_100days_T4_0_myTable1_???_16373_1}
    //    {myTable1_16374_16404_1}
    //      -> {merged_100days_T4_1_16374_16399_0, merged_100days_T4_1_16400_16404_1}
    // 5.
    //    {merged_100days_T4_1_16400_16404_1, myTable1_16405_16435_2}
    //      -> {merged_100days_T5_0_myTable1_16400_16435_0}

    String sqlQuery = ""SELECT count(*) FROM myTable1""; // 115545 rows for the test table
    JsonNode expectedJson = postSqlQuery(sqlQuery, _brokerBaseApiUrl);
    int[] expectedNumSubTasks = {1, 2, 2, 2, 1};
    int[] expectedNumSegmentsQueried = {13, 12, 13, 13, 12};
    long expectedWatermark = 16000 * 86_400_000L;
    String offlineTableName = TableNameBuilder.OFFLINE.tableNameWithType(SINGLE_LEVEL_CONCAT_TEST_TABLE);
    int numTasks = 0;
    for (String tasks = _taskManager.scheduleTasks(offlineTableName).get(MinionConstants.MergeRollupTask.TASK_TYPE);
        tasks != null; tasks =
        _taskManager.scheduleTasks(offlineTableName).get(MinionConstants.MergeRollupTask.TASK_TYPE), numTasks++) {
      assertEquals(_helixTaskResourceManager.getTaskConfigs(tasks).size(), expectedNumSubTasks[numTasks]);
      assertTrue(_helixTaskResourceManager.getTaskQueues()
          .contains(PinotHelixTaskResourceManager.getHelixJobQueueName(MinionConstants.MergeRollupTask.TASK_TYPE)));
      // Will not schedule task if there's incomplete task
      assertNull(
          _taskManager.scheduleTasks(offlineTableName).get(MinionConstants.RealtimeToOfflineSegmentsTask.TASK_TYPE));
      waitForTaskToComplete();

      // Check watermark
      MergeRollupTaskMetadata minionTaskMetadata = MergeRollupTaskMetadata
          .fromZNRecord(_taskManager.getClusterInfoAccessor().getMinionMergeRollupTaskZNRecord(offlineTableName));
      assertNotNull(minionTaskMetadata);
      assertEquals((long) minionTaskMetadata.getWatermarkMap().get(""100days""), expectedWatermark);
      expectedWatermark += 100 * 86_400_000L;

      // Check metadata of merged segments
      for (SegmentZKMetadata metadata : _pinotHelixResourceManager.getSegmentsZKMetadata(offlineTableName)) {
        if (metadata.getSegmentName().startsWith(""merged"")) {
          // Check merged segment zk metadata
          assertNotNull(metadata.getCustomMap());
          assertEquals(""100days"",
              metadata.getCustomMap().get(MinionConstants.MergeRollupTask.SEGMENT_ZK_METADATA_MERGE_LEVEL_KEY));
          // Check merged segments are time partitioned
          assertEquals(metadata.getEndTimeMs() / (86_400_000L * 100), metadata.getStartTimeMs() / (86_400_000L * 100));
        }
      }

      // Check num total doc of merged segments are the same as the original segments
      JsonNode actualJson = postSqlQuery(sqlQuery, _brokerBaseApiUrl);
      SqlResultComparator.areEqual(actualJson, expectedJson, sqlQuery);
      // Check query routing
      int numSegmentsQueried = actualJson.get(""numSegmentsQueried"").asInt();
      assertEquals(numSegmentsQueried, expectedNumSegmentsQueried[numTasks]);
    }
    // Check total tasks
    assertEquals(numTasks, 5);

    assertTrue(_controllerStarter.getControllerMetrics()
        .containsGauge(""mergeRollupTaskDelayInNumBuckets.myTable1_OFFLINE.100days""));

    // Drop the table
    dropOfflineTable(SINGLE_LEVEL_CONCAT_TEST_TABLE);

    // Check if the task metadata is cleaned up on table deletion
    verifyTableDelete(offlineTableName);
  }
"
"  @Test
  public void testSingleLevelRollup()
      throws Exception {
    // The original segments are time partitioned by month:
    // segmentName (totalDocs)
    // myTable2_16071_16101_3_1, myTable2_16071_16101_3_2 (9746)
    // myTable2_16102_16129_4_1, myTable2_16102_16129_4_2 (8690)
    // myTable2_16130_16159_5_1, myTable2_16130_16159_5_2 (9621)
    // myTable2_16160_16189_6_1, myTable2_16160_16189_6_2 (9454)
    // myTable2_16190_16220_7_1, myTable2_16190_16220_7_2 (10329)
    // myTable2_16221_16250_8_1, myTable2_16221_16250_8_2 (10468)
    // myTable2_16251_16281_9_1, myTable2_16251_16281_9_2 (10499)
    // myTable2_16282_16312_10_1, myTable2_16282_16312_10_2 (10196)
    // myTable2_16313_16342_11_1, myTable2_16313_16342_11_2 (9136)
    // myTable2_16343_16373_0_1, myTable2_16343_16373_0_2 (9292)
    // myTable2_16374_16404_1_1, myTable2_16374_16404_1_2 (8736)
    // myTable2_16405_16435_2_1, myTable2_16405_16435_2_2 (9378)

    // Expected merge tasks and result segments:
    // 1.
    //    {myTable2_16071_16101_3_1, myTable2_16071_16101_3_2, myTable2_16102_16129_4_1, myTable2_16102_16129_4_2,
    //     myTable2_16130_16159_5_1, myTable2_16130_16159_5_2, myTable2_16160_16189_6_1, myTable2_16160_16189_6_2
    //     myTable2_16190_16220_7}
    //      -> {merged_150days_T1_0_myTable2_16065_16198_0, merged_150days_T1_0_myTable2_16205_16219_1}
    // 2.
    //    {merged_150days_T1_0_myTable2_16205_16219_1, myTable2_16221_16250_8_1, myTable2_16221_16250_8_2,
    //     myTable2_16251_16281_9_1, myTable2_16251_16281_9_2, myTable2_16282_16312_10_1
    //     myTable2_16282_16312_10_2, myTable2_16313_16342_11_1, myTable2_16313_16342_11_2,
    //     myTable2_16343_16373_0_1, myTable2_16343_16373_0_2}
    //      -> {merged_150days_1628644088146_0_myTable2_16205_16345_0,
    //          merged_150days_1628644088146_0_myTable2_16352_16373_1}
    // 3.
    //    {merged_150days_1628644088146_0_myTable2_16352_16373_1, myTable2_16374_16404_1_1, myTable2_16374_16404_1_2
    //     myTable2_16405_16435_2_1, myTable2_16405_16435_2_2}
    //      -> {merged_150days_1628644105127_0_myTable2_16352_16429_0}

    String sqlQuery = ""SELECT count(*) FROM myTable2""; // 115545 rows for the test table
    JsonNode expectedJson = postSqlQuery(sqlQuery, _brokerBaseApiUrl);
    int[] expectedNumSegmentsQueried = {16, 7, 3};
    long expectedWatermark = 16050 * 86_400_000L;
    String offlineTableName = TableNameBuilder.OFFLINE.tableNameWithType(SINGLE_LEVEL_ROLLUP_TEST_TABLE);
    int numTasks = 0;
    for (String tasks = _taskManager.scheduleTasks(offlineTableName).get(MinionConstants.MergeRollupTask.TASK_TYPE);
        tasks != null; tasks =
        _taskManager.scheduleTasks(offlineTableName).get(MinionConstants.MergeRollupTask.TASK_TYPE), numTasks++) {
      assertEquals(_helixTaskResourceManager.getTaskConfigs(tasks).size(), 1);
      assertTrue(_helixTaskResourceManager.getTaskQueues()
          .contains(PinotHelixTaskResourceManager.getHelixJobQueueName(MinionConstants.MergeRollupTask.TASK_TYPE)));
      // Will not schedule task if there's incomplete task
      assertNull(
          _taskManager.scheduleTasks(offlineTableName).get(MinionConstants.RealtimeToOfflineSegmentsTask.TASK_TYPE));
      waitForTaskToComplete();

      // Check watermark
      MergeRollupTaskMetadata minionTaskMetadata = MergeRollupTaskMetadata
          .fromZNRecord(_taskManager.getClusterInfoAccessor().getMinionMergeRollupTaskZNRecord(offlineTableName));
      assertNotNull(minionTaskMetadata);
      assertEquals((long) minionTaskMetadata.getWatermarkMap().get(""150days""), expectedWatermark);
      expectedWatermark += 150 * 86_400_000L;

      // Check metadata of merged segments
      for (SegmentZKMetadata metadata : _pinotHelixResourceManager.getSegmentsZKMetadata(offlineTableName)) {
        if (metadata.getSegmentName().startsWith(""merged"")) {
          // Check merged segment zk metadata
          assertNotNull(metadata.getCustomMap());
          assertEquals(""150days"",
              metadata.getCustomMap().get(MinionConstants.MergeRollupTask.SEGMENT_ZK_METADATA_MERGE_LEVEL_KEY));
          // Check merged segments are time partitioned
          assertEquals(metadata.getEndTimeMs() / (86_400_000L * 150), metadata.getStartTimeMs() / (86_400_000L * 150));
        }
      }

      // Check total doc of merged segments are less than the original segments
      JsonNode actualJson = postSqlQuery(sqlQuery, _brokerBaseApiUrl);
      assertTrue(
          actualJson.get(""resultTable"").get(""rows"").get(0).get(0).asInt() < expectedJson.get(""resultTable"").get(""rows"")
              .get(0).get(0).asInt());
      // Check query routing
      int numSegmentsQueried = actualJson.get(""numSegmentsQueried"").asInt();
      assertEquals(numSegmentsQueried, expectedNumSegmentsQueried[numTasks]);
    }

    // Check total doc is half of the original after all merge tasks are finished
    JsonNode actualJson = postSqlQuery(sqlQuery, _brokerBaseApiUrl);
    assertEquals(actualJson.get(""resultTable"").get(""rows"").get(0).get(0).asInt(),
        expectedJson.get(""resultTable"").get(""rows"").get(0).get(0).asInt() / 2);
    // Check time column is rounded
    JsonNode responseJson =
        postSqlQuery(""SELECT count(*), DaysSinceEpoch FROM myTable2 GROUP BY DaysSinceEpoch ORDER BY DaysSinceEpoch"");
    for (int i = 0; i < responseJson.get(""resultTable"").get(""rows"").size(); i++) {
      int daysSinceEpoch = responseJson.get(""resultTable"").get(""rows"").get(i).get(1).asInt();
      assertTrue(daysSinceEpoch % 7 == 0);
    }
    // Check total tasks
    assertEquals(numTasks, 3);

    assertTrue(_controllerStarter.getControllerMetrics()
        .containsGauge(""mergeRollupTaskDelayInNumBuckets.myTable2_OFFLINE.150days""));
  }
"
"  @Test
  public void testMultiLevelConcat()
      throws Exception {
    // The original segments are time partitioned by month:
    // segmentName (totalDocs)
    // myTable3_16071_16101_3 (9746)
    // myTable3_16102_16129_4 (8690)
    // myTable3_16130_16159_5 (9621)
    // myTable3_16160_16189_6 (9454)
    // myTable3_16190_16220_7 (10329)
    // myTable3_16221_16250_8 (10468)
    // myTable3_16251_16281_9 (10499)
    // myTable3_16282_16312_10 (10196)
    // myTable3_16313_16342_11 (9136)
    // myTable3_16343_16373_0 (9292)
    // myTable3_16374_16404_1 (8736)
    // myTable3_16405_16435_2 (9378)

    // Expected merge tasks and results:
    // 1.
    //    45days: {myTable3_16071_16101_3, myTable3_16102_16129_4}
    //      -> {merged_45days_T1_0_myTable3_16071_16109_0, merged_45days_T1_0_myTable3_16110_16129_1}
    //    watermark: {45days: 16065, 90days: null}
    // 2.
    //    45days: {merged_45days_T1_0_myTable3_16110_16129_1, myTable3_16130_16159_5}
    //      -> {merged_45days_T2_0_myTable3_16110_16154_0, merged_45days_T2_0_myTable3_16155_16159_1}
    //    90days: {merged_45days_T1_0_myTable3_16071_16109_0}
    //      -> {merged_90days_T2_0_myTable3_16071_16109_0}
    //    watermark: {45days: 16110, 90days: 16020}
    // 3.
    //    45days: {merged_45days_T2_0_myTable3_16155_16159_1, myTable3_16160_16189_6, myTable3_16190_16220_7}
    //      -> {merged_45days_T3_0_myTable3_16155_16199_0, merged_45days_T3_0_myTable3_16200_16220_1}
    //    watermark: {45days: 16155, 90days: 16020}
    // 4.
    //    45days: {merged_45days_T3_-_myTable3_16200_16220_1, myTable3_16221_16250_8}
    //      -> {merged_45days_T4_0_myTable3_16200_16244_0, merged_45days_T4_0_myTable3_16245_16250_1}
    //    90days: {merged_45days_T2_0_myTable3_16110_16154_0, merged_45days_T3_0_myTable3_16155_16199_0}
    //      -> {merged_90days_T4_0_myTable3_16110_16199_0}
    //    watermark: {45days: 16200, 90days: 16110}
    // 5.
    //    45days: {merged_45days_T4_0_myTable3_16245_16250_1, myTable3_16251_16281_9, myTable3_16282_16312_10}
    //      -> {merged_45days_T5_0_myTable3_16245_16289_0, merged_45days_T5_0_myTable3_16290_16312_1}
    //    watermark: {45days: 16245, 90days: 16110}
    // 6.
    //    45days: {merged_45days_T5_0_myTable3_16290_16312_1, myTable3_16313_16342_11}
    //      -> {merged_45days_T6_0_myTable3_16290_16334_0, merged_45days_T6_0_myTable3_16335_16342_1}
    //    90days: {merged_45days_T4_0_myTable3_16200_16244_0, merged_45days_T5_0_myTable3_16245_16289_0}
    //      -> {merged_90days_T6_0_myTable3_16200_16289_0}
    //    watermark: {45days: 16290, 90days: 16200}
    // 7.
    //    45days: {merged_45days_T6_0_myTable3_16335_16342_1, myTable_16343_16373_0, myTable_16374_16404_1}
    //      -> {merged_45days_T7_0_myTable3_16335_16379_0, merged_45days_T7_0_myTable3_16380_16404_1}
    //    watermark: {45days: 16335, 90days: 16200}
    // 8.
    //    45days: {merged_45days_T7_0_myTable3_16380_16404_1, myTable3_16405_16435_2}
    //      -> {merged_45days_T8_0_myTable3_16380_16424_0, merged_45days_T8_1_myTable3_16425_16435_1}
    //    90days: {merged_45days_T6_0_myTable3_16290_16334_0, merged_45days_T7_0_myTable3_16335_16379_0}
    //      -> {merged_90days_T8_0_myTable3_16290_16379_0}
    //    watermark: {45days:16380, 90days: 16290}
    // 9.
    //    45days: no segment left, not scheduling
    //    90days: [16380, 16470) is not a valid merge window because windowEndTime > 45days watermark, not scheduling

    String sqlQuery = ""SELECT count(*) FROM myTable3""; // 115545 rows for the test table
    JsonNode expectedJson = postSqlQuery(sqlQuery, _brokerBaseApiUrl);
    int[] expectedNumSubTasks = {1, 2, 1, 2, 1, 2, 1, 2, 1};
    int[] expectedNumSegmentsQueried = {12, 12, 11, 10, 9, 8, 7, 6, 5};
    Long[] expectedWatermarks45Days = {16065L, 16110L, 16155L, 16200L, 16245L, 16290L, 16335L, 16380L};
    Long[] expectedWatermarks90Days = {null, 16020L, 16020L, 16110L, 16110L, 16200L, 16200L, 16290L};
    for (int i = 0; i < expectedWatermarks45Days.length; i++) {
      expectedWatermarks45Days[i] *= 86_400_000L;
    }
    for (int i = 1; i < expectedWatermarks90Days.length; i++) {
      expectedWatermarks90Days[i] *= 86_400_000L;
    }

    String offlineTableName = TableNameBuilder.OFFLINE.tableNameWithType(MULTI_LEVEL_CONCAT_TEST_TABLE);
    int numTasks = 0;
    for (String tasks = _taskManager.scheduleTasks(offlineTableName).get(MinionConstants.MergeRollupTask.TASK_TYPE);
        tasks != null; tasks =
        _taskManager.scheduleTasks(offlineTableName).get(MinionConstants.MergeRollupTask.TASK_TYPE), numTasks++) {
      assertEquals(_helixTaskResourceManager.getTaskConfigs(tasks).size(), expectedNumSubTasks[numTasks]);
      assertTrue(_helixTaskResourceManager.getTaskQueues()
          .contains(PinotHelixTaskResourceManager.getHelixJobQueueName(MinionConstants.MergeRollupTask.TASK_TYPE)));
      // Will not schedule task if there's incomplete task
      assertNull(
          _taskManager.scheduleTasks(offlineTableName).get(MinionConstants.RealtimeToOfflineSegmentsTask.TASK_TYPE));
      waitForTaskToComplete();

      // Check watermark
      MergeRollupTaskMetadata minionTaskMetadata = MergeRollupTaskMetadata
          .fromZNRecord(_taskManager.getClusterInfoAccessor().getMinionMergeRollupTaskZNRecord(offlineTableName));
      assertNotNull(minionTaskMetadata);
      assertEquals(minionTaskMetadata.getWatermarkMap().get(""45days""), expectedWatermarks45Days[numTasks]);
      assertEquals(minionTaskMetadata.getWatermarkMap().get(""90days""), expectedWatermarks90Days[numTasks]);

      // Check metadata of merged segments
      for (SegmentZKMetadata metadata : _pinotHelixResourceManager.getSegmentsZKMetadata(offlineTableName)) {
        if (metadata.getSegmentName().startsWith(""merged"")) {
          // Check merged segment zk metadata
          assertNotNull(metadata.getCustomMap());
          if (metadata.getSegmentName().startsWith(""merged_45days"")) {
            assertEquals(""45days"",
                metadata.getCustomMap().get(MinionConstants.MergeRollupTask.SEGMENT_ZK_METADATA_MERGE_LEVEL_KEY));
            assertEquals(metadata.getEndTimeMs() / (86_400_000L * 45), metadata.getStartTimeMs() / (86_400_000L * 45));
          }
          if (metadata.getSegmentName().startsWith(""merged_90days"")) {
            assertEquals(""90days"",
                metadata.getCustomMap().get(MinionConstants.MergeRollupTask.SEGMENT_ZK_METADATA_MERGE_LEVEL_KEY));
            assertEquals(metadata.getEndTimeMs() / (86_400_000L * 90), metadata.getStartTimeMs() / (86_400_000L * 90));
          }
        }
      }

      // Check total doc of merged segments are the same as the original segments
      JsonNode actualJson = postSqlQuery(sqlQuery, _brokerBaseApiUrl);
      SqlResultComparator.areEqual(actualJson, expectedJson, sqlQuery);
      // Check query routing
      int numSegmentsQueried = actualJson.get(""numSegmentsQueried"").asInt();
      assertEquals(numSegmentsQueried, expectedNumSegmentsQueried[numTasks]);
    }
    // Check total tasks
    assertEquals(numTasks, 8);

    assertTrue(_controllerStarter.getControllerMetrics()
        .containsGauge(""mergeRollupTaskDelayInNumBuckets.myTable3_OFFLINE.45days""));
    assertTrue(_controllerStarter.getControllerMetrics()
        .containsGauge(""mergeRollupTaskDelayInNumBuckets.myTable3_OFFLINE.90days""));
  }
"
"  @Test
  public void testInstancesStarted() {
    assertEquals(_serviceStatusCallbacks.size(), getNumBrokers() + getNumServers());
    for (ServiceStatus.ServiceStatusCallback serviceStatusCallback : _serviceStatusCallbacks) {
      assertEquals(serviceStatusCallback.getServiceStatus(), ServiceStatus.Status.GOOD);
    }
  }
"
"  @Test
  public void testInvalidTableConfig() {
    TableConfig tableConfig = new TableConfigBuilder(TableType.OFFLINE).setTableName(""badTable"").build();
    ObjectNode tableConfigJson = (ObjectNode) tableConfig.toJsonNode();
    // Remove a mandatory field
    tableConfigJson.remove(TableConfig.VALIDATION_CONFIG_KEY);
    try {
      sendPostRequest(_controllerRequestURLBuilder.forTableCreate(), tableConfigJson.toString());
      fail();
    } catch (IOException e) {
      // Should get response code 400 (BAD_REQUEST)
      assertTrue(e.getMessage().startsWith(""Server returned HTTP response code: 400""));
    }
  }
"
"  @Test
  public void testRefreshTableConfigAndQueryTimeout()
      throws Exception {
    // Set timeout as 5ms so that query will timeout
    TableConfig tableConfig = getOfflineTableConfig();
    tableConfig.setQueryConfig(new QueryConfig(5L));
    updateTableConfig(tableConfig);

    // Wait for at most 1 minute for broker to receive and process the table config refresh message
    TestUtils.waitForCondition(aVoid -> {
      try {
        JsonNode queryResponse = postQuery(TEST_TIMEOUT_QUERY);
        JsonNode exceptions = queryResponse.get(""exceptions"");
        if (exceptions.isEmpty()) {
          return false;
        }
        int errorCode = exceptions.get(0).get(""errorCode"").asInt();
        if (errorCode == QueryException.BROKER_TIMEOUT_ERROR_CODE) {
          // Timed out on broker side
          return true;
        }
        if (errorCode == QueryException.SERVER_NOT_RESPONDING_ERROR_CODE) {
          // Timed out on server side
          int numServersQueried = queryResponse.get(""numServersQueried"").asInt();
          int numServersResponded = queryResponse.get(""numServersResponded"").asInt();
          int numDocsScanned = queryResponse.get(""numDocsScanned"").asInt();
          return numServersQueried == getNumServers() && numServersResponded == 0 && numDocsScanned == 0;
        }
        return false;
      } catch (Exception e) {
        throw new RuntimeException(e);
      }
    }, 60_000L, ""Failed to refresh table config"");

    // Remove timeout so that query will finish
    tableConfig.setQueryConfig(null);
    updateTableConfig(tableConfig);

    // Wait for at most 1 minute for broker to receive and process the table config refresh message
    TestUtils.waitForCondition(aVoid -> {
      try {
        JsonNode queryResponse = postQuery(TEST_TIMEOUT_QUERY);
        JsonNode exceptions = queryResponse.get(""exceptions"");
        if (!exceptions.isEmpty()) {
          return false;
        }
        int numServersQueried = queryResponse.get(""numServersQueried"").asInt();
        int numServersResponded = queryResponse.get(""numServersResponded"").asInt();
        int numDocsScanned = queryResponse.get(""numDocsScanned"").asInt();
        return numServersQueried == getNumServers() && numServersResponded == getNumServers()
            && numDocsScanned == getCountStarResult();
      } catch (Exception e) {
        throw new RuntimeException(e);
      }
    }, 60_000L, ""Failed to refresh table config"");
  }
"
"  @Test
  public void testUploadSameSegments()
      throws Exception {
    String offlineTableName = TableNameBuilder.OFFLINE.tableNameWithType(getTableName());
    SegmentZKMetadata segmentZKMetadata = _helixResourceManager.getSegmentsZKMetadata(offlineTableName).get(0);
    String segmentName = segmentZKMetadata.getSegmentName();
    long crc = segmentZKMetadata.getCrc();
    // Creation time is when the segment gets created
    long creationTime = segmentZKMetadata.getCreationTime();
    // Push time is when the segment gets first pushed (new segment)
    long pushTime = segmentZKMetadata.getPushTime();
    // Refresh time is when the segment gets refreshed (existing segment)
    long refreshTime = segmentZKMetadata.getRefreshTime();

    uploadSegments(offlineTableName, _tarDir);
    for (SegmentZKMetadata segmentZKMetadataAfterUpload : _helixResourceManager
        .getSegmentsZKMetadata(offlineTableName)) {
      // Only check one segment
      if (segmentZKMetadataAfterUpload.getSegmentName().equals(segmentName)) {
        assertEquals(segmentZKMetadataAfterUpload.getCrc(), crc);
        assertEquals(segmentZKMetadataAfterUpload.getCreationTime(), creationTime);
        assertEquals(segmentZKMetadataAfterUpload.getPushTime(), pushTime);
        // Refresh time should change
        assertTrue(segmentZKMetadataAfterUpload.getRefreshTime() > refreshTime);
        return;
      }
    }
  }
"
"  @Test
  public void testUploadSegmentRefreshOnly()
      throws Exception {
    TableConfig segmentUploadTestTableConfig =
        new TableConfigBuilder(TableType.OFFLINE).setTableName(SEGMENT_UPLOAD_TEST_TABLE).setSchemaName(getSchemaName())
            .setTimeColumnName(getTimeColumnName()).setSortedColumn(getSortedColumn())
            .setInvertedIndexColumns(getInvertedIndexColumns()).setNoDictionaryColumns(getNoDictionaryColumns())
            .setRangeIndexColumns(getRangeIndexColumns()).setBloomFilterColumns(getBloomFilterColumns())
            .setFieldConfigList(getFieldConfigs()).setNumReplicas(getNumReplicas())
            .setSegmentVersion(getSegmentVersion())
            .setLoadMode(getLoadMode()).setTaskConfig(getTaskConfig()).setBrokerTenant(getBrokerTenant())
            .setServerTenant(getServerTenant()).setIngestionConfig(getIngestionConfig())
            .setNullHandlingEnabled(getNullHandlingEnabled()).build();
    addTableConfig(segmentUploadTestTableConfig);
    String offlineTableName = segmentUploadTestTableConfig.getTableName();
    File[] segmentTarFiles = _tarDir.listFiles();
    assertNotNull(segmentTarFiles);
    int numSegments = segmentTarFiles.length;
    assertTrue(numSegments > 0);
    List<Header> headers = new ArrayList<>();
    headers.add(new BasicHeader(FileUploadDownloadClient.CustomHeaders.REFRESH_ONLY, ""true""));
    List<NameValuePair> parameters = new ArrayList<>();
    NameValuePair tableNameParameter = new BasicNameValuePair(FileUploadDownloadClient.QueryParameters.TABLE_NAME,
        TableNameBuilder.extractRawTableName(offlineTableName));
    parameters.add(tableNameParameter);

    URI uploadSegmentHttpURI = FileUploadDownloadClient.getUploadSegmentHttpURI(LOCAL_HOST, _controllerPort);
    try (FileUploadDownloadClient fileUploadDownloadClient = new FileUploadDownloadClient()) {
      // Refresh non-existing segment
      File segmentTarFile = segmentTarFiles[0];
      try {
        fileUploadDownloadClient
            .uploadSegment(uploadSegmentHttpURI, segmentTarFile.getName(), segmentTarFile, headers, parameters,
                FileUploadDownloadClient.DEFAULT_SOCKET_TIMEOUT_MS);
        fail();
      } catch (HttpErrorStatusException e) {
        assertEquals(e.getStatusCode(), HttpStatus.SC_GONE);
        assertTrue(_helixResourceManager.getSegmentsZKMetadata(SEGMENT_UPLOAD_TEST_TABLE).isEmpty());
      }

      // Upload segment
      SimpleHttpResponse response = fileUploadDownloadClient
          .uploadSegment(uploadSegmentHttpURI, segmentTarFile.getName(), segmentTarFile, null, parameters,
              FileUploadDownloadClient.DEFAULT_SOCKET_TIMEOUT_MS);
      assertEquals(response.getStatusCode(), HttpStatus.SC_OK);
      System.out.println(response.getResponse());
      List<SegmentZKMetadata> segmentsZKMetadata = _helixResourceManager.getSegmentsZKMetadata(offlineTableName);
      assertEquals(segmentsZKMetadata.size(), 1);

      // Refresh existing segment
      response = fileUploadDownloadClient
          .uploadSegment(uploadSegmentHttpURI, segmentTarFile.getName(), segmentTarFile, headers, parameters,
              FileUploadDownloadClient.DEFAULT_SOCKET_TIMEOUT_MS);
      assertEquals(response.getStatusCode(), HttpStatus.SC_OK);
      segmentsZKMetadata = _helixResourceManager.getSegmentsZKMetadata(offlineTableName);
      assertEquals(segmentsZKMetadata.size(), 1);
      assertNotEquals(segmentsZKMetadata.get(0).getRefreshTime(), Long.MIN_VALUE);
    }
    dropOfflineTable(SEGMENT_UPLOAD_TEST_TABLE);
  }
"
"  @Test(dependsOnMethods = ""testRangeIndexTriggering"")
  public void testInvertedIndexTriggering()
      throws Exception {
    long numTotalDocs = getCountStarResult();

    // Without index on DivActualElapsedTime, all docs are scanned at filtering stage.
    assertEquals(postQuery(TEST_UPDATED_INVERTED_INDEX_QUERY).get(""numEntriesScannedInFilter"").asLong(), numTotalDocs);

    addInvertedIndex();
    long tableSizeWithNewIndex = getTableSize(getTableName());

    // Update table config to remove the new inverted index, and
    // reload table to clean the new inverted indices physically.
    TableConfig tableConfig = getOfflineTableConfig();
    tableConfig.getIndexingConfig().setInvertedIndexColumns(getInvertedIndexColumns());
    updateTableConfig(tableConfig);
    reloadOfflineTable(getTableName());
    TestUtils.waitForCondition(aVoid -> {
      try {
        JsonNode queryResponse = postQuery(TEST_UPDATED_INVERTED_INDEX_QUERY);
        // Total docs should not change during reload, but num entries scanned
        // gets back to total number of documents as the index is removed.
        assertEquals(queryResponse.get(""totalDocs"").asLong(), numTotalDocs);
        return queryResponse.get(""numEntriesScannedInFilter"").asLong() == numTotalDocs;
      } catch (Exception e) {
        throw new RuntimeException(e);
      }
    }, 600_000L, ""Failed to cleanup obsolete index"");
    assertEquals(getTableSize(getTableName()), _tableSizeAfterRemovingIndex);

    // Add the inverted index back to test index removal via force download.
    addInvertedIndex();
    long tableSizeAfterAddingIndexAgain = getTableSize(getTableName());
    assertEquals(tableSizeAfterAddingIndexAgain, tableSizeWithNewIndex);

    // Update table config to remove the new inverted index.
    tableConfig = getOfflineTableConfig();
    tableConfig.getIndexingConfig().setInvertedIndexColumns(getInvertedIndexColumns());
    updateTableConfig(tableConfig);

    // Force to download a single segment, and disk usage should drop a bit.
    SegmentZKMetadata segmentZKMetadata =
        _helixResourceManager.getSegmentsZKMetadata(TableNameBuilder.OFFLINE.tableNameWithType(getTableName())).get(0);
    String segmentName = segmentZKMetadata.getSegmentName();
    reloadOfflineSegment(getTableName(), segmentName, true);
    TestUtils.waitForCondition(aVoid -> {
      try {
        JsonNode queryResponse = postQuery(TEST_UPDATED_INVERTED_INDEX_QUERY);
        // Total docs should not change during reload
        assertEquals(queryResponse.get(""totalDocs"").asLong(), numTotalDocs);
        return getTableSize(getTableName()) < tableSizeAfterAddingIndexAgain;
      } catch (Exception e) {
        throw new RuntimeException(e);
      }
    }, 600_000L, ""Failed to clean up obsolete index in segment"");

    // Force to download the whole table and expect disk usage drops further.
    reloadOfflineTable(getTableName(), true);
    TestUtils.waitForCondition(aVoid -> {
      try {
        JsonNode queryResponse = postQuery(TEST_UPDATED_INVERTED_INDEX_QUERY);
        // Total docs should not change during reload, but num entries scanned
        // gets back to total number of documents as the index is removed.
        assertEquals(queryResponse.get(""totalDocs"").asLong(), numTotalDocs);
        return queryResponse.get(""numEntriesScannedInFilter"").asLong() == numTotalDocs;
      } catch (Exception e) {
        throw new RuntimeException(e);
      }
    }, 600_000L, ""Failed to cleanup obsolete index in table"");
    // With force download, the table size gets back to the initial value.
    assertEquals(getTableSize(getTableName()), DISK_SIZE_IN_BYTES);
  }
"
"  @Test
  public void testTimeFunc()
      throws Exception {
    String sqlQuery = ""SELECT toDateTime(now(), 'yyyy-MM-dd z'), toDateTime(ago('PT1H'), 'yyyy-MM-dd z') FROM mytable"";
    JsonNode response = postSqlQuery(sqlQuery, _brokerBaseApiUrl);
    String todayStr = response.get(""resultTable"").get(""rows"").get(0).get(0).asText();
    String expectedTodayStr =
        Instant.now().atZone(ZoneId.of(""UTC"")).format(DateTimeFormatter.ofPattern(""yyyy-MM-dd z""));
    assertEquals(todayStr, expectedTodayStr);

    String oneHourAgoTodayStr = response.get(""resultTable"").get(""rows"").get(0).get(1).asText();
    String expectedOneHourAgoTodayStr = Instant.now().minus(Duration.parse(""PT1H"")).atZone(ZoneId.of(""UTC""))
        .format(DateTimeFormatter.ofPattern(""yyyy-MM-dd z""));
    assertEquals(oneHourAgoTodayStr, expectedOneHourAgoTodayStr);
  }
"
"  @Test
  public void testLiteralOnlyFunc()
      throws Exception {
    long currentTsMin = System.currentTimeMillis();
    long oneHourAgoTsMin = currentTsMin - ONE_HOUR_IN_MS;
    String sqlQuery =
        ""SELECT 1, now() as currentTs, ago('PT1H') as oneHourAgoTs, 'abc', toDateTime(now(), 'yyyy-MM-dd z') as ""
            + ""today, now(), ago('PT1H')"";
    JsonNode response = postSqlQuery(sqlQuery, _brokerBaseApiUrl);
    long currentTsMax = System.currentTimeMillis();
    long oneHourAgoTsMax = currentTsMax - ONE_HOUR_IN_MS;

    assertEquals(response.get(""resultTable"").get(""dataSchema"").get(""columnNames"").get(0).asText(), ""1"");
    assertEquals(response.get(""resultTable"").get(""dataSchema"").get(""columnNames"").get(1).asText(), ""currentTs"");
    assertEquals(response.get(""resultTable"").get(""dataSchema"").get(""columnNames"").get(2).asText(), ""oneHourAgoTs"");
    assertEquals(response.get(""resultTable"").get(""dataSchema"").get(""columnNames"").get(3).asText(), ""abc"");
    assertEquals(response.get(""resultTable"").get(""dataSchema"").get(""columnNames"").get(4).asText(), ""today"");
    String nowColumnName = response.get(""resultTable"").get(""dataSchema"").get(""columnNames"").get(5).asText();
    String oneHourAgoColumnName = response.get(""resultTable"").get(""dataSchema"").get(""columnNames"").get(6).asText();
    assertTrue(Long.parseLong(nowColumnName) > currentTsMin);
    assertTrue(Long.parseLong(nowColumnName) < currentTsMax);
    assertTrue(Long.parseLong(oneHourAgoColumnName) > oneHourAgoTsMin);
    assertTrue(Long.parseLong(oneHourAgoColumnName) < oneHourAgoTsMax);

    assertEquals(response.get(""resultTable"").get(""dataSchema"").get(""columnDataTypes"").get(0).asText(), ""LONG"");
    assertEquals(response.get(""resultTable"").get(""dataSchema"").get(""columnDataTypes"").get(1).asText(), ""LONG"");
    assertEquals(response.get(""resultTable"").get(""dataSchema"").get(""columnDataTypes"").get(2).asText(), ""LONG"");
    assertEquals(response.get(""resultTable"").get(""dataSchema"").get(""columnDataTypes"").get(3).asText(), ""STRING"");
    assertEquals(response.get(""resultTable"").get(""dataSchema"").get(""columnDataTypes"").get(4).asText(), ""STRING"");
    assertEquals(response.get(""resultTable"").get(""dataSchema"").get(""columnDataTypes"").get(5).asText(), ""LONG"");
    assertEquals(response.get(""resultTable"").get(""dataSchema"").get(""columnDataTypes"").get(6).asText(), ""LONG"");

    int first = response.get(""resultTable"").get(""rows"").get(0).get(0).asInt();
    long second = response.get(""resultTable"").get(""rows"").get(0).get(1).asLong();
    long third = response.get(""resultTable"").get(""rows"").get(0).get(2).asLong();
    String fourth = response.get(""resultTable"").get(""rows"").get(0).get(3).asText();
    assertEquals(first, 1);
    assertTrue(second > currentTsMin);
    assertTrue(second < currentTsMax);
    assertTrue(third > oneHourAgoTsMin);
    assertTrue(third < oneHourAgoTsMax);
    assertEquals(fourth, ""abc"");
    String todayStr = response.get(""resultTable"").get(""rows"").get(0).get(4).asText();
    String expectedTodayStr =
        Instant.now().atZone(ZoneId.of(""UTC"")).format(DateTimeFormatter.ofPattern(""yyyy-MM-dd z""));
    assertEquals(todayStr, expectedTodayStr);
    long nowValue = response.get(""resultTable"").get(""rows"").get(0).get(5).asLong();
    assertEquals(nowValue, Long.parseLong(nowColumnName));
    long oneHourAgoValue = response.get(""resultTable"").get(""rows"").get(0).get(6).asLong();
    assertEquals(oneHourAgoValue, Long.parseLong(oneHourAgoColumnName));
  }
"
"  @Test(dependsOnMethods = ""testBloomFilterTriggering"")
  public void testRangeIndexTriggering()
      throws Exception {
    long numTotalDocs = getCountStarResult();
    assertEquals(postQuery(TEST_UPDATED_RANGE_INDEX_QUERY).get(""numEntriesScannedInFilter"").asLong(), numTotalDocs);

    // Update table config and trigger reload
    TableConfig tableConfig = getOfflineTableConfig();
    tableConfig.getIndexingConfig().setRangeIndexColumns(UPDATED_RANGE_INDEX_COLUMNS);
    updateTableConfig(tableConfig);
    reloadOfflineTable(getTableName());
    TestUtils.waitForCondition(aVoid -> {
      try {
        JsonNode queryResponse = postQuery(TEST_UPDATED_RANGE_INDEX_QUERY);
        // Total docs should not change during reload
        assertEquals(queryResponse.get(""totalDocs"").asLong(), numTotalDocs);
        return queryResponse.get(""numEntriesScannedInFilter"").asLong() < numTotalDocs;
      } catch (Exception e) {
        throw new RuntimeException(e);
      }
    }, 600_000L, ""Failed to generate range index"");

    // Update table config to remove the new range index, and
    // reload table to clean the new range index physically.
    tableConfig = getOfflineTableConfig();
    tableConfig.getIndexingConfig().setRangeIndexColumns(getRangeIndexColumns());
    updateTableConfig(tableConfig);
    reloadOfflineTable(getTableName());
    TestUtils.waitForCondition(aVoid -> {
      try {
        JsonNode queryResponse = postQuery(TEST_UPDATED_RANGE_INDEX_QUERY);
        // Total docs should not change during reload, but num entries scanned
        // gets back to total number of documents as the index is removed.
        assertEquals(queryResponse.get(""totalDocs"").asLong(), numTotalDocs);
        return queryResponse.get(""numEntriesScannedInFilter"").asLong() == numTotalDocs;
      } catch (Exception e) {
        throw new RuntimeException(e);
      }
    }, 600_000L, ""Failed to cleanup obsolete index"");

    assertEquals(getTableSize(getTableName()), _tableSizeAfterRemovingIndex);
  }
"
"  @Test(dependsOnMethods = ""testDefaultColumns"")
  public void testBloomFilterTriggering()
      throws Exception {
    long numTotalDocs = getCountStarResult();
    assertEquals(postQuery(TEST_UPDATED_BLOOM_FILTER_QUERY).get(""numSegmentsProcessed"").asLong(), NUM_SEGMENTS);

    // Update table config and trigger reload
    TableConfig tableConfig = getOfflineTableConfig();
    tableConfig.getIndexingConfig().setBloomFilterColumns(UPDATED_BLOOM_FILTER_COLUMNS);
    updateTableConfig(tableConfig);
    reloadOfflineTable(getTableName());
    TestUtils.waitForCondition(aVoid -> {
      try {
        JsonNode queryResponse = postQuery(TEST_UPDATED_BLOOM_FILTER_QUERY);
        // Total docs should not change during reload
        assertEquals(queryResponse.get(""totalDocs"").asLong(), numTotalDocs);
        return queryResponse.get(""numSegmentsProcessed"").asLong() == 0L;
      } catch (Exception e) {
        throw new RuntimeException(e);
      }
    }, 600_000L, ""Failed to generate bloom filter"");

    // Update table config to remove the new bloom filter, and
    // reload table to clean the new bloom filter physically.
    tableConfig = getOfflineTableConfig();
    tableConfig.getIndexingConfig().setBloomFilterColumns(getBloomFilterColumns());
    updateTableConfig(tableConfig);
    reloadOfflineTable(getTableName());
    TestUtils.waitForCondition(aVoid -> {
      try {
        JsonNode queryResponse = postQuery(TEST_UPDATED_BLOOM_FILTER_QUERY);
        // Total docs should not change during reload, but num entries scanned
        // gets back to total number of documents as bloom filter is removed.
        assertEquals(queryResponse.get(""totalDocs"").asLong(), numTotalDocs);
        return queryResponse.get(""numSegmentsProcessed"").asLong() == NUM_SEGMENTS;
      } catch (Exception e) {
        throw new RuntimeException(e);
      }
    }, 600_000L, ""Failed to cleanup obsolete index"");
    assertEquals(getTableSize(getTableName()), _tableSizeAfterRemovingIndex);
  }
"
"  @Test
  public void testServerErrorWithBrokerTimeout()
      throws Exception {
    // Set query timeout
    long queryTimeout = 5000;
    TableConfig tableConfig = getOfflineTableConfig();
    tableConfig.setQueryConfig(new QueryConfig(queryTimeout));
    updateTableConfig(tableConfig);

    long startTime = System.currentTimeMillis();
    // The query below will fail execution due to JSON_MATCH on column without json index
    JsonNode queryResponse = postSqlQuery(""SELECT count(*) FROM mytable WHERE JSON_MATCH(Dest, '$=123')"");

    assertTrue(System.currentTimeMillis() - startTime < queryTimeout);
    assertTrue(queryResponse.get(""exceptions"").get(0).get(""message"").toString().startsWith(""\""QueryExecutionError""));

    // Remove timeout
    tableConfig.setQueryConfig(null);
    updateTableConfig(tableConfig);
  }
"
"  @Test
  public void testStarTreeTriggering()
      throws Exception {
    long numTotalDocs = getCountStarResult();
    long tableSizeWithDefaultIndex = getTableSize(getTableName());

    // Test the first query
    JsonNode firstQueryResponse = postQuery(TEST_STAR_TREE_QUERY_1);
    int firstQueryResult = firstQueryResponse.get(""aggregationResults"").get(0).get(""value"").asInt();
    assertEquals(firstQueryResponse.get(""totalDocs"").asLong(), numTotalDocs);
    // Initially 'numDocsScanned' should be the same as 'COUNT(*)' result
    assertEquals(firstQueryResponse.get(""numDocsScanned"").asInt(), firstQueryResult);

    // Update table config and trigger reload
    TableConfig tableConfig = getOfflineTableConfig();
    IndexingConfig indexingConfig = tableConfig.getIndexingConfig();
    indexingConfig.setStarTreeIndexConfigs(Collections.singletonList(STAR_TREE_INDEX_CONFIG_1));
    indexingConfig.setEnableDynamicStarTreeCreation(true);
    updateTableConfig(tableConfig);
    reloadOfflineTable(getTableName());

    TestUtils.waitForCondition(aVoid -> {
      try {
        JsonNode queryResponse = postQuery(TEST_STAR_TREE_QUERY_1);
        // Result should not change during reload
        assertEquals(queryResponse.get(""aggregationResults"").get(0).get(""value"").asInt(), firstQueryResult);
        // Total docs should not change during reload
        assertEquals(queryResponse.get(""totalDocs"").asLong(), numTotalDocs);
        // With star-tree, 'numDocsScanned' should be the same as number of segments (1 per segment)
        return queryResponse.get(""numDocsScanned"").asInt() == NUM_SEGMENTS;
      } catch (Exception e) {
        throw new RuntimeException(e);
      }
    }, 600_000L, ""Failed to add first star-tree index"");

    // Reload again should have no effect
    reloadOfflineTable(getTableName());
    firstQueryResponse = postQuery(TEST_STAR_TREE_QUERY_1);
    assertEquals(firstQueryResponse.get(""aggregationResults"").get(0).get(""value"").asInt(), firstQueryResult);
    assertEquals(firstQueryResponse.get(""totalDocs"").asLong(), numTotalDocs);
    assertEquals(firstQueryResponse.get(""numDocsScanned"").asInt(), NUM_SEGMENTS);

    // Should be able to use the star-tree with an additional match-all predicate on another dimension
    firstQueryResponse = postQuery(TEST_STAR_TREE_QUERY_1 + "" AND DaysSinceEpoch > 16070"");
    assertEquals(firstQueryResponse.get(""aggregationResults"").get(0).get(""value"").asInt(), firstQueryResult);
    assertEquals(firstQueryResponse.get(""totalDocs"").asLong(), numTotalDocs);
    assertEquals(firstQueryResponse.get(""numDocsScanned"").asInt(), NUM_SEGMENTS);

    // Test the second query
    JsonNode secondQueryResponse = postQuery(TEST_STAR_TREE_QUERY_2);
    int secondQueryResult = secondQueryResponse.get(""aggregationResults"").get(0).get(""value"").asInt();
    assertEquals(secondQueryResponse.get(""totalDocs"").asLong(), numTotalDocs);
    // Initially 'numDocsScanned' should be the same as 'COUNT(*)' result
    assertEquals(secondQueryResponse.get(""numDocsScanned"").asInt(), secondQueryResult);

    // Update table config with a different star-tree index config and trigger reload
    indexingConfig.setStarTreeIndexConfigs(Collections.singletonList(STAR_TREE_INDEX_CONFIG_2));
    updateTableConfig(tableConfig);
    reloadOfflineTable(getTableName());

    TestUtils.waitForCondition(aVoid -> {
      try {
        JsonNode queryResponse = postQuery(TEST_STAR_TREE_QUERY_2);
        // Result should not change during reload
        assertEquals(queryResponse.get(""aggregationResults"").get(0).get(""value"").asInt(), secondQueryResult);
        // Total docs should not change during reload
        assertEquals(queryResponse.get(""totalDocs"").asLong(), numTotalDocs);
        // With star-tree, 'numDocsScanned' should be the same as number of segments (1 per segment)
        return queryResponse.get(""numDocsScanned"").asInt() == NUM_SEGMENTS;
      } catch (Exception e) {
        throw new RuntimeException(e);
      }
    }, 600_000L, ""Failed to change to second star-tree index"");

    // First query should not be able to use the star-tree
    firstQueryResponse = postQuery(TEST_STAR_TREE_QUERY_1);
    assertEquals(firstQueryResponse.get(""numDocsScanned"").asInt(), firstQueryResult);

    // Reload again should have no effect
    reloadOfflineTable(getTableName());
    firstQueryResponse = postQuery(TEST_STAR_TREE_QUERY_1);
    assertEquals(firstQueryResponse.get(""aggregationResults"").get(0).get(""value"").asInt(), firstQueryResult);
    assertEquals(firstQueryResponse.get(""totalDocs"").asLong(), numTotalDocs);
    assertEquals(firstQueryResponse.get(""numDocsScanned"").asInt(), firstQueryResult);
    secondQueryResponse = postQuery(TEST_STAR_TREE_QUERY_2);
    assertEquals(secondQueryResponse.get(""aggregationResults"").get(0).get(""value"").asInt(), secondQueryResult);
    assertEquals(secondQueryResponse.get(""totalDocs"").asLong(), numTotalDocs);
    assertEquals(secondQueryResponse.get(""numDocsScanned"").asInt(), NUM_SEGMENTS);

    // Should be able to use the star-tree with an additional match-all predicate on another dimension
    secondQueryResponse = postQuery(TEST_STAR_TREE_QUERY_2 + "" AND DaysSinceEpoch > 16070"");
    assertEquals(secondQueryResponse.get(""aggregationResults"").get(0).get(""value"").asInt(), secondQueryResult);
    assertEquals(secondQueryResponse.get(""totalDocs"").asLong(), numTotalDocs);
    assertEquals(secondQueryResponse.get(""numDocsScanned"").asInt(), NUM_SEGMENTS);

    // Remove the star-tree index config and trigger reload
    indexingConfig.setStarTreeIndexConfigs(null);
    updateTableConfig(tableConfig);
    reloadOfflineTable(getTableName());

    TestUtils.waitForCondition(aVoid -> {
      try {
        JsonNode queryResponse = postQuery(TEST_STAR_TREE_QUERY_2);
        // Result should not change during reload
        assertEquals(queryResponse.get(""aggregationResults"").get(0).get(""value"").asInt(), secondQueryResult);
        // Total docs should not change during reload
        assertEquals(queryResponse.get(""totalDocs"").asLong(), numTotalDocs);
        // Without star-tree, 'numDocsScanned' should be the same as the 'COUNT(*)' result
        return queryResponse.get(""numDocsScanned"").asInt() == secondQueryResult;
      } catch (Exception e) {
        throw new RuntimeException(e);
      }
    }, 600_000L, ""Failed to remove star-tree index"");
    assertEquals(getTableSize(getTableName()), tableSizeWithDefaultIndex);

    // First query should not be able to use the star-tree
    firstQueryResponse = postQuery(TEST_STAR_TREE_QUERY_1);
    assertEquals(firstQueryResponse.get(""numDocsScanned"").asInt(), firstQueryResult);

    // Reload again should have no effect
    reloadOfflineTable(getTableName());
    firstQueryResponse = postQuery(TEST_STAR_TREE_QUERY_1);
    assertEquals(firstQueryResponse.get(""aggregationResults"").get(0).get(""value"").asInt(), firstQueryResult);
    assertEquals(firstQueryResponse.get(""totalDocs"").asLong(), numTotalDocs);
    assertEquals(firstQueryResponse.get(""numDocsScanned"").asInt(), firstQueryResult);
    secondQueryResponse = postQuery(TEST_STAR_TREE_QUERY_2);
    assertEquals(secondQueryResponse.get(""aggregationResults"").get(0).get(""value"").asInt(), secondQueryResult);
    assertEquals(secondQueryResponse.get(""totalDocs"").asLong(), numTotalDocs);
    assertEquals(secondQueryResponse.get(""numDocsScanned"").asInt(), secondQueryResult);
  }
"
"  @Test(dependsOnMethods = ""testAggregateMetadataAPI"")
  public void testDefaultColumns()
      throws Exception {
    long numTotalDocs = getCountStarResult();

    reloadWithExtraColumns();
    JsonNode queryResponse = postQuery(SELECT_STAR_QUERY);
    assertEquals(queryResponse.get(""totalDocs"").asLong(), numTotalDocs);
    assertEquals(queryResponse.get(""selectionResults"").get(""columns"").size(), 91);

    testNewAddedColumns();

    reloadWithMissingColumns();
    queryResponse = postQuery(SELECT_STAR_QUERY);
    assertEquals(queryResponse.get(""totalDocs"").asLong(), numTotalDocs);
    assertEquals(queryResponse.get(""selectionResults"").get(""columns"").size(), 75);

    reloadWithRegularColumns();
    queryResponse = postQuery(SELECT_STAR_QUERY);
    assertEquals(queryResponse.get(""totalDocs"").asLong(), numTotalDocs);
    assertEquals(queryResponse.get(""selectionResults"").get(""columns"").size(), 79);

    _tableSizeAfterRemovingIndex = getTableSize(getTableName());
  }
"
"  @Test
  public void testBrokerResponseMetadata()
      throws Exception {
    super.testBrokerResponseMetadata();
  }
"
"  @Test
  public void testGroupByUDF()
      throws Exception {
    String pqlQuery = ""SELECT COUNT(*) FROM mytable GROUP BY timeConvert(DaysSinceEpoch,'DAYS','SECONDS')"";
    JsonNode response = postQuery(pqlQuery);
    JsonNode groupByResult = response.get(""aggregationResults"").get(0);
    JsonNode groupByEntry = groupByResult.get(""groupByResult"").get(0);
    assertEquals(groupByEntry.get(""value"").asDouble(), 605.0);
    assertEquals(groupByEntry.get(""group"").get(0).asInt(), 16138 * 24 * 3600);
    assertEquals(groupByResult.get(""groupByColumns"").get(0).asText(), ""timeconvert(DaysSinceEpoch,'DAYS','SECONDS')"");

    pqlQuery =
        ""SELECT COUNT(*) FROM mytable GROUP BY dateTimeConvert(DaysSinceEpoch,'1:DAYS:EPOCH','1:HOURS:EPOCH',""
            + ""'1:HOURS')"";
    response = postQuery(pqlQuery);
    groupByResult = response.get(""aggregationResults"").get(0);
    groupByEntry = groupByResult.get(""groupByResult"").get(0);
    assertEquals(groupByEntry.get(""value"").asDouble(), 605.0);
    assertEquals(groupByEntry.get(""group"").get(0).asInt(), 16138 * 24);
    assertEquals(groupByResult.get(""groupByColumns"").get(0).asText(),
        ""datetimeconvert(DaysSinceEpoch,'1:DAYS:EPOCH','1:HOURS:EPOCH','1:HOURS')"");

    pqlQuery = ""SELECT COUNT(*) FROM mytable GROUP BY add(DaysSinceEpoch,DaysSinceEpoch,15)"";
    response = postQuery(pqlQuery);
    groupByResult = response.get(""aggregationResults"").get(0);
    groupByEntry = groupByResult.get(""groupByResult"").get(0);
    assertEquals(groupByEntry.get(""value"").asDouble(), 605.0);
    assertEquals(groupByEntry.get(""group"").get(0).asDouble(), 16138.0 + 16138 + 15);
    assertEquals(groupByResult.get(""groupByColumns"").get(0).asText(), ""add(DaysSinceEpoch,DaysSinceEpoch,'15')"");

    pqlQuery = ""SELECT COUNT(*) FROM mytable GROUP BY sub(DaysSinceEpoch,25)"";
    response = postQuery(pqlQuery);
    groupByResult = response.get(""aggregationResults"").get(0);
    groupByEntry = groupByResult.get(""groupByResult"").get(0);
    assertEquals(groupByEntry.get(""value"").asDouble(), 605.0);
    assertEquals(groupByEntry.get(""group"").get(0).asDouble(), 16138.0 - 25);
    assertEquals(groupByResult.get(""groupByColumns"").get(0).asText(), ""sub(DaysSinceEpoch,'25')"");

    pqlQuery = ""SELECT COUNT(*) FROM mytable GROUP BY mult(DaysSinceEpoch,24,3600)"";
    response = postQuery(pqlQuery);
    groupByResult = response.get(""aggregationResults"").get(0);
    groupByEntry = groupByResult.get(""groupByResult"").get(0);
    assertEquals(groupByEntry.get(""value"").asDouble(), 605.0);
    assertEquals(groupByEntry.get(""group"").get(0).asDouble(), 16138.0 * 24 * 3600);
    assertEquals(groupByResult.get(""groupByColumns"").get(0).asText(), ""mult(DaysSinceEpoch,'24','3600')"");

    pqlQuery = ""SELECT COUNT(*) FROM mytable GROUP BY div(DaysSinceEpoch,2)"";
    response = postQuery(pqlQuery);
    groupByResult = response.get(""aggregationResults"").get(0);
    groupByEntry = groupByResult.get(""groupByResult"").get(0);
    assertEquals(groupByEntry.get(""value"").asDouble(), 605.0);
    assertEquals(groupByEntry.get(""group"").get(0).asDouble(), 16138.0 / 2);
    assertEquals(groupByResult.get(""groupByColumns"").get(0).asText(), ""div(DaysSinceEpoch,'2')"");

    pqlQuery = ""SELECT COUNT(*) FROM mytable GROUP BY arrayLength(DivAirports)"";
    response = postQuery(pqlQuery);
    groupByResult = response.get(""aggregationResults"").get(0);
    groupByEntry = groupByResult.get(""groupByResult"").get(0);
    assertEquals(groupByEntry.get(""value"").asDouble(), 115545.0);
    assertEquals(groupByEntry.get(""group"").get(0).asText(), ""5"");
    assertEquals(groupByResult.get(""groupByColumns"").get(0).asText(), ""arraylength(DivAirports)"");

    pqlQuery = ""SELECT COUNT(*) FROM mytable GROUP BY arrayLength(valueIn(DivAirports,'DFW','ORD'))"";
    response = postQuery(pqlQuery);
    groupByResult = response.get(""aggregationResults"").get(0);
    groupByEntry = groupByResult.get(""groupByResult"").get(0);
    assertEquals(groupByEntry.get(""value"").asDouble(), 114895.0);
    assertEquals(groupByEntry.get(""group"").get(0).asText(), ""0"");
    groupByEntry = groupByResult.get(""groupByResult"").get(1);
    assertEquals(groupByEntry.get(""value"").asDouble(), 648.0);
    assertEquals(groupByEntry.get(""group"").get(0).asText(), ""1"");
    groupByEntry = groupByResult.get(""groupByResult"").get(2);
    assertEquals(groupByEntry.get(""value"").asDouble(), 2.0);
    assertEquals(groupByEntry.get(""group"").get(0).asText(), ""2"");
    assertEquals(groupByResult.get(""groupByColumns"").get(0).asText(), ""arraylength(valuein(DivAirports,'DFW','ORD'))"");

    pqlQuery = ""SELECT COUNT(*) FROM mytable GROUP BY valueIn(DivAirports,'DFW','ORD')"";
    response = postQuery(pqlQuery);
    groupByResult = response.get(""aggregationResults"").get(0);
    groupByEntry = groupByResult.get(""groupByResult"").get(0);
    assertEquals(groupByEntry.get(""value"").asDouble(), 336.0);
    assertEquals(groupByEntry.get(""group"").get(0).asText(), ""ORD"");
    assertEquals(groupByResult.get(""groupByColumns"").get(0).asText(), ""valuein(DivAirports,'DFW','ORD')"");

    pqlQuery = ""SELECT MAX(timeConvert(DaysSinceEpoch,'DAYS','SECONDS')) FROM mytable"";
    response = postQuery(pqlQuery);
    JsonNode aggregationResult = response.get(""aggregationResults"").get(0);
    assertEquals(aggregationResult.get(""function"").asText(), ""max_timeconvert(DaysSinceEpoch,'DAYS','SECONDS')"");
    assertEquals(aggregationResult.get(""value"").asDouble(), 16435.0 * 24 * 3600);

    pqlQuery = ""SELECT MIN(div(DaysSinceEpoch,2)) FROM mytable"";
    response = postQuery(pqlQuery);
    aggregationResult = response.get(""aggregationResults"").get(0);
    assertEquals(aggregationResult.get(""function"").asText(), ""min_div(DaysSinceEpoch,'2')"");
    assertEquals(aggregationResult.get(""value"").asDouble(), 16071.0 / 2);
  }
"
"  @Test
  public void testAggregationUDF()
      throws Exception {

    String pqlQuery = ""SELECT MAX(timeConvert(DaysSinceEpoch,'DAYS','SECONDS')) FROM mytable"";
    JsonNode response = postQuery(pqlQuery);
    JsonNode aggregationResult = response.get(""aggregationResults"").get(0);
    assertEquals(aggregationResult.get(""function"").asText(), ""max_timeconvert(DaysSinceEpoch,'DAYS','SECONDS')"");
    assertEquals(aggregationResult.get(""value"").asDouble(), 16435.0 * 24 * 3600);

    pqlQuery = ""SELECT MIN(div(DaysSinceEpoch,2)) FROM mytable"";
    response = postQuery(pqlQuery);
    aggregationResult = response.get(""aggregationResults"").get(0);
    assertEquals(aggregationResult.get(""function"").asText(), ""min_div(DaysSinceEpoch,'2')"");
    assertEquals(aggregationResult.get(""value"").asDouble(), 16071.0 / 2);
  }
"
"  @Test
  public void testSelectionUDF()
      throws Exception {
    String pqlQuery = ""SELECT DaysSinceEpoch, timeConvert(DaysSinceEpoch,'DAYS','SECONDS') FROM mytable"";
    JsonNode response = postQuery(pqlQuery);
    ArrayNode selectionResults = (ArrayNode) response.get(""selectionResults"").get(""results"");
    assertNotNull(selectionResults);
    assertFalse(selectionResults.isEmpty());
    for (int i = 0; i < selectionResults.size(); i++) {
      long daysSinceEpoch = selectionResults.get(i).get(0).asLong();
      long secondsSinceEpoch = selectionResults.get(i).get(1).asLong();
      assertEquals(daysSinceEpoch * 24 * 60 * 60, secondsSinceEpoch);
    }

    pqlQuery =
        ""SELECT DaysSinceEpoch, timeConvert(DaysSinceEpoch,'DAYS','SECONDS') FROM mytable order by DaysSinceEpoch ""
            + ""limit 10000"";
    response = postQuery(pqlQuery);
    selectionResults = (ArrayNode) response.get(""selectionResults"").get(""results"");
    assertNotNull(selectionResults);
    assertFalse(selectionResults.isEmpty());
    long prevValue = -1;
    for (int i = 0; i < selectionResults.size(); i++) {
      long daysSinceEpoch = selectionResults.get(i).get(0).asLong();
      long secondsSinceEpoch = selectionResults.get(i).get(1).asLong();
      assertEquals(daysSinceEpoch * 24 * 60 * 60, secondsSinceEpoch);
      assertTrue(daysSinceEpoch >= prevValue);
      prevValue = daysSinceEpoch;
    }

    pqlQuery =
        ""SELECT DaysSinceEpoch, timeConvert(DaysSinceEpoch,'DAYS','SECONDS') FROM mytable order by timeConvert""
            + ""(DaysSinceEpoch,'DAYS','SECONDS') DESC limit 10000"";
    response = postQuery(pqlQuery);
    selectionResults = (ArrayNode) response.get(""selectionResults"").get(""results"");
    assertNotNull(selectionResults);
    assertFalse(selectionResults.isEmpty());
    prevValue = Long.MAX_VALUE;
    for (int i = 0; i < selectionResults.size(); i++) {
      long daysSinceEpoch = selectionResults.get(i).get(0).asLong();
      long secondsSinceEpoch = selectionResults.get(i).get(1).asLong();
      assertEquals(daysSinceEpoch * 24 * 60 * 60, secondsSinceEpoch);
      assertTrue(secondsSinceEpoch <= prevValue);
      prevValue = secondsSinceEpoch;
    }
  }
"
"  @Test
  public void testFilterUDF()
      throws Exception {
    int daysSinceEpoch = 16138;
    long secondsSinceEpoch = 16138 * 24 * 60 * 60;

    String pqlQuery;
    pqlQuery = ""SELECT count(*) FROM mytable WHERE DaysSinceEpoch = "" + daysSinceEpoch;
    long expectedResult = postQuery(pqlQuery).get(""aggregationResults"").get(0).get(""value"").asLong();

    pqlQuery = ""SELECT count(*) FROM mytable WHERE timeConvert(DaysSinceEpoch,'DAYS','SECONDS') = "" + secondsSinceEpoch;
    assertEquals(postQuery(pqlQuery).get(""aggregationResults"").get(0).get(""value"").asLong(), expectedResult);

    pqlQuery = ""SELECT count(*) FROM mytable WHERE DaysSinceEpoch = "" + daysSinceEpoch
        + "" OR timeConvert(DaysSinceEpoch,'DAYS','SECONDS') = "" + secondsSinceEpoch;
    assertEquals(postQuery(pqlQuery).get(""aggregationResults"").get(0).get(""value"").asLong(), expectedResult);

    pqlQuery = ""SELECT count(*) FROM mytable WHERE DaysSinceEpoch = "" + daysSinceEpoch
        + "" AND timeConvert(DaysSinceEpoch,'DAYS','SECONDS') = "" + secondsSinceEpoch;
    assertEquals(postQuery(pqlQuery).get(""aggregationResults"").get(0).get(""value"").asLong(), expectedResult);

    pqlQuery =
        ""SELECT count(*) FROM mytable WHERE DIV(timeConvert(DaysSinceEpoch,'DAYS','SECONDS'),1) = "" + secondsSinceEpoch;
    assertEquals(postQuery(pqlQuery).get(""aggregationResults"").get(0).get(""value"").asLong(), expectedResult);

    pqlQuery = String
        .format(""SELECT count(*) FROM mytable WHERE timeConvert(DaysSinceEpoch,'DAYS','SECONDS') IN (%d, %d)"",
            secondsSinceEpoch - 100, secondsSinceEpoch);
    assertEquals(postQuery(pqlQuery).get(""aggregationResults"").get(0).get(""value"").asLong(), expectedResult);

    pqlQuery = String
        .format(""SELECT count(*) FROM mytable WHERE timeConvert(DaysSinceEpoch,'DAYS','SECONDS') BETWEEN %d AND %d"",
            secondsSinceEpoch - 100, secondsSinceEpoch);
    assertEquals(postQuery(pqlQuery).get(""aggregationResults"").get(0).get(""value"").asLong(), expectedResult);
  }
"
"  @Test
  public void testCaseStatementInSelection()
      throws Exception {
    List<String> origins = Arrays
        .asList(""ATL"", ""ORD"", ""DFW"", ""DEN"", ""LAX"", ""IAH"", ""SFO"", ""PHX"", ""LAS"", ""EWR"", ""MCO"", ""BOS"", ""SLC"", ""SEA"", ""MSP"",
            ""CLT"", ""LGA"", ""DTW"", ""JFK"", ""BWI"");
    StringBuilder caseStatementBuilder = new StringBuilder(""CASE "");
    for (int i = 0; i < origins.size(); i++) {
      // WHEN origin = 'ATL' THEN 1
      // WHEN origin = 'ORD' THEN 2
      // WHEN origin = 'DFW' THEN 3
      // ....
      caseStatementBuilder.append(String.format(""WHEN origin = '%s' THEN %d "", origins.get(i), i + 1));
    }
    caseStatementBuilder.append(""ELSE 0 END"");
    String sqlQuery = ""SELECT origin, "" + caseStatementBuilder + "" AS origin_code FROM mytable LIMIT 1000"";
    JsonNode response = postSqlQuery(sqlQuery, _brokerBaseApiUrl);
    JsonNode rows = response.get(""resultTable"").get(""rows"");
    assertEquals(response.get(""exceptions"").size(), 0);
    for (int i = 0; i < rows.size(); i++) {
      String origin = rows.get(i).get(0).asText();
      int originCode = rows.get(i).get(1).asInt();
      if (originCode > 0) {
        assertEquals(origin, origins.get(originCode - 1));
      } else {
        assertFalse(origins.contains(origin));
      }
    }
  }
"
"  @Test
  public void testCaseStatementInSelectionWithTransformFunctionInThen()
      throws Exception {
    String sqlQuery =
        ""SELECT ArrDelay, CASE WHEN ArrDelay > 0 THEN ArrDelay WHEN ArrDelay < 0 THEN ArrDelay * -1 ELSE 0 END AS ""
            + ""ArrTimeDiff FROM mytable LIMIT 1000"";
    JsonNode response = postSqlQuery(sqlQuery, _brokerBaseApiUrl);
    JsonNode rows = response.get(""resultTable"").get(""rows"");
    assertEquals(response.get(""exceptions"").size(), 0);
    for (int i = 0; i < rows.size(); i++) {
      int arrDelay = rows.get(i).get(0).asInt();
      int arrDelayDiff = rows.get(i).get(1).asInt();
      if (arrDelay > 0) {
        assertEquals(arrDelay, arrDelayDiff);
      } else {
        assertEquals(arrDelay, arrDelayDiff * -1);
      }
    }
  }
"
"  @Test
  public void testCaseStatementWithLogicalTransformFunction()
      throws Exception {
    String sqlQuery = ""SELECT ArrDelay"" + "", CASE WHEN ArrDelay > 50 OR ArrDelay < 10 THEN 10 ELSE 0 END""
        + "", CASE WHEN ArrDelay < 50 AND ArrDelay >= 10 THEN 10 ELSE 0 END"" + "" FROM mytable LIMIT 1000"";
    JsonNode response = postSqlQuery(sqlQuery, _brokerBaseApiUrl);
    JsonNode rows = response.get(""resultTable"").get(""rows"");
    assertEquals(response.get(""exceptions"").size(), 0);
    for (int i = 0; i < rows.size(); i++) {
      int row0 = rows.get(i).get(0).asInt();
      int row1 = rows.get(i).get(1).asInt();
      int row2 = rows.get(i).get(2).asInt();
      if (row0 > 50 || row0 < 10) {
        assertEquals(row1, 10);
      } else {
        assertEquals(row1, 0);
      }
      if (row0 < 50 && row0 >= 10) {
        assertEquals(row2, 10);
      } else {
        assertEquals(row2, 0);
      }
    }
  }
"
"  @Test
  public void testCaseStatementWithInAggregation()
      throws Exception {
    testCountVsCaseQuery(""origin = 'ATL'"");
    testCountVsCaseQuery(""origin <> 'ATL'"");

    testCountVsCaseQuery(""DaysSinceEpoch > 16312"");
    testCountVsCaseQuery(""DaysSinceEpoch >= 16312"");
    testCountVsCaseQuery(""DaysSinceEpoch < 16312"");
    testCountVsCaseQuery(""DaysSinceEpoch <= 16312"");
    testCountVsCaseQuery(""DaysSinceEpoch = 16312"");
    testCountVsCaseQuery(""DaysSinceEpoch <> 16312"");
  }
"
"  @Test
  public void testFilterWithInvertedIndexUDF()
      throws Exception {
    int daysSinceEpoch = 16138;
    long secondsSinceEpoch = 16138 * 24 * 60 * 60;

    String[] origins = new String[]{
        ""ATL"", ""ORD"", ""DFW"", ""DEN"", ""LAX"", ""IAH"", ""SFO"", ""PHX"", ""LAS"", ""EWR"", ""MCO"", ""BOS"", ""SLC"", ""SEA"", ""MSP"", ""CLT"",
        ""LGA"", ""DTW"", ""JFK"", ""BWI""
    };
    String pqlQuery;
    for (String origin : origins) {
      pqlQuery =
          ""SELECT count(*) FROM mytable WHERE Origin = \"""" + origin + ""\"" AND DaysSinceEpoch = "" + daysSinceEpoch;
      JsonNode response1 = postQuery(pqlQuery);
      pqlQuery = ""SELECT count(*) FROM mytable WHERE Origin = \"""" + origin
          + ""\"" AND timeConvert(DaysSinceEpoch,'DAYS','SECONDS') = "" + secondsSinceEpoch;
      JsonNode response2 = postQuery(pqlQuery);
      double val1 = response1.get(""aggregationResults"").get(0).get(""value"").asDouble();
      double val2 = response2.get(""aggregationResults"").get(0).get(""value"").asDouble();
      assertEquals(val1, val2);
    }
  }
"
"  @Test
  public void testQueryWithRepeatedColumns()
      throws Exception {
    //test repeated columns in selection query
    String query = ""SELECT ArrTime, ArrTime FROM mytable WHERE DaysSinceEpoch <= 16312 AND Carrier = 'DL'"";
    testQuery(query, Collections.singletonList(query));

    //test repeated columns in selection query with order by
    query = ""SELECT ArrTime, ArrTime FROM mytable WHERE DaysSinceEpoch <= 16312 AND Carrier = 'DL' order by ArrTime"";
    testQuery(query, Collections.singletonList(query));

    //test repeated columns in agg query
    query = ""SELECT count(*), count(*) FROM mytable WHERE DaysSinceEpoch <= 16312 AND Carrier = 'DL'"";
    testQuery(query, Arrays.asList(""SELECT count(*) FROM mytable WHERE DaysSinceEpoch <= 16312 AND Carrier = 'DL'"",
        ""SELECT count(*) FROM mytable WHERE DaysSinceEpoch <= 16312 AND Carrier = 'DL'""));

    //test repeated columns in agg group by query
    query =
        ""SELECT ArrTime, ArrTime, count(*), count(*) FROM mytable WHERE DaysSinceEpoch <= 16312 AND Carrier = 'DL' ""
            + ""group by ArrTime, ArrTime"";
    testQuery(query, Arrays.asList(
        ""SELECT ArrTime, ArrTime, count(*) FROM mytable WHERE DaysSinceEpoch <= 16312 AND Carrier = 'DL' group by ""
            + ""ArrTime, ArrTime"",
        ""SELECT ArrTime, ArrTime, count(*) FROM mytable WHERE DaysSinceEpoch <= 16312 AND Carrier = 'DL' group by ""
            + ""ArrTime, ArrTime""));
  }
"
"  @Test
  public void testQueryWithOrderby()
      throws Exception {
    //test repeated columns in selection query
    String query = ""SELECT ArrTime, Carrier, DaysSinceEpoch FROM mytable ORDER BY DaysSinceEpoch DESC"";
    testQuery(query, Collections.singletonList(query));

    //test repeated columns in selection query
    query = ""SELECT ArrTime, DaysSinceEpoch, Carrier FROM mytable ORDER BY Carrier DESC"";
    testQuery(query, Collections.singletonList(query));

    //test repeated columns in selection query
    query = ""SELECT ArrTime, DaysSinceEpoch, Carrier FROM mytable ORDER BY Carrier DESC, ArrTime DESC"";
    testQuery(query, Collections.singletonList(query));
  }
"
"  @Test
  public void testQueryWithAlias()
      throws Exception {
    {
      //test same alias name with column name
      String query =
          ""SELECT ArrTime AS ArrTime, Carrier AS Carrier, DaysSinceEpoch AS DaysSinceEpoch FROM mytable ORDER BY ""
              + ""DaysSinceEpoch DESC"";
      testSqlQuery(query, Collections.singletonList(query));

      query =
          ""SELECT ArrTime AS ArrTime, DaysSinceEpoch AS DaysSinceEpoch, Carrier AS Carrier FROM mytable ORDER BY ""
              + ""Carrier DESC"";
      testSqlQuery(query, Collections.singletonList(query));

      query =
          ""SELECT ArrTime AS ArrTime, DaysSinceEpoch AS DaysSinceEpoch, Carrier AS Carrier FROM mytable ORDER BY ""
              + ""Carrier DESC, ArrTime DESC"";
      testSqlQuery(query, Collections.singletonList(query));
    }
    {
      //test single alias
      String query = ""SELECT ArrTime, Carrier AS CarrierName, DaysSinceEpoch FROM mytable ORDER BY DaysSinceEpoch DESC"";
      testSqlQuery(query, Collections.singletonList(query));

      query = ""SELECT count(*) AS cnt, max(ArrTime) as maxArrTime FROM mytable"";
      testSqlQuery(query, Collections.singletonList(query));

      query = ""SELECT count(*) AS cnt, Carrier AS CarrierName FROM mytable GROUP BY CarrierName ORDER BY cnt"";
      testSqlQuery(query, Collections.singletonList(query));
    }
    {
      //test multiple alias
      String query =
          ""SELECT ArrTime, Carrier, Carrier AS CarrierName1, Carrier AS CarrierName2, DaysSinceEpoch FROM mytable ""
              + ""ORDER BY DaysSinceEpoch DESC"";
      testSqlQuery(query, Collections.singletonList(query));

      query = ""SELECT count(*) AS cnt, max(ArrTime) as maxArrTime1, max(ArrTime) as maxArrTime2 FROM mytable"";
      testSqlQuery(query, Collections.singletonList(query));

      query =
          ""SELECT count(*), count(*) AS cnt1, count(*) AS cnt2, Carrier AS CarrierName FROM mytable GROUP BY ""
              + ""CarrierName ORDER BY cnt2"";
      testSqlQuery(query, Collections.singletonList(query));
    }
  }
"
"  @Test
  public void testDistinctQuery()
      throws Exception {
    // by default 10 rows will be returned, so use high limit
    String pql = ""SELECT DISTINCT(Carrier) FROM mytable LIMIT 1000000"";
    String sql = ""SELECT DISTINCT Carrier FROM mytable"";
    testQuery(pql, Collections.singletonList(sql));
    pql = ""SELECT DISTINCT Carrier FROM mytable LIMIT 1000000"";
    testSqlQuery(pql, Collections.singletonList(sql));

    pql = ""SELECT DISTINCT(Carrier, DestAirportID) FROM mytable LIMIT 1000000"";
    sql = ""SELECT DISTINCT Carrier, DestAirportID FROM mytable"";
    testQuery(pql, Collections.singletonList(sql));
    pql = ""SELECT DISTINCT Carrier, DestAirportID FROM mytable LIMIT 1000000"";
    testSqlQuery(pql, Collections.singletonList(sql));

    pql = ""SELECT DISTINCT(Carrier, DestAirportID, DestStateName) FROM mytable LIMIT 1000000"";
    sql = ""SELECT DISTINCT Carrier, DestAirportID, DestStateName FROM mytable"";
    testQuery(pql, Collections.singletonList(sql));
    pql = ""SELECT DISTINCT Carrier, DestAirportID, DestStateName FROM mytable LIMIT 1000000"";
    testSqlQuery(pql, Collections.singletonList(sql));

    pql = ""SELECT DISTINCT(Carrier, DestAirportID, DestCityName) FROM mytable LIMIT 1000000"";
    sql = ""SELECT DISTINCT Carrier, DestAirportID, DestCityName FROM mytable"";
    testQuery(pql, Collections.singletonList(sql));
    pql = ""SELECT DISTINCT Carrier, DestAirportID, DestCityName FROM mytable LIMIT 1000000"";
    testSqlQuery(pql, Collections.singletonList(sql));
  }
"
"  @Test
  public void testNonAggregationGroupByQuery()
      throws Exception {
    // by default 10 rows will be returned, so use high limit
    String pql = ""SELECT Carrier FROM mytable GROUP BY Carrier LIMIT 1000000"";
    String sql = ""SELECT Carrier FROM mytable GROUP BY Carrier"";
    testSqlQuery(pql, Collections.singletonList(sql));

    pql = ""SELECT Carrier, DestAirportID FROM mytable GROUP BY Carrier, DestAirportID LIMIT 1000000"";
    sql = ""SELECT Carrier, DestAirportID FROM mytable GROUP BY Carrier, DestAirportID"";
    testSqlQuery(pql, Collections.singletonList(sql));

    pql =
        ""SELECT Carrier, DestAirportID, DestStateName FROM mytable GROUP BY Carrier, DestAirportID, DestStateName ""
            + ""LIMIT 1000000"";
    sql = ""SELECT Carrier, DestAirportID, DestStateName FROM mytable GROUP BY Carrier, DestAirportID, DestStateName"";
    testSqlQuery(pql, Collections.singletonList(sql));

    pql =
        ""SELECT Carrier, DestAirportID, DestCityName FROM mytable GROUP BY Carrier, DestAirportID, DestCityName LIMIT""
            + "" 1000000"";
    sql = ""SELECT Carrier, DestAirportID, DestCityName FROM mytable GROUP BY Carrier, DestAirportID, DestCityName"";
    testSqlQuery(pql, Collections.singletonList(sql));

    pql = ""SELECT ArrTime-DepTime FROM mytable GROUP BY ArrTime, DepTime LIMIT 1000000"";
    sql = ""SELECT ArrTime-DepTime FROM mytable GROUP BY ArrTime, DepTime"";
    testSqlQuery(pql, Collections.singletonList(sql));

    pql = ""SELECT ArrTime-DepTime,ArrTime/3,DepTime*2 FROM mytable GROUP BY ArrTime, DepTime LIMIT 1000000"";
    sql = ""SELECT ArrTime-DepTime,ArrTime/3,DepTime*2 FROM mytable GROUP BY ArrTime, DepTime"";
    testSqlQuery(pql, Collections.singletonList(sql));

    pql = ""SELECT ArrTime+DepTime FROM mytable GROUP BY ArrTime + DepTime LIMIT 1000000"";
    sql = ""SELECT ArrTime+DepTime FROM mytable GROUP BY ArrTime + DepTime"";
    testSqlQuery(pql, Collections.singletonList(sql));
  }
"
"  @Test
  public void testCaseInsensitivity() {
    int daysSinceEpoch = 16138;
    int hoursSinceEpoch = 16138 * 24;
    int secondsSinceEpoch = 16138 * 24 * 60 * 60;
    List<String> baseQueries = Arrays.asList(""SELECT * FROM mytable"",
        ""SELECT DaysSinceEpoch, timeConvert(DaysSinceEpoch,'DAYS','SECONDS') FROM mytable"",
        ""SELECT DaysSinceEpoch, timeConvert(DaysSinceEpoch,'DAYS','SECONDS') FROM mytable order by DaysSinceEpoch ""
            + ""limit 10000"",
        ""SELECT DaysSinceEpoch, timeConvert(DaysSinceEpoch,'DAYS','SECONDS') FROM mytable order by timeConvert""
            + ""(DaysSinceEpoch,'DAYS','SECONDS') DESC limit 10000"",
        ""SELECT count(*) FROM mytable WHERE DaysSinceEpoch = "" + daysSinceEpoch,
        ""SELECT count(*) FROM mytable WHERE timeConvert(DaysSinceEpoch,'DAYS','HOURS') = "" + hoursSinceEpoch,
        ""SELECT count(*) FROM mytable WHERE timeConvert(DaysSinceEpoch,'DAYS','SECONDS') = "" + secondsSinceEpoch,
        ""SELECT MAX(timeConvert(DaysSinceEpoch,'DAYS','SECONDS')) FROM mytable"",
        ""SELECT COUNT(*) FROM mytable GROUP BY dateTimeConvert(DaysSinceEpoch,'1:DAYS:EPOCH','1:HOURS:EPOCH',""
            + ""'1:HOURS')"");
    List<String> queries = new ArrayList<>();
    baseQueries.forEach(q -> queries.add(q.replace(""mytable"", ""MYTABLE"").replace(""DaysSinceEpoch"", ""DAYSSinceEpOch"")));
    baseQueries
        .forEach(q -> queries.add(q.replace(""mytable"", ""MYDB.MYTABLE"").replace(""DaysSinceEpoch"", ""DAYSSinceEpOch"")));

    for (String query : queries) {
      try {
        JsonNode response = postQuery(query);
        assertTrue(response.get(""numSegmentsProcessed"").asLong() >= 1L, ""PQL: "" + query + "" failed"");

        response = postSqlQuery(query);
        assertTrue(response.get(""numSegmentsProcessed"").asLong() >= 1L, ""SQL: "" + query + "" failed"");
      } catch (Exception e) {
        // Fail the test when exception caught
        throw new RuntimeException(""Got Exceptions from query - "" + query);
      }
    }
  }
"
"  @Test
  public void testColumnNameContainsTableName() {
    int daysSinceEpoch = 16138;
    int hoursSinceEpoch = 16138 * 24;
    int secondsSinceEpoch = 16138 * 24 * 60 * 60;
    List<String> baseQueries = Arrays.asList(""SELECT * FROM mytable"",
        ""SELECT DaysSinceEpoch, timeConvert(DaysSinceEpoch,'DAYS','SECONDS') FROM mytable"",
        ""SELECT DaysSinceEpoch, timeConvert(DaysSinceEpoch,'DAYS','SECONDS') FROM mytable order by DaysSinceEpoch ""
            + ""limit 10000"",
        ""SELECT DaysSinceEpoch, timeConvert(DaysSinceEpoch,'DAYS','SECONDS') FROM mytable order by timeConvert""
            + ""(DaysSinceEpoch,'DAYS','SECONDS') DESC limit 10000"",
        ""SELECT count(*) FROM mytable WHERE DaysSinceEpoch = "" + daysSinceEpoch,
        ""SELECT count(*) FROM mytable WHERE timeConvert(DaysSinceEpoch,'DAYS','HOURS') = "" + hoursSinceEpoch,
        ""SELECT count(*) FROM mytable WHERE timeConvert(DaysSinceEpoch,'DAYS','SECONDS') = "" + secondsSinceEpoch,
        ""SELECT MAX(timeConvert(DaysSinceEpoch,'DAYS','SECONDS')) FROM mytable"",
        ""SELECT COUNT(*) FROM mytable GROUP BY dateTimeConvert(DaysSinceEpoch,'1:DAYS:EPOCH','1:HOURS:EPOCH',""
            + ""'1:HOURS')"");
    List<String> queries = new ArrayList<>();
    baseQueries.forEach(q -> queries.add(q.replace(""DaysSinceEpoch"", ""mytable.DAYSSinceEpOch"")));
    baseQueries.forEach(q -> queries.add(q.replace(""DaysSinceEpoch"", ""mytable.DAYSSinceEpOch"")));

    for (String query : queries) {
      try {
        JsonNode response = postQuery(query);
        assertTrue(response.get(""numSegmentsProcessed"").asLong() >= 1L, ""PQL: "" + query + "" failed"");

        response = postSqlQuery(query);
        assertTrue(response.get(""numSegmentsProcessed"").asLong() >= 1L, ""SQL: "" + query + "" failed"");
      } catch (Exception e) {
        // Fail the test when exception caught
        throw new RuntimeException(""Got Exceptions from query - "" + query);
      }
    }
  }
"
"  @Test
  public void testCaseInsensitivityWithColumnNameContainsTableName() {
    int daysSinceEpoch = 16138;
    int hoursSinceEpoch = 16138 * 24;
    int secondsSinceEpoch = 16138 * 24 * 60 * 60;
    List<String> baseQueries = Arrays.asList(""SELECT * FROM mytable"",
        ""SELECT DaysSinceEpoch, timeConvert(DaysSinceEpoch,'DAYS','SECONDS') FROM mytable"",
        ""SELECT DaysSinceEpoch, timeConvert(DaysSinceEpoch,'DAYS','SECONDS') FROM mytable order by DaysSinceEpoch ""
            + ""limit 10000"",
        ""SELECT DaysSinceEpoch, timeConvert(DaysSinceEpoch,'DAYS','SECONDS') FROM mytable order by timeConvert""
            + ""(DaysSinceEpoch,'DAYS','SECONDS') DESC limit 10000"",
        ""SELECT count(*) FROM mytable WHERE DaysSinceEpoch = "" + daysSinceEpoch,
        ""SELECT count(*) FROM mytable WHERE timeConvert(DaysSinceEpoch,'DAYS','HOURS') = "" + hoursSinceEpoch,
        ""SELECT count(*) FROM mytable WHERE timeConvert(DaysSinceEpoch,'DAYS','SECONDS') = "" + secondsSinceEpoch,
        ""SELECT MAX(timeConvert(DaysSinceEpoch,'DAYS','SECONDS')) FROM mytable"",
        ""SELECT COUNT(*) FROM mytable GROUP BY dateTimeConvert(DaysSinceEpoch,'1:DAYS:EPOCH','1:HOURS:EPOCH',""
            + ""'1:HOURS')"");
    List<String> queries = new ArrayList<>();
    baseQueries
        .forEach(q -> queries.add(q.replace(""mytable"", ""MYTABLE"").replace(""DaysSinceEpoch"", ""MYTABLE.DAYSSinceEpOch"")));
    baseQueries.forEach(
        q -> queries.add(q.replace(""mytable"", ""MYDB.MYTABLE"").replace(""DaysSinceEpoch"", ""MYTABLE.DAYSSinceEpOch"")));

    for (String query : queries) {
      try {
        JsonNode response = postQuery(query);
        assertTrue(response.get(""numSegmentsProcessed"").asLong() >= 1L, ""PQL: "" + query + "" failed"");

        response = postSqlQuery(query);
        assertTrue(response.get(""numSegmentsProcessed"").asLong() >= 1L, ""SQL: "" + query + "" failed"");
      } catch (Exception e) {
        // Fail the test when exception caught
        throw new RuntimeException(""Got Exceptions from query - "" + query);
      }
    }
  }
"
"  @Test
  public void testQuerySourceWithDatabaseName()
      throws Exception {
    // by default 10 rows will be returned, so use high limit
    String pql = ""SELECT DISTINCT(Carrier) FROM mytable LIMIT 1000000"";
    String sql = ""SELECT DISTINCT Carrier FROM mytable"";
    testQuery(pql, Collections.singletonList(sql));
    pql = ""SELECT DISTINCT Carrier FROM db.mytable LIMIT 1000000"";
    testSqlQuery(pql, Collections.singletonList(sql));
  }
"
"  @Test
  public void testDistinctCountHll()
      throws Exception {
    String query;

    // The Accurate value is 6538.
    query = ""SELECT distinctCount(FlightNum) FROM mytable "";
    assertEquals(postQuery(query).get(""aggregationResults"").get(0).get(""value"").asLong(), 6538);
    assertEquals(postSqlQuery(query, _brokerBaseApiUrl).get(""resultTable"").get(""rows"").get(0).get(0).asLong(), 6538);

    // Expected distinctCountHll with different log2m value from 2 to 19. The Accurate value is 6538.
    long[] expectedResults = new long[]{
        3504, 6347, 8877, 9729, 9046, 7672, 7538, 6993, 6649, 6651, 6553, 6525, 6459, 6523, 6532, 6544, 6538, 6539
    };

    for (int i = 2; i < 20; i++) {
      query = String.format(""SELECT distinctCountHLL(FlightNum, %d) FROM mytable "", i);
      assertEquals(postQuery(query).get(""aggregationResults"").get(0).get(""value"").asLong(), expectedResults[i - 2]);
      assertEquals(postSqlQuery(query, _brokerBaseApiUrl).get(""resultTable"").get(""rows"").get(0).get(0).asLong(),
          expectedResults[i - 2]);
    }

    // Default HLL is set as log2m=12
    query = ""SELECT distinctCountHLL(FlightNum) FROM mytable "";
    assertEquals(postQuery(query).get(""aggregationResults"").get(0).get(""value"").asLong(), expectedResults[10]);
    assertEquals(postSqlQuery(query, _brokerBaseApiUrl).get(""resultTable"").get(""rows"").get(0).get(0).asLong(),
        expectedResults[10]);
  }
"
"  @Test
  public void testAggregationFunctionsWithUnderscore()
      throws Exception {
    String query;

    // The Accurate value is 6538.
    query = ""SELECT distinct_count(FlightNum) FROM mytable "";
    assertEquals(postQuery(query).get(""aggregationResults"").get(0).get(""value"").asLong(), 6538);
    assertEquals(postSqlQuery(query, _brokerBaseApiUrl).get(""resultTable"").get(""rows"").get(0).get(0).asLong(), 6538);

    // The Accurate value is 6538.
    query = ""SELECT c_o_u_n_t(FlightNum) FROM mytable "";
    assertEquals(postQuery(query).get(""aggregationResults"").get(0).get(""value"").asLong(), 115545);
    assertEquals(postSqlQuery(query, _brokerBaseApiUrl).get(""resultTable"").get(""rows"").get(0).get(0).asLong(), 115545);
  }
"
"  @Test
  public void testGrpcQueryServer()
      throws Exception {
    GrpcQueryClient queryClient = new GrpcQueryClient(""localhost"", CommonConstants.Server.DEFAULT_GRPC_PORT);
    String sql = ""SELECT * FROM mytable_OFFLINE LIMIT 1000000"";
    BrokerRequest brokerRequest = new Pql2Compiler().compileToBrokerRequest(sql);
    List<String> segments = _helixResourceManager.getSegmentsFor(""mytable_OFFLINE"");

    GrpcRequestBuilder requestBuilder = new GrpcRequestBuilder().setSegments(segments);
    testNonStreamingRequest(queryClient.submit(requestBuilder.setSql(sql).build()));
    testNonStreamingRequest(queryClient.submit(requestBuilder.setBrokerRequest(brokerRequest).build()));

    requestBuilder.setEnableStreaming(true);
    testStreamingRequest(queryClient.submit(requestBuilder.setSql(sql).build()));
    testStreamingRequest(queryClient.submit(requestBuilder.setBrokerRequest(brokerRequest).build()));
  }
"
"  @Test
  public void testHardcodedServerPartitionedSqlQueries()
      throws Exception {
    super.testHardcodedServerPartitionedSqlQueries();
  }
"
"  @Test
  public void testAggregateMetadataAPI()
      throws IOException {
    JsonNode oneColumnResponse = JsonUtils
        .stringToJsonNode(sendGetRequest(_controllerBaseApiUrl + ""/tables/mytable/metadata?columns=DestCityMarketID""));
    assertEquals(oneColumnResponse.get(DISK_SIZE_IN_BYTES_KEY).asInt(), DISK_SIZE_IN_BYTES);
    assertEquals(oneColumnResponse.get(NUM_SEGMENTS_KEY).asInt(), NUM_SEGMENTS);
    assertEquals(oneColumnResponse.get(NUM_ROWS_KEY).asInt(), NUM_ROWS);
    assertEquals(oneColumnResponse.get(COLUMN_LENGTH_MAP_KEY).size(), 1);
    assertEquals(oneColumnResponse.get(COLUMN_CARDINALITY_MAP_KEY).size(), 1);

    JsonNode threeColumnsResponse = JsonUtils.stringToJsonNode(sendGetRequest(_controllerBaseApiUrl
        + ""/tables/mytable/metadata?columns=DivActualElapsedTime&columns=CRSElapsedTime&columns=OriginStateName""));
    assertEquals(threeColumnsResponse.get(DISK_SIZE_IN_BYTES_KEY).asInt(), DISK_SIZE_IN_BYTES);
    assertEquals(threeColumnsResponse.get(NUM_SEGMENTS_KEY).asInt(), NUM_SEGMENTS);
    assertEquals(threeColumnsResponse.get(NUM_ROWS_KEY).asInt(), NUM_ROWS);
    assertEquals(threeColumnsResponse.get(COLUMN_LENGTH_MAP_KEY).size(), 3);
    assertEquals(threeColumnsResponse.get(COLUMN_CARDINALITY_MAP_KEY).size(), 3);

    JsonNode zeroColumnResponse =
        JsonUtils.stringToJsonNode(sendGetRequest(_controllerBaseApiUrl + ""/tables/mytable/metadata""));
    assertEquals(zeroColumnResponse.get(DISK_SIZE_IN_BYTES_KEY).asInt(), DISK_SIZE_IN_BYTES);
    assertEquals(zeroColumnResponse.get(NUM_SEGMENTS_KEY).asInt(), NUM_SEGMENTS);
    assertEquals(zeroColumnResponse.get(NUM_ROWS_KEY).asInt(), NUM_ROWS);
    assertEquals(zeroColumnResponse.get(COLUMN_LENGTH_MAP_KEY).size(), 0);
    assertEquals(zeroColumnResponse.get(COLUMN_CARDINALITY_MAP_KEY).size(), 0);

    JsonNode allColumnResponse =
        JsonUtils.stringToJsonNode(sendGetRequest(_controllerBaseApiUrl + ""/tables/mytable/metadata?columns=*""));
    assertEquals(allColumnResponse.get(DISK_SIZE_IN_BYTES_KEY).asInt(), DISK_SIZE_IN_BYTES);
    assertEquals(allColumnResponse.get(NUM_SEGMENTS_KEY).asInt(), NUM_SEGMENTS);
    assertEquals(allColumnResponse.get(NUM_ROWS_KEY).asInt(), NUM_ROWS);
    assertEquals(allColumnResponse.get(COLUMN_LENGTH_MAP_KEY).size(), 82);
    assertEquals(allColumnResponse.get(COLUMN_CARDINALITY_MAP_KEY).size(), 82);

    allColumnResponse = JsonUtils.stringToJsonNode(sendGetRequest(
        _controllerBaseApiUrl + ""/tables/mytable/metadata?columns=CRSElapsedTime&columns=*&columns=OriginStateName""));
    assertEquals(allColumnResponse.get(DISK_SIZE_IN_BYTES_KEY).asInt(), DISK_SIZE_IN_BYTES);
    assertEquals(allColumnResponse.get(NUM_SEGMENTS_KEY).asInt(), NUM_SEGMENTS);
    assertEquals(allColumnResponse.get(NUM_ROWS_KEY).asInt(), NUM_ROWS);
    assertEquals(allColumnResponse.get(COLUMN_LENGTH_MAP_KEY).size(), 82);
    assertEquals(allColumnResponse.get(COLUMN_CARDINALITY_MAP_KEY).size(), 82);
  }
"
"  @Test
  public void testRecords()
      throws Exception {
    Assert.assertNotEquals(_totalRecordsPushedInStream, 0);

    ResultSet pinotResultSet = getPinotConnection()
        .execute(new Request(""sql"", ""SELECT * FROM "" + getTableName() + "" ORDER BY Origin LIMIT 10000""))
        .getResultSet(0);

    Assert.assertNotEquals(pinotResultSet.getRowCount(), 0);

    Statement h2statement =
        _h2Connection.createStatement(java.sql.ResultSet.TYPE_FORWARD_ONLY, java.sql.ResultSet.CONCUR_READ_ONLY);
    h2statement.execute(""SELECT * FROM "" + getTableName() + "" ORDER BY Origin"");
    java.sql.ResultSet h2ResultSet = h2statement.getResultSet();

    Assert.assertFalse(h2ResultSet.isLast());

    h2ResultSet.beforeFirst();
    int row = 0;
    Map<String, Integer> columnToIndex = new HashMap<>();
    for (int i = 0; i < _h2FieldNameAndTypes.size(); i++) {
      columnToIndex.put(pinotResultSet.getColumnName(i), i);
    }

    while (h2ResultSet.next()) {

      for (String fieldNameAndDatatype : _h2FieldNameAndTypes) {
        String[] fieldNameAndDatatypeList = fieldNameAndDatatype.split("" "");
        String fieldName = fieldNameAndDatatypeList[0];
        String h2DataType = fieldNameAndDatatypeList[1];
        switch (h2DataType) {
          case ""int"": {
            int expectedValue = h2ResultSet.getInt(fieldName);
            int actualValue = pinotResultSet.getInt(row, columnToIndex.get(fieldName));
            Assert.assertEquals(expectedValue, actualValue);
            break;
          }
          case ""varchar(128)"": {
            String expectedValue = h2ResultSet.getString(fieldName);
            String actualValue = pinotResultSet.getString(row, columnToIndex.get(fieldName));
            Assert.assertEquals(expectedValue, actualValue);
            break;
          }
          default:
            break;
        }
      }

      row++;

      if (row >= pinotResultSet.getRowCount()) {
        int cnt = 0;
        while (h2ResultSet.next()) {
          cnt++;
        }
        Assert.assertEquals(cnt, 0);
        break;
      }
    }
  }
"
"  @Test
  public void testCountRecords() {
    long count =
        getPinotConnection().execute(new Request(""sql"", ""SELECT COUNT(*) FROM "" + getTableName())).getResultSet(0)
            .getLong(0);

    Assert.assertEquals(count, _totalRecordsPushedInStream);
  }
"
"  @Test
  public void testGeneratedQueries()
      throws Exception {
    for (int i = 0; i < NUM_QUERIES_TO_GENERATE; i += 2) {
      testStarQuery(_starTree1QueryGenerator.nextQuery());
      testStarQuery(_starTree2QueryGenerator.nextQuery());
    }
  }
"
"  @Test
  public void testPredicateOnMetrics()
      throws Exception {
    String starQuery;

    // Query containing predicate on one metric only
    starQuery = ""SELECT SUM(DepDelayMinutes) FROM myStarTable WHERE DepDelay > 0"";
    testStarQuery(starQuery);
    starQuery = ""SELECT SUM(DepDelayMinutes) FROM myStarTable WHERE DepDelay BETWEEN 0 and 10000"";
    testStarQuery(starQuery);

    // Query containing predicate on multiple metrics
    starQuery = ""SELECT SUM(DepDelayMinutes) FROM myStarTable WHERE DepDelay > 0 AND ArrDelay > 0"";
    testStarQuery(starQuery);

    // Query containing predicate on multiple metrics and dimensions
    starQuery =
        ""SELECT SUM(DepDelayMinutes) FROM myStarTable WHERE DepDelay > 0 AND ArrDelay > 0 AND OriginStateName = ""
            + ""'Massachusetts'"";
    testStarQuery(starQuery);
  }
"
"  @Test
  public void testTextSearchCountQuery()
      throws Exception {
    // Keep posting queries until all records are consumed
    long previousResult = 0;
    while (getCurrentCountStarResult() < NUM_RECORDS) {
      long result = getTextColumnQueryResult();
      assertTrue(result >= previousResult);
      previousResult = result;
      Thread.sleep(100);
    }

    //Lucene index on consuming segments to update the latest records
    TestUtils.waitForCondition(aVoid -> {
      try {
        return getTextColumnQueryResult() == NUM_MATCHING_RECORDS;
      } catch (Exception e) {
        fail(""Caught exception while getting text column query result"");
        return false;
      }
    }, 10_000L, ""Failed to reach expected number of matching records"");
  }
"
"  @Test
  public void testRealtimeToOfflineSegmentsTask()
      throws IOException {
    List<SegmentZKMetadata> segmentsZKMetadata = _pinotHelixResourceManager.getSegmentsZKMetadata(_offlineTableName);
    Assert.assertTrue(segmentsZKMetadata.isEmpty());

    long expectedWatermark = _dataSmallestTimeMs + 86400000;
    int numOfflineSegments = 0;
    for (int i = 0; i < 3; i++) {
      // Schedule task
      Assert.assertNotNull(_taskManager.scheduleTasks().get(MinionConstants.RealtimeToOfflineSegmentsTask.TASK_TYPE));
      Assert.assertTrue(_helixTaskResourceManager.getTaskQueues().contains(
          PinotHelixTaskResourceManager.getHelixJobQueueName(MinionConstants.RealtimeToOfflineSegmentsTask.TASK_TYPE)));
      // Should not generate more tasks
      Assert.assertNull(_taskManager.scheduleTasks().get(MinionConstants.RealtimeToOfflineSegmentsTask.TASK_TYPE));

      // Wait at most 600 seconds for all tasks COMPLETED
      waitForTaskToComplete(expectedWatermark);
      // check segment is in offline
      segmentsZKMetadata = _pinotHelixResourceManager.getSegmentsZKMetadata(_offlineTableName);
      numOfflineSegments++;
      Assert.assertEquals(segmentsZKMetadata.size(), numOfflineSegments);
      long expectedOfflineSegmentTimeMs = expectedWatermark - 86400000;
      Assert.assertEquals(segmentsZKMetadata.get(i).getStartTimeMs(), expectedOfflineSegmentTimeMs);
      Assert.assertEquals(segmentsZKMetadata.get(i).getEndTimeMs(), expectedOfflineSegmentTimeMs);

      expectedWatermark += 86400000;
    }
    testHardcodedSqlQueries();

    // Delete the table
    dropRealtimeTable(_realtimeTableName);

    // Check if the metadata is cleaned up on table deletion
    verifyTableDelete(_realtimeTableName);
  }
"
"  @Test(enabled = false)
  public void testSegmentListApi() {
  }
"
"  @Test(enabled = false)
  public void testBrokerDebugOutput() {
  }
"
"  @Test(enabled = false)
  public void testBrokerDebugRoutingTableSQL() {
  }
"
"  @Test(enabled = false)
  public void testBrokerResponseMetadata() {
  }
"
"  @Test(enabled = false)
  public void testDictionaryBasedQueries() {
  }
"
"  @Test(enabled = false)
  public void testGeneratedQueriesWithMultiValues() {
  }
"
"  @Test(enabled = false)
  public void testGeneratedQueriesWithoutMultiValues() {
  }
"
"  @Test(enabled = false)
  public void testHardcodedQueries() {
  }
"
"  @Test(enabled = false)
  public void testHardcodedSqlQueries() {
  }
"
"  @Test(enabled = false)
  public void testInstanceShutdown() {
  }
"
"  @Test(enabled = false)
  public void testQueriesFromQueryFile() {
  }
"
"  @Test(enabled = false)
  public void testQueryExceptions() {
  }
"
"  @Test(enabled = false)
  public void testReload(boolean includeOfflineTable) {
  }
"
"  @Test(enabled = false)
  public void testSqlQueriesFromQueryFile() {
  }
"
"  @Test(enabled = false)
  public void testVirtualColumnQueries() {
  }
"
"  @Test
  public void testJsonPathQueries()
      throws Exception {
    // Selection only
    String query = ""SELECT stringKeyMapStr FROM "" + getTableName();
    JsonNode pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    JsonNode selectionResults = pinotResponse.get(""selectionResults"").get(""results"");
    assertEquals(selectionResults.size(), 10);
    for (int i = 0; i < 10; i++) {
      assertEquals(selectionResults.get(i).get(0).textValue(), String.format(""{\""k1\"":%d,\""k2\"":100%d}"", i, i));
    }
    query = ""SELECT jsonExtractScalar(stringKeyMapStr, '$.k1', 'INT') FROM "" + getTableName();
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    selectionResults = pinotResponse.get(""selectionResults"").get(""results"");
    assertEquals(selectionResults.size(), 10);
    for (int i = 0; i < 10; i++) {
      assertEquals(Integer.parseInt(selectionResults.get(i).get(0).textValue()), i);
    }
    query = ""SELECT jsonExtractScalar(intKeyMapStr, '$.95', 'INT') FROM "" + getTableName();
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    selectionResults = pinotResponse.get(""selectionResults"").get(""results"");
    assertEquals(selectionResults.size(), 10);
    for (int i = 0; i < 10; i++) {
      assertEquals(Integer.parseInt(selectionResults.get(i).get(0).textValue()), i);
    }

    // Selection order-by
    query = ""SELECT jsonExtractScalar(stringKeyMapStr, '$.k2', 'INT') FROM "" + getTableName()
        + "" ORDER BY jsonExtractScalar(stringKeyMapStr, '$.k1', 'INT')"";
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    selectionResults = pinotResponse.get(""selectionResults"").get(""results"");
    assertEquals(selectionResults.size(), 10);
    for (int i = 0; i < 10; i++) {
      assertEquals(Integer.parseInt(selectionResults.get(i).get(0).textValue()), NUM_DOCS + i);
    }
    query = ""SELECT jsonExtractScalar(intKeyMapStr, '$.717', 'INT') FROM "" + getTableName()
        + "" ORDER BY jsonExtractScalar(intKeyMapStr, '$.95', 'INT')"";
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    selectionResults = pinotResponse.get(""selectionResults"").get(""results"");
    assertEquals(selectionResults.size(), 10);
    for (int i = 0; i < 10; i++) {
      assertEquals(Integer.parseInt(selectionResults.get(i).get(0).textValue()), NUM_DOCS + i);
    }

    // Aggregation only
    query = ""SELECT MAX(jsonExtractScalar(stringKeyMapStr, '$.k1', 'INT')) FROM "" + getTableName();
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    JsonNode aggregationResult = pinotResponse.get(""aggregationResults"").get(0).get(""value"");
    assertEquals((int) Double.parseDouble(aggregationResult.textValue()), NUM_DOCS - 1);
    query = ""SELECT MAX(jsonExtractScalar(intKeyMapStr, '$.95', 'INT')) FROM "" + getTableName();
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    aggregationResult = pinotResponse.get(""aggregationResults"").get(0).get(""value"");
    assertEquals((int) Double.parseDouble(aggregationResult.textValue()), NUM_DOCS - 1);

    // Aggregation group-by
    query = ""SELECT MIN(jsonExtractScalar(stringKeyMapStr, '$.k2', 'INT')) FROM "" + getTableName()
        + "" GROUP BY jsonExtractScalar(stringKeyMapStr, '$.k1', 'INT')"";
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    JsonNode groupByResults = pinotResponse.get(""aggregationResults"").get(0).get(""groupByResult"");
    assertEquals(groupByResults.size(), 10);
    for (int i = 0; i < 10; i++) {
      JsonNode groupByResult = groupByResults.get(i);
      assertEquals(Integer.parseInt(groupByResult.get(""group"").get(0).asText()), i);
      assertEquals((int) Double.parseDouble(groupByResult.get(""value"").asText()), NUM_DOCS + i);
    }
    query = ""SELECT MIN(jsonExtractScalar(intKeyMapStr, '$.717', 'INT')) FROM "" + getTableName()
        + "" GROUP BY jsonExtractScalar(intKeyMapStr, '$.95', 'INT')"";
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    groupByResults = pinotResponse.get(""aggregationResults"").get(0).get(""groupByResult"");
    assertEquals(groupByResults.size(), 10);
    for (int i = 0; i < 10; i++) {
      JsonNode groupByResult = groupByResults.get(i);
      assertEquals(Integer.parseInt(groupByResult.get(""group"").get(0).asText()), i);
      assertEquals((int) Double.parseDouble(groupByResult.get(""value"").asText()), NUM_DOCS + i);
    }

    // Filter
    query = ""SELECT jsonExtractScalar(stringKeyMapStr, '$.k2', 'INT') FROM "" + getTableName()
        + "" WHERE jsonExtractScalar(stringKeyMapStr, '$.k1', 'INT') = 25"";
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    selectionResults = pinotResponse.get(""selectionResults"").get(""results"");
    assertEquals(selectionResults.size(), 1);
    assertEquals(Integer.parseInt(selectionResults.get(0).get(0).textValue()), NUM_DOCS + 25);
    query = ""SELECT jsonExtractScalar(intKeyMapStr, '$.717', 'INT') FROM "" + getTableName()
        + "" WHERE jsonExtractScalar(intKeyMapStr, '$.95', 'INT') = 25"";
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    selectionResults = pinotResponse.get(""selectionResults"").get(""results"");
    assertEquals(selectionResults.size(), 1);
    assertEquals(Integer.parseInt(selectionResults.get(0).get(0).textValue()), NUM_DOCS + 25);

    // Filter on non-existing key
    query = ""SELECT jsonExtractScalar(stringKeyMapStr, '$.k2', 'INT') FROM "" + getTableName()
        + "" WHERE jsonExtractScalar(stringKeyMapStr, '$.k3', 'INT_ARRAY') = 25"";
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    selectionResults = pinotResponse.get(""selectionResults"").get(""results"");
    assertEquals(selectionResults.size(), 0);
    query = ""SELECT jsonExtractScalar(intKeyMapStr, '$.717', 'INT') FROM "" + getTableName()
        + "" WHERE jsonExtractScalar(intKeyMapStr, '$.123', 'INT_ARRAY') = 25"";
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    selectionResults = pinotResponse.get(""selectionResults"").get(""results"");
    assertEquals(selectionResults.size(), 0);

    // Select non-existing key (illegal query)
    query = ""SELECT jsonExtractScalar(stringKeyMapStr, '$.k3', 'INT') FROM "" + getTableName();
    pinotResponse = postQuery(query);
    assertNotEquals(pinotResponse.get(""exceptions"").size(), 0);
    query = ""SELECT jsonExtractScalar(stringKeyMapStr, '$.123', 'INT') FROM "" + getTableName();
    pinotResponse = postQuery(query);
    assertNotEquals(pinotResponse.get(""exceptions"").size(), 0);

    // Select non-existing key with default value
    query = ""SELECT jsonExtractScalar(stringKeyMapStr, '$.k3', 'INT', '0') FROM "" + getTableName();
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    query = ""SELECT jsonExtractScalar(stringKeyMapStr, '$.123', 'INT', '0') FROM "" + getTableName();
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);

    // Select non-existing key with proper filter
    query = ""SELECT jsonExtractScalar(intKeyMapStr, '$.123', 'INT') FROM "" + getTableName()
        + "" WHERE jsonExtractKey(intKeyMapStr, '$.*') = \""$['123']\"""";
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    selectionResults = pinotResponse.get(""selectionResults"").get(""results"");
    assertEquals(selectionResults.size(), 0);
    query = ""SELECT jsonExtractScalar(stringKeyMapStr, '$.k3', 'INT') FROM "" + getTableName()
        + "" WHERE jsonExtractKey(stringKeyMapStr, '$.*') = \""$['k3']\"""";
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    selectionResults = pinotResponse.get(""selectionResults"").get(""results"");
    assertEquals(selectionResults.size(), 0);
  }
"
"  @Test
  public void testQueries()
      throws Exception {
    // Selection only
    String query = ""SELECT mapValue(stringKeyMap__KEYS, 'k1', stringKeyMap__VALUES) FROM "" + getTableName();
    JsonNode pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    JsonNode selectionResults = pinotResponse.get(""selectionResults"").get(""results"");
    assertEquals(selectionResults.size(), 10);
    for (int i = 0; i < 10; i++) {
      assertEquals(Integer.parseInt(selectionResults.get(i).get(0).textValue()), i);
    }
    query = ""SELECT mapValue(intKeyMap__KEYS, 95, intKeyMap__VALUES) FROM "" + getTableName();
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    selectionResults = pinotResponse.get(""selectionResults"").get(""results"");
    assertEquals(selectionResults.size(), 10);
    for (int i = 0; i < 10; i++) {
      assertEquals(Integer.parseInt(selectionResults.get(i).get(0).textValue()), i);
    }

    // Selection order-by
    query = ""SELECT mapValue(stringKeyMap__KEYS, 'k2', stringKeyMap__VALUES) FROM "" + getTableName()
        + "" ORDER BY mapValue(stringKeyMap__KEYS, 'k1', stringKeyMap__VALUES)"";
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    selectionResults = pinotResponse.get(""selectionResults"").get(""results"");
    assertEquals(selectionResults.size(), 10);
    for (int i = 0; i < 10; i++) {
      assertEquals(Integer.parseInt(selectionResults.get(i).get(0).textValue()), NUM_DOCS + i);
    }
    query = ""SELECT mapValue(intKeyMap__KEYS, 717, intKeyMap__VALUES) FROM "" + getTableName()
        + "" ORDER BY mapValue(intKeyMap__KEYS, 95, intKeyMap__VALUES)"";
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    selectionResults = pinotResponse.get(""selectionResults"").get(""results"");
    assertEquals(selectionResults.size(), 10);
    for (int i = 0; i < 10; i++) {
      assertEquals(Integer.parseInt(selectionResults.get(i).get(0).textValue()), NUM_DOCS + i);
    }

    // Aggregation only
    query = ""SELECT MAX(mapValue(stringKeyMap__KEYS, 'k1', stringKeyMap__VALUES)) FROM "" + getTableName();
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    JsonNode aggregationResult = pinotResponse.get(""aggregationResults"").get(0).get(""value"");
    assertEquals((int) Double.parseDouble(aggregationResult.textValue()), NUM_DOCS - 1);
    query = ""SELECT MAX(mapValue(intKeyMap__KEYS, 95, intKeyMap__VALUES)) FROM "" + getTableName();
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    aggregationResult = pinotResponse.get(""aggregationResults"").get(0).get(""value"");
    assertEquals((int) Double.parseDouble(aggregationResult.textValue()), NUM_DOCS - 1);

    // Aggregation group-by
    query = ""SELECT MIN(mapValue(stringKeyMap__KEYS, 'k2', stringKeyMap__VALUES)) FROM "" + getTableName()
        + "" GROUP BY mapValue(stringKeyMap__KEYS, 'k1', stringKeyMap__VALUES)"";
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    JsonNode groupByResults = pinotResponse.get(""aggregationResults"").get(0).get(""groupByResult"");
    assertEquals(groupByResults.size(), 10);
    for (int i = 0; i < 10; i++) {
      JsonNode groupByResult = groupByResults.get(i);
      assertEquals(Integer.parseInt(groupByResult.get(""group"").get(0).asText()), i);
      assertEquals((int) Double.parseDouble(groupByResult.get(""value"").asText()), NUM_DOCS + i);
    }
    query = ""SELECT MIN(mapValue(intKeyMap__KEYS, 717, intKeyMap__VALUES)) FROM "" + getTableName()
        + "" GROUP BY mapValue(intKeyMap__KEYS, 95, intKeyMap__VALUES)"";
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    groupByResults = pinotResponse.get(""aggregationResults"").get(0).get(""groupByResult"");
    assertEquals(groupByResults.size(), 10);
    for (int i = 0; i < 10; i++) {
      JsonNode groupByResult = groupByResults.get(i);
      assertEquals(Integer.parseInt(groupByResult.get(""group"").get(0).asText()), i);
      assertEquals((int) Double.parseDouble(groupByResult.get(""value"").asText()), NUM_DOCS + i);
    }

    // Filter
    query = ""SELECT mapValue(stringKeyMap__KEYS, 'k2', stringKeyMap__VALUES) FROM "" + getTableName()
        + "" WHERE mapValue(stringKeyMap__KEYS, 'k1', stringKeyMap__VALUES) = 25"";
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    selectionResults = pinotResponse.get(""selectionResults"").get(""results"");
    assertEquals(selectionResults.size(), 1);
    assertEquals(Integer.parseInt(selectionResults.get(0).get(0).textValue()), NUM_DOCS + 25);
    query = ""SELECT mapValue(intKeyMap__KEYS, 717, intKeyMap__VALUES) FROM "" + getTableName()
        + "" WHERE mapValue(intKeyMap__KEYS, 95, intKeyMap__VALUES) = 25"";
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    selectionResults = pinotResponse.get(""selectionResults"").get(""results"");
    assertEquals(selectionResults.size(), 1);
    assertEquals(Integer.parseInt(selectionResults.get(0).get(0).textValue()), NUM_DOCS + 25);

    // Filter on non-existing key
    query = ""SELECT mapValue(stringKeyMap__KEYS, 'k2', stringKeyMap__VALUES) FROM "" + getTableName()
        + "" WHERE mapValue(stringKeyMap__KEYS, 'k3', stringKeyMap__VALUES) = 25"";
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    selectionResults = pinotResponse.get(""selectionResults"").get(""results"");
    assertEquals(selectionResults.size(), 0);
    query = ""SELECT mapValue(intKeyMap__KEYS, 717, intKeyMap__VALUES) FROM "" + getTableName()
        + "" WHERE mapValue(intKeyMap__KEYS, 123, intKeyMap__VALUES) = 25"";
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    selectionResults = pinotResponse.get(""selectionResults"").get(""results"");
    assertEquals(selectionResults.size(), 0);

    // Select non-existing key (illegal query)
    query = ""SELECT mapValue(stringKeyMap__KEYS, 'k3', stringKeyMap__VALUES) FROM "" + getTableName();
    pinotResponse = postQuery(query);
    assertNotEquals(pinotResponse.get(""exceptions"").size(), 0);
    query = ""SELECT mapValue(stringKeyMap__KEYS, 123, stringKeyMap__VALUES) FROM "" + getTableName();
    pinotResponse = postQuery(query);
    assertNotEquals(pinotResponse.get(""exceptions"").size(), 0);

    // Select non-existing key with proper filter
    query = ""SELECT mapValue(stringKeyMap__KEYS, 'k3', stringKeyMap__VALUES) FROM "" + getTableName()
        + "" WHERE stringKeyMap__KEYS = 'k3'"";
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    selectionResults = pinotResponse.get(""selectionResults"").get(""results"");
    assertEquals(selectionResults.size(), 0);
    query = ""SELECT mapValue(intKeyMap__KEYS, 123, intKeyMap__VALUES) FROM "" + getTableName()
        + "" WHERE stringKeyMap__KEYS = 123"";
    pinotResponse = postQuery(query);
    assertEquals(pinotResponse.get(""exceptions"").size(), 0);
    selectionResults = pinotResponse.get(""selectionResults"").get(""results"");
    assertEquals(selectionResults.size(), 0);
  }
"
"  @Test
  public void testSegmentAssignment()
      throws Exception {
    IdealState idealState = HelixHelper.getTableIdealState(_helixManager, TABLE_NAME_WITH_TYPE);
    Assert.assertEquals(getCurrentCountStarResult(), getCountStarResult());
    verifyTableIdealStates(idealState);
    // Wait 3 seconds to let the realtime validation thread to run.
    Thread.sleep(3000);
    // Verify the result again.
    Assert.assertEquals(getCurrentCountStarResult(), getCountStarResult());
    verifyTableIdealStates(idealState);
  }
"
"  @Test
  public void testSegmentUploadDownload()
      throws Exception {
    final Request query = new Request(""sql"", ""SELECT count(*) FROM "" + getTableName());

    ResultSetGroup resultBeforeOffline = getPinotConnection().execute(query);
    Assert.assertTrue(resultBeforeOffline.getResultSet(0).getLong(0) > 0);

    // schedule offline segment generation
    Assert.assertNotNull(_controllerStarter.getTaskManager().scheduleTasks());

    // wait for offline segments
    JsonNode offlineSegments = TestUtils.waitForResult(() -> {
      JsonNode segmentSets = JsonUtils.stringToJsonNode(
          sendGetRequest(_controllerRequestURLBuilder.forSegmentListAPI(getTableName()), AUTH_HEADER));
      JsonNode currentOfflineSegments =
          new IntRange(0, segmentSets.size()).stream().map(segmentSets::get).filter(s -> s.has(""OFFLINE""))
              .map(s -> s.get(""OFFLINE"")).findFirst().get();
      Assert.assertFalse(currentOfflineSegments.isEmpty());
      return currentOfflineSegments;
    }, 30000);

    // Verify constant row count
    ResultSetGroup resultAfterOffline = getPinotConnection().execute(query);
    Assert.assertEquals(resultBeforeOffline.getResultSet(0).getLong(0), resultAfterOffline.getResultSet(0).getLong(0));

    // download and sanity-check size of offline segment(s)
    for (int i = 0; i < offlineSegments.size(); i++) {
      String segment = offlineSegments.get(i).asText();
      Assert.assertTrue(
          sendGetRequest(_controllerRequestURLBuilder.forSegmentDownload(getTableName(), segment), AUTH_HEADER).length()
              > 200000); // download segment
    }
  }
"
"  @Test
  public void testConvertToRawIndexTask()
      throws Exception {
    String offlineTableName = TableNameBuilder.OFFLINE.tableNameWithType(getTableName());

    File testDataDir = new File(CommonConstants.Server.DEFAULT_INSTANCE_DATA_DIR + ""-0"", offlineTableName);
    if (!testDataDir.isDirectory()) {
      testDataDir = new File(CommonConstants.Server.DEFAULT_INSTANCE_DATA_DIR + ""-1"", offlineTableName);
    }
    Assert.assertTrue(testDataDir.isDirectory());
    File tableDataDir = testDataDir;

    // Check that all columns have dictionary
    File[] indexDirs = tableDataDir.listFiles();
    Assert.assertNotNull(indexDirs);
    for (File indexDir : indexDirs) {
      SegmentMetadata segmentMetadata = new SegmentMetadataImpl(indexDir);
      for (String columnName : segmentMetadata.getSchema().getColumnNames()) {
        Assert.assertTrue(segmentMetadata.getColumnMetadataFor(columnName).hasDictionary());
      }
    }

    // Should create the task queues and generate a ConvertToRawIndexTask task with 5 child tasks
    Assert.assertNotNull(_taskManager.scheduleTasks().get(ConvertToRawIndexTask.TASK_TYPE));
    Assert.assertTrue(_helixTaskResourceManager.getTaskQueues()
        .contains(PinotHelixTaskResourceManager.getHelixJobQueueName(ConvertToRawIndexTask.TASK_TYPE)));

    // Should generate one more ConvertToRawIndexTask task with 3 child tasks
    Assert.assertNotNull(_taskManager.scheduleTasks().get(ConvertToRawIndexTask.TASK_TYPE));

    // Should not generate more tasks
    Assert.assertNull(_taskManager.scheduleTasks().get(ConvertToRawIndexTask.TASK_TYPE));

    // Wait at most 600 seconds for all tasks COMPLETED and new segments refreshed
    TestUtils.waitForCondition(input -> {
      // Check task state
      for (TaskState taskState : _helixTaskResourceManager.getTaskStates(ConvertToRawIndexTask.TASK_TYPE).values()) {
        if (taskState != TaskState.COMPLETED) {
          return false;
        }
      }

      // Check segment ZK metadata
      for (SegmentZKMetadata segmentZKMetadata : _helixResourceManager.getSegmentsZKMetadata(offlineTableName)) {
        Map<String, String> customMap = segmentZKMetadata.getCustomMap();
        if (customMap == null || customMap.size() != 1 || !customMap
            .containsKey(ConvertToRawIndexTask.TASK_TYPE + MinionConstants.TASK_TIME_SUFFIX)) {
          return false;
        }
      }

      // Check segment metadata
      File[] indexDirs1 = tableDataDir.listFiles();
      Assert.assertNotNull(indexDirs1);
      for (File indexDir : indexDirs1) {
        SegmentMetadata segmentMetadata;

        // Segment metadata file might not exist if the segment is refreshing
        try {
          segmentMetadata = new SegmentMetadataImpl(indexDir);
        } catch (Exception e) {
          return false;
        }

        // The columns in COLUMNS_TO_CONVERT should have raw index
        List<String> rawIndexColumns = Arrays.asList(StringUtils.split(COLUMNS_TO_CONVERT, ','));
        for (String columnName : segmentMetadata.getSchema().getColumnNames()) {
          if (rawIndexColumns.contains(columnName)) {
            if (segmentMetadata.getColumnMetadataFor(columnName).hasDictionary()) {
              return false;
            }
          } else {
            if (!segmentMetadata.getColumnMetadataFor(columnName).hasDictionary()) {
              return false;
            }
          }
        }
      }

      return true;
    }, 600_000L, ""Failed to get all tasks COMPLETED and new segments refreshed"");
  }
"
"  @Test
  public void testPinotHelixResourceManagerAPIs() {
    // Instance APIs
    Assert.assertEquals(_helixResourceManager.getAllInstances().size(), 5);
    Assert.assertEquals(_helixResourceManager.getOnlineInstanceList().size(), 5);
    Assert.assertEquals(_helixResourceManager.getOnlineUnTaggedBrokerInstanceList().size(), 0);
    Assert.assertEquals(_helixResourceManager.getOnlineUnTaggedServerInstanceList().size(), 0);

    // Table APIs
    String rawTableName = getTableName();
    String offlineTableName = TableNameBuilder.OFFLINE.tableNameWithType(rawTableName);
    String realtimeTableName = TableNameBuilder.REALTIME.tableNameWithType(rawTableName);
    List<String> tableNames = _helixResourceManager.getAllTables();
    Assert.assertEquals(tableNames.size(), 2);
    Assert.assertTrue(tableNames.contains(offlineTableName));
    Assert.assertTrue(tableNames.contains(realtimeTableName));
    Assert.assertEquals(_helixResourceManager.getAllRawTables(), Collections.singletonList(rawTableName));
    Assert.assertEquals(_helixResourceManager.getAllRealtimeTables(), Collections.singletonList(realtimeTableName));

    // Tenant APIs
    Assert.assertEquals(_helixResourceManager.getAllBrokerTenantNames(), Collections.singleton(""TestTenant""));
    Assert.assertEquals(_helixResourceManager.getAllServerTenantNames(), Collections.singleton(""TestTenant""));
  }
"
" * <p>To enable the test, override it and add @Test annotation.
  public void testHardcodedQueries()
      throws Exception {
    // Here are some sample queries.
    String query;
    query = ""SELECT COUNT(*) FROM mytable WHERE DaysSinceEpoch = 16312 AND Carrier = 'DL'"";
    testQuery(query, Collections.singletonList(query));
    query = ""SELECT COUNT(*) FROM mytable WHERE DaysSinceEpoch <> 16312 AND Carrier = 'DL'"";
    testQuery(query, Collections.singletonList(query));
    query = ""SELECT COUNT(*) FROM mytable WHERE DaysSinceEpoch > 16312 AND Carrier = 'DL'"";
    testQuery(query, Collections.singletonList(query));
    query = ""SELECT COUNT(*) FROM mytable WHERE DaysSinceEpoch >= 16312 AND Carrier = 'DL'"";
    testQuery(query, Collections.singletonList(query));
    query = ""SELECT COUNT(*) FROM mytable WHERE DaysSinceEpoch < 16312 AND Carrier = 'DL'"";
    testQuery(query, Collections.singletonList(query));
    query = ""SELECT COUNT(*) FROM mytable WHERE DaysSinceEpoch <= 16312 AND Carrier = 'DL'"";
    testQuery(query, Collections.singletonList(query));
    query = ""SELECT MAX(ArrTime), MIN(ArrTime) FROM mytable WHERE DaysSinceEpoch >= 16312"";
    testQuery(query, Arrays.asList(""SELECT MAX(ArrTime) FROM mytable WHERE DaysSinceEpoch >= 15312"",
        ""SELECT MIN(ArrTime) FROM mytable WHERE DaysSinceEpoch >= 15312""));
    query =
        ""SELECT SUM(TotalAddGTime) FROM mytable WHERE DivArrDelay NOT IN (67, 260) AND Carrier IN ('F9', 'B6') OR ""
            + ""DepTime BETWEEN 2144 AND 1926"";
    testQuery(query, Collections.singletonList(query));
  }
"
"  @Test
  public void testConsumerDirectoryExists() {
    File consumerDirectory = new File(CONSUMER_DIRECTORY, ""mytable_REALTIME"");
    assertEquals(consumerDirectory.exists(), _isConsumerDirConfigured,
        ""The off heap consumer directory does not exist"");
  }
"
"  @Test
  public void testSegmentFlushSize() {
    String realtimeTableName = TableNameBuilder.REALTIME.tableNameWithType(getTableName());
    List<SegmentZKMetadata> segmentsZKMetadata =
        ZKMetadataProvider.getSegmentsZKMetadata(_propertyStore, realtimeTableName);
    for (SegmentZKMetadata segmentZKMetadata : segmentsZKMetadata) {
      assertEquals(segmentZKMetadata.getSizeThresholdToFlushSegment(),
          getRealtimeSegmentFlushSize() / getNumKafkaPartitions());
    }
  }
"
"  @Test
  public void testInvertedIndexTriggering()
      throws Exception {
    long numTotalDocs = getCountStarResult();

    JsonNode queryResponse = postQuery(TEST_UPDATED_INVERTED_INDEX_QUERY);
    assertEquals(queryResponse.get(""totalDocs"").asLong(), numTotalDocs);
    assertTrue(queryResponse.get(""numEntriesScannedInFilter"").asLong() > 0L);

    TableConfig tableConfig = getRealtimeTableConfig();
    tableConfig.getIndexingConfig().setInvertedIndexColumns(UPDATED_INVERTED_INDEX_COLUMNS);
    updateTableConfig(tableConfig);
    reloadRealtimeTable(getTableName());

    TestUtils.waitForCondition(aVoid -> {
      try {
        JsonNode queryResponse1 = postQuery(TEST_UPDATED_INVERTED_INDEX_QUERY);
        // Total docs should not change during reload
        assertEquals(queryResponse1.get(""totalDocs"").asLong(), numTotalDocs);
        assertEquals(queryResponse1.get(""numConsumingSegmentsQueried"").asLong(), 2);
        assertTrue(queryResponse1.get(""minConsumingFreshnessTimeMs"").asLong() > _startTime);
        assertTrue(queryResponse1.get(""minConsumingFreshnessTimeMs"").asLong() < System.currentTimeMillis());
        return queryResponse1.get(""numEntriesScannedInFilter"").asLong() == 0;
      } catch (Exception e) {
        throw new RuntimeException(e);
      }
    }, 600_000L, ""Failed to generate inverted index"");
  }
"
"  @Test(expectedExceptions = IOException.class)
  public void testAddHLCTableShouldFail()
      throws IOException {
    TableConfig tableConfig = new TableConfigBuilder(TableType.REALTIME).setTableName(""testTable"")
        .setStreamConfigs(Collections.singletonMap(""stream.kafka.consumer.type"", ""HIGHLEVEL"")).build();
    sendPostRequest(_controllerRequestURLBuilder.forTableCreate(), tableConfig.toJsonString());
  }
"
"  @Test
  public void testReload()
      throws Exception {
    testReload(false);
  }
"
"  @Test
  public void testHardcodedServerPartitionedSqlQueries()
      throws Exception {
    super.testHardcodedServerPartitionedSqlQueries();
  }
"
"  @Test
  public void testFileBasedSegmentWriterAndDefaultUploader()
      throws Exception {

    TableConfig offlineTableConfig = createOfflineTableConfig();
    addTableConfig(offlineTableConfig);

    SegmentWriter segmentWriter = new FileBasedSegmentWriter();
    segmentWriter.init(offlineTableConfig, _schema);
    SegmentUploader segmentUploader = new SegmentUploaderDefault();
    segmentUploader.init(offlineTableConfig);

    GenericRow reuse = new GenericRow();
    long totalDocs = 0;
    for (int i = 0; i < 3; i++) {
      AvroRecordReader avroRecordReader = new AvroRecordReader();
      avroRecordReader.init(_avroFiles.get(i), null, null);

      long numDocsInSegment = 0;
      while (avroRecordReader.hasNext()) {
        avroRecordReader.next(reuse);
        segmentWriter.collect(reuse);
        numDocsInSegment++;
        totalDocs++;
      }
      // flush to segment
      URI segmentTarURI = segmentWriter.flush();
      // upload
      segmentUploader.uploadSegment(segmentTarURI, null);

      // check num segments
      Assert.assertEquals(getNumSegments(), i + 1);
      // check numDocs in latest segment
      Assert.assertEquals(getNumDocsInLatestSegment(), numDocsInSegment);
      // check totalDocs in query
      checkTotalDocsInQuery(totalDocs);
    }
    segmentWriter.close();

    dropAllSegments(_tableNameWithType, TableType.OFFLINE);
    checkNumSegments(0);

    // upload all together using dir
    segmentUploader.uploadSegmentsFromDir(_tarDir.toURI(), null);
    // check num segments
    Assert.assertEquals(getNumSegments(), 3);
    // check totalDocs in query
    checkTotalDocsInQuery(totalDocs);

    dropOfflineTable(_tableNameWithType);
  }
"
"  @Test
  public void testSegmentUploadDownload()
      throws Exception {
    final Request query = new Request(""sql"", ""SELECT count(*) FROM "" + getTableName());

    ResultSetGroup resultBeforeOffline = getPinotConnection().execute(query);
    Assert.assertTrue(resultBeforeOffline.getResultSet(0).getLong(0) > 0);

    // schedule offline segment generation
    Assert.assertNotNull(_controllerStarter.getTaskManager().scheduleTasks());

    // wait for offline segments
    JsonNode offlineSegments = TestUtils.waitForResult(() -> {
      JsonNode segmentSets = JsonUtils.stringToJsonNode(
          sendGetRequest(_controllerRequestURLBuilder.forSegmentListAPI(getTableName()), AUTH_HEADER));
      JsonNode currentOfflineSegments =
          new IntRange(0, segmentSets.size()).stream().map(segmentSets::get).filter(s -> s.has(""OFFLINE""))
              .map(s -> s.get(""OFFLINE"")).findFirst().get();
      Assert.assertFalse(currentOfflineSegments.isEmpty());
      return currentOfflineSegments;
    }, 30000);

    // Verify constant row count
    ResultSetGroup resultAfterOffline = getPinotConnection().execute(query);
    Assert.assertEquals(resultBeforeOffline.getResultSet(0).getLong(0), resultAfterOffline.getResultSet(0).getLong(0));

    // download and sanity-check size of offline segment(s)
    for (int i = 0; i < offlineSegments.size(); i++) {
      String segment = offlineSegments.get(i).asText();
      Assert.assertTrue(
          sendGetRequest(_controllerRequestURLBuilder.forSegmentDownload(getTableName(), segment), AUTH_HEADER).length()
              > 200000); // download segment
    }
  }
"
"  @Test
  public void testDefaultServerConf()
      throws Exception {
    String expectedHost = NetUtils.getHostAddress();
    String expectedInstanceId = PREFIX_OF_SERVER_INSTANCE + expectedHost + ""_"" + DEFAULT_SERVER_NETTY_PORT;

    verifyInstanceConfig(new PinotConfiguration(), expectedInstanceId, expectedHost, DEFAULT_SERVER_NETTY_PORT);
  }
"
"  @Test
  public void testSetInstanceIdToHostname()
      throws Exception {
    String expectedHost = NetUtils.getHostnameOrAddress();
    String expectedInstanceId = PREFIX_OF_SERVER_INSTANCE + expectedHost + ""_"" + DEFAULT_SERVER_NETTY_PORT;

    Map<String, Object> properties = new HashMap<>();
    properties.put(SET_INSTANCE_ID_TO_HOSTNAME_KEY, true);

    verifyInstanceConfig(new PinotConfiguration(properties), expectedInstanceId, expectedHost,
        DEFAULT_SERVER_NETTY_PORT);
  }
"
"  @Test
  public void testCustomInstanceId()
      throws Exception {
    Map<String, Object> properties = new HashMap<>();
    properties.put(CONFIG_OF_INSTANCE_ID, CUSTOM_INSTANCE_ID);

    verifyInstanceConfig(new PinotConfiguration(properties), CUSTOM_INSTANCE_ID, NetUtils.getHostAddress(),
        DEFAULT_SERVER_NETTY_PORT);
  }
"
"  @Test
  public void testCustomHost()
      throws Exception {
    String expectedInstanceId = PREFIX_OF_SERVER_INSTANCE + CUSTOM_HOST + ""_"" + DEFAULT_SERVER_NETTY_PORT;

    Map<String, Object> properties = new HashMap<>();
    properties.put(KEY_OF_SERVER_NETTY_HOST, CUSTOM_HOST);

    verifyInstanceConfig(new PinotConfiguration(properties), expectedInstanceId, CUSTOM_HOST,
        DEFAULT_SERVER_NETTY_PORT);
  }
"
"  @Test
  public void testCustomPort()
      throws Exception {
    String expectedHost = NetUtils.getHostAddress();
    String expectedInstanceId = PREFIX_OF_SERVER_INSTANCE + expectedHost + ""_"" + CUSTOM_PORT;

    Map<String, Object> properties = new HashMap<>();
    properties.put(KEY_OF_SERVER_NETTY_PORT, CUSTOM_PORT);

    verifyInstanceConfig(new PinotConfiguration(properties), expectedInstanceId, expectedHost, CUSTOM_PORT);
  }
"
"  @Test
  public void testAllCustomServerConf()
      throws Exception {
    Map<String, Object> properties = new HashMap<>();
    properties.put(CONFIG_OF_INSTANCE_ID, CUSTOM_INSTANCE_ID);
    properties.put(KEY_OF_SERVER_NETTY_HOST, CUSTOM_HOST);
    properties.put(KEY_OF_SERVER_NETTY_PORT, CUSTOM_PORT);
    verifyInstanceConfig(new PinotConfiguration(properties), CUSTOM_INSTANCE_ID, CUSTOM_HOST, CUSTOM_PORT);
  }
"
"  @Test
  public void testPqlQueries()
      throws Exception {

    //Selection Query
    String pqlQuery = ""Select "" + MY_MAP_STR_FIELD_NAME + "" from "" + DEFAULT_TABLE_NAME;
    JsonNode pinotResponse = postQuery(pqlQuery);
    ArrayNode selectionResults = (ArrayNode) pinotResponse.get(""selectionResults"").get(""results"");
    Assert.assertNotNull(selectionResults);
    Assert.assertFalse(selectionResults.isEmpty());
    for (int i = 0; i < selectionResults.size(); i++) {
      String value = selectionResults.get(i).get(0).textValue();
      Assert.assertTrue(value.indexOf(""-k1-"") > 0);
    }

    //Filter Query
    pqlQuery = ""Select jsonExtractScalar(myMapStr,'$.k1','STRING') from "" + DEFAULT_TABLE_NAME
        + ""  where jsonExtractScalar(myMapStr,'$.k1','STRING') = 'value-k1-0'"";
    pinotResponse = postQuery(pqlQuery);
    selectionResults = (ArrayNode) pinotResponse.get(""selectionResults"").get(""results"");
    Assert.assertNotNull(selectionResults);
    Assert.assertFalse(selectionResults.isEmpty());
    for (int i = 0; i < selectionResults.size(); i++) {
      String value = selectionResults.get(i).get(0).textValue();
      Assert.assertEquals(value, ""value-k1-0"");
    }
    pqlQuery =
        ""Select "" + MY_MAP_STR_K1_FIELD_NAME + "" from "" + DEFAULT_TABLE_NAME + ""  where "" + MY_MAP_STR_K1_FIELD_NAME
            + "" = 'value-k1-0'"";
    pinotResponse = postQuery(pqlQuery);
    selectionResults = (ArrayNode) pinotResponse.get(""selectionResults"").get(""results"");
    Assert.assertNotNull(selectionResults);
    Assert.assertFalse(selectionResults.isEmpty());
    for (int i = 0; i < selectionResults.size(); i++) {
      String value = selectionResults.get(i).get(0).textValue();
      Assert.assertEquals(value, ""value-k1-0"");
    }

    //selection order by
    pqlQuery = ""Select jsonExtractScalar(myMapStr,'$.k1','STRING') from "" + DEFAULT_TABLE_NAME
        + "" order by jsonExtractScalar(myMapStr,'$.k1','STRING')"";
    pinotResponse = postQuery(pqlQuery);
    selectionResults = (ArrayNode) pinotResponse.get(""selectionResults"").get(""results"");
    Assert.assertNotNull(selectionResults);
    Assert.assertFalse(selectionResults.isEmpty());
    for (int i = 0; i < selectionResults.size(); i++) {
      String value = selectionResults.get(i).get(0).textValue();
      Assert.assertTrue(value.indexOf(""-k1-"") > 0);
    }
    pqlQuery =
        ""Select "" + MY_MAP_STR_K1_FIELD_NAME + "" from "" + DEFAULT_TABLE_NAME + "" order by "" + MY_MAP_STR_K1_FIELD_NAME;
    pinotResponse = postQuery(pqlQuery);
    selectionResults = (ArrayNode) pinotResponse.get(""selectionResults"").get(""results"");
    Assert.assertNotNull(selectionResults);
    Assert.assertFalse(selectionResults.isEmpty());
    for (int i = 0; i < selectionResults.size(); i++) {
      String value = selectionResults.get(i).get(0).textValue();
      Assert.assertTrue(value.indexOf(""-k1-"") > 0);
    }

    //Group By Query
    pqlQuery = ""Select count(*) from "" + DEFAULT_TABLE_NAME + "" group by jsonExtractScalar(myMapStr,'$.k1','STRING')"";
    pinotResponse = postQuery(pqlQuery);
    Assert.assertNotNull(pinotResponse.get(""aggregationResults""));
    JsonNode groupByResult = pinotResponse.get(""aggregationResults"").get(0).get(""groupByResult"");
    Assert.assertNotNull(groupByResult);
    Assert.assertTrue(groupByResult.isArray());
    Assert.assertFalse(groupByResult.isEmpty());

    pqlQuery = ""Select count(*) from "" + DEFAULT_TABLE_NAME + "" group by "" + MY_MAP_STR_K1_FIELD_NAME;
    pinotResponse = postQuery(pqlQuery);
    Assert.assertNotNull(pinotResponse.get(""aggregationResults""));
    groupByResult = pinotResponse.get(""aggregationResults"").get(0).get(""groupByResult"");
    Assert.assertNotNull(groupByResult);
    Assert.assertTrue(groupByResult.isArray());
    Assert.assertFalse(groupByResult.isEmpty());
  }
"
"  @Test
  public void testSqlQueries()
      throws Exception {
    //Selection Query
    String sqlQuery = ""Select myMapStr from "" + DEFAULT_TABLE_NAME;
    JsonNode pinotResponse = postSqlQuery(sqlQuery);
    ArrayNode rows = (ArrayNode) pinotResponse.get(""resultTable"").get(""rows"");
    Assert.assertNotNull(rows);
    Assert.assertFalse(rows.isEmpty());
    for (int i = 0; i < rows.size(); i++) {
      String value = rows.get(i).get(0).textValue();
      Assert.assertTrue(value.indexOf(""-k1-"") > 0);
    }

    //Filter Query
    sqlQuery = ""Select jsonExtractScalar(myMapStr,'$.k1','STRING') from "" + DEFAULT_TABLE_NAME
        + ""  where jsonExtractScalar(myMapStr,'$.k1','STRING') = 'value-k1-0'"";
    pinotResponse = postSqlQuery(sqlQuery);
    rows = (ArrayNode) pinotResponse.get(""resultTable"").get(""rows"");
    Assert.assertNotNull(rows);
    Assert.assertFalse(rows.isEmpty());
    for (int i = 0; i < rows.size(); i++) {
      String value = rows.get(i).get(0).textValue();
      Assert.assertEquals(value, ""value-k1-0"");
    }

    //selection order by
    sqlQuery = ""Select jsonExtractScalar(myMapStr,'$.k1','STRING') from "" + DEFAULT_TABLE_NAME
        + "" order by jsonExtractScalar(myMapStr,'$.k1','STRING')"";
    pinotResponse = postSqlQuery(sqlQuery);
    rows = (ArrayNode) pinotResponse.get(""resultTable"").get(""rows"");
    Assert.assertNotNull(rows);
    Assert.assertFalse(rows.isEmpty());
    for (int i = 0; i < rows.size(); i++) {
      String value = rows.get(i).get(0).textValue();
      Assert.assertTrue(value.indexOf(""-k1-"") > 0);
    }

    //Group By Query
    sqlQuery = ""Select jsonExtractScalar(myMapStr,'$.k1','STRING'), count(*) from "" + DEFAULT_TABLE_NAME
        + "" group by jsonExtractScalar(myMapStr,'$.k1','STRING')"";
    pinotResponse = postSqlQuery(sqlQuery);
    Assert.assertNotNull(pinotResponse.get(""resultTable""));
    rows = (ArrayNode) pinotResponse.get(""resultTable"").get(""rows"");
    for (int i = 0; i < rows.size(); i++) {
      String value = rows.get(i).get(0).textValue();
      Assert.assertTrue(value.indexOf(""-k1-"") > 0);
    }
  }
"
"    @Test
    public void repeatsOperationWithQueryByChangeOfTable() {
        User user = putUserBlocking();

        PreparedGetObject<User> operation = storIOSQLite
                .get()
                .object(User.class)
                .withQuery(query)
                .prepare();

        verifyChangesReceived(operation, tableChanges, user);
    }
"
"    @Test
    public void repeatsOperationWithRawQueryByChangeOfTable() {
        User user = putUserBlocking();

        PreparedGetObject<User> operation = storIOSQLite
                .get()
                .object(User.class)
                .withQuery(query)
                .prepare();

        verifyChangesReceived(operation, tableChanges, user);
    }
"
"    @Test
    public void repeatsOperationWithQueryByChangeOfTag() {
        User user = putUserBlocking();

        PreparedGetObject<User> operation = storIOSQLite
                .get()
                .object(User.class)
                .withQuery(query)
                .prepare();

        verifyChangesReceived(operation, tagChanges, user);
    }
"
"    @Test
    public void repeatsOperationWithRawQueryByChangeOfTag() {
        User user = putUserBlocking();

        PreparedGetObject<User> operation = storIOSQLite
                .get()
                .object(User.class)
                .withQuery(query)
                .prepare();

        verifyChangesReceived(operation, tagChanges, user);
    }
"
"    @Test
    public void repeatsOperationWithQueryByChangeOfTable() {
        User user = putUserBlocking();

        PreparedGetListOfObjects<User> operation = storIOSQLite
                .get()
                .listOfObjects(User.class)
                .withQuery(query)
                .prepare();

        verifyChangesReceived(operation, tableChanges, singletonList(user));
    }
"
"    @Test
    public void repeatsOperationWithRawQueryByChangeOfTable() {
        User user = putUserBlocking();

        PreparedGetListOfObjects<User> operation = storIOSQLite
                .get()
                .listOfObjects(User.class)
                .withQuery(rawQuery)
                .prepare();

        verifyChangesReceived(operation, tableChanges, singletonList(user));
    }
"
"    @Test
    public void repeatsOperationWithQueryByChangeOfTag() {
        User user = putUserBlocking();

        PreparedGetListOfObjects<User> operation = storIOSQLite
                .get()
                .listOfObjects(User.class)
                .withQuery(query)
                .prepare();

        verifyChangesReceived(operation, tagChanges, singletonList(user));
    }
"
"    @Test
    public void repeatsOperationWithRawQueryByChangeOfTag() {
        User user = putUserBlocking();

        PreparedGetListOfObjects<User> operation = storIOSQLite
                .get()
                .listOfObjects(User.class)
                .withQuery(rawQuery)
                .prepare();

        verifyChangesReceived(operation, tagChanges, singletonList(user));
    }
"
"    @Test
    public void insertEmission() {
        final List<User> users = TestFactory.newUsers(10);

        final Queue<Changes> expectedChanges = new LinkedList<Changes>();
        expectedChanges.add(Changes.newInstance(UserTableMeta.TABLE, UserTableMeta.NOTIFICATION_TAG));

        final EmissionChecker emissionChecker = new EmissionChecker(expectedChanges);
        final Subscription subscription = emissionChecker.subscribe();

        putUsersBlocking(users);

        // Should receive changes of Users table
        emissionChecker.awaitNextExpectedValue();

        emissionChecker.assertThatNoExpectedValuesLeft();

        subscription.unsubscribe();
    }
"
"    @Test
    public void updateEmission() {
        final List<User> users = putUsersBlocking(10);
        final List<User> updated = new ArrayList<User>(users.size());

        for (User user : users) {
            updated.add(User.newInstance(user.id(), user.email()));
        }

        final Queue<Changes> expectedChanges = new LinkedList<Changes>();
        expectedChanges.add(Changes.newInstance(UserTableMeta.TABLE, UserTableMeta.NOTIFICATION_TAG));

        final EmissionChecker emissionChecker = new EmissionChecker(expectedChanges);
        final Subscription subscription = emissionChecker.subscribe();

        storIOSQLite
                .put()
                .objects(updated)
                .prepare()
                .executeAsBlocking();

        // Should receive changes of Users table
        emissionChecker.awaitNextExpectedValue();

        emissionChecker.assertThatNoExpectedValuesLeft();

        subscription.unsubscribe();
    }
"
"    @Test
    public void deleteEmission() {
        final List<User> users = putUsersBlocking(10);

        final Queue<Changes> expectedChanges = new LinkedList<Changes>();
        expectedChanges.add(Changes.newInstance(UserTableMeta.TABLE, UserTableMeta.NOTIFICATION_TAG));

        final EmissionChecker emissionChecker = new EmissionChecker(expectedChanges);
        final Subscription subscription = emissionChecker.subscribe();

        deleteUsersBlocking(users);

        // Should receive changes of Users table
        emissionChecker.awaitNextExpectedValue();

        emissionChecker.assertThatNoExpectedValuesLeft();

        subscription.unsubscribe();
    }
"
"    @Test
    public void queryAll() {
        final List<User> users = putUsersBlocking(3);
        final List<User> usersFromQuery = getAllUsersBlocking();
        assertThat(users.equals(usersFromQuery)).isTrue();
    }
"
"    @Test
    public void queryOneByField() {
        final List<User> users = putUsersBlocking(3);

        for (User user : users) {
            final List<User> usersFromQuery = storIOSQLite
                    .get()
                    .listOfObjects(User.class)
                    .withQuery(Query.builder()
                            .table(UserTableMeta.TABLE)
                            .where(UserTableMeta.COLUMN_EMAIL + ""=?"")
                            .whereArgs(user.email())
                            .build())
                    .prepare()
                    .executeAsBlocking();

            assertThat(usersFromQuery).isNotNull();
            assertThat(usersFromQuery).hasSize(1);
            assertThat(usersFromQuery.get(0)).isEqualTo(user);
        }
    }
"
"    @Test
    public void queryOrdered() {
        final List<User> users = TestFactory.newUsers(3);

        // Reverse sorting by email before inserting, for the purity of the experiment.
        Collections.reverse(users);

        putUsersBlocking(users);

        final List<User> usersFromQueryOrdered = storIOSQLite
                .get()
                .listOfObjects(User.class)
                .withQuery(Query.builder()
                        .table(UserTableMeta.TABLE)
                        .orderBy(UserTableMeta.COLUMN_EMAIL)
                        .build())
                .prepare()
                .executeAsBlocking();

        assertThat(usersFromQueryOrdered).isNotNull();
        assertThat(usersFromQueryOrdered).hasSize(users.size());

        // Sorting by email for check ordering.
        Collections.sort(users);

        for (int i = 0; i < users.size(); i++) {
            assertThat(usersFromQueryOrdered.get(i)).isEqualTo(users.get(i));
        }
    }
"
"    @Test
    public void queryOrderedDesc() {
        final List<User> users = TestFactory.newUsers(3);

        // Sorting by email before inserting, for the purity of the experiment.
        Collections.sort(users);

        putUsersBlocking(users);

        final List<User> usersFromQueryOrdered = storIOSQLite
                .get()
                .listOfObjects(User.class)
                .withQuery(Query.builder()
                        .table(UserTableMeta.TABLE)
                        .orderBy(UserTableMeta.COLUMN_EMAIL + "" DESC"")
                        .build())
                .prepare()
                .executeAsBlocking();

        assertThat(usersFromQueryOrdered).isNotNull();
        assertThat(usersFromQueryOrdered).hasSize(users.size());

        // Reverse sorting by email for check ordering.
        Collections.reverse(users);

        for (int i = 0; i < users.size(); i++) {
            assertThat(usersFromQueryOrdered.get(i)).isEqualTo(users.get(i));
        }
    }
"
"    @Test
    public void querySingleLimit() {
        putUsersBlocking(10);

        final int limit = 8;
        final List<User> usersFromQuery = storIOSQLite
                .get()
                .listOfObjects(User.class)
                .withQuery(Query.builder()
                        .table(UserTableMeta.TABLE)
                        .limit(String.valueOf(limit))
                        .build())
                .prepare()
                .executeAsBlocking();

        assertThat(usersFromQuery).isNotNull();
        assertThat(usersFromQuery).hasSize(limit);
    }
"
"    @Test
    public void queryLimitOffset() {
        final List<User> users = putUsersBlocking(10);

        final int offset = 5;
        final int limit = 3;
        final List<User> usersFromQuery = storIOSQLite
                .get()
                .listOfObjects(User.class)
                .withQuery(Query.builder()
                        .table(UserTableMeta.TABLE)
                        .orderBy(UserTableMeta.COLUMN_EMAIL)
                        .limit(offset + "", "" + limit)
                        .build())
                .prepare()
                .executeAsBlocking();

        assertThat(usersFromQuery).isNotNull();
        assertThat(usersFromQuery).hasSize(Math.min(limit, users.size() - offset));

        Collections.sort(users);

        int position = 0;
        for (int i = offset; i < offset + limit; i++) {
            assertThat(usersFromQuery.get(position++)).isEqualTo(users.get(i));
        }
    }
"
"    @Test
    public void queryIntegerLimit() {
        putUsersBlocking(10);

        final int limit = 8;
        final List<User> usersFromQuery = storIOSQLite
                .get()
                .listOfObjects(User.class)
                .withQuery(Query.builder()
                        .table(UserTableMeta.TABLE)
                        .limit(limit)
                        .build())
                .prepare()
                .executeAsBlocking();

        assertThat(usersFromQuery).isNotNull();
        assertThat(usersFromQuery).hasSize(limit);
    }
"
"    @Test
    public void queryLimitOffsetQuantity() {
        final List<User> users = putUsersBlocking(10);

        final int offset = 5;
        final int quantity = 3;
        final List<User> usersFromQuery = storIOSQLite
                .get()
                .listOfObjects(User.class)
                .withQuery(Query.builder()
                        .table(UserTableMeta.TABLE)
                        .orderBy(UserTableMeta.COLUMN_EMAIL)
                        .limit(offset, quantity)
                        .build())
                .prepare()
                .executeAsBlocking();

        assertThat(usersFromQuery).isNotNull();
        assertThat(usersFromQuery).hasSize(Math.min(quantity, users.size() - offset));

        Collections.sort(users);

        int position = 0;
        for (int i = offset; i < offset + quantity; i++) {
            assertThat(usersFromQuery.get(position++)).isEqualTo(users.get(i));
        }
    }
"
"    @Test
    public void queryGroupBy() {
        final List<User> users = TestFactory.newUsers(10);

        for (int i = 0; i < users.size(); i++) {
            final String commonEmail;
            if (i < 3) {
                commonEmail = ""first_group@gmail.com"";
            } else {
                commonEmail = ""second_group@gmail.com"";
            }

            users.set(i, User.newInstance(null, commonEmail));
        }

        putUsersBlocking(users);

        final List<User> groupsOfUsers = storIOSQLite
                .get()
                .listOfObjects(User.class)
                .withQuery(Query.builder()
                        .table(UserTableMeta.TABLE)
                        .columns(UserTableMeta.COLUMN_EMAIL)
                        .groupBy(UserTableMeta.COLUMN_EMAIL)
                        .build())
                .withGetResolver(new DefaultGetResolver<User>() {
                    @NonNull
                    @Override
                    public User mapFromCursor(@NonNull StorIOSQLite storIOSQLite, @NonNull Cursor cursor) {
                        return User.newInstance(null, cursor.getString(cursor.getColumnIndex(UserTableMeta.COLUMN_EMAIL)));
                    }
"
"    @Test
    public void queryHaving() {
        final List<User> users = TestFactory.newUsers(10);

        for (int i = 0; i < users.size(); i++) {
            final String commonEmail;
            if (i < 3) {
                commonEmail = ""first_group@gmail.com"";
            } else {
                commonEmail = ""second_group@gmail.com"";
            }

            users.set(i, User.newInstance(null, commonEmail));
        }

        putUsersBlocking(users);

        final int bigGroupThreshold = 5;

        final List<User> groupsOfUsers = storIOSQLite
                .get()
                .listOfObjects(User.class)
                .withQuery(Query.builder()
                        .table(UserTableMeta.TABLE)
                        .columns(UserTableMeta.COLUMN_EMAIL)
                        .groupBy(UserTableMeta.COLUMN_EMAIL)
                        .having(""COUNT(*) >= "" + bigGroupThreshold)
                        .build())
                .withGetResolver(new DefaultGetResolver<User>() {
                    @NonNull
                    @Override
                    public User mapFromCursor(@NonNull StorIOSQLite storIOSQLite, @NonNull Cursor cursor) {
                        return User.newInstance(null, cursor.getString(cursor.getColumnIndex(UserTableMeta.COLUMN_EMAIL)));
                    }
"
"    @Test
    public void queryDistinct() {
        final List<User> users = new ArrayList<User>();

        for (int i = 0; i < 10; i++) {
            users.add(User.newInstance((long) i, ""same@email.com""));
        }

        putUsersBlocking(users);

        final GetResolver<User> customGetResolver = new DefaultGetResolver<User>() {
            @NonNull
            @Override
            public User mapFromCursor(@NonNull StorIOSQLite storIOSQLite, @NonNull Cursor cursor) {
                return User.newInstance(null, cursor.getString(cursor.getColumnIndex(UserTableMeta.COLUMN_EMAIL)));
            }
"
"    @Test
    public void queryWithRawQuery() {
        final List<User> users = TestFactory.newUsers(20);

        int counter = 1;

        for (int i = 0; i < users.size(); i++) {
            char[] chars = new char[counter++];
            Arrays.fill(chars, '*'); // wtf is going on?
            users.set(i, User.newInstance(null, new String(chars)));
        }

        putUsersBlocking(users);

        final List<User> usersWithLongName = new ArrayList<User>(users.size());

        int lengthSum = 0;
        for (User user : users) {
            lengthSum += user.email().length();
        }

        final int avrLength = lengthSum / users.size();

        for (User user : users) {
            if (user.email().length() > avrLength) {
                usersWithLongName.add(user);
            }
        }

        final String query = ""Select * from "" + UserTableMeta.TABLE
                + "" where length("" + UserTableMeta.COLUMN_EMAIL + "") > ""
                + ""(select avg(length("" + UserTableMeta.COLUMN_EMAIL + "")) from users)"";

        final List<User> usersFromQuery = storIOSQLite
                .get()
                .listOfObjects(User.class)
                .withQuery(RawQuery.builder()
                        .query(query)
                        .build())
                .prepare()
                .executeAsBlocking();

        assertThat(usersFromQuery).isEqualTo(usersWithLongName);
    }
"
"    @Test
    public void queryWithRawQueryAndArguments() {
        final User testUser = User.newInstance(null, ""testUserName"");

        final List<User> users = TestFactory.newUsers(10);
        users.add(testUser);
        putUsersBlocking(users);

        final String query = ""SELECT * FROM "" + UserTableMeta.TABLE
                + "" WHERE "" + UserTableMeta.COLUMN_EMAIL + "" LIKE ?"";

        final List<User> usersFromQuery = storIOSQLite
                .get()
                .listOfObjects(User.class)
                .withQuery(RawQuery.builder()
                        .query(query)
                        .args(testUser.email())
                        .build())
                .prepare()
                .executeAsBlocking();

        assertThat(usersFromQuery).isNotNull();
        assertThat(usersFromQuery).hasSize(1);
        assertThat(usersFromQuery.get(0)).isEqualTo(testUser);
    }
"
"    @Test
    public void queryWithRawQuerySqlInjectionFail() {
        final List<User> users = putUsersBlocking(10);

        final String query = ""SELECT * FROM "" + UserTableMeta.TABLE
                + "" WHERE "" + UserTableMeta.COLUMN_EMAIL + "" LIKE ?"";

        final String arg = ""(DELETE FROM "" + UserTableMeta.TABLE + "")"";

        storIOSQLite.get()
                .listOfObjects(User.class)
                .withQuery(RawQuery.builder()
                        .query(query)
                        .args(arg)
                        .build())
                .prepare()
                .executeAsBlocking();

        assertThat(getAllUsersBlocking()).isEqualTo(users);
    }
"
"    @Test
    public void getNumberOfResults() {
        putUsersBlocking(8);

        Integer numberOfResults = storIOSQLite
                .get()
                .numberOfResults()
                .withQuery(UserTableMeta.QUERY_ALL)
                .prepare()
                .executeAsBlocking();

        assertThat(numberOfResults).isEqualTo(8);
    }
"
"    @Test
    public void queryOneExistedObject() {
        final List<User> users = putUsersBlocking(3);
        final User user = users.get(0);

        final User userFromQuery = storIOSQLite
                .get()
                .object(User.class)
                .withQuery(Query.builder()
                        .table(UserTableMeta.TABLE)
                        .where(UserTableMeta.COLUMN_EMAIL + ""=?"")
                        .whereArgs(user.email())
                        .build())
                .prepare()
                .executeAsBlocking();

        assertThat(userFromQuery).isNotNull();
        assertThat(userFromQuery).isEqualTo(user);
    }
"
"    @Test
    public void queryOneNonExistedObject() {
        putUsersBlocking(3);

        final User userFromQuery = storIOSQLite
                .get()
                .object(User.class)
                .withQuery(Query.builder()
                        .table(UserTableMeta.TABLE)
                        .where(UserTableMeta.COLUMN_EMAIL + ""=?"")
                        .whereArgs(""some arg"")
                        .build())
                .prepare()
                .executeAsBlocking();

        assertThat(userFromQuery).isNull();
    }
"
"    @Test
    public void deleteOne() {
        final User user = putUserBlocking();

        final Cursor cursorAfterInsert = db.query(UserTableMeta.TABLE, null, null, null, null, null, null);
        assertThat(cursorAfterInsert.getCount()).isEqualTo(1);
        cursorAfterInsert.close();

        deleteUserBlocking(user);

        final Cursor cursorAfterDelete = db.query(UserTableMeta.TABLE, null, null, null, null, null, null);
        assertThat(cursorAfterDelete.getCount()).isEqualTo(0);
        cursorAfterDelete.close();
    }
"
"    @Test
    public void deleteCollection() {
        final List<User> allUsers = putUsersBlocking(10);

        final List<User> usersToDelete = new ArrayList<User>();

        for (int i = 0; i < allUsers.size(); i += 2) {  // Delete every second user
            usersToDelete.add(allUsers.get(i));
        }

        final DeleteResults<User> deleteResults = storIOSQLite
                .delete()
                .objects(usersToDelete)
                .prepare()
                .executeAsBlocking();

        final List<User> usersAfterDelete = getAllUsersBlocking();

        assertThat(usersAfterDelete).hasSize(allUsers.size() / 2);

        for (User user : allUsers) {
            final boolean shouldBeDeleted = usersToDelete.contains(user);

            // Check that we deleted what we going to.
            assertThat(deleteResults.wasDeleted(user)).isEqualTo(shouldBeDeleted);

            // Check that we didn't delete users that we didn't want to
            assertThat(usersAfterDelete.contains(user)).isEqualTo(!shouldBeDeleted);
        }
    }
"
"    @Test
    public void insertEmission() {
        final List<User> initialUsers = putUsersBlocking(10);
        final List<User> usersForInsert = TestFactory.newUsers(10);
        final List<User> allUsers = new ArrayList<User>(initialUsers.size() + usersForInsert.size());

        allUsers.addAll(initialUsers);
        allUsers.addAll(usersForInsert);

        final Queue<List<User>> expectedUsers = new LinkedList<List<User>>();
        expectedUsers.add(initialUsers);
        expectedUsers.add(allUsers);

        final EmissionChecker emissionChecker = new EmissionChecker(expectedUsers);
        final Subscription subscription = emissionChecker.subscribe();

        // Should receive initial users
        emissionChecker.awaitNextExpectedValue();

        putUsersBlocking(usersForInsert);

        // Should receive initial users + inserted users
        emissionChecker.awaitNextExpectedValue();

        emissionChecker.assertThatNoExpectedValuesLeft();

        subscription.unsubscribe();
    }
"
"    @Test
    public void updateEmission() {
        final List<User> users = putUsersBlocking(10);

        final Queue<List<User>> expectedUsers = new LinkedList<List<User>>();

        final List<User> updatedList = new ArrayList<User>(users.size());

        int count = 1;
        for (User user : users) {
            updatedList.add(User.newInstance(user.id(), ""new_email"" + count++));
        }
        expectedUsers.add(users);
        expectedUsers.add(updatedList);
        final EmissionChecker emissionChecker = new EmissionChecker(expectedUsers);
        final Subscription subscription = emissionChecker.subscribe();

        // Should receive all users
        emissionChecker.awaitNextExpectedValue();

        storIOSQLite
                .put()
                .objects(updatedList)
                .prepare()
                .executeAsBlocking();

        // Should receive updated users
        emissionChecker.awaitNextExpectedValue();

        emissionChecker.assertThatNoExpectedValuesLeft();

        subscription.unsubscribe();
    }
"
"    @Test
    public void deleteEmission() {
        final List<User> usersThatShouldBeSaved = TestFactory.newUsers(10);
        final List<User> usersThatShouldBeDeleted = TestFactory.newUsers(10);
        final List<User> allUsers = new ArrayList<User>();

        allUsers.addAll(usersThatShouldBeSaved);
        allUsers.addAll(usersThatShouldBeDeleted);

        putUsersBlocking(allUsers);

        final Queue<List<User>> expectedUsers = new LinkedList<List<User>>();

        expectedUsers.add(allUsers);
        expectedUsers.add(usersThatShouldBeSaved);

        final EmissionChecker emissionChecker = new EmissionChecker(expectedUsers);
        final Subscription subscription = emissionChecker.subscribe();

        // Should receive all users
        emissionChecker.awaitNextExpectedValue();

        deleteUsersBlocking(usersThatShouldBeDeleted);

        // Should receive users that should be saved
        emissionChecker.awaitNextExpectedValue();

        emissionChecker.assertThatNoExpectedValuesLeft();

        subscription.unsubscribe();
    }
"
"    @Test
    public void concurrentPutWithoutGlobalTransaction() throws InterruptedException {
        final int numberOfConcurrentPuts = ConcurrencyTesting.optimalTestThreadsCount();

        TestSubscriber<Changes> testSubscriber = new TestSubscriber<Changes>();

        storIOSQLite
                .observeChangesInTable(TweetTableMeta.TABLE)
                .subscribe(testSubscriber);

        final CountDownLatch concurrentPutLatch = new CountDownLatch(1);
        final CountDownLatch allPutsDoneLatch = new CountDownLatch(numberOfConcurrentPuts);

        for (int i = 0; i < numberOfConcurrentPuts; i++) {
            final int iCopy = i;

            new Thread(new Runnable() {
                @Override
                public void run() {
                    try {
                        concurrentPutLatch.await();
                    } catch (InterruptedException e) {
                        throw new RuntimeException(e);
                    }

                    storIOSQLite
                            .put()
                            .object(Tweet.newInstance(null, 1L, ""Some text: "" + iCopy))
                            .prepare()
                            .executeAsBlocking();

                    allPutsDoneLatch.countDown();
                }
"
"    @Test
    public void nestedTransaction() {
        storIOSQLite.lowLevel().beginTransaction();

        storIOSQLite.lowLevel().beginTransaction();

        storIOSQLite.lowLevel().setTransactionSuccessful();
        storIOSQLite.lowLevel().endTransaction();

        storIOSQLite.lowLevel().setTransactionSuccessful();
        storIOSQLite.lowLevel().endTransaction();
    }
"
"    @Test
    public void queryOneExistedObjectObservable() {
        final List<User> users = putUsersBlocking(3);
        final User expectedUser = users.get(0);

        final Observable<User> userObservable = storIOSQLite
                .get()
                .object(User.class)
                .withQuery(Query.builder()
                        .table(UserTableMeta.TABLE)
                        .where(UserTableMeta.COLUMN_EMAIL + ""=?"")
                        .whereArgs(expectedUser.email())
                        .build())
                .prepare()
                .asRxObservable()
                .take(1);

        TestSubscriber<User> testSubscriber = new TestSubscriber<User>();
        userObservable.subscribe(testSubscriber);

        testSubscriber.awaitTerminalEvent(5, SECONDS);
        testSubscriber.assertNoErrors();
        testSubscriber.assertValue(expectedUser);
    }
"
"    @Test
    public void queryOneNonExistedObjectObservable() {
        putUsersBlocking(3);

        final Observable<User> userObservable = storIOSQLite
                .get()
                .object(User.class)
                .withQuery(Query.builder()
                        .table(UserTableMeta.TABLE)
                        .where(UserTableMeta.COLUMN_EMAIL + ""=?"")
                        .whereArgs(""some arg"")
                        .build())
                .prepare()
                .asRxObservable()
                .take(1);

        TestSubscriber<User> testSubscriber = new TestSubscriber<User>();
        userObservable.subscribe(testSubscriber);

        testSubscriber.awaitTerminalEvent(5, SECONDS);
        testSubscriber.assertNoErrors();
        testSubscriber.assertValue(null);
    }
"
"    @Test
    public void queryOneExistedObjectTableUpdate() {
        User expectedUser = User.newInstance(null, ""such@email.com"");
        putUsersBlocking(3);

        final Observable<User> userObservable = storIOSQLite
                .get()
                .object(User.class)
                .withQuery(Query.builder()
                        .table(UserTableMeta.TABLE)
                        .where(UserTableMeta.COLUMN_EMAIL + ""=?"")
                        .whereArgs(expectedUser.email())
                        .build())
                .prepare()
                .asRxObservable()
                .take(2);

        TestSubscriber<User> testSubscriber = new TestSubscriber<User>();
        userObservable.subscribe(testSubscriber);

        testSubscriber.awaitTerminalEvent(5, SECONDS);
        testSubscriber.assertNoErrors();
        testSubscriber.assertValue(null);

        putUserBlocking(expectedUser);

        testSubscriber.awaitTerminalEvent(5, SECONDS);
        testSubscriber.assertNoErrors();
        testSubscriber.assertValues(null, expectedUser);
    }
"
"    @Test
    public void queryOneNonexistedObjectTableUpdate() {
        final Observable<User> userObservable = storIOSQLite
                .get()
                .object(User.class)
                .withQuery(Query.builder()
                        .table(UserTableMeta.TABLE)
                        .where(UserTableMeta.COLUMN_EMAIL + ""=?"")
                        .whereArgs(""some arg"")
                        .build())
                .prepare()
                .asRxObservable()
                .take(2);

        TestSubscriber<User> testSubscriber = new TestSubscriber<User>();
        userObservable.subscribe(testSubscriber);

        testSubscriber.awaitTerminalEvent(5, SECONDS);
        testSubscriber.assertNoErrors();
        testSubscriber.assertValue(null);

        putUserBlocking();

        testSubscriber.awaitTerminalEvent(5, SECONDS);
        testSubscriber.assertNoErrors();
        testSubscriber.assertValues(null, null);
    }
"
"    @Test
    public void queryListOfObjectsAsSingle() {
        final List<User> users = putUsersBlocking(10);

        final Single<List<User>> usersSingle = storIOSQLite
                .get()
                .listOfObjects(User.class)
                .withQuery(UserTableMeta.QUERY_ALL)
                .prepare()
                .asRxSingle();

        TestSubscriber<List<User>> testSubscriber = new TestSubscriber<List<User>>();
        usersSingle.subscribe(testSubscriber);

        testSubscriber.awaitTerminalEvent(5, SECONDS);
        testSubscriber.assertNoErrors();
        testSubscriber.assertValue(users);
        testSubscriber.assertCompleted();
    }
"
"    @Test
    public void queryObjectAsSingle() {
        final List<User> users = putUsersBlocking(3);

        final Single<User> usersSingle = storIOSQLite
                .get()
                .object(User.class)
                .withQuery(UserTableMeta.QUERY_ALL)
                .prepare()
                .asRxSingle();

        TestSubscriber<User> testSubscriber = new TestSubscriber<User>();
        usersSingle.subscribe(testSubscriber);

        testSubscriber.awaitTerminalEvent(5, SECONDS);
        testSubscriber.assertNoErrors();
        testSubscriber.assertValues(users.get(0));
        testSubscriber.assertCompleted();
    }
"
"    @Test
    public void queryNumberOfResultsAsSingle() {
        final List<User> users = putUsersBlocking(3);

        final Single<Integer> usersSingle = storIOSQLite
                .get()
                .numberOfResults()
                .withQuery(UserTableMeta.QUERY_ALL)
                .prepare()
                .asRxSingle();

        TestSubscriber<Integer> testSubscriber = new TestSubscriber<Integer>();
        usersSingle.subscribe(testSubscriber);

        testSubscriber.awaitTerminalEvent(5, SECONDS);
        testSubscriber.assertNoErrors();
        testSubscriber.assertValue(users.size());
        testSubscriber.assertCompleted();
    }
"
"    @Test
    public void deleteByQuery() {
        storIOSQLite.delete()
                .byQuery(DeleteQuery.builder()
                        .table(TweetTableMeta.TABLE)
                        .build()
                )
                .prepare()
                .executeAsBlocking();
        checkInterceptorsCalls();
    }
"
"    @Test
    public void deleteCollectionOfObjects() {
        storIOSQLite.delete()
                .objects(Collections.singleton(createTweet()))
                .prepare()
                .executeAsBlocking();
        checkInterceptorsCalls();
    }
"
"    @Test
    public void deleteObject() {
        storIOSQLite.delete()
                .object(createTweet())
                .prepare()
                .executeAsBlocking();
        checkInterceptorsCalls();
    }
"
"    @Test
    public void execSql() {
        storIOSQLite.executeSQL()
                .withQuery(RawQuery.builder()
                        .query(""select * from "" + TweetTableMeta.TABLE)
                        .build()
                )
                .prepare()
                .executeAsBlocking();
        checkInterceptorsCalls();
    }
"
"    @Test
    public void getCursorWithRawQuery() {
        storIOSQLite.get()
                .cursor()
                .withQuery(RawQuery.builder()
                        .query(""select * from "" + TweetTableMeta.TABLE)
                        .build()
                )
                .prepare()
                .executeAsBlocking();
        checkInterceptorsCalls();
    }
"
"    @Test
    public void getCursorWithQuery() {
        storIOSQLite.get()
                .cursor()
                .withQuery(Query.builder()
                        .table(TweetTableMeta.TABLE)
                        .build()
                )
                .prepare()
                .executeAsBlocking();
        checkInterceptorsCalls();
    }
"
"    @Test
    public void getListOfObjectsWithRawQuery() {
        storIOSQLite.get()
                .listOfObjects(Tweet.class)
                .withQuery(RawQuery.builder()
                        .query(""select * from "" + TweetTableMeta.TABLE)
                        .build()
                )
                .prepare()
                .executeAsBlocking();
        checkInterceptorsCalls();
    }
"
"    @Test
    public void getListOfObjectsWithQuery() {
        storIOSQLite.get()
                .listOfObjects(Tweet.class)
                .withQuery(Query.builder()
                        .table(TweetTableMeta.TABLE)
                        .build()
                )
                .prepare()
                .executeAsBlocking();
        checkInterceptorsCalls();
    }
"
"    @Test
    public void getNumberOfResultsWithRawQuery() {
        storIOSQLite.get()
                .numberOfResults()
                .withQuery(RawQuery.builder()
                        .query(""select * from "" + TweetTableMeta.TABLE)
                        .build()
                )
                .prepare()
                .executeAsBlocking();
        checkInterceptorsCalls();
    }
"
"    @Test
    public void getNumberOfResultsWithQuery() {
        storIOSQLite.get()
                .numberOfResults()
                .withQuery(Query.builder()
                        .table(TweetTableMeta.TABLE)
                        .build()
                )
                .prepare()
                .executeAsBlocking();
        checkInterceptorsCalls();
    }
"
"    @Test
    public void getObjectWithRawQuery() {
        storIOSQLite.get()
                .object(Tweet.class)
                .withQuery(RawQuery.builder()
                        .query(""select * from "" + TweetTableMeta.TABLE)
                        .build()
                )
                .prepare()
                .executeAsBlocking();
        checkInterceptorsCalls();
    }
"
"    @Test
    public void getObjectWithQuery() {
        storIOSQLite.get()
                .object(Tweet.class)
                .withQuery(Query.builder()
                        .table(TweetTableMeta.TABLE)
                        .build()
                )
                .prepare()
                .executeAsBlocking();
        checkInterceptorsCalls();
    }
"
"    @Test
    public void putCollection() {
        storIOSQLite.put()
                .objects(Collections.singleton(createTweet()))
                .prepare()
                .executeAsBlocking();
        checkInterceptorsCalls();
    }
"
"    @Test
    public void putContentValues() {
        storIOSQLite.put()
                .contentValues(createContentValues())
                .withPutResolver(createCVPutResolver())
                .prepare()
                .executeAsBlocking();
        checkInterceptorsCalls();
    }
"
"    @Test
    public void putContentValuesIterable() {
        storIOSQLite.put()
                .contentValues(createContentValues(), createContentValues())
                .withPutResolver(createCVPutResolver())
                .prepare()
                .executeAsBlocking();
        checkInterceptorsCalls();
    }
"
"    @Test
    public void putObject() {
        storIOSQLite.put()
                .object(createTweet())
                .prepare()
                .executeAsBlocking();
        checkInterceptorsCalls();
    }
"
"    @Test
    public void insertOne() {
        final User user = putUserBlocking();

        // why we created StorIOSQLite: nobody loves nulls
        final Cursor cursor = db.query(UserTableMeta.TABLE, null, null, null, null, null, null);

        // asserting that values was really inserted to db
        assertThat(cursor.getCount()).isEqualTo(1);
        assertThat(cursor.moveToFirst()).isTrue();

        final User insertedUser = UserTableMeta.GET_RESOLVER.mapFromCursor(storIOSQLite, cursor);

        assertThat(insertedUser.id()).isNotNull();
        assertThat(user.equalsExceptId(insertedUser)).isTrue();

        cursor.close();
    }
"
"    @Test
    public void insertCollection() {
        final List<User> users = putUsersBlocking(3);

        // asserting that values was really inserted to db
        final Cursor cursor = db.query(UserTableMeta.TABLE, null, null, null, null, null, null);

        assertThat(cursor.getCount()).isEqualTo(users.size());

        for (int i = 0; i < users.size(); i++) {
            assertThat(cursor.moveToNext()).isTrue();
            assertThat(UserTableMeta.GET_RESOLVER.mapFromCursor(storIOSQLite, cursor)).isEqualTo(users.get(i));
        }

        cursor.close();
    }
"
"    @Test
    public void insertAndDeleteTwice() {
        final User user = TestFactory.newUser();

        for (int i = 0; i < 2; i++) {
            putUserBlocking(user);

            final List<User> existUsers = getAllUsersBlocking();

            assertThat(existUsers).isNotNull();
            assertThat(existUsers).hasSize(1);

            final Cursor cursorAfterPut = db.query(UserTableMeta.TABLE, null, null, null, null, null, null);
            assertThat(cursorAfterPut.getCount()).isEqualTo(1);
            cursorAfterPut.close();

            deleteUserBlocking(user);

            final Cursor cursorAfterDelete = db.query(UserTableMeta.TABLE, null, null, null, null, null, null);
            assertThat(cursorAfterDelete.getCount()).isEqualTo(0);
            cursorAfterDelete.close();
        }
    }
"
"    @Test
    public void insertOneWithNullField() {
        User user = User.newInstance(null, ""user@example.com"", null); // phone is null
        putUserBlocking(user);

        final Cursor cursor = db.query(UserTableMeta.TABLE, null, null, null, null, null, null);

        // asserting that values was really inserted to db
        assertThat(cursor.getCount()).isEqualTo(1);
        assertThat(cursor.moveToFirst()).isTrue();

        final User insertedUser = UserTableMeta.GET_RESOLVER.mapFromCursor(storIOSQLite, cursor);

        assertThat(insertedUser.id()).isNotNull();
        assertThat(user.equalsExceptId(insertedUser)).isTrue();

        cursor.close();
    }
"
"    @Test
    public void updateOne() {
        final User userForInsert = putUserBlocking();

        final User userForUpdate = User.newInstance(
                userForInsert.id(), // using id of inserted user
                ""new@email.com"" // new value
        );

        updateUserBlocking(userForUpdate);
        checkOnlyOneItemInStorage(userForUpdate);  // update should not add new rows!
    }
"
"    @Test
    public void updateNullFieldToNotNull() {
        final User userForInsert = User.newInstance(null, ""user@email.com"", null); // phone is null

        putUserBlocking(userForInsert);

        final User userForUpdate = User.newInstance(
                userForInsert.id(),
                userForInsert.email(),
                ""1-999-547867""  // phone not null
        );

        updateUserBlocking(userForUpdate);
        checkOnlyOneItemInStorage(userForUpdate);
    }
"
"    @Test
    public void updateNotNullFieldToNull() {
        final User userForInsert = User.newInstance(null, ""user@email.com"", ""1-999-547867""); // phone not null

        putUserBlocking(userForInsert);

        final User userForUpdate = User.newInstance(
                userForInsert.id(),
                userForInsert.email(),
                null    // phone is null
        );

        updateUserBlocking(userForUpdate);
        checkOnlyOneItemInStorage(userForUpdate);
    }
"
"    @Test
    public void updateCollection() {
        final List<User> usersForInsert = TestFactory.newUsers(3);

        final PutResults<User> insertResults = storIOSQLite
                .put()
                .objects(usersForInsert)
                .prepare()
                .executeAsBlocking();

        assertThat(insertResults.numberOfInserts()).isEqualTo(usersForInsert.size());

        final List<User> usersForUpdate = new ArrayList<User>(usersForInsert.size());

        for (int i = 0; i < usersForInsert.size(); i++) {
            usersForUpdate.add(User.newInstance(usersForInsert.get(i).id(), ""new"" + i + ""@email.com"" + i));
        }

        final PutResults<User> updateResults = storIOSQLite
                .put()
                .objects(usersForUpdate)
                .prepare()
                .executeAsBlocking();

        assertThat(updateResults.numberOfUpdates()).isEqualTo(usersForUpdate.size());

        final Cursor cursor = db.query(UserTableMeta.TABLE, null, null, null, null, null, null);

        assertThat(cursor.getCount()).isEqualTo(usersForUpdate.size()); // update should not add new rows!

        for (int i = 0; i < usersForUpdate.size(); i++) {
            assertThat(cursor.moveToNext()).isTrue();
            assertThat(UserTableMeta.GET_RESOLVER.mapFromCursor(storIOSQLite, cursor)).isEqualTo(usersForUpdate.get(i));
        }

        cursor.close();
    }
"
"    @Test
    public void insertEmission() {
        final List<User> users = TestFactory.newUsers(10);

        final Queue<Changes> expectedChanges = new LinkedList<Changes>();
        expectedChanges.add(Changes.newInstance(UserTableMeta.TABLE, UserTableMeta.NOTIFICATION_TAG));

        final EmissionChecker emissionChecker = new EmissionChecker(expectedChanges);
        final Subscription subscription = emissionChecker.subscribe();

        putUsersBlocking(users);

        // Should receive changes of Users table
        emissionChecker.awaitNextExpectedValue();

        emissionChecker.assertThatNoExpectedValuesLeft();

        subscription.unsubscribe();
    }
"
"    @Test
    public void updateEmission() {
        final List<User> users = putUsersBlocking(10);
        final List<User> updated = new ArrayList<User>(users.size());

        for (User user : users) {
            updated.add(User.newInstance(user.id(), user.email()));
        }

        final Queue<Changes> expectedChanges = new LinkedList<Changes>();
        expectedChanges.add(Changes.newInstance(UserTableMeta.TABLE, UserTableMeta.NOTIFICATION_TAG));

        final EmissionChecker emissionChecker = new EmissionChecker(expectedChanges);
        final Subscription subscription = emissionChecker.subscribe();

        storIOSQLite
                .put()
                .objects(updated)
                .prepare()
                .executeAsBlocking();

        // Should receive changes of Users table
        emissionChecker.awaitNextExpectedValue();

        emissionChecker.assertThatNoExpectedValuesLeft();

        subscription.unsubscribe();
    }
"
"    @Test
    public void deleteEmission() {
        final List<User> users = putUsersBlocking(10);

        final Queue<Changes> expectedChanges = new LinkedList<Changes>();
        expectedChanges.add(Changes.newInstance(UserTableMeta.TABLE, UserTableMeta.NOTIFICATION_TAG));

        final EmissionChecker emissionChecker = new EmissionChecker(expectedChanges);
        final Subscription subscription = emissionChecker.subscribe();

        deleteUsersBlocking(users);

        // Should receive changes of Users table
        emissionChecker.awaitNextExpectedValue();

        emissionChecker.assertThatNoExpectedValuesLeft();

        subscription.unsubscribe();
    }
"
"    @Test
    public void shouldReturnQueryInGetData() {
        final RawQuery query = RawQuery.builder()
                .query(""DROP TABLE IF EXISTS no_such_table"") // we don't want to really delete table
                .build();
        final PreparedExecuteSQL operation = storIOSQLite
                .executeSQL()
                .withQuery(query)
                .prepare();

        assertThat(operation.getData()).isEqualTo(query);
    }
"
"    @Test
    public void execSQLWithEmptyArgs() {
        // Should not throw exceptions!
        storIOSQLite
                .executeSQL()
                .withQuery(RawQuery.builder()
                        .query(""DROP TABLE IF EXISTS no_such_table"") // we don't want to really delete table
                        .build())
                .prepare()
                .executeAsBlocking();
    }
"
"    @Test
    public void shouldPassArgsAsObjects() {
        final User user = putUserBlocking();

        assertThat(user.id()).isNotNull();
        //noinspection ConstantConditions
        final long uid = user.id();

        final String query = ""UPDATE "" + UserTableMeta.TABLE
                + "" SET "" + UserTableMeta.COLUMN_ID + "" = MIN("" + UserTableMeta.COLUMN_ID + "", ?)"";

        storIOSQLite
                .executeSQL()
                .withQuery(
                        RawQuery.builder()
                                .query(query)
                                .args(uid - 1)  // as integer is less, as string is greater
                                .build())
                .prepare()
                .executeAsBlocking();

        List<User> users = getAllUsersBlocking();

        assertThat(users.size()).isEqualTo(1);

        // Was updated, because (uid - 1) passed as object, not string, and (uid - 1) < uid.
        assertThat(users.get(0).id()).isEqualTo(uid - 1);
    }
"
"    @Test
    public void repeatsOperationWithQueryByChangeOfTable() {
        putUserBlocking();

        TestSubscriber<Cursor> testSubscriber = new TestSubscriber<Cursor>();
        storIOSQLite
                .get()
                .cursor()
                .withQuery(query)
                .prepare()
                .asRxObservable()
                .subscribe(testSubscriber);

        testSubscriber.assertValueCount(1);

        storIOSQLite.lowLevel().notifyAboutChanges(tableChanges);

        testSubscriber.assertValueCount(2);
    }
"
"    @Test
    public void repeatsOperationWithRawQueryByChangeOfTable() {
        putUserBlocking();

        TestSubscriber<Cursor> testSubscriber = new TestSubscriber<Cursor>();
        storIOSQLite
                .get()
                .cursor()
                .withQuery(rawQuery)
                .prepare()
                .asRxObservable()
                .subscribe(testSubscriber);

        testSubscriber.assertValueCount(1);

        storIOSQLite.lowLevel().notifyAboutChanges(tableChanges);

        testSubscriber.assertValueCount(2);
    }
"
"    @Test
    public void repeatsOperationWithQueryByChangeOfTag() {
        putUserBlocking();

        TestSubscriber<Cursor> testSubscriber = new TestSubscriber<Cursor>();
        storIOSQLite
                .get()
                .cursor()
                .withQuery(query)
                .prepare()
                .asRxObservable()
                .subscribe(testSubscriber);

        testSubscriber.assertValueCount(1);

        storIOSQLite.lowLevel().notifyAboutChanges(tagChanges);

        testSubscriber.assertValueCount(2);
    }
"
"    @Test
    public void repeatsOperationWithRawQueryByChangeOfTag() {
        putUserBlocking();

        TestSubscriber<Cursor> testSubscriber = new TestSubscriber<Cursor>();
        storIOSQLite
                .get()
                .cursor()
                .withQuery(rawQuery)
                .prepare()
                .asRxObservable()
                .subscribe(testSubscriber);

        testSubscriber.assertValueCount(1);

        storIOSQLite.lowLevel().notifyAboutChanges(tagChanges);

        testSubscriber.assertValueCount(2);
    }
"
"    @Test
    public void repeatsOperationWithQueryByChangeOfTable() {
        putUserBlocking();

        PreparedGetNumberOfResults operation = storIOSQLite
                .get()
                .numberOfResults()
                .withQuery(query)
                .prepare();

        verifyChangesReceived(operation, tableChanges, 1);
    }
"
"    @Test
    public void repeatsOperationWithRawQueryByChangeOfTable() {
        putUserBlocking();

        PreparedGetNumberOfResults operation = storIOSQLite
                .get()
                .numberOfResults()
                .withQuery(rawQuery)
                .prepare();

        verifyChangesReceived(operation, tableChanges, 1);
    }
"
"    @Test
    public void repeatsOperationWithQueryByChangeOfTag() {
        putUserBlocking();

        PreparedGetNumberOfResults operation = storIOSQLite
                .get()
                .numberOfResults()
                .withQuery(query)
                .prepare();

        verifyChangesReceived(operation, tagChanges, 1);
    }
"
"    @Test
    public void repeatsOperationWithRawQueryByChangeOfTag() {
        putUserBlocking();

        PreparedGetNumberOfResults operation = storIOSQLite
                .get()
                .numberOfResults()
                .withQuery(rawQuery)
                .prepare();

        verifyChangesReceived(operation, tagChanges, 1);
    }
"
"    @Test
    public void insertObject() {
        final Book book = Book.builder()
                .id(1)
                .title(""What a great book"")
                .author(""Somebody"")
                .build();

        final PutResult putResult = storIOSQLite
                .put()
                .object(book)
                .prepare()
                .executeAsBlocking();

        assertThat(putResult.wasInserted()).isTrue();

        final List<Book> storedBooks = storIOSQLite
                .get()
                .listOfObjects(Book.class)
                .withQuery(Query.builder()
                        .table(BookTableMeta.TABLE)
                        .build())
                .prepare()
                .executeAsBlocking();

        assertThat(storedBooks).hasSize(1);

        assertThat(storedBooks.get(0)).isEqualTo(book);
    }
"
"    @Test
    public void updateObject() {
        final Book book = Book.builder()
                .id(1)
                .title(""What a great book"")
                .author(""Somebody"")
                .build();

        final PutResult putResult1 = storIOSQLite
                .put()
                .object(book)
                .prepare()
                .executeAsBlocking();

        assertThat(putResult1.wasInserted()).isTrue();

        final Book bookWithUpdatedInfo = Book.builder()
                .id(1) // Same id, should be updated
                .title(""Corrected title"")
                .author(""Corrected author"")
                .build();

        final PutResult putResult2 = storIOSQLite
                .put()
                .object(bookWithUpdatedInfo)
                .prepare()
                .executeAsBlocking();

        assertThat(putResult2.wasUpdated()).isTrue();

        final List<Book> storedBooks = storIOSQLite
                .get()
                .listOfObjects(Book.class)
                .withQuery(Query.builder()
                        .table(BookTableMeta.TABLE)
                        .build())
                .prepare()
                .executeAsBlocking();

        assertThat(storedBooks).hasSize(1);

        assertThat(storedBooks.get(0)).isEqualTo(bookWithUpdatedInfo);
    }
"
"    @Test
    public void deleteObject() {
        final Book book = Book.builder()
                .id(1)
                .title(""What a great book"")
                .author(""Somebody"")
                .build();

        final PutResult putResult = storIOSQLite
                .put()
                .object(book)
                .prepare()
                .executeAsBlocking();

        assertThat(putResult.wasInserted()).isTrue();

        final DeleteResult deleteResult = storIOSQLite
                .delete()
                .object(book)
                .prepare()
                .executeAsBlocking();

        assertThat(deleteResult.numberOfRowsDeleted()).isEqualTo(1);

        final List<Book> storedBooks = storIOSQLite
                .get()
                .listOfObjects(Book.class)
                .withQuery(Query.builder()
                        .table(BookTableMeta.TABLE)
                        .build())
                .prepare()
                .executeAsBlocking();

        assertThat(storedBooks).hasSize(0);
    }
"
"    @Test(expected = NullPointerException.class)
    public void nullPutResolver() {
        SQLiteTypeMapping.builder()
                .putResolver(null)
                .getResolver(mock(GetResolver.class))
                .deleteResolver(mock(DeleteResolver.class))
                .build();
    }
"
"    @Test(expected = NullPointerException.class)
    public void nullMapFromCursor() {
        SQLiteTypeMapping.builder()
                .putResolver(mock(PutResolver.class))
                .getResolver(null)
                .deleteResolver(mock(DeleteResolver.class))
                .build();
    }
"
"    @Test(expected = NullPointerException.class)
    public void nullMapToDeleteQuery() {
        SQLiteTypeMapping.builder()
                .putResolver(mock(PutResolver.class))
                .getResolver(mock(GetResolver.class))
                .deleteResolver(null)
                .build();
    }
"
"    @Test
    public void build() {
        class TestItem {

        }

        final PutResolver<TestItem> putResolver = mock(PutResolver.class);
        final GetResolver<TestItem> getResolver = mock(GetResolver.class);
        final DeleteResolver<TestItem> deleteResolver = mock(DeleteResolver.class);

        final SQLiteTypeMapping<TestItem> typeMapping = SQLiteTypeMapping.<TestItem>builder()
                .putResolver(putResolver)
                .getResolver(getResolver)
                .deleteResolver(deleteResolver)
                .build();

        assertThat(typeMapping.putResolver()).isSameAs(putResolver);
        assertThat(typeMapping.getResolver()).isSameAs(getResolver);
        assertThat(typeMapping.deleteResolver()).isSameAs(deleteResolver);
    }
"
"    @Test
    public void shouldNotAllowNullTable() {
        expectedException.expect(NullPointerException.class);
        expectedException.expectMessage(equalTo(""Table name is null or empty""));
        expectedException.expectCause(nullValue(Throwable.class));

        //noinspection ConstantConditions
        InsertQuery.builder().table(null);
    }
"
"    @Test
    public void shouldNotAllowEmptyTable() {
        expectedException.expect(IllegalStateException.class);
        expectedException.expectMessage(equalTo(""Table name is null or empty""));
        expectedException.expectCause(nullValue(Throwable.class));

        InsertQuery.builder().table("""");
    }
"
"    @Test
    public void nullColumnHackShouldBeNullByDefault() {
        InsertQuery insertQuery = InsertQuery.builder()
                .table(""test_table"")
                .build();

        assertThat(insertQuery.nullColumnHack()).isNull();
    }
"
"    @Test
    public void completeBuilderShouldNotAllowNullTable() {
        try {
            //noinspection ConstantConditions
            InsertQuery.builder()
                    .table(""test_table"")
                    .table(null);
            failBecauseExceptionWasNotThrown(NullPointerException.class);
        } catch (NullPointerException expected) {
            assertThat(expected)
                    .hasMessage(""Table name is null or empty"")
                    .hasNoCause();
        }
    }
"
"    @Test
    public void completeBuilderShouldNotAllowEmptyTable() {
        try {
            InsertQuery.builder()
                    .table(""test_table"")
                    .table("""");
            failBecauseExceptionWasNotThrown(IllegalStateException.class);
        } catch (IllegalStateException expected) {
            assertThat(expected)
                    .hasMessage(""Table name is null or empty"")
                    .hasNoCause();
        }
    }
"
"    @Test
    public void completeBuilderShouldUpdateTable() {
        InsertQuery query = InsertQuery.builder()
                .table(""old_table"")
                .table(""new_table"")
                .build();

        assertThat(query.table()).isEqualTo(""new_table"");
    }
"
"    @Test
    public void createdThroughToBuilderQueryShouldBeEqual() {
        final String table = ""test_table"";
        final String nullColumnHack = ""test_null_column_hack"";
        final String tag = ""test_tag"";

        final InsertQuery firstQuery = InsertQuery.builder()
                .table(table)
                .nullColumnHack(nullColumnHack)
                .affectsTags(tag)
                .build();

        final InsertQuery secondQuery = firstQuery.toBuilder().build();

        assertThat(secondQuery).isEqualTo(firstQuery);
    }
"
"    @Test
    public void affectsTagsCollectionShouldRewrite() {
        InsertQuery insertQuery = InsertQuery.builder()
                .table(""table"")
                .affectsTags(new HashSet<String>((singletonList(""first_call_collection""))))
                .affectsTags(new HashSet<String>((singletonList(""second_call_collection""))))
                .build();

        assertThat(insertQuery.affectsTags()).isEqualTo(singleton(""second_call_collection""));
    }
"
"    @Test
    public void affectsTagsVarargShouldRewrite() {
        InsertQuery insertQuery = InsertQuery.builder()
                .table(""table"")
                .affectsTags(""first_call_vararg"")
                .affectsTags(""second_call_vararg"")
                .build();

        assertThat(insertQuery.affectsTags()).isEqualTo(singleton(""second_call_vararg""));
    }
"
"    @Test
    public void affectsTagsCollectionAllowsNull() {
        InsertQuery insertQuery = InsertQuery.builder()
                .table(""table"")
                .affectsTags(new HashSet<String>((singletonList(""first_call_collection""))))
                .affectsTags(null)
                .build();

        assertThat(insertQuery.affectsTags()).isEmpty();
    }
"
"    @Test
    public void buildWithNormalValues() {
        final String table = ""test_table"";
        final String nullColumnHack = ""test_null_column_hack"";
        final Set<String> tags = singleton(""tag"");

        final InsertQuery insertQuery = InsertQuery.builder()
                .table(table)
                .nullColumnHack(nullColumnHack)
                .affectsTags(tags)
                .build();

        assertThat(insertQuery.table()).isEqualTo(table);
        assertThat(insertQuery.nullColumnHack()).isEqualTo(nullColumnHack);
        assertThat(insertQuery.affectsTags()).isEqualTo(tags);
    }
"
"    @Test
    public void shouldNotAllowNullTag() {
        expectedException.expect(NullPointerException.class);
        expectedException.expectMessage(startsWith(""affectsTag must not be null or empty, affectsTags = ""));
        expectedException.expectCause(nullValue(Throwable.class));

        //noinspection ConstantConditions
        InsertQuery.builder()
                .table(""table"")
                .affectsTags((String) null)
                .build();
    }
"
"    @Test
    public void shouldNotAllowEmptyTag() {
        expectedException.expect(IllegalStateException.class);
        expectedException.expectMessage(startsWith(""affectsTag must not be null or empty, affectsTags = ""));
        expectedException.expectCause(nullValue(Throwable.class));

        //noinspection ConstantConditions
        InsertQuery.builder()
                .table(""table"")
                .affectsTags("""")
                .build();
    }
"
"	@BeforeEach
	public void beforeEach() {
		super.beforeEach();
	}
"
"	@Test
	public void concatMapCB() throws Exception {
		System.out.println(""Start concatMapCB"");
		System.out.println(""\n******** Using concatMap() *********"");
		ParallelFlux<GetResult> concat = Flux.fromIterable(keyList).parallel(2).runOn(Schedulers.parallel())
				.concatMap(item -> cbGet(item)
						/* rCollection.get(item) */.doOnSubscribe((x) -> System.out.println("" +"" + rCat.incrementAndGet()))
						.doOnTerminate(() -> System.out.println("" -"" + rCat.decrementAndGet())));
		System.out.println(concat.sequential().collectList().block());
	}
"
"	@Test
	public void cbse() {
		LinkedList<LinkedList<Airport>> listOfLists = new LinkedList<>();
		Airport a = new Airport(UUID.randomUUID().toString(), ""iata"", ""lowp"");
		String last = null;
		for (int i = 0; i < 5; i++) {
			LinkedList<Airport> list = new LinkedList<>();
			for (int j = 0; j < 10; j++) {
				list.add(a.withId(UUID.randomUUID().toString()));
				last = a.getId();
			}
			listOfLists.add(list);
		}
		Flux<Object> af = Flux.fromIterable(listOfLists).concatMap(catalogToStore -> Flux.fromIterable(catalogToStore)
				.parallel(4).runOn(Schedulers.parallel()).concatMap((entity) -> airportRepository.save(entity)));
		List<Object> saved = af.collectList().block();
		System.out.println(""results.size() : "" + saved.size());

		String statement = ""select * from `"" + /*config().bucketname()*/ ""_default"" + ""` where META().id >= '"" + last + ""'"";
		System.out.println(""statement: "" + statement);
		try {
			QueryResult qr = couchbaseTemplate.getCouchbaseClientFactory().getScope().query(statement,
					QueryOptions.queryOptions().profile(QueryProfile.PHASES));
			List<RemoveResult> rr = couchbaseTemplate.removeByQuery(Airport.class)
					.withOptions(QueryOptions.queryOptions().scanConsistency(QueryScanConsistency.REQUEST_PLUS)).all();
			System.out.println(qr.metaData().profile().get());
		} catch (Exception e) {
			e.printStackTrace();
			throw e;
		}
		List<Airport> airports = airportRepository.findAll().collectList().block();
		assertEquals(0, airports.size(), ""should have been all deleted"");
	}
"
"	@Test
	public void pairIdAndResult() {
		LinkedList<Airport> list = new LinkedList<>();
		Airport a = new Airport(UUID.randomUUID().toString(), ""iata"", ""lowp"");
		for (int i = 0; i < 5; i++) {
			list.add(a.withId(UUID.randomUUID().toString()));
		}
		Flux<Object> af = Flux.fromIterable(list).concatMap((entity) -> airportRepository.save(entity));
		List<Object> saved = af.collectList().block();
		System.out.println(""results.size() : "" + saved.size());
		Flux<Pair<String, Mono<Airport>>> pairFlux = Flux.fromIterable(list)
				.map((airport) -> Pair.of(airport.getId(), airportRepository.findById(airport.getId())));
		List<Pair<String, Mono<Airport>>> airportPairs = pairFlux.collectList().block();
		for (Pair<String, Mono<Airport>> airportPair : airportPairs) {
			System.out.println(""id: "" + airportPair.getFirst() + "" airport: "" + airportPair.getSecond().block());
		}

	}
"
"	@Test
	public void flatMapCB() throws Exception {
		System.out.println(""Start flatMapCB"");
		ParallelFlux<GetResult> concat = Flux.fromIterable(keyList).parallel(2).runOn(Schedulers.parallel())
				.flatMap(item -> cbGet(item) /* rCollection.get(item) */
						.doOnSubscribe((x) -> System.out.println("" +"" + rCat.incrementAndGet()))
						.doOnTerminate(() -> System.out.println("" -"" + rCat.decrementAndGet())));
		System.out.println(concat.sequential().collectList().block());
	}
"
"	@Test
	public void flatMapSyncCB() throws Exception {
		System.out.println(""Start flatMapSyncCB"");
		System.out.println(""\n******** Using flatSyncMap() *********"");
		ParallelFlux<GetResult> concat = Flux.fromIterable(keyList).parallel(2).runOn(Schedulers.parallel())
				.flatMap(item -> Flux.just(cbGetSync(item) /* collection.get(item) */));
		System.out.println(concat.sequential().collectList().block());
		;
	}
"
"	@Test
	public void flatMapVsConcatMapCB2() throws Exception {
		System.out.println(""Start flatMapCB2"");
		System.out.println(""\n******** Using flatMap() *********"");
		ParallelFlux<GetResult> flat = Flux.fromIterable(keyList).parallel(1).runOn(Schedulers.parallel())
				.flatMap(item -> rCollection.get(item).doOnSubscribe((x) -> System.out.println("" +"" + rCat.incrementAndGet()))
						.doOnTerminate(() -> System.out.println("" -"" + rCat.getAndDecrement())));
		System.out.println(flat.sequential().collectList().block());
		System.out.println(""Start concatMapCB"");
		System.out.println(""\n******** Using concatMap() *********"");
		ParallelFlux<GetResult> concat = Flux.fromIterable(keyList).parallel(2).runOn(Schedulers.parallel())
				.concatMap(item -> cbGet(item).doOnSubscribe((x) -> System.out.println("" +"" + rCat.incrementAndGet()))
						.doOnTerminate(() -> System.out.println("" -"" + rCat.getAndDecrement())));
		System.out.println(concat.sequential().collectList().block());
		;
	}
"
"	@BeforeEach
	public void beforeEach() {
		super.beforeEach();
		// already setup by JavaIntegrationTests.beforeAll()
		// ApplicationContext ac = new AnnotationConfigApplicationContext(Config.class);
		// couchbaseTemplate = (CouchbaseTemplate) ac.getBean(COUCHBASE_TEMPLATE);
		// reactiveCouchbaseTemplate = (ReactiveCouchbaseTemplate) ac.getBean(REACTIVE_COUCHBASE_TEMPLATE);
		// ensure each test starts with clean state

		couchbaseTemplate.removeByQuery(User.class).all();
		couchbaseTemplate.findByQuery(User.class).withConsistency(QueryScanConsistency.REQUEST_PLUS).all();
	}
"
"	@BeforeEach
	public void beforeEach() {
		super.beforeEach();
		couchbaseTemplate.removeByQuery(User.class).all();
		couchbaseTemplate.removeByQuery(UserAnnotated.class).all();
		couchbaseTemplate.removeByQuery(UserAnnotated2.class).all();
		couchbaseTemplate.removeByQuery(UserAnnotated3.class).all();
		couchbaseTemplate.removeByQuery(User.class).withConsistency(QueryScanConsistency.REQUEST_PLUS).all();
	}
"
"	@BeforeEach
	public void beforeEach() {
		super.beforeEach();
		List<RemoveResult> r1 = reactiveCouchbaseTemplate.removeByQuery(User.class).all().collectList().block();
		List<RemoveResult> r2 = reactiveCouchbaseTemplate.removeByQuery(UserAnnotated.class).all().collectList().block();
		List<RemoveResult> r3 = reactiveCouchbaseTemplate.removeByQuery(UserAnnotated2.class).all().collectList().block();
	}
"
"	@BeforeEach
	public void beforeEach() {
		// first call the super method
		super.beforeEach();
		// then do processing for this class
		couchbaseTemplate.removeByQuery(User.class).inCollection(collectionName).all();
		couchbaseTemplate.findByQuery(User.class).withConsistency(QueryScanConsistency.REQUEST_PLUS)
				.inCollection(collectionName).all();
		couchbaseTemplate.removeByQuery(Airport.class).inScope(scopeName).inCollection(collectionName).all();
		couchbaseTemplate.findByQuery(Airport.class).withConsistency(QueryScanConsistency.REQUEST_PLUS).inScope(scopeName)
				.inCollection(collectionName).all();
		couchbaseTemplate.removeByQuery(Airport.class).inScope(otherScope).inCollection(otherCollection).all();
		couchbaseTemplate.findByQuery(Airport.class).withConsistency(QueryScanConsistency.REQUEST_PLUS).inScope(otherScope)
				.inCollection(otherCollection).all();
	}
"
"	@AfterEach
	public void afterEach() {
		// first do processing for this class
		couchbaseTemplate.removeByQuery(User.class).inCollection(collectionName).all();
		// query with REQUEST_PLUS to ensure that the remove has completed.
		couchbaseTemplate.findByQuery(User.class).withConsistency(QueryScanConsistency.REQUEST_PLUS)
				.inCollection(collectionName).all();
		// then call the super method
		super.afterEach();
	}
"
"	@Test
	public void existsById() { // 1
		GetOptions options = GetOptions.getOptions().timeout(Duration.ofSeconds(10));
		ExistsOptions existsOptions = ExistsOptions.existsOptions().timeout(Duration.ofSeconds(10));
		Airport saved = template.insertById(Airport.class).inScope(scopeName).inCollection(collectionName).one(vie.withIcao(""low7"")).block();
		try {
			Boolean exists = template.existsById().inScope(scopeName).inCollection(collectionName).withOptions(existsOptions)
					.one(saved.getId()).block();
			assertTrue(exists, ""Airport should exist: "" + saved.getId());
		} finally {
			template.removeById().inScope(scopeName).inCollection(collectionName).one(saved.getId()).block();
		}
	}
"
"	@Test
	public void findByAnalytics() { // 2
		AnalyticsOptions options = AnalyticsOptions.analyticsOptions().timeout(Duration.ofSeconds(10));
		Airport saved = template.insertById(Airport.class).inScope(scopeName).inCollection(collectionName).one(vie.withIcao(""low8"")).block();
		try {
			List<Airport> found = template.findByAnalytics(Airport.class).inScope(scopeName).inCollection(collectionName)
					.withOptions(options).all().collectList().block();
			assertEquals(saved, found);
		} finally {
			template.removeById().inScope(scopeName).inCollection(collectionName).one(saved.getId()).block();
		}
	}
"
"	@Test
	public void findById() { // 3
		GetOptions options = GetOptions.getOptions().timeout(Duration.ofSeconds(10));
		Airport saved = template.insertById(Airport.class).inScope(scopeName).inCollection(collectionName).one(vie.withIcao(""low9"")).block();
		try {
			Airport found = template.findById(Airport.class).inScope(scopeName).inCollection(collectionName)
					.withOptions(options).one(saved.getId()).block();
			assertEquals(saved, found);
		} finally {
			template.removeById().inScope(scopeName).inCollection(collectionName).one(saved.getId()).block();
		}
	}
"
"	@Test
	public void findByQuery() { // 4
		QueryOptions options = QueryOptions.queryOptions().timeout(Duration.ofSeconds(10));
		Airport saved = template.insertById(Airport.class).inScope(scopeName).inCollection(collectionName).one(vie.withIcao(""lowa"")).block();
		try {
			List<Airport> found = template.findByQuery(Airport.class).withConsistency(QueryScanConsistency.REQUEST_PLUS)
					.inScope(scopeName).inCollection(collectionName).withOptions(options).all().collectList().block();
			assertEquals(saved.getId(), found.get(0).getId());
		} finally {
			template.removeById().inScope(scopeName).inCollection(collectionName).one(saved.getId()).block();
		}
	}
"
"	@Test
	public void findFromReplicasById() { // 5
		GetAnyReplicaOptions options = GetAnyReplicaOptions.getAnyReplicaOptions().timeout(Duration.ofSeconds(10));
		Airport saved = template.insertById(Airport.class).inScope(scopeName).inCollection(collectionName).one(vie.withIcao(""lowb"")).block();
		try {
			Airport found = template.findFromReplicasById(Airport.class).inScope(scopeName).inCollection(collectionName)
					.withOptions(options).any(saved.getId()).block();
			assertEquals(saved, found);
		} finally {
			template.removeById().inScope(scopeName).inCollection(collectionName).one(saved.getId()).block();
		}
	}
"
"	@Test
	public void insertById() { // 6
		InsertOptions options = InsertOptions.insertOptions().timeout(Duration.ofSeconds(10));
		GetOptions getOptions = GetOptions.getOptions().timeout(Duration.ofSeconds(10));
		Airport saved = template.insertById(Airport.class).inScope(scopeName).inCollection(collectionName)
				.withOptions(options).one(vie.withIcao(""lowc"").withId(UUID.randomUUID().toString())).block();
		try {
			Airport found = template.findById(Airport.class).inScope(scopeName).inCollection(collectionName)
					.withOptions(getOptions).one(saved.getId()).block();
			assertEquals(saved, found);
		} finally {
			template.removeById().inScope(scopeName).inCollection(collectionName).one(saved.getId()).block();
		}
	}
"
"	@Test
	public void removeById() { // 7
		RemoveOptions options = RemoveOptions.removeOptions().timeout(Duration.ofSeconds(10));
		Airport saved = template.insertById(Airport.class).inScope(scopeName).inCollection(collectionName).one(vie.withIcao(""lowd"")).block();
		RemoveResult removeResult = template.removeById().inScope(scopeName).inCollection(collectionName)
				.withOptions(options).one(saved.getId()).block();
		assertEquals(saved.getId(), removeResult.getId());
	}
"
"	@Test
	public void removeByQuery() { // 8
		QueryOptions options = QueryOptions.queryOptions().timeout(Duration.ofSeconds(10));
		Airport saved = template.insertById(Airport.class).inScope(scopeName).inCollection(collectionName).one(vie.withIcao(""lowe"")).block();
		List<RemoveResult> removeResults = template.removeByQuery(Airport.class)
				.withConsistency(QueryScanConsistency.REQUEST_PLUS).inScope(scopeName).inCollection(collectionName)
				.withOptions(options).matching(Query.query(QueryCriteria.where(""iata"").is(vie.getIata()))).all().collectList()
				.block();
		assertEquals(saved.getId(), removeResults.get(0).getId());
	}
"
"	@Test
	public void replaceById() { // 9
		InsertOptions insertOptions = InsertOptions.insertOptions().timeout(Duration.ofSeconds(10));
		ReplaceOptions options = ReplaceOptions.replaceOptions().timeout(Duration.ofSeconds(10));
		GetOptions getOptions = GetOptions.getOptions().timeout(Duration.ofSeconds(10));
		Airport saved = template.insertById(Airport.class).inScope(scopeName).inCollection(collectionName)
				.withOptions(insertOptions).one(vie.withIcao(""lowe"")).block();
		Airport replaced = template.replaceById(Airport.class).inScope(scopeName).inCollection(collectionName)
				.withOptions(options).one(vie.withIcao(""newIcao"")).block();
		try {
			Airport found = template.findById(Airport.class).inScope(scopeName).inCollection(collectionName)
					.withOptions(getOptions).one(saved.getId()).block();
			assertEquals(replaced, found);
		} finally {
			template.removeById().inScope(scopeName).inCollection(collectionName).one(saved.getId()).block();
		}
	}
"
"	@Test
	public void upsertById() { // 10
		UpsertOptions options = UpsertOptions.upsertOptions().timeout(Duration.ofSeconds(10));
		GetOptions getOptions = GetOptions.getOptions().timeout(Duration.ofSeconds(10));

		Airport saved = template.upsertById(Airport.class).inScope(scopeName).inCollection(collectionName)
				.withOptions(options).one(vie.withIcao(""lowf"")).block();
		try {
			Airport found = template.findById(Airport.class).inScope(scopeName).inCollection(collectionName)
					.withOptions(getOptions).one(saved.getId()).block();
			assertEquals(saved, found);
		} finally {
			template.removeById().inScope(scopeName).inCollection(collectionName).one(saved.getId()).block();
		}
	}
"
"	@Test
	public void existsByIdOther() { // 1
		GetOptions options = GetOptions.getOptions().timeout(Duration.ofSeconds(10));
		ExistsOptions existsOptions = ExistsOptions.existsOptions().timeout(Duration.ofSeconds(10));
		Airport saved = template.insertById(Airport.class).inScope(otherScope).inCollection(otherCollection).one(vie.withIcao(""lowg""))
				.block();
		try {
			Boolean exists = template.existsById().inScope(otherScope).inCollection(otherCollection)
					.withOptions(existsOptions).one(saved.getId()).block();
			assertTrue(exists, ""Airport should exist: "" + saved.getId());
		} finally {
			template.removeById().inScope(otherScope).inCollection(otherCollection).one(saved.getId()).block();
		}
	}
"
"	@Test
	public void findByAnalyticsOther() { // 2
		AnalyticsOptions options = AnalyticsOptions.analyticsOptions().timeout(Duration.ofSeconds(10));
		Airport saved = template.insertById(Airport.class).inScope(otherScope).inCollection(otherCollection).one(vie.withIcao(""lowh""))
				.block();
		try {
			List<Airport> found = template.findByAnalytics(Airport.class).inScope(otherScope).inCollection(otherCollection)
					.withOptions(options).all().collectList().block();
			assertEquals(saved, found);
		} finally {
			template.removeById().inScope(otherScope).inCollection(otherCollection).one(saved.getId()).block();
		}
	}
"
"	@Test
	public void findByIdOther() { // 3
		GetOptions options = GetOptions.getOptions().timeout(Duration.ofSeconds(10));
		Airport saved = template.insertById(Airport.class).inScope(otherScope).inCollection(otherCollection).one(vie.withIcao(""lowi""))
				.block();
		try {
			Airport found = template.findById(Airport.class).inScope(otherScope).inCollection(otherCollection)
					.withOptions(options).one(saved.getId()).block();
			assertEquals(saved, found);
		} finally {
			template.removeById().inScope(otherScope).inCollection(otherCollection).one(saved.getId()).block();
		}
	}
"
"	@Test
	public void findByQueryOther() { // 4
		QueryOptions options = QueryOptions.queryOptions().timeout(Duration.ofSeconds(10));
		Airport saved = template.insertById(Airport.class).inScope(otherScope).inCollection(otherCollection).one(vie.withIcao(""lowj""))
				.block();
		try {
			List<Airport> found = template.findByQuery(Airport.class).withConsistency(QueryScanConsistency.REQUEST_PLUS)
					.inScope(otherScope).inCollection(otherCollection).withOptions(options).all().collectList().block();
			assertEquals(saved.getId(), found.get(0).getId());
		} finally {
			template.removeById().inScope(otherScope).inCollection(otherCollection).one(saved.getId()).block();
		}
	}
"
"	@Test
	public void findFromReplicasByIdOther() { // 5
		GetAnyReplicaOptions options = GetAnyReplicaOptions.getAnyReplicaOptions().timeout(Duration.ofSeconds(10));
		Airport saved = template.insertById(Airport.class).inScope(otherScope).inCollection(otherCollection).one(vie.withIcao(""lowk""))
				.block();
		try {
			Airport found = template.findFromReplicasById(Airport.class).inScope(otherScope).inCollection(otherCollection)
					.withOptions(options).any(saved.getId()).block();
			assertEquals(saved, found);
		} finally {
			template.removeById().inScope(otherScope).inCollection(otherCollection).one(saved.getId()).block();
		}
	}
"
"	@Test
	public void insertByIdOther() { // 6
		InsertOptions options = InsertOptions.insertOptions().timeout(Duration.ofSeconds(10));
		GetOptions getOptions = GetOptions.getOptions().timeout(Duration.ofSeconds(10));
		Airport saved = template.insertById(Airport.class).inScope(otherScope).inCollection(otherCollection)
				.withOptions(options).one(vie.withIcao(""lowl"").withId(UUID.randomUUID().toString())).block();
		try {
			Airport found = template.findById(Airport.class).inScope(otherScope).inCollection(otherCollection)
					.withOptions(getOptions).one(saved.getId()).block();
			assertEquals(saved, found);
		} finally {
			template.removeById().inScope(otherScope).inCollection(otherCollection).one(saved.getId()).block();
		}
	}
"
"	@Test
	public void removeByIdOther() { // 7
		RemoveOptions options = RemoveOptions.removeOptions().timeout(Duration.ofSeconds(10));
		Airport saved = template.insertById(Airport.class).inScope(otherScope).inCollection(otherCollection).one(vie.withIcao(""lowm""))
				.block();
		RemoveResult removeResult = template.removeById().inScope(otherScope).inCollection(otherCollection)
				.withOptions(options).one(saved.getId()).block();
		assertEquals(saved.getId(), removeResult.getId());
	}
"
"	@Test
	public void removeByQueryOther() { // 8
		QueryOptions options = QueryOptions.queryOptions().timeout(Duration.ofSeconds(10));
		Airport saved = template.insertById(Airport.class).inScope(otherScope).inCollection(otherCollection).one(vie.withIcao(""lown""))
				.block();
		List<RemoveResult> removeResults = template.removeByQuery(Airport.class)
				.withConsistency(QueryScanConsistency.REQUEST_PLUS).inScope(otherScope).inCollection(otherCollection)
				.withOptions(options).matching(Query.query(QueryCriteria.where(""iata"").is(vie.getIata()))).all().collectList()
				.block();
		assertEquals(saved.getId(), removeResults.get(0).getId());
	}
"
"	@Test
	public void replaceByIdOther() { // 9
		InsertOptions insertOptions = InsertOptions.insertOptions().timeout(Duration.ofSeconds(10));
		ReplaceOptions options = ReplaceOptions.replaceOptions().timeout(Duration.ofSeconds(10));
		GetOptions getOptions = GetOptions.getOptions().timeout(Duration.ofSeconds(10));
		Airport saved = template.insertById(Airport.class).inScope(otherScope).inCollection(otherCollection)
				.withOptions(insertOptions).one(vie.withIcao(""lown"")).block();
		Airport replaced = template.replaceById(Airport.class).inScope(otherScope).inCollection(otherCollection)
				.withOptions(options).one(vie.withIcao(""newIcao"")).block();
		try {
			Airport found = template.findById(Airport.class).inScope(otherScope).inCollection(otherCollection)
					.withOptions(getOptions).one(saved.getId()).block();
			assertEquals(replaced, found);
		} finally {
			template.removeById().inScope(otherScope).inCollection(otherCollection).one(saved.getId()).block();
		}
	}
"
"	@Test
	public void upsertByIdOther() { // 10
		UpsertOptions options = UpsertOptions.upsertOptions().timeout(Duration.ofSeconds(10));
		GetOptions getOptions = GetOptions.getOptions().timeout(Duration.ofSeconds(10));

		Airport saved = template.upsertById(Airport.class).inScope(otherScope).inCollection(otherCollection)
				.withOptions(options).one(vie.withIcao(""lowo"")).block();
		try {
			Airport found = template.findById(Airport.class).inScope(otherScope).inCollection(otherCollection)
					.withOptions(getOptions).one(saved.getId()).block();
			assertEquals(saved, found);
		} finally {
			template.removeById().inScope(otherScope).inCollection(otherCollection).one(saved.getId()).block();
		}
	}
"
"	@Test
	public void existsByIdOptions() { // 1 - Options
		ExistsOptions options = ExistsOptions.existsOptions().timeout(Duration.ofNanos(10));
		assertThrows(UnambiguousTimeoutException.class, () -> template.existsById().inScope(otherScope)
				.inCollection(otherCollection).withOptions(options).one(vie.getId()).block());
	}
"
"	@Test
	public void findByAnalyticsOptions() { // 2
		AnalyticsOptions options = AnalyticsOptions.analyticsOptions().timeout(Duration.ofNanos(10));
		assertThrows(AmbiguousTimeoutException.class, () -> template.findByAnalytics(Airport.class).inScope(otherScope)
				.inCollection(otherCollection).withOptions(options).all().collectList().block());
	}
"
"	@Test
	public void findByIdOptions() { // 3
		GetOptions options = GetOptions.getOptions().timeout(Duration.ofNanos(10));
		assertThrows(UnambiguousTimeoutException.class, () -> template.findById(Airport.class).inScope(otherScope)
				.inCollection(otherCollection).withOptions(options).one(vie.getId()).block());
	}
"
"	@Test
	public void findByQueryOptions() { // 4
		QueryOptions options = QueryOptions.queryOptions().timeout(Duration.ofNanos(10));
		assertThrows(AmbiguousTimeoutException.class,
				() -> template.findByQuery(Airport.class).withConsistency(QueryScanConsistency.REQUEST_PLUS).inScope(otherScope)
						.inCollection(otherCollection).withOptions(options).all().collectList().block());
	}
"
"	@Test
	public void findFromReplicasByIdOptions() { // 5
		GetAnyReplicaOptions options = GetAnyReplicaOptions.getAnyReplicaOptions().timeout(Duration.ofNanos(1000));
		Airport saved = template.insertById(Airport.class).inScope(otherScope).inCollection(otherCollection).one(vie)
				.block();
		try {
			Airport found = template.findFromReplicasById(Airport.class).inScope(otherScope).inCollection(otherCollection)
					.withOptions(options).any(saved.getId()).block();
			assertNull(found, ""should not have found document in short timeout"");
		} finally {
			template.removeById().inScope(otherScope).inCollection(otherCollection).one(saved.getId()).block();
		}
	}
"
"	@Test
	public void insertByIdOptions() { // 6
		InsertOptions options = InsertOptions.insertOptions().timeout(Duration.ofNanos(10));
		assertThrows(AmbiguousTimeoutException.class, () -> template.insertById(Airport.class).inScope(otherScope)
				.inCollection(otherCollection).withOptions(options).one(vie.withId(UUID.randomUUID().toString())).block());
	}
"
"	@Test
	public void removeByIdOptions() { // 7 - options
		Airport saved = template.insertById(Airport.class).inScope(otherScope).inCollection(otherCollection).one(vie)
				.block();
		RemoveOptions options = RemoveOptions.removeOptions().timeout(Duration.ofNanos(10));
		assertThrows(AmbiguousTimeoutException.class, () -> template.removeById().inScope(otherScope)
				.inCollection(otherCollection).withOptions(options).one(vie.getId()).block());

	}
"
"	@Test
	public void removeByQueryOptions() { // 8 - options
		QueryOptions options = QueryOptions.queryOptions().timeout(Duration.ofNanos(10));
		assertThrows(AmbiguousTimeoutException.class,
				() -> template.removeByQuery(Airport.class).withConsistency(QueryScanConsistency.REQUEST_PLUS)
						.inScope(otherScope).inCollection(otherCollection).withOptions(options)
						.matching(Query.query(QueryCriteria.where(""iata"").is(vie.getIata()))).all().collectList().block());
	}
"
"	@Test
	public void replaceByIdOptions() { // 9 - options
		ReplaceOptions options = ReplaceOptions.replaceOptions().timeout(Duration.ofNanos(10));
		assertThrows(AmbiguousTimeoutException.class, () -> template.replaceById(Airport.class).inScope(otherScope)
				.inCollection(otherCollection).withOptions(options).one(vie.withIcao(""newIcao"")).block());
	}
"
"	@Test
	public void upsertByIdOptions() { // 10 - options
		UpsertOptions options = UpsertOptions.upsertOptions().timeout(Duration.ofNanos(10));
		assertThrows(AmbiguousTimeoutException.class, () -> template.upsertById(Airport.class).inScope(otherScope)
				.inCollection(otherCollection).withOptions(options).one(vie).block());
	}
"
"	@Test
	public void testNullValue() {
		QueryCriteria c = where(i(""name"")).is(null);
		assertEquals(""`name` = null"", c.export());
	}
"
"	@BeforeEach
	public void beforeEach() {
		// first call the super method
		super.beforeEach();
		// then do processing for this class
		couchbaseTemplate.removeByQuery(User.class).inCollection(collectionName).all();
		couchbaseTemplate.findByQuery(User.class).withConsistency(QueryScanConsistency.REQUEST_PLUS)
				.inCollection(collectionName).all();
		couchbaseTemplate.removeByQuery(Airport.class).inScope(scopeName).inCollection(collectionName).all();
		couchbaseTemplate.findByQuery(Airport.class).withConsistency(QueryScanConsistency.REQUEST_PLUS).inScope(scopeName)
				.inCollection(collectionName).all();
		couchbaseTemplate.removeByQuery(Airport.class).inScope(otherScope).inCollection(otherCollection).all();
		couchbaseTemplate.findByQuery(Airport.class).withConsistency(QueryScanConsistency.REQUEST_PLUS).inScope(otherScope)
				.inCollection(otherCollection).all();
	}
"
"	@AfterEach
	public void afterEach() {
		// first do processing for this class
		couchbaseTemplate.removeByQuery(User.class).inCollection(collectionName).all();
		// query with REQUEST_PLUS to ensure that the remove has completed.
		couchbaseTemplate.findByQuery(User.class).withConsistency(QueryScanConsistency.REQUEST_PLUS)
				.inCollection(collectionName).all();
		// then call the super method
		super.afterEach();
	}
"
"	@Test
	public void existsById() { // 1
		GetOptions options = GetOptions.getOptions().timeout(Duration.ofSeconds(10));
		ExistsOptions existsOptions = ExistsOptions.existsOptions().timeout(Duration.ofSeconds(10));
		Airport saved = couchbaseTemplate.insertById(Airport.class).inScope(scopeName).inCollection(collectionName)
				.one(vie);
		try {
			Boolean exists = couchbaseTemplate.existsById().inScope(scopeName).inCollection(collectionName)
					.withOptions(existsOptions).one(saved.getId());
			assertTrue(exists, ""Airport should exist: "" + saved.getId());
		} finally {
			couchbaseTemplate.removeById().inScope(scopeName).inCollection(collectionName).one(saved.getId());
		}
	}
"
"	@Test
	public void findByAnalytics() { // 2
		AnalyticsOptions options = AnalyticsOptions.analyticsOptions().timeout(Duration.ofSeconds(10));
		Airport saved = couchbaseTemplate.insertById(Airport.class).inScope(scopeName).inCollection(collectionName)
				.one(vie);
		try {
			List<Airport> found = couchbaseTemplate.findByAnalytics(Airport.class).inScope(scopeName)
					.inCollection(collectionName).withOptions(options).all();
			assertEquals(saved, found);
		} finally {
			couchbaseTemplate.removeById().inScope(scopeName).inCollection(collectionName).one(saved.getId());
		}
	}
"
"	@Test
	public void findById() { // 3
		GetOptions options = GetOptions.getOptions().timeout(Duration.ofSeconds(10));
		Airport saved = couchbaseTemplate.insertById(Airport.class).inScope(scopeName).inCollection(collectionName)
				.one(vie);
		try {
			Airport found = couchbaseTemplate.findById(Airport.class).inScope(scopeName).inCollection(collectionName)
					.withOptions(options).one(saved.getId());
			assertEquals(saved, found);
		} finally {
			couchbaseTemplate.removeById().inScope(scopeName).inCollection(collectionName).one(saved.getId());
		}
	}
"
"	@Test
	public void findByQuery() { // 4
		QueryOptions options = QueryOptions.queryOptions().timeout(Duration.ofSeconds(10));
		Airport saved = couchbaseTemplate.insertById(Airport.class).inScope(scopeName).inCollection(collectionName)
				.one(vie);
		try {
			List<Airport> found = couchbaseTemplate.findByQuery(Airport.class)
					.withConsistency(QueryScanConsistency.REQUEST_PLUS).inScope(scopeName).inCollection(collectionName)
					.withOptions(options).all();
			assertEquals(saved.getId(), found.get(0).getId());
		} finally {
			couchbaseTemplate.removeById().inScope(scopeName).inCollection(collectionName).one(saved.getId());
		}
	}
"
"	@Test
	public void findFromReplicasById() { // 5
		GetAnyReplicaOptions options = GetAnyReplicaOptions.getAnyReplicaOptions().timeout(Duration.ofSeconds(10));
		Airport saved = couchbaseTemplate.insertById(Airport.class).inScope(scopeName).inCollection(collectionName)
				.one(vie);
		try {
			Airport found = couchbaseTemplate.findFromReplicasById(Airport.class).inScope(scopeName)
					.inCollection(collectionName).withOptions(options).any(saved.getId());
			assertEquals(saved, found);
		} finally {
			couchbaseTemplate.removeById().inScope(scopeName).inCollection(collectionName).one(saved.getId());
		}
	}
"
"	@Test
	public void insertById() { // 6
		InsertOptions options = InsertOptions.insertOptions().timeout(Duration.ofSeconds(10));
		GetOptions getOptions = GetOptions.getOptions().timeout(Duration.ofSeconds(10));
		Airport saved = couchbaseTemplate.insertById(Airport.class).inScope(scopeName).inCollection(collectionName)
				.withOptions(options).one(vie.withId(UUID.randomUUID().toString()));
		try {
			Airport found = couchbaseTemplate.findById(Airport.class).inScope(scopeName).inCollection(collectionName)
					.withOptions(getOptions).one(saved.getId());
			assertEquals(saved, found);
		} finally {
			couchbaseTemplate.removeById().inScope(scopeName).inCollection(collectionName).one(saved.getId());
		}
	}
"
"	@Test
	public void removeById() { // 7
		RemoveOptions options = RemoveOptions.removeOptions().timeout(Duration.ofSeconds(10));
		Airport saved = couchbaseTemplate.insertById(Airport.class).inScope(scopeName).inCollection(collectionName)
				.one(vie);
		RemoveResult removeResult = couchbaseTemplate.removeById().inScope(scopeName).inCollection(collectionName)
				.withOptions(options).one(saved.getId());
		assertEquals(saved.getId(), removeResult.getId());
	}
"
"	@Test
	public void removeByQuery() { // 8
		QueryOptions options = QueryOptions.queryOptions().timeout(Duration.ofSeconds(10));
		Airport saved = couchbaseTemplate.insertById(Airport.class).inScope(scopeName).inCollection(collectionName)
				.one(vie);
		List<RemoveResult> removeResults = couchbaseTemplate.removeByQuery(Airport.class)
				.withConsistency(QueryScanConsistency.REQUEST_PLUS).inScope(scopeName).inCollection(collectionName)
				.withOptions(options).matching(Query.query(QueryCriteria.where(""iata"").is(vie.getIata()))).all();
		assertEquals(saved.getId(), removeResults.get(0).getId());
	}
"
"	@Test
	public void replaceById() { // 9
		InsertOptions insertOptions = InsertOptions.insertOptions().timeout(Duration.ofSeconds(10));
		ReplaceOptions options = ReplaceOptions.replaceOptions().timeout(Duration.ofSeconds(10));
		GetOptions getOptions = GetOptions.getOptions().timeout(Duration.ofSeconds(10));
		Airport saved = couchbaseTemplate.insertById(Airport.class).inScope(scopeName).inCollection(collectionName)
				.withOptions(insertOptions).one(vie);
		Airport replaced = couchbaseTemplate.replaceById(Airport.class).inScope(scopeName).inCollection(collectionName)
				.withOptions(options).one(vie.withIcao(""newIcao""));
		try {
			Airport found = couchbaseTemplate.findById(Airport.class).inScope(scopeName).inCollection(collectionName)
					.withOptions(getOptions).one(saved.getId());
			assertEquals(replaced, found);
		} finally {
			couchbaseTemplate.removeById().inScope(scopeName).inCollection(collectionName).one(saved.getId());
		}
	}
"
"	@Test
	public void upsertById() { // 10
		UpsertOptions options = UpsertOptions.upsertOptions().timeout(Duration.ofSeconds(10));
		GetOptions getOptions = GetOptions.getOptions().timeout(Duration.ofSeconds(10));

		Airport saved = couchbaseTemplate.upsertById(Airport.class).inScope(scopeName).inCollection(collectionName)
				.withOptions(options).one(vie);
		try {
			Airport found = couchbaseTemplate.findById(Airport.class).inScope(scopeName).inCollection(collectionName)
					.withOptions(getOptions).one(saved.getId());
			assertEquals(saved, found);
		} finally {
			couchbaseTemplate.removeById().inScope(scopeName).inCollection(collectionName).one(saved.getId());
		}
	}
"
"	@Test
	public void existsByIdOther() { // 1
		GetOptions options = GetOptions.getOptions().timeout(Duration.ofSeconds(10));
		ExistsOptions existsOptions = ExistsOptions.existsOptions().timeout(Duration.ofSeconds(10));
		Airport saved = couchbaseTemplate.insertById(Airport.class).inScope(otherScope).inCollection(otherCollection)
				.one(vie);
		try {
			Boolean exists = couchbaseTemplate.existsById().inScope(otherScope).inCollection(otherCollection)
					.withOptions(existsOptions).one(saved.getId());
			assertTrue(exists, ""Airport should exist: "" + saved.getId());
		} finally {
			couchbaseTemplate.removeById().inScope(otherScope).inCollection(otherCollection).one(saved.getId());
		}
	}
"
"	@Test
	public void findByAnalyticsOther() { // 2
		AnalyticsOptions options = AnalyticsOptions.analyticsOptions().timeout(Duration.ofSeconds(10));
		Airport saved = couchbaseTemplate.insertById(Airport.class).inScope(otherScope).inCollection(otherCollection)
				.one(vie);
		try {
			List<Airport> found = couchbaseTemplate.findByAnalytics(Airport.class).inScope(otherScope)
					.inCollection(otherCollection).withOptions(options).all();
			assertEquals(saved, found);
		} finally {
			couchbaseTemplate.removeById().inScope(otherScope).inCollection(otherCollection).one(saved.getId());
		}
	}
"
"	@Test
	public void findByIdOther() { // 3
		GetOptions options = GetOptions.getOptions().timeout(Duration.ofSeconds(10));
		Airport saved = couchbaseTemplate.insertById(Airport.class).inScope(otherScope).inCollection(otherCollection)
				.one(vie);
		try {
			Airport found = couchbaseTemplate.findById(Airport.class).inScope(otherScope).inCollection(otherCollection)
					.withOptions(options).one(saved.getId());
			assertEquals(saved, found);
		} finally {
			couchbaseTemplate.removeById().inScope(otherScope).inCollection(otherCollection).one(saved.getId());
		}
	}
"
"	@Test
	public void findByQueryOther() { // 4
		QueryOptions options = QueryOptions.queryOptions().timeout(Duration.ofSeconds(10));
		Airport saved = couchbaseTemplate.insertById(Airport.class).inScope(otherScope).inCollection(otherCollection)
				.one(vie);
		try {
			List<Airport> found = couchbaseTemplate.findByQuery(Airport.class)
					.withConsistency(QueryScanConsistency.REQUEST_PLUS).inScope(otherScope).inCollection(otherCollection)
					.withOptions(options).all();
			assertEquals(saved.getId(), found.get(0).getId());
		} finally {
			couchbaseTemplate.removeById().inScope(otherScope).inCollection(otherCollection).one(saved.getId());
		}
	}
"
"	@Test
	public void findFromReplicasByIdOther() { // 5
		GetAnyReplicaOptions options = GetAnyReplicaOptions.getAnyReplicaOptions().timeout(Duration.ofSeconds(10));
		Airport saved = couchbaseTemplate.insertById(Airport.class).inScope(otherScope).inCollection(otherCollection)
				.one(vie);
		try {
			Airport found = couchbaseTemplate.findFromReplicasById(Airport.class).inScope(otherScope)
					.inCollection(otherCollection).withOptions(options).any(saved.getId());
			assertEquals(saved, found);
		} finally {
			couchbaseTemplate.removeById().inScope(otherScope).inCollection(otherCollection).one(saved.getId());
		}
	}
"
"	@Test
	public void insertByIdOther() { // 6
		InsertOptions options = InsertOptions.insertOptions().timeout(Duration.ofSeconds(10));
		GetOptions getOptions = GetOptions.getOptions().timeout(Duration.ofSeconds(10));
		Airport saved = couchbaseTemplate.insertById(Airport.class).inScope(otherScope).inCollection(otherCollection)
				.withOptions(options).one(vie.withId(UUID.randomUUID().toString()));
		try {
			Airport found = couchbaseTemplate.findById(Airport.class).inScope(otherScope).inCollection(otherCollection)
					.withOptions(getOptions).one(saved.getId());
			assertEquals(saved, found);
		} finally {
			couchbaseTemplate.removeById().inScope(otherScope).inCollection(otherCollection).one(saved.getId());
		}
	}
"
"	@Test
	public void removeByIdOther() { // 7
		RemoveOptions options = RemoveOptions.removeOptions().timeout(Duration.ofSeconds(10));
		Airport saved = couchbaseTemplate.insertById(Airport.class).inScope(otherScope).inCollection(otherCollection)
				.one(vie);
		RemoveResult removeResult = couchbaseTemplate.removeById().inScope(otherScope).inCollection(otherCollection)
				.withOptions(options).one(saved.getId());
		assertEquals(saved.getId(), removeResult.getId());
	}
"
"	@Test
	public void removeByQueryOther() { // 8
		QueryOptions options = QueryOptions.queryOptions().timeout(Duration.ofSeconds(10));
		Airport saved = couchbaseTemplate.insertById(Airport.class).inScope(otherScope).inCollection(otherCollection)
				.one(vie);
		List<RemoveResult> removeResults = couchbaseTemplate.removeByQuery(Airport.class)
				.withConsistency(QueryScanConsistency.REQUEST_PLUS).inScope(otherScope).inCollection(otherCollection)
				.withOptions(options).matching(Query.query(QueryCriteria.where(""iata"").is(vie.getIata()))).all();
		assertEquals(saved.getId(), removeResults.get(0).getId());
	}
"
"	@Test
	public void replaceByIdOther() { // 9
		InsertOptions insertOptions = InsertOptions.insertOptions().timeout(Duration.ofSeconds(10));
		ReplaceOptions options = ReplaceOptions.replaceOptions().timeout(Duration.ofSeconds(10));
		GetOptions getOptions = GetOptions.getOptions().timeout(Duration.ofSeconds(10));
		Airport saved = couchbaseTemplate.insertById(Airport.class).inScope(otherScope).inCollection(otherCollection)
				.withOptions(insertOptions).one(vie);
		Airport replaced = couchbaseTemplate.replaceById(Airport.class).inScope(otherScope).inCollection(otherCollection)
				.withOptions(options).one(vie.withIcao(""newIcao""));
		try {
			Airport found = couchbaseTemplate.findById(Airport.class).inScope(otherScope).inCollection(otherCollection)
					.withOptions(getOptions).one(saved.getId());
			assertEquals(replaced, found);
		} finally {
			couchbaseTemplate.removeById().inScope(otherScope).inCollection(otherCollection).one(saved.getId());
		}
	}
"
"	@Test
	public void upsertByIdOther() { // 10
		UpsertOptions options = UpsertOptions.upsertOptions().timeout(Duration.ofSeconds(10));
		GetOptions getOptions = GetOptions.getOptions().timeout(Duration.ofSeconds(10));

		Airport saved = couchbaseTemplate.upsertById(Airport.class).inScope(otherScope).inCollection(otherCollection)
				.withOptions(options).one(vie);
		try {
			Airport found = couchbaseTemplate.findById(Airport.class).inScope(otherScope).inCollection(otherCollection)
					.withOptions(getOptions).one(saved.getId());
			assertEquals(saved, found);
		} finally {
			couchbaseTemplate.removeById().inScope(otherScope).inCollection(otherCollection).one(saved.getId());
		}
	}
"
"	@Test
	public void existsByIdOptions() { // 1 - Options
		ExistsOptions options = ExistsOptions.existsOptions().timeout(Duration.ofNanos(10));
		assertThrows(UnambiguousTimeoutException.class, () -> couchbaseTemplate.existsById().inScope(otherScope)
				.inCollection(otherCollection).withOptions(options).one(vie.getId()));
	}
"
"	@Test
	public void findByAnalyticsOptions() { // 2
		AnalyticsOptions options = AnalyticsOptions.analyticsOptions().timeout(Duration.ofNanos(10));
		assertThrows(AmbiguousTimeoutException.class, () -> couchbaseTemplate.findByAnalytics(Airport.class)
				.inScope(otherScope).inCollection(otherCollection).withOptions(options).all());
	}
"
"	@Test
	public void findByIdOptions() { // 3
		GetOptions options = GetOptions.getOptions().timeout(Duration.ofNanos(10));
		assertThrows(UnambiguousTimeoutException.class, () -> couchbaseTemplate.findById(Airport.class).inScope(otherScope)
				.inCollection(otherCollection).withOptions(options).one(vie.getId()));
	}
"
"	@Test
	public void findByQueryOptions() { // 4
		QueryOptions options = QueryOptions.queryOptions().timeout(Duration.ofNanos(10));
		assertThrows(AmbiguousTimeoutException.class,
				() -> couchbaseTemplate.findByQuery(Airport.class).withConsistency(QueryScanConsistency.REQUEST_PLUS)
						.inScope(otherScope).inCollection(otherCollection).withOptions(options).all());
	}
"
"	@Test
	public void findFromReplicasByIdOptions() { // 5
		GetAnyReplicaOptions options = GetAnyReplicaOptions.getAnyReplicaOptions().timeout(Duration.ofNanos(1000));
		Airport saved = couchbaseTemplate.insertById(Airport.class).inScope(otherScope).inCollection(otherCollection)
				.one(vie);
		try {
			Airport found = couchbaseTemplate.findFromReplicasById(Airport.class).inScope(otherScope)
					.inCollection(otherCollection).withOptions(options).any(saved.getId());
			assertNull(found, ""should not have found document in short timeout"");
		} finally {
			couchbaseTemplate.removeById().inScope(otherScope).inCollection(otherCollection).one(saved.getId());
		}
	}
"
"	@Test
	public void insertByIdOptions() { // 6
		InsertOptions options = InsertOptions.insertOptions().timeout(Duration.ofNanos(10));
		assertThrows(AmbiguousTimeoutException.class, () -> couchbaseTemplate.insertById(Airport.class).inScope(otherScope)
				.inCollection(otherCollection).withOptions(options).one(vie.withId(UUID.randomUUID().toString())));
	}
"
"	@Test
	public void removeByIdOptions() { // 7 - options
		Airport saved = couchbaseTemplate.insertById(Airport.class).inScope(otherScope).inCollection(otherCollection)
				.one(vie);
		RemoveOptions options = RemoveOptions.removeOptions().timeout(Duration.ofNanos(10));
		assertThrows(AmbiguousTimeoutException.class, () -> couchbaseTemplate.removeById().inScope(otherScope)
				.inCollection(otherCollection).withOptions(options).one(vie.getId()));

	}
"
"	@Test
	public void removeByQueryOptions() { // 8 - options
		QueryOptions options = QueryOptions.queryOptions().timeout(Duration.ofNanos(10));
		assertThrows(AmbiguousTimeoutException.class,
				() -> couchbaseTemplate.removeByQuery(Airport.class).withConsistency(QueryScanConsistency.REQUEST_PLUS)
						.inScope(otherScope).inCollection(otherCollection).withOptions(options)
						.matching(Query.query(QueryCriteria.where(""iata"").is(vie.getIata()))).all());
	}
"
"	@Test
	public void replaceByIdOptions() { // 9 - options
		ReplaceOptions options = ReplaceOptions.replaceOptions().timeout(Duration.ofNanos(10));
		assertThrows(AmbiguousTimeoutException.class, () -> couchbaseTemplate.replaceById(Airport.class).inScope(otherScope)
				.inCollection(otherCollection).withOptions(options).one(vie.withIcao(""newIcao"")));
	}
"
"	@Test
	public void upsertByIdOptions() { // 10 - options
		UpsertOptions options = UpsertOptions.upsertOptions().timeout(Duration.ofNanos(10));
		assertThrows(AmbiguousTimeoutException.class, () -> couchbaseTemplate.upsertById(Airport.class).inScope(otherScope)
				.inCollection(otherCollection).withOptions(options).one(vie));
	}
"
"	@Test
	public void testScopeCollectionAnnotation() {
		UserCol user = new UserCol(""1"", ""Dave"", ""Wilson"");
		Query query = Query.query(QueryCriteria.where(""firstname"").is(user.getFirstname()));
		try {
			UserCol saved = couchbaseTemplate.insertById(UserCol.class).inScope(scopeName).inCollection(collectionName)
					.one(user);
			List<UserCol> found = couchbaseTemplate.findByQuery(UserCol.class)
					.withConsistency(QueryScanConsistency.REQUEST_PLUS).inScope(scopeName).inCollection(collectionName)
					.matching(query).all();
			assertEquals(saved, found.get(0), ""should have found what was saved"");
			List<UserCol> notfound = couchbaseTemplate.findByQuery(UserCol.class).inScope(CollectionIdentifier.DEFAULT_SCOPE)
					.inCollection(CollectionIdentifier.DEFAULT_COLLECTION).matching(query).all();
			assertEquals(0, notfound.size(), ""should not have found what was saved"");
			couchbaseTemplate.removeByQuery(UserCol.class).inScope(scopeName).inCollection(collectionName).matching(query)
					.all();
		} finally {
			try {
				couchbaseTemplate.removeByQuery(UserCol.class).inScope(scopeName).inCollection(collectionName).matching(query)
						.all();
			} catch (DataRetrievalFailureException drfe) {}
		}
	}
"
"	@Test
	public void testScopeCollectionRepoWith() {
		UserCol user = new UserCol(""1"", ""Dave"", ""Wilson"");
		Query query = Query.query(QueryCriteria.where(""firstname"").is(user.getFirstname()));
		try {
			UserCol saved = couchbaseTemplate.insertById(UserCol.class).inScope(scopeName).inCollection(collectionName)
					.one(user);
			List<UserCol> found = couchbaseTemplate.findByQuery(UserCol.class)
					.withConsistency(QueryScanConsistency.REQUEST_PLUS).inScope(scopeName).inCollection(collectionName)
					.matching(query).all();
			assertEquals(saved, found.get(0), ""should have found what was saved"");
			List<UserCol> notfound = couchbaseTemplate.findByQuery(UserCol.class).inScope(CollectionIdentifier.DEFAULT_SCOPE)
					.inCollection(CollectionIdentifier.DEFAULT_COLLECTION).matching(query).all();
			assertEquals(0, notfound.size(), ""should not have found what was saved"");
			couchbaseTemplate.removeByQuery(UserCol.class).inScope(scopeName).inCollection(collectionName).matching(query)
					.all();
		} finally {
			try {
				couchbaseTemplate.removeByQuery(UserCol.class).inScope(scopeName).inCollection(collectionName).matching(query)
						.all();
			} catch (DataRetrievalFailureException drfe) {}
		}
	}
"
"	@Test
		public String getConnectionString() {
			return connectionString();
		}
"
"	@Test
		public String getId() {
			return springId;
		}
"
"	@Test
		public String convert(Integer source) {
			return source % 2 == 0 ? ""even"" : ""odd"";
		}
"
"	@BeforeEach
	public void beforeEach() {
		context = new CouchbaseMappingContext();
		converter = new MappingCouchbaseConverter(context);
		bucketName = ""sample-bucket"";
	}
"
"	@BeforeEach
	public void beforeEach() {
		context = new CouchbaseMappingContext();
		converter = new MappingCouchbaseConverter(context);
		ApplicationContext ac = new AnnotationConfigApplicationContext(Config.class);
		couchbaseTemplate = (CouchbaseTemplate) ac.getBean(COUCHBASE_TEMPLATE);
	}
"
"	@Test
		public String getConnectionString() {
			return connectionString();
		}
"
"	@BeforeEach
	public void beforeEach() {
		context = new CouchbaseMappingContext();
		converter = new MappingCouchbaseConverter(context);
		ApplicationContext ac = new AnnotationConfigApplicationContext(Config.class);
		couchbaseTemplate = (CouchbaseTemplate) ac.getBean(COUCHBASE_TEMPLATE);
	}
"
"	@Test
		public String getConnectionString() {
			return connectionString();
		}
"
"	@BeforeEach
	public void beforeEach() {
		// first call the super method
		super.beforeEach();
		// then do processing for this class
		couchbaseTemplate.removeByQuery(User.class).inCollection(collectionName).all();
		couchbaseTemplate.removeByQuery(UserCol.class).inScope(otherScope).inCollection(otherCollection).all();
		ApplicationContext ac = new AnnotationConfigApplicationContext(Config.class);
		// seems that @Autowired is not adequate, so ...
		airportRepository = (AirportRepository) ac.getBean(""airportRepository"");
		userColRepository = (UserColRepository) ac.getBean(""userColRepository"");
	}
"
"	@AfterEach
	public void afterEach() {
		// first do processing for this class
		// no-op
		// then call the super method
		super.afterEach();
	}
"
"	@Test
	public void myTest() {

		AirportRepository ar = airportRepository.withScope(scopeName).withCollection(collectionName);
		Airport vie = new Airport(""airports::vie"", ""vie"", ""loww"");
		try {
			Airport saved = ar.save(vie);
			Airport airport2 = ar.save(saved);
		} catch (Exception e) {
			e.printStackTrace();
			throw e;
		} finally {
			ar.delete(vie);
		}

	}
"
"	@Test
	public void testScopeCollectionAnnotation() {
		// template default scope is my_scope
		// UserCol annotation scope is other_scope
		UserCol user = new UserCol(""1"", ""Dave"", ""Wilson"");
		try {
			UserCol saved = userColRepository.withCollection(otherCollection).save(user); // should use UserCol annotation
																																										// scope
			List<UserCol> found = userColRepository.withCollection(otherCollection).findByFirstname(user.getFirstname());
			assertEquals(saved, found.get(0), ""should have found what was saved"");
			List<UserCol> notfound = userColRepository.withScope(DEFAULT_SCOPE)
					.withCollection(CollectionIdentifier.DEFAULT_COLLECTION).findByFirstname(user.getFirstname());
			assertEquals(0, notfound.size(), ""should not have found what was saved"");
		} finally {
			try {
				userColRepository.withScope(otherScope).withCollection(otherCollection).delete(user);
			} catch (DataRetrievalFailureException drfe) {}
		}
	}
"
"	@Test
	public void testScopeCollectionAnnotationSwap() {
		// UserCol annotation scope is other_scope, collection is other_collection
		// airportRepository relies on Config.setScopeName(scopeName) (""my_scope"") from CollectionAwareIntegrationTests.
		// using airportRepository without specified a collection should fail.
		// This test ensures that airportRepository.save(airport) doesn't get the
		// collection from CrudMethodMetadata of UserCol.save()
		UserCol userCol = new UserCol(""1"", ""Dave"", ""Wilson"");
		Airport airport = new Airport(""3"", ""myIata"", ""myIcao"");
		UserCol savedCol = userColRepository.save(userCol); // uses UserCol annotation scope, populates CrudMethodMetadata
		userColRepository.delete(userCol); // uses UserCol annotation scope, populates CrudMethodMetadata
		assertThrows(IllegalStateException.class, () -> airportRepository.save(airport));
	}
"
"	@Test
	public void testScopeCollectionRepoWith() {
		UserCol user = new UserCol(""1"", ""Dave"", ""Wilson"");
		try {
			UserCol saved = userColRepository.withScope(scopeName).withCollection(collectionName).save(user);
			List<UserCol> found = userColRepository.withScope(scopeName).withCollection(collectionName)
					.findByFirstname(user.getFirstname());
			assertEquals(saved, found.get(0), ""should have found what was saved"");
			List<UserCol> notfound = userColRepository.withScope(DEFAULT_SCOPE)
					.withCollection(CollectionIdentifier.DEFAULT_COLLECTION).findByFirstname(user.getFirstname());
			assertEquals(0, notfound.size(), ""should not have found what was saved"");
			userColRepository.withScope(scopeName).withCollection(collectionName).delete(user);
		} finally {
			try {
				userColRepository.withScope(scopeName).withCollection(collectionName).delete(user);
			} catch (DataRetrievalFailureException drfe) {}
		}
	}
"
"	@BeforeEach
	public void beforeEach() {
		// first call the super method
		super.beforeEach();
		// then do processing for this class
		couchbaseTemplate.removeByQuery(User.class).inCollection(collectionName).all();
		couchbaseTemplate.removeByQuery(UserCol.class).inScope(otherScope).inCollection(otherCollection).all();

		ApplicationContext ac = new AnnotationConfigApplicationContext(Config.class);
		// seems that @Autowired is not adequate, so ...
		airportRepository = (ReactiveAirportRepository) ac.getBean(""reactiveAirportRepository"");
		userColRepository = (ReactiveUserColRepository) ac.getBean(""reactiveUserColRepository"");
	}
"
"	@AfterEach
	public void afterEach() {
		// first do processing for this class
		// no-op
		// then call the super method
		super.afterEach();
	}
"
"	@Test
	public void myTest() {

		ReactiveAirportRepository ar = airportRepository.withScope(scopeName).withCollection(collectionName);
		Airport vie = new Airport(""airports::vie"", ""vie"", ""loww"");
		try {
			Airport saved = ar.save(vie).block();
			Airport airport2 = ar.save(saved).block();
		} catch (Exception e) {
			e.printStackTrace();
			throw e;
		} finally {
			ar.delete(vie).block();
		}

	}
"
"	@Test
	public void testScopeCollectionAnnotation() {
		// template default scope is my_scope
		// UserCol annotation scope is other_scope
		UserCol user = new UserCol(""1"", ""Dave"", ""Wilson"");
		try {
			UserCol saved = userColRepository.withCollection(otherCollection).save(user).block(); // should use UserCol
																																														// annotation
			// scope
			List<UserCol> found = userColRepository.withCollection(otherCollection).findByFirstname(user.getFirstname())
					.collectList().block();
			assertEquals(saved, found.get(0), ""should have found what was saved"");
			List<UserCol> notfound = userColRepository.withScope(CollectionIdentifier.DEFAULT_SCOPE)
					.withCollection(CollectionIdentifier.DEFAULT_COLLECTION).findByFirstname(user.getFirstname()).collectList()
					.block();
			assertEquals(0, notfound.size(), ""should not have found what was saved"");
		} finally {
			try {
				userColRepository.withScope(otherScope).withCollection(otherCollection).delete(user);
			} catch (DataRetrievalFailureException drfe) {}
		}
	}
"
"	@Test
	public void testScopeCollectionRepoWith() {
		UserCol user = new UserCol(""1"", ""Dave"", ""Wilson"");
		try {
			UserCol saved = userColRepository.withScope(scopeName).withCollection(collectionName).save(user).block();
			List<UserCol> found = userColRepository.withScope(scopeName).withCollection(collectionName)
					.findByFirstname(user.getFirstname()).collectList().block();
			assertEquals(saved, found.get(0), ""should have found what was saved"");
			List<UserCol> notfound = userColRepository.withScope(CollectionIdentifier.DEFAULT_SCOPE)
					.withCollection(CollectionIdentifier.DEFAULT_COLLECTION).findByFirstname(user.getFirstname()).collectList()
					.block();
			assertEquals(0, notfound.size(), ""should not have found what was saved"");
			userColRepository.withScope(scopeName).withCollection(collectionName).delete(user).block();
		} finally {
			try {
				userColRepository.withScope(scopeName).withCollection(collectionName).delete(user).block();
			} catch (DataRetrievalFailureException drfe) {}
		}
	}
"
"	@Test
		public String getConnectionString() {
			return connectionString();
		}
"
"	@Test
	public void testCas() {
		User user = new User(""1"", ""Dave"", ""Wilson"");
		userRepository.save(user).block();
		user.setVersion(user.getVersion() - 1);
		assertThrows(DataIntegrityViolationException.class, () -> userRepository.save(user).block());
		user.setVersion(0);
		userRepository.save(user).block();
		userRepository.delete(user).block();
	}
"
"	@Test
	public Mono<Airport> getPolicyByIdAndEffectiveDateTime(String policyId, Instant effectiveDateTime) {
		return airportRepository
				.findPolicySnapshotByPolicyIdAndEffectiveDateTime(policyId, effectiveDateTime.toEpochMilli())
				// .map(Airport::getEntity)
				.doOnError(
						error -> System.out.println(""MSG='Exception happened while retrieving Policy by Id and effectiveDateTime', ""
								+ ""policyId={}, effectiveDateTime={}""));
	}
"
"	@Test
		public String getConnectionString() {
			return connectionString();
		}
"
"    @Test
    public void shouldDecodeRequestWithSimpleXml() {
        Object temp;

        write(XML1);

        temp = channel.readInbound();
        assertThat(temp, instanceOf(XmlDocumentStart.class));
        assertThat(((XmlDocumentStart) temp).version(), is(""1.0""));
        assertThat(((XmlDocumentStart) temp).encoding(), is(""UTF-8""));
        assertThat(((XmlDocumentStart) temp).standalone(), is(false));
        assertThat(((XmlDocumentStart) temp).encodingScheme(), is(nullValue()));

        temp = channel.readInbound();
        assertThat(temp, instanceOf(XmlDTD.class));
        assertThat(((XmlDTD) temp).text(), is(""employee.dtd""));

        temp = channel.readInbound();
        assertThat(temp, instanceOf(XmlProcessingInstruction.class));
        assertThat(((XmlProcessingInstruction) temp).target(), is(""xml-stylesheet""));
        assertThat(((XmlProcessingInstruction) temp).data(), is(""type=\""text/css\"" href=\""netty.css\""""));

        temp = channel.readInbound();
        assertThat(temp, instanceOf(XmlProcessingInstruction.class));
        assertThat(((XmlProcessingInstruction) temp).target(), is(""xml-test""));
        assertThat(((XmlProcessingInstruction) temp).data(), is(""""));

        temp = channel.readInbound();
        assertThat(temp, instanceOf(XmlElementStart.class));
        assertThat(((XmlElementStart) temp).name(), is(""employee""));
        assertThat(((XmlElementStart) temp).prefix(), is(""""));
        assertThat(((XmlElementStart) temp).namespace(), is(""""));
        assertThat(((XmlElementStart) temp).attributes().size(), is(0));
        assertThat(((XmlElementStart) temp).namespaces().size(), is(1));
        assertThat(((XmlElementStart) temp).namespaces().get(0).prefix(), is(""nettya""));
        assertThat(((XmlElementStart) temp).namespaces().get(0).uri(), is(""http://netty.io/netty/a""));

        temp = channel.readInbound();
        assertThat(temp, instanceOf(XmlElementStart.class));
        assertThat(((XmlElementStart) temp).name(), is(""id""));
        assertThat(((XmlElementStart) temp).prefix(), is(""nettya""));
        assertThat(((XmlElementStart) temp).namespace(), is(""http://netty.io/netty/a""));
        assertThat(((XmlElementStart) temp).attributes().size(), is(0));
        assertThat(((XmlElementStart) temp).namespaces().size(), is(0));

        temp = channel.readInbound();
        assertThat(temp, instanceOf(XmlEntityReference.class));
        assertThat(((XmlEntityReference) temp).name(), is(""plusmn""));
        assertThat(((XmlEntityReference) temp).text(), is(""""));

        temp = channel.readInbound();
        assertThat(temp, instanceOf(XmlCharacters.class));
        assertThat(((XmlCharacters) temp).data(), is(""1""));

        temp = channel.readInbound();
        assertThat(temp, instanceOf(XmlElementEnd.class));
        assertThat(((XmlElementEnd) temp).name(), is(""id""));
        assertThat(((XmlElementEnd) temp).prefix(), is(""nettya""));
        assertThat(((XmlElementEnd) temp).namespace(), is(""http://netty.io/netty/a""));

        temp = channel.readInbound();
        assertThat(temp, instanceOf(XmlCharacters.class));
        assertThat(((XmlCharacters) temp).data(), is(""\n""));

        temp = channel.readInbound();
        assertThat(temp, nullValue());

        write(XML2);

        temp = channel.readInbound();
        assertThat(temp, instanceOf(XmlElementStart.class));
        assertThat(((XmlElementStart) temp).name(), is(""name""));
        assertThat(((XmlElementStart) temp).prefix(), is(""""));
        assertThat(((XmlElementStart) temp).namespace(), is(""""));
        assertThat(((XmlElementStart) temp).attributes().size(), is(1));
        assertThat(((XmlElementStart) temp).attributes().get(0).name(), is(""type""));
        assertThat(((XmlElementStart) temp).attributes().get(0).value(), is(""given""));
        assertThat(((XmlElementStart) temp).attributes().get(0).prefix(), is(""""));
        assertThat(((XmlElementStart) temp).attributes().get(0).namespace(), is(""""));
        assertThat(((XmlElementStart) temp).namespaces().size(), is(0));

        temp = channel.readInbound();
        assertThat(temp, instanceOf(XmlCharacters.class));
        assertThat(((XmlCharacters) temp).data(), is(""Alba""));

        temp = channel.readInbound();
        assertThat(temp, instanceOf(XmlElementEnd.class));
        assertThat(((XmlElementEnd) temp).name(), is(""name""));
        assertThat(((XmlElementEnd) temp).prefix(), is(""""));
        assertThat(((XmlElementEnd) temp).namespace(), is(""""));

        temp = channel.readInbound();
        assertThat(temp, instanceOf(XmlCdata.class));
        assertThat(((XmlCdata) temp).data(), is("" <some data &gt;/> ""));

        temp = channel.readInbound();
        assertThat(temp, instanceOf(XmlCharacters.class));
        assertThat(((XmlCharacters) temp).data(), is(""   ""));

        temp = channel.readInbound();
        assertThat(temp, instanceOf(XmlComment.class));
        assertThat(((XmlComment) temp).data(), is("" namespaced ""));

        temp = channel.readInbound();
        assertThat(temp, instanceOf(XmlElementStart.class));
        assertThat(((XmlElementStart) temp).name(), is(""salary""));
        assertThat(((XmlElementStart) temp).prefix(), is(""nettyb""));
        assertThat(((XmlElementStart) temp).namespace(), is(""http://netty.io/netty/b""));
        assertThat(((XmlElementStart) temp).attributes().size(), is(1));
        assertThat(((XmlElementStart) temp).attributes().get(0).name(), is(""period""));
        assertThat(((XmlElementStart) temp).attributes().get(0).value(), is(""weekly""));
        assertThat(((XmlElementStart) temp).attributes().get(0).prefix(), is(""nettyb""));
        assertThat(((XmlElementStart) temp).attributes().get(0).namespace(), is(""http://netty.io/netty/b""));
        assertThat(((XmlElementStart) temp).namespaces().size(), is(1));
        assertThat(((XmlElementStart) temp).namespaces().get(0).prefix(), is(""nettyb""));
        assertThat(((XmlElementStart) temp).namespaces().get(0).uri(), is(""http://netty.io/netty/b""));

        temp = channel.readInbound();
        assertThat(temp, instanceOf(XmlCharacters.class));
        assertThat(((XmlCharacters) temp).data(), is(""100""));

        temp = channel.readInbound();
        assertThat(temp, instanceOf(XmlElementEnd.class));
        assertThat(((XmlElementEnd) temp).name(), is(""salary""));
        assertThat(((XmlElementEnd) temp).prefix(), is(""nettyb""));
        assertThat(((XmlElementEnd) temp).namespace(), is(""http://netty.io/netty/b""));
        assertThat(((XmlElementEnd) temp).namespaces().size(), is(1));
        assertThat(((XmlElementEnd) temp).namespaces().get(0).prefix(), is(""nettyb""));
        assertThat(((XmlElementEnd) temp).namespaces().get(0).uri(), is(""http://netty.io/netty/b""));

        temp = channel.readInbound();
        assertThat(temp, instanceOf(XmlElementStart.class));
        assertThat(((XmlElementStart) temp).name(), is(""last""));
        assertThat(((XmlElementStart) temp).prefix(), is(""""));
        assertThat(((XmlElementStart) temp).namespace(), is(""""));
        assertThat(((XmlElementStart) temp).attributes().size(), is(0));
        assertThat(((XmlElementStart) temp).namespaces().size(), is(0));

        temp = channel.readInbound();
        assertThat(temp, instanceOf(XmlElementEnd.class));
        assertThat(((XmlElementEnd) temp).name(), is(""last""));
        assertThat(((XmlElementEnd) temp).prefix(), is(""""));
        assertThat(((XmlElementEnd) temp).namespace(), is(""""));
        assertThat(((XmlElementEnd) temp).namespaces().size(), is(0));

        temp = channel.readInbound();
        assertThat(temp, instanceOf(XmlElementEnd.class));
        assertThat(((XmlElementEnd) temp).name(), is(""employee""));
        assertThat(((XmlElementEnd) temp).prefix(), is(""""));
        assertThat(((XmlElementEnd) temp).namespace(), is(""""));
        assertThat(((XmlElementEnd) temp).namespaces().size(), is(1));
        assertThat(((XmlElementEnd) temp).namespaces().get(0).prefix(), is(""nettya""));
        assertThat(((XmlElementEnd) temp).namespaces().get(0).uri(), is(""http://netty.io/netty/a""));

        temp = channel.readInbound();
        assertThat(temp, nullValue());
    }
"
"    @Test
    public void shouldDecodeXmlHeader() {
        Object temp;

        write(XML3);

        temp = channel.readInbound();
        assertThat(temp, instanceOf(XmlDocumentStart.class));
        assertThat(((XmlDocumentStart) temp).version(), is(""1.1""));
        assertThat(((XmlDocumentStart) temp).encoding(), is(""UTF-8""));
        assertThat(((XmlDocumentStart) temp).standalone(), is(true));
        assertThat(((XmlDocumentStart) temp).encodingScheme(), is(""UTF-8""));

        temp = channel.readInbound();
        assertThat(temp, instanceOf(XmlElementStart.class));
        assertThat(((XmlElementStart) temp).name(), is(""netty""));
        assertThat(((XmlElementStart) temp).prefix(), is(""""));
        assertThat(((XmlElementStart) temp).namespace(), is(""""));
        assertThat(((XmlElementStart) temp).attributes().size(), is(0));
        assertThat(((XmlElementStart) temp).namespaces().size(), is(0));

        temp = channel.readInbound();
        assertThat(temp, instanceOf(XmlElementEnd.class));
        assertThat(((XmlElementEnd) temp).name(), is(""netty""));
        assertThat(((XmlElementEnd) temp).prefix(), is(""""));
        assertThat(((XmlElementEnd) temp).namespace(), is(""""));
        assertThat(((XmlElementEnd) temp).namespaces().size(), is(0));

        temp = channel.readInbound();
        assertThat(temp, nullValue());
    }
"
"    @Test
    public void shouldDecodeWithoutHeader() {
        Object temp;

        write(XML4);

        temp = channel.readInbound();
        assertThat(temp, instanceOf(XmlDocumentStart.class));
        assertThat(((XmlDocumentStart) temp).version(), is(nullValue()));
        assertThat(((XmlDocumentStart) temp).encoding(), is(""UTF-8""));
        assertThat(((XmlDocumentStart) temp).standalone(), is(false));
        assertThat(((XmlDocumentStart) temp).encodingScheme(), is(nullValue()));

        temp = channel.readInbound();
        assertThat(temp, instanceOf(XmlElementStart.class));
        assertThat(((XmlElementStart) temp).name(), is(""netty""));
        assertThat(((XmlElementStart) temp).prefix(), is(""""));
        assertThat(((XmlElementStart) temp).namespace(), is(""""));
        assertThat(((XmlElementStart) temp).attributes().size(), is(0));
        assertThat(((XmlElementStart) temp).namespaces().size(), is(0));

        temp = channel.readInbound();
        assertThat(temp, instanceOf(XmlElementEnd.class));
        assertThat(((XmlElementEnd) temp).name(), is(""netty""));
        assertThat(((XmlElementEnd) temp).prefix(), is(""""));
        assertThat(((XmlElementEnd) temp).namespace(), is(""""));
        assertThat(((XmlElementEnd) temp).namespaces().size(), is(0));

        temp = channel.readInbound();
        assertThat(temp, nullValue());
    }
"
"    @Test
    public void getCommandFromCache() {
        assertSame(SmtpCommand.DATA, SmtpCommand.valueOf(""DATA""));
        assertSame(SmtpCommand.EHLO, SmtpCommand.valueOf(""EHLO""));
        assertNotSame(SmtpCommand.EHLO, SmtpCommand.valueOf(""ehlo""));
    }
"
"    @Test
    public void equalsIgnoreCase() {
        assertEquals(SmtpCommand.MAIL, SmtpCommand.valueOf(""mail""));
        assertEquals(SmtpCommand.valueOf(""test""), SmtpCommand.valueOf(""TEST""));
    }
"
"    @Test
    public void isContentExpected() {
        assertTrue(SmtpCommand.valueOf(""DATA"").isContentExpected());
        assertTrue(SmtpCommand.valueOf(""data"").isContentExpected());

        assertFalse(SmtpCommand.HELO.isContentExpected());
        assertFalse(SmtpCommand.HELP.isContentExpected());
        assertFalse(SmtpCommand.valueOf(""DATA2"").isContentExpected());
    }
"
"    @Test
    public void testEncodeEhlo() {
        testEncode(SmtpRequests.ehlo(""localhost""), ""EHLO localhost\r\n"");
    }
"
"    @Test
    public void testEncodeHelo() {
        testEncode(SmtpRequests.helo(""localhost""), ""HELO localhost\r\n"");
    }
"
"    @Test
    public void testEncodeMail() {
        testEncode(SmtpRequests.mail(""me@netty.io""), ""MAIL FROM:<me@netty.io>\r\n"");
    }
"
"    @Test
    public void testEncodeMailNullSender() {
        testEncode(SmtpRequests.mail(null), ""MAIL FROM:<>\r\n"");
    }
"
"    @Test
    public void testEncodeRcpt() {
        testEncode(SmtpRequests.rcpt(""me@netty.io""), ""RCPT TO:<me@netty.io>\r\n"");
    }
"
"    @Test
    public void testEncodeNoop() {
        testEncode(SmtpRequests.noop(), ""NOOP\r\n"");
    }
"
"    @Test
    public void testEncodeRset() {
        testEncode(SmtpRequests.rset(), ""RSET\r\n"");
    }
"
"    @Test
    public void testEncodeHelp() {
        testEncode(SmtpRequests.help(null), ""HELP\r\n"");
    }
"
"    @Test
    public void testEncodeHelpWithArg() {
        testEncode(SmtpRequests.help(""MAIL""), ""HELP MAIL\r\n"");
    }
"
"    @Test
    public void testEncodeData() {
        testEncode(SmtpRequests.data(), ""DATA\r\n"");
    }
"
"    @Test
    public void testEncodeDataAndContent() {
        EmbeddedChannel channel = new EmbeddedChannel(new SmtpRequestEncoder());
        assertTrue(channel.writeOutbound(SmtpRequests.data()));
        assertTrue(channel.writeOutbound(
                new DefaultSmtpContent(Unpooled.copiedBuffer(""Subject: Test\r\n\r\n"", CharsetUtil.US_ASCII))));
        assertTrue(channel.writeOutbound(
                new DefaultLastSmtpContent(Unpooled.copiedBuffer(""Test\r\n"", CharsetUtil.US_ASCII))));
        assertTrue(channel.finish());

        assertEquals(""DATA\r\nSubject: Test\r\n\r\nTest\r\n.\r\n"", getWrittenString(channel));
    }
"
"    @Test(expected = EncoderException.class)
    public void testThrowsIfContentExpected() {
        EmbeddedChannel channel = new EmbeddedChannel(new SmtpRequestEncoder());
        try {
            assertTrue(channel.writeOutbound(SmtpRequests.data()));
            channel.writeOutbound(SmtpRequests.noop());
        } finally {
            channel.finishAndReleaseAll();
        }
    }
"
"    @Test
    public void testRsetClearsContentExpectedFlag() {
        EmbeddedChannel channel = new EmbeddedChannel(new SmtpRequestEncoder());

        assertTrue(channel.writeOutbound(SmtpRequests.data()));
        assertTrue(channel.writeOutbound(SmtpRequests.rset()));
        assertTrue(channel.writeOutbound(SmtpRequests.noop()));
        assertTrue(channel.finish());

        assertEquals(""DATA\r\nRSET\r\nNOOP\r\n"", getWrittenString(channel));
    }
"
"    @Test
    public void testDecodeOneLineResponse() {
        EmbeddedChannel channel = newChannel();
        assertTrue(channel.writeInbound(newBuffer(""200 Ok\r\n"")));
        assertTrue(channel.finish());

        SmtpResponse response = channel.readInbound();
        assertEquals(200, response.code());
        List<CharSequence> sequences = response.details();
        assertEquals(1, sequences.size());

        assertEquals(""Ok"", sequences.get(0).toString());
        assertNull(channel.readInbound());
    }
"
"    @Test
    public void testDecodeOneLineResponseNoDetails() {
        EmbeddedChannel channel = newChannel();
        assertTrue(channel.writeInbound(newBuffer(""250 \r\n"")));
        assertTrue(channel.finish());

        SmtpResponse response = channel.readInbound();
        assertEquals(250, response.code());
        List<CharSequence> sequences = response.details();
        assertEquals(0, sequences.size());
    }
"
"    @Test
    public void testDecodeOneLineResponseChunked() {
        EmbeddedChannel channel = newChannel();
        assertFalse(channel.writeInbound(newBuffer(""200 Ok"")));
        assertTrue(channel.writeInbound(newBuffer(""\r\n"")));
        assertTrue(channel.finish());

        SmtpResponse response = channel.readInbound();
        assertEquals(200, response.code());
        List<CharSequence> sequences = response.details();
        assertEquals(1, sequences.size());

        assertEquals(""Ok"", sequences.get(0).toString());
        assertNull(channel.readInbound());
    }
"
"    @Test
    public void testDecodeTwoLineResponse() {
        EmbeddedChannel channel = newChannel();
        assertTrue(channel.writeInbound(newBuffer(""200-Hello\r\n200 Ok\r\n"")));
        assertTrue(channel.finish());

        SmtpResponse response = channel.readInbound();
        assertEquals(200, response.code());
        List<CharSequence> sequences = response.details();
        assertEquals(2, sequences.size());

        assertEquals(""Hello"", sequences.get(0).toString());
        assertEquals(""Ok"", sequences.get(1).toString());
        assertNull(channel.readInbound());
    }
"
"    @Test
    public void testDecodeTwoLineResponseChunked() {
        EmbeddedChannel channel = newChannel();
        assertFalse(channel.writeInbound(newBuffer(""200-"")));
        assertFalse(channel.writeInbound(newBuffer(""Hello\r\n2"")));
        assertFalse(channel.writeInbound(newBuffer(""00 Ok"")));
        assertTrue(channel.writeInbound(newBuffer(""\r\n"")));
        assertTrue(channel.finish());

        SmtpResponse response = channel.readInbound();
        assertEquals(200, response.code());
        List<CharSequence> sequences = response.details();
        assertEquals(2, sequences.size());

        assertEquals(""Hello"", sequences.get(0).toString());
        assertEquals(""Ok"", sequences.get(1).toString());
        assertNull(channel.readInbound());
    }
"
"    @Test(expected = DecoderException.class)
    public void testDecodeInvalidSeparator() {
        EmbeddedChannel channel = newChannel();
        assertTrue(channel.writeInbound(newBuffer(""200:Ok\r\n"")));
    }
"
"    @Test(expected = DecoderException.class)
    public void testDecodeInvalidCode() {
        EmbeddedChannel channel = newChannel();
        assertTrue(channel.writeInbound(newBuffer(""xyz Ok\r\n"")));
    }
"
"    @Test(expected = DecoderException.class)
    public void testDecodeInvalidLine() {
        EmbeddedChannel channel = newChannel();
        assertTrue(channel.writeInbound(newBuffer(""Ok\r\n"")));
    }
"
"    @Test
    public void testPooledAllocatorIsBufferCopyNeededForWrite() {
        testIsBufferCopyNeededForWrite(PooledByteBufAllocator.DEFAULT);
    }
"
"    @Test
    public void testUnPooledAllocatorIsBufferCopyNeededForWrite() {
        testIsBufferCopyNeededForWrite(UnpooledByteBufAllocator.DEFAULT);
    }
"
"    @Test
    public void testKeepAlive() throws Exception {
        assertFalse(socket.isKeepAlive());
        socket.setKeepAlive(true);
        assertTrue(socket.isKeepAlive());
    }
"
"    @Test
    public void testTcpNoDelay() throws Exception {
        assertFalse(socket.isTcpNoDelay());
        socket.setTcpNoDelay(true);
        assertTrue(socket.isTcpNoDelay());
    }
"
"    @Test
    public void testReceivedBufferSize() throws Exception {
        int size = socket.getReceiveBufferSize();
        int newSize = 65535;
        assertTrue(size > 0);
        socket.setReceiveBufferSize(newSize);
        // Linux usually set it to double what is specified
        assertTrue(newSize <= socket.getReceiveBufferSize());
    }
"
"    @Test
    public void testSendBufferSize() throws Exception {
        int size = socket.getSendBufferSize();
        int newSize = 65535;
        assertTrue(size > 0);
        socket.setSendBufferSize(newSize);
        // Linux usually set it to double what is specified
        assertTrue(newSize <= socket.getSendBufferSize());
    }
"
"    @Test
    public void testSoLinger() throws Exception {
        assertEquals(-1, socket.getSoLinger());
        socket.setSoLinger(10);
        assertEquals(10, socket.getSoLinger());
    }
"
"    @Test
    public void testDoubleCloseDoesNotThrow() throws IOException {
        Socket socket = Socket.newSocketStream();
        socket.close();
        socket.close();
    }
"
"    @Test
    public void testTrafficClass() throws IOException {
        // IPTOS_THROUGHPUT
        final int value = 0x08;
        socket.setTrafficClass(value);
        assertEquals(value, socket.getTrafficClass());
    }
"
"    @Test(timeout = 10000)
    public void clientCloseWithoutServerReadIsDetectedNoExtraReadRequested() throws InterruptedException {
        clientCloseWithoutServerReadIsDetected0(false);
    }
"
"    @Test(timeout = 10000)
    public void clientCloseWithoutServerReadIsDetectedExtraReadRequested() throws InterruptedException {
        clientCloseWithoutServerReadIsDetected0(true);
    }
"
"    @Test(timeout = 10000)
    public void serverCloseWithoutClientReadIsDetectedNoExtraReadRequested() throws InterruptedException {
        serverCloseWithoutClientReadIsDetected0(false);
    }
"
"    @Test(timeout = 10000)
    public void serverCloseWithoutClientReadIsDetectedExtraReadRequested() throws InterruptedException {
        serverCloseWithoutClientReadIsDetected0(true);
    }
"
"    @Test(expected = UnsupportedOperationException.class)
    public void testInternalNioBufferAfterRelease() {
        super.testInternalNioBufferAfterRelease();
    }
"
"    @Test
    public void testForward() {
        final ByteBuf buf =
                Unpooled.copiedBuffer(""abc\r\n\ndef\r\rghi\n\njkl\0\0mno  \t\tx"", CharsetUtil.ISO_8859_1);
        final int length = buf.readableBytes();

        assertEquals(3,  buf.forEachByte(0,  length, ByteProcessor.FIND_CRLF));
        assertEquals(6,  buf.forEachByte(3,  length - 3, ByteProcessor.FIND_NON_CRLF));
        assertEquals(9,  buf.forEachByte(6,  length - 6, ByteProcessor.FIND_CR));
        assertEquals(11, buf.forEachByte(9,  length - 9, ByteProcessor.FIND_NON_CR));
        assertEquals(14, buf.forEachByte(11, length - 11, ByteProcessor.FIND_LF));
        assertEquals(16, buf.forEachByte(14, length - 14, ByteProcessor.FIND_NON_LF));
        assertEquals(19, buf.forEachByte(16, length - 16, ByteProcessor.FIND_NUL));
        assertEquals(21, buf.forEachByte(19, length - 19, ByteProcessor.FIND_NON_NUL));
        assertEquals(24, buf.forEachByte(19, length - 19, ByteProcessor.FIND_ASCII_SPACE));
        assertEquals(24, buf.forEachByte(21, length - 21, ByteProcessor.FIND_LINEAR_WHITESPACE));
        assertEquals(28, buf.forEachByte(24, length - 24, ByteProcessor.FIND_NON_LINEAR_WHITESPACE));
        assertEquals(-1, buf.forEachByte(28, length - 28, ByteProcessor.FIND_LINEAR_WHITESPACE));

        buf.release();
    }
"
"    @Test
    public void testBackward() {
        final ByteBuf buf =
                Unpooled.copiedBuffer(""abc\r\n\ndef\r\rghi\n\njkl\0\0mno  \t\tx"", CharsetUtil.ISO_8859_1);
        final int length = buf.readableBytes();

        assertEquals(27, buf.forEachByteDesc(0, length, ByteProcessor.FIND_LINEAR_WHITESPACE));
        assertEquals(25, buf.forEachByteDesc(0, length, ByteProcessor.FIND_ASCII_SPACE));
        assertEquals(23, buf.forEachByteDesc(0, 28, ByteProcessor.FIND_NON_LINEAR_WHITESPACE));
        assertEquals(20, buf.forEachByteDesc(0, 24, ByteProcessor.FIND_NUL));
        assertEquals(18, buf.forEachByteDesc(0, 21, ByteProcessor.FIND_NON_NUL));
        assertEquals(15, buf.forEachByteDesc(0, 19, ByteProcessor.FIND_LF));
        assertEquals(13, buf.forEachByteDesc(0, 16, ByteProcessor.FIND_NON_LF));
        assertEquals(10, buf.forEachByteDesc(0, 14, ByteProcessor.FIND_CR));
        assertEquals(8,  buf.forEachByteDesc(0, 11, ByteProcessor.FIND_NON_CR));
        assertEquals(5,  buf.forEachByteDesc(0, 9, ByteProcessor.FIND_CRLF));
        assertEquals(2,  buf.forEachByteDesc(0, 6, ByteProcessor.FIND_NON_CRLF));
        assertEquals(-1, buf.forEachByteDesc(0, 3, ByteProcessor.FIND_CRLF));

        buf.release();
    }
"
"    @Test(expected = NullPointerException.class)
    public void shouldNotAllowNullInConstructor1() {
        new UnpooledHeapByteBuf(null, new byte[1], 0);
    }
"
"    @Test(expected = NullPointerException.class)
    public void shouldNotAllowNullInConstructor2() {
        new UnpooledHeapByteBuf(UnpooledByteBufAllocator.DEFAULT, null, 0);
    }
"
"    @Test
    public void testIsWritable() {
        EmptyByteBuf empty = new EmptyByteBuf(UnpooledByteBufAllocator.DEFAULT);
        assertFalse(empty.isWritable());
        assertFalse(empty.isWritable(1));
    }
"
"    @Test
    public void testWriteEmptyByteBuf() {
        EmptyByteBuf empty = new EmptyByteBuf(UnpooledByteBufAllocator.DEFAULT);
        empty.writeBytes(Unpooled.EMPTY_BUFFER); // Ok
        ByteBuf nonEmpty = UnpooledByteBufAllocator.DEFAULT.buffer().writeBoolean(false);
        try {
            empty.writeBytes(nonEmpty);
            fail();
        } catch (IndexOutOfBoundsException ignored) {
            // Ignore.
        } finally {
            nonEmpty.release();
        }
    }
"
"    @Test
    public void testIsReadable() {
        EmptyByteBuf empty = new EmptyByteBuf(UnpooledByteBufAllocator.DEFAULT);
        assertFalse(empty.isReadable());
        assertFalse(empty.isReadable(1));
    }
"
"    @Test
    public void testArray() {
        EmptyByteBuf empty = new EmptyByteBuf(UnpooledByteBufAllocator.DEFAULT);
        assertThat(empty.hasArray(), is(true));
        assertThat(empty.array().length, is(0));
        assertThat(empty.arrayOffset(), is(0));
    }
"
"    @Test
    public void testNioBuffer() {
        EmptyByteBuf empty = new EmptyByteBuf(UnpooledByteBufAllocator.DEFAULT);
        assertThat(empty.nioBufferCount(), is(1));
        assertThat(empty.nioBuffer().position(), is(0));
        assertThat(empty.nioBuffer().limit(), is(0));
        assertThat(empty.nioBuffer(), is(sameInstance(empty.nioBuffer())));
        assertThat(empty.nioBuffer(), is(sameInstance(empty.internalNioBuffer(empty.readerIndex(), 0))));
    }
"
"    @Test
    public void testMemoryAddress() {
        EmptyByteBuf empty = new EmptyByteBuf(UnpooledByteBufAllocator.DEFAULT);
        if (empty.hasMemoryAddress()) {
            assertThat(empty.memoryAddress(), is(not(0L)));
        } else {
            try {
                empty.memoryAddress();
                fail();
            } catch (UnsupportedOperationException ignored) {
                // Ignore.
            }
        }
    }
"
"    @Test
    public void consistentEqualsAndHashCodeWithAbstractBytebuf() {
        ByteBuf empty = new EmptyByteBuf(UnpooledByteBufAllocator.DEFAULT);
        ByteBuf emptyAbstract = new UnpooledHeapByteBuf(UnpooledByteBufAllocator.DEFAULT, 0, 0);
        assertEquals(emptyAbstract, empty);
        assertEquals(emptyAbstract.hashCode(), empty.hashCode());
        assertEquals(EmptyByteBuf.EMPTY_BYTE_BUF_HASH_CODE, empty.hashCode());
        assertTrue(emptyAbstract.release());
        assertFalse(empty.release());
    }
"
"    @Test
    public void decodeRandomHexBytesWithEvenLength() {
        decodeRandomHexBytes(256);
    }
"
"    @Test
    public void decodeRandomHexBytesWithOddLength() {
        decodeRandomHexBytes(257);
    }
"
"    @Test(expected = IllegalArgumentException.class)
    public void decodeHexDumpWithOddLength() {
        ByteBufUtil.decodeHexDump(""abc"");
    }
"
"    @Test(expected = IllegalArgumentException.class)
    public void decodeHexDumpWithInvalidChar() {
        ByteBufUtil.decodeHexDump(""fg"");
    }
"
"    @Test
    public void equalsBufferSubsections() {
        byte[] b1 = new byte[128];
        byte[] b2 = new byte[256];
        Random rand = new Random();
        rand.nextBytes(b1);
        rand.nextBytes(b2);
        final int iB1 = b1.length / 2;
        final int iB2 = iB1 + b1.length;
        final int length = b1.length - iB1;
        System.arraycopy(b1, iB1, b2, iB2, length);
        assertTrue(ByteBufUtil.equals(Unpooled.wrappedBuffer(b1), iB1, Unpooled.wrappedBuffer(b2), iB2, length));
    }
"
"    @Test
    public void notEqualsBufferSubsections() {
        byte[] b1 = new byte[50];
        byte[] b2 = new byte[256];
        Random rand = new Random();
        rand.nextBytes(b1);
        rand.nextBytes(b2);
        final int iB1 = b1.length / 2;
        final int iB2 = iB1 + b1.length;
        final int length = b1.length - iB1;
        System.arraycopy(b1, iB1, b2, iB2, length);
        // Randomly pick an index in the range that will be compared and make the value at that index differ between
        // the 2 arrays.
        int diffIndex = random(rand, iB1, iB1 + length - 1);
        ++b1[diffIndex];
        assertFalse(ByteBufUtil.equals(Unpooled.wrappedBuffer(b1), iB1, Unpooled.wrappedBuffer(b2), iB2, length));
    }
"
"    @Test
    public void notEqualsBufferOverflow() {
        byte[] b1 = new byte[8];
        byte[] b2 = new byte[16];
        Random rand = new Random();
        rand.nextBytes(b1);
        rand.nextBytes(b2);
        final int iB1 = b1.length / 2;
        final int iB2 = iB1 + b1.length;
        final int length = b1.length - iB1;
        System.arraycopy(b1, iB1, b2, iB2, length - 1);
        assertFalse(ByteBufUtil.equals(Unpooled.wrappedBuffer(b1), iB1, Unpooled.wrappedBuffer(b2), iB2,
                Math.max(b1.length, b2.length) * 2));
    }
"
"    @Test (expected = IllegalArgumentException.class)
    public void notEqualsBufferUnderflow() {
        byte[] b1 = new byte[8];
        byte[] b2 = new byte[16];
        Random rand = new Random();
        rand.nextBytes(b1);
        rand.nextBytes(b2);
        final int iB1 = b1.length / 2;
        final int iB2 = iB1 + b1.length;
        final int length = b1.length - iB1;
        System.arraycopy(b1, iB1, b2, iB2, length - 1);
        assertFalse(ByteBufUtil.equals(Unpooled.wrappedBuffer(b1), iB1, Unpooled.wrappedBuffer(b2), iB2,
                -1));
    }
"
"    @Test
    public void writeShortBE() {
        int expected = 0x1234;

        ByteBuf buf = Unpooled.buffer(2).order(ByteOrder.BIG_ENDIAN);
        ByteBufUtil.writeShortBE(buf, expected);
        assertEquals(expected, buf.readShort());
        buf.resetReaderIndex();
        assertEquals(ByteBufUtil.swapShort((short) expected), buf.readShortLE());
        buf.release();

        buf = Unpooled.buffer(2).order(ByteOrder.LITTLE_ENDIAN);
        ByteBufUtil.writeShortBE(buf, expected);
        assertEquals((short) expected, buf.readShortLE());
        buf.resetReaderIndex();
        assertEquals(ByteBufUtil.swapShort((short) expected), buf.readShort());
        buf.release();
    }
"
"    @Test
    public void setShortBE() {
        int shortValue = 0x1234;

        ByteBuf buf = Unpooled.wrappedBuffer(new byte[2]).order(ByteOrder.BIG_ENDIAN);
        ByteBufUtil.setShortBE(buf, 0, shortValue);
        assertEquals(shortValue, buf.readShort());
        buf.resetReaderIndex();
        assertEquals(ByteBufUtil.swapShort((short) shortValue), buf.readShortLE());
        buf.release();

        buf = Unpooled.wrappedBuffer(new byte[2]).order(ByteOrder.LITTLE_ENDIAN);
        ByteBufUtil.setShortBE(buf, 0, shortValue);
        assertEquals((short) shortValue, buf.readShortLE());
        buf.resetReaderIndex();
        assertEquals(ByteBufUtil.swapShort((short) shortValue), buf.readShort());
        buf.release();
    }
"
"    @Test
    public void writeMediumBE() {
        int mediumValue = 0x123456;

        ByteBuf buf = Unpooled.buffer(4).order(ByteOrder.BIG_ENDIAN);
        ByteBufUtil.writeMediumBE(buf, mediumValue);
        assertEquals(mediumValue, buf.readMedium());
        buf.resetReaderIndex();
        assertEquals(ByteBufUtil.swapMedium(mediumValue), buf.readMediumLE());
        buf.release();

        buf = Unpooled.buffer(4).order(ByteOrder.LITTLE_ENDIAN);
        ByteBufUtil.writeMediumBE(buf, mediumValue);
        assertEquals(mediumValue, buf.readMediumLE());
        buf.resetReaderIndex();
        assertEquals(ByteBufUtil.swapMedium(mediumValue), buf.readMedium());
        buf.release();
    }
"
"    @Test
    public void testWriteUsAscii() {
        String usAscii = ""NettyRocks"";
        ByteBuf buf = Unpooled.buffer(16);
        buf.writeBytes(usAscii.getBytes(CharsetUtil.US_ASCII));
        ByteBuf buf2 = Unpooled.buffer(16);
        ByteBufUtil.writeAscii(buf2, usAscii);

        assertEquals(buf, buf2);

        buf.release();
        buf2.release();
    }
"
"    @Test
    public void testWriteUsAsciiSwapped() {
        String usAscii = ""NettyRocks"";
        ByteBuf buf = Unpooled.buffer(16);
        buf.writeBytes(usAscii.getBytes(CharsetUtil.US_ASCII));
        SwappedByteBuf buf2 = new SwappedByteBuf(Unpooled.buffer(16));
        ByteBufUtil.writeAscii(buf2, usAscii);

        assertEquals(buf, buf2);

        buf.release();
        buf2.release();
    }
"
"    @Test
    public void testWriteUsAsciiWrapped() {
        String usAscii = ""NettyRocks"";
        ByteBuf buf = unreleasableBuffer(Unpooled.buffer(16));
        assertWrapped(buf);
        buf.writeBytes(usAscii.getBytes(CharsetUtil.US_ASCII));
        ByteBuf buf2 = unreleasableBuffer(Unpooled.buffer(16));
        assertWrapped(buf2);
        ByteBufUtil.writeAscii(buf2, usAscii);

        assertEquals(buf, buf2);

        buf.unwrap().release();
        buf2.unwrap().release();
    }
"
"    @Test
    public void testWriteUsAsciiComposite() {
        String usAscii = ""NettyRocks"";
        ByteBuf buf = Unpooled.buffer(16);
        buf.writeBytes(usAscii.getBytes(CharsetUtil.US_ASCII));
        ByteBuf buf2 = Unpooled.compositeBuffer().addComponent(
                Unpooled.buffer(8)).addComponent(Unpooled.buffer(24));
        // write some byte so we start writing with an offset.
        buf2.writeByte(1);
        ByteBufUtil.writeAscii(buf2, usAscii);

        // Skip the previously written byte.
        assertEquals(buf, buf2.skipBytes(1));

        buf.release();
        buf2.release();
    }
"
"    @Test
    public void testWriteUsAsciiCompositeWrapped() {
        String usAscii = ""NettyRocks"";
        ByteBuf buf = Unpooled.buffer(16);
        buf.writeBytes(usAscii.getBytes(CharsetUtil.US_ASCII));
        ByteBuf buf2 = new WrappedCompositeByteBuf(Unpooled.compositeBuffer().addComponent(
                Unpooled.buffer(8)).addComponent(Unpooled.buffer(24)));
        // write some byte so we start writing with an offset.
        buf2.writeByte(1);
        ByteBufUtil.writeAscii(buf2, usAscii);

        // Skip the previously written byte.
        assertEquals(buf, buf2.skipBytes(1));

        buf.release();
        buf2.release();
    }
"
"    @Test
    public void testWriteUtf8() {
        String usAscii = ""Some UTF-8 like Ã¤ÃâÅÅ"";
        ByteBuf buf = Unpooled.buffer(16);
        buf.writeBytes(usAscii.getBytes(CharsetUtil.UTF_8));
        ByteBuf buf2 = Unpooled.buffer(16);
        ByteBufUtil.writeUtf8(buf2, usAscii);

        assertEquals(buf, buf2);

        buf.release();
        buf2.release();
    }
"
"    @Test
    public void testWriteUtf8Composite() {
        String utf8 = ""Some UTF-8 like Ã¤ÃâÅÅ"";
        ByteBuf buf = Unpooled.buffer(16);
        buf.writeBytes(utf8.getBytes(CharsetUtil.UTF_8));
        ByteBuf buf2 = Unpooled.compositeBuffer().addComponent(
                Unpooled.buffer(8)).addComponent(Unpooled.buffer(24));
        // write some byte so we start writing with an offset.
        buf2.writeByte(1);
        ByteBufUtil.writeUtf8(buf2, utf8);

        // Skip the previously written byte.
        assertEquals(buf, buf2.skipBytes(1));

        buf.release();
        buf2.release();
    }
"
"    @Test
    public void testWriteUtf8CompositeWrapped() {
        String utf8 = ""Some UTF-8 like Ã¤ÃâÅÅ"";
        ByteBuf buf = Unpooled.buffer(16);
        buf.writeBytes(utf8.getBytes(CharsetUtil.UTF_8));
        ByteBuf buf2 = new WrappedCompositeByteBuf(Unpooled.compositeBuffer().addComponent(
                Unpooled.buffer(8)).addComponent(Unpooled.buffer(24)));
        // write some byte so we start writing with an offset.
        buf2.writeByte(1);
        ByteBufUtil.writeUtf8(buf2, utf8);

        // Skip the previously written byte.
        assertEquals(buf, buf2.skipBytes(1));

        buf.release();
        buf2.release();
    }
"
"    @Test
    public void testWriteUtf8Surrogates() {
        // leading surrogate + trailing surrogate
        String surrogateString = new StringBuilder(2)
                                .append('a')
                                .append('\uD800')
                                .append('\uDC00')
                                .append('b')
                                .toString();
        ByteBuf buf = Unpooled.buffer(16);
        buf.writeBytes(surrogateString.getBytes(CharsetUtil.UTF_8));
        ByteBuf buf2 = Unpooled.buffer(16);
        ByteBufUtil.writeUtf8(buf2, surrogateString);

        assertEquals(buf, buf2);
        assertEquals(buf.readableBytes(), ByteBufUtil.utf8Bytes(surrogateString));

        buf.release();
        buf2.release();
    }
"
"    @Test
    public void testWriteUtf8InvalidOnlyTrailingSurrogate() {
        String surrogateString = new StringBuilder(2)
                                .append('a')
                                .append('\uDC00')
                                .append('b')
                                .toString();
        ByteBuf buf = Unpooled.buffer(16);
        buf.writeBytes(surrogateString.getBytes(CharsetUtil.UTF_8));
        ByteBuf buf2 = Unpooled.buffer(16);
        ByteBufUtil.writeUtf8(buf2, surrogateString);

        assertEquals(buf, buf2);
        assertEquals(buf.readableBytes(), ByteBufUtil.utf8Bytes(surrogateString));

        buf.release();
        buf2.release();
    }
"
"    @Test
    public void testWriteUtf8InvalidOnlyLeadingSurrogate() {
        String surrogateString = new StringBuilder(2)
                                .append('a')
                                .append('\uD800')
                                .append('b')
                                .toString();
        ByteBuf buf = Unpooled.buffer(16);
        buf.writeBytes(surrogateString.getBytes(CharsetUtil.UTF_8));
        ByteBuf buf2 = Unpooled.buffer(16);
        ByteBufUtil.writeUtf8(buf2, surrogateString);

        assertEquals(buf, buf2);
        assertEquals(buf.readableBytes(), ByteBufUtil.utf8Bytes(surrogateString));

        buf.release();
        buf2.release();
    }
"
"    @Test
    public void testWriteUtf8InvalidSurrogatesSwitched() {
        String surrogateString = new StringBuilder(2)
                                .append('a')
                                .append('\uDC00')
                                .append('\uD800')
                                .append('b')
                                .toString();
        ByteBuf buf = Unpooled.buffer(16);
        buf.writeBytes(surrogateString.getBytes(CharsetUtil.UTF_8));
        ByteBuf buf2 = Unpooled.buffer(16);
        ByteBufUtil.writeUtf8(buf2, surrogateString);

        assertEquals(buf, buf2);
        assertEquals(buf.readableBytes(), ByteBufUtil.utf8Bytes(surrogateString));

        buf.release();
        buf2.release();
    }
"
"    @Test
    public void testWriteUtf8InvalidTwoLeadingSurrogates() {
        String surrogateString = new StringBuilder(2)
                                .append('a')
                                .append('\uD800')
                                .append('\uD800')
                                .append('b')
                                .toString();
        ByteBuf buf = Unpooled.buffer(16);
        buf.writeBytes(surrogateString.getBytes(CharsetUtil.UTF_8));
        ByteBuf buf2 = Unpooled.buffer(16);
        ByteBufUtil.writeUtf8(buf2, surrogateString);

        assertEquals(buf, buf2);
        assertEquals(buf.readableBytes(), ByteBufUtil.utf8Bytes(surrogateString));
        buf.release();
        buf2.release();
    }
"
"    @Test
    public void testWriteUtf8InvalidTwoTrailingSurrogates() {
        String surrogateString = new StringBuilder(2)
                                .append('a')
                                .append('\uDC00')
                                .append('\uDC00')
                                .append('b')
                                .toString();
        ByteBuf buf = Unpooled.buffer(16);
        buf.writeBytes(surrogateString.getBytes(CharsetUtil.UTF_8));
        ByteBuf buf2 = Unpooled.buffer(16);
        ByteBufUtil.writeUtf8(buf2, surrogateString);

        assertEquals(buf, buf2);
        assertEquals(buf.readableBytes(), ByteBufUtil.utf8Bytes(surrogateString));

        buf.release();
        buf2.release();
    }
"
"    @Test
    public void testWriteUtf8InvalidEndOnLeadingSurrogate() {
        String surrogateString = new StringBuilder(2)
                                .append('\uD800')
                                .toString();
        ByteBuf buf = Unpooled.buffer(16);
        buf.writeBytes(surrogateString.getBytes(CharsetUtil.UTF_8));
        ByteBuf buf2 = Unpooled.buffer(16);
        ByteBufUtil.writeUtf8(buf2, surrogateString);

        assertEquals(buf, buf2);
        assertEquals(buf.readableBytes(), ByteBufUtil.utf8Bytes(surrogateString));

        buf.release();
        buf2.release();
    }
"
"    @Test
    public void testWriteUtf8InvalidEndOnTrailingSurrogate() {
        String surrogateString = new StringBuilder(2)
                                .append('\uDC00')
                                .toString();
        ByteBuf buf = Unpooled.buffer(16);
        buf.writeBytes(surrogateString.getBytes(CharsetUtil.UTF_8));
        ByteBuf buf2 = Unpooled.buffer(16);
        ByteBufUtil.writeUtf8(buf2, surrogateString);

        assertEquals(buf, buf2);
        assertEquals(buf.readableBytes(), ByteBufUtil.utf8Bytes(surrogateString));

        buf.release();
        buf2.release();
    }
"
"    @Test
    public void testWriteUsAsciiString() {
        AsciiString usAscii = new AsciiString(""NettyRocks"");
        ByteBuf buf = Unpooled.buffer(16);
        buf.writeBytes(usAscii.toString().getBytes(CharsetUtil.US_ASCII));
        ByteBuf buf2 = Unpooled.buffer(16);
        ByteBufUtil.writeAscii(buf2, usAscii);

        assertEquals(buf, buf2);

        buf.release();
        buf2.release();
    }
"
"    @Test
    public void testWriteUtf8Wrapped() {
        String usAscii = ""Some UTF-8 like Ã¤ÃâÅÅ"";
        ByteBuf buf = unreleasableBuffer(Unpooled.buffer(16));
        assertWrapped(buf);
        buf.writeBytes(usAscii.getBytes(CharsetUtil.UTF_8));
        ByteBuf buf2 = unreleasableBuffer(Unpooled.buffer(16));
        assertWrapped(buf2);
        ByteBufUtil.writeUtf8(buf2, usAscii);

        assertEquals(buf, buf2);

        buf.release();
        buf2.release();
    }
"
"    @Test
    public void testDecodeUsAscii() {
        testDecodeString(""This is a test"", CharsetUtil.US_ASCII);
    }
"
"    @Test
    public void testDecodeUtf8() {
        testDecodeString(""Some UTF-8 like Ã¤ÃâÅÅ"", CharsetUtil.UTF_8);
    }
"
"    @Test
    public void testToStringDoesNotThrowIndexOutOfBounds() {
        CompositeByteBuf buffer = Unpooled.compositeBuffer();
        try {
            byte[] bytes = ""1234"".getBytes(CharsetUtil.UTF_8);
            buffer.addComponent(Unpooled.buffer(bytes.length).writeBytes(bytes));
            buffer.addComponent(Unpooled.buffer(bytes.length).writeBytes(bytes));
            assertEquals(""1234"", buffer.toString(bytes.length, bytes.length, CharsetUtil.UTF_8));
        } finally {
            buffer.release();
        }
    }
"
"    @Test
    public void testIsTextWithUtf8() {
        byte[][] validUtf8Bytes = {
                ""netty"".getBytes(CharsetUtil.UTF_8),
                {(byte) 0x24},
                {(byte) 0xC2, (byte) 0xA2},
                {(byte) 0xE2, (byte) 0x82, (byte) 0xAC},
                {(byte) 0xF0, (byte) 0x90, (byte) 0x8D, (byte) 0x88},
                {(byte) 0x24,
                        (byte) 0xC2, (byte) 0xA2,
                        (byte) 0xE2, (byte) 0x82, (byte) 0xAC,
                        (byte) 0xF0, (byte) 0x90, (byte) 0x8D, (byte) 0x88} // multiple characters
        };
        for (byte[] bytes : validUtf8Bytes) {
            assertIsText(bytes, true, CharsetUtil.UTF_8);
        }
        byte[][] invalidUtf8Bytes = {
                {(byte) 0x80},
                {(byte) 0xF0, (byte) 0x82, (byte) 0x82, (byte) 0xAC}, // Overlong encodings
                {(byte) 0xC2},                                        // not enough bytes
                {(byte) 0xE2, (byte) 0x82},                           // not enough bytes
                {(byte) 0xF0, (byte) 0x90, (byte) 0x8D},              // not enough bytes
                {(byte) 0xC2, (byte) 0xC0},                           // not correct bytes
                {(byte) 0xE2, (byte) 0x82, (byte) 0xC0},              // not correct bytes
                {(byte) 0xF0, (byte) 0x90, (byte) 0x8D, (byte) 0xC0}, // not correct bytes
                {(byte) 0xC1, (byte) 0x80},                           // out of lower bound
                {(byte) 0xE0, (byte) 0x80, (byte) 0x80},              // out of lower bound
                {(byte) 0xED, (byte) 0xAF, (byte) 0x80}               // out of upper bound
        };
        for (byte[] bytes : invalidUtf8Bytes) {
            assertIsText(bytes, false, CharsetUtil.UTF_8);
        }
    }
"
"    @Test
    public void testIsTextWithoutOptimization() {
        byte[] validBytes = {(byte) 0x01, (byte) 0xD8, (byte) 0x37, (byte) 0xDC};
        byte[] invalidBytes = {(byte) 0x01, (byte) 0xD8};

        assertIsText(validBytes, true, CharsetUtil.UTF_16LE);
        assertIsText(invalidBytes, false, CharsetUtil.UTF_16LE);
    }
"
"    @Test
    public void testIsTextWithAscii() {
        byte[] validBytes = {(byte) 0x00, (byte) 0x01, (byte) 0x37, (byte) 0x7F};
        byte[] invalidBytes = {(byte) 0x80, (byte) 0xFF};

        assertIsText(validBytes, true, CharsetUtil.US_ASCII);
        assertIsText(invalidBytes, false, CharsetUtil.US_ASCII);
    }
"
"    @Test
    public void testIsTextWithInvalidIndexAndLength() {
        ByteBuf buffer = Unpooled.buffer();
        try {
            buffer.writeBytes(new byte[4]);
            int[][] validIndexLengthPairs = {
                    {4, 0},
                    {0, 4},
                    {1, 3},
            };
            for (int[] pair : validIndexLengthPairs) {
                assertTrue(ByteBufUtil.isText(buffer, pair[0], pair[1], CharsetUtil.US_ASCII));
            }
            int[][] invalidIndexLengthPairs = {
                    {4, 1},
                    {-1, 2},
                    {3, -1},
                    {3, -2},
                    {5, 0},
                    {1, 5},
            };
            for (int[] pair : invalidIndexLengthPairs) {
                try {
                    ByteBufUtil.isText(buffer, pair[0], pair[1], CharsetUtil.US_ASCII);
                    fail(""Expected IndexOutOfBoundsException"");
                } catch (IndexOutOfBoundsException e) {
                    // expected
                }
            }
        } finally {
            buffer.release();
        }
    }
"
"    @Test
    public void testUtf8Bytes() {
        final String s = ""Some UTF-8 like Ã¤ÃâÅÅ"";
        checkUtf8Bytes(s);
    }
"
"    @Test
    public void testUtf8BytesWithSurrogates() {
        final String s = ""a\uD800\uDC00b"";
        checkUtf8Bytes(s);
    }
"
"    @Test
    public void testUtf8BytesWithNonSurrogates3Bytes() {
        final String s = ""a\uE000b"";
        checkUtf8Bytes(s);
    }
"
"    @Test
    public void testUtf8BytesWithNonSurrogatesNonAscii() {
        final char nonAscii = (char) 0x81;
        final String s = ""a"" + nonAscii + ""b"";
        checkUtf8Bytes(s);
    }
"
"    @Test
    public void testIsTextMultiThreaded() throws Throwable {
        final ByteBuf buffer = Unpooled.copiedBuffer(""Hello, World!"", CharsetUtil.ISO_8859_1);

        try {
            final AtomicInteger counter = new AtomicInteger(60000);
            final AtomicReference<Throwable> errorRef = new AtomicReference<Throwable>();
            List<Thread> threads = new ArrayList<Thread>();
            for (int i = 0; i < 10; i++) {
                Thread thread = new Thread(new Runnable() {
                    @Override
                    public void run() {
                        try {
                            while (errorRef.get() == null && counter.decrementAndGet() > 0) {
                                assertTrue(ByteBufUtil.isText(buffer, CharsetUtil.ISO_8859_1));
                            }
                        } catch (Throwable cause) {
                            errorRef.compareAndSet(null, cause);
                        }
                    }
"
"    @Test(expected = IllegalReferenceCountException.class)
    public void testRetainOverflow() {
        AbstractReferenceCountedByteBuf referenceCounted = newReferenceCounted();
        referenceCounted.setRefCnt(Integer.MAX_VALUE);
        assertEquals(Integer.MAX_VALUE, referenceCounted.refCnt());
        referenceCounted.retain();
    }
"
"    @Test(expected = IllegalReferenceCountException.class)
    public void testRetainOverflow2() {
        AbstractReferenceCountedByteBuf referenceCounted = newReferenceCounted();
        assertEquals(1, referenceCounted.refCnt());
        referenceCounted.retain(Integer.MAX_VALUE);
    }
"
"    @Test(expected = IllegalReferenceCountException.class)
    public void testReleaseOverflow() {
        AbstractReferenceCountedByteBuf referenceCounted = newReferenceCounted();
        referenceCounted.setRefCnt(0);
        assertEquals(0, referenceCounted.refCnt());
        referenceCounted.release(Integer.MAX_VALUE);
    }
"
"    @Test
    public void testReleaseErrorMessage() {
        AbstractReferenceCountedByteBuf referenceCounted = newReferenceCounted();
        assertTrue(referenceCounted.release());
        try {
            referenceCounted.release(1);
            fail(""IllegalReferenceCountException didn't occur"");
        } catch (IllegalReferenceCountException e) {
            assertEquals(""refCnt: 0, decrement: 1"", e.getMessage());
        }
    }
"
"    @Test(expected = IllegalReferenceCountException.class)
    public void testRetainResurrect() {
        AbstractReferenceCountedByteBuf referenceCounted = newReferenceCounted();
        assertTrue(referenceCounted.release());
        assertEquals(0, referenceCounted.refCnt());
        referenceCounted.retain();
    }
"
"    @Test(expected = IllegalReferenceCountException.class)
    public void testRetainResurrect2() {
        AbstractReferenceCountedByteBuf referenceCounted = newReferenceCounted();
        assertTrue(referenceCounted.release());
        assertEquals(0, referenceCounted.refCnt());
        referenceCounted.retain(2);
    }
"
"    @Test
    public void testCopyDirect() {
        testCopy(true);
    }
"
"    @Test
    public void testAuthenticatedWebsocket() throws Exception {
        ProgramaticClientEndpoint endpoint = new ProgramaticClientEndpoint();
        ClientEndpointConfig clientEndpointConfig = ClientEndpointConfig.Builder.create().configurator(new ClientConfigurator(){
            @Override
            public void beforeRequest(Map<String, List<String>> headers) {
                headers.put(AUTHORIZATION.toString(), Collections.singletonList(BASIC + "" "" + FlexBase64.encodeString(""user1:password1"".getBytes(), false)));
            }
"
"    @Test
    public void testWrappedRequest() throws Exception {
        ProgramaticClientEndpoint endpoint = new ProgramaticClientEndpoint();
        ClientEndpointConfig clientEndpointConfig = ClientEndpointConfig.Builder.create().build();
        ContainerProvider.getWebSocketContainer().connectToServer(endpoint, clientEndpointConfig, new URI(""ws://"" + DefaultServer.getHostAddress(""default"") + "":"" + DefaultServer.getHostPort(""default"") + ""/servletContext/wrapper""));
        Assert.assertEquals(""wrapped"", endpoint.getResponses().poll(15, TimeUnit.SECONDS));
        endpoint.session.close();
        endpoint.closeLatch.await(10, TimeUnit.SECONDS);
    }
"
"    @Test
    public void testConnectionWaitsForMessageEnd() throws Exception {
        final CountDownLatch done = new CountDownLatch(1);
        final AtomicReference<String> message = new AtomicReference<>();
        WebSocketChannel channel = WebSocketClient.connectionBuilder(DefaultServer.getWorker(), DefaultServer.getBufferPool(), new URI(DefaultServer.getDefaultServerURL() + ""/""))
                .connect().get();
        channel.getReceiveSetter().set(new AbstractReceiveListener() {
            @Override
            protected void onFullTextMessage(WebSocketChannel channel, BufferedTextMessage msg) throws IOException {
                message.set(msg.getData());
                done.countDown();
            }

            @Override
            protected void onError(WebSocketChannel channel, Throwable error) {
                error.printStackTrace();
                message.set(""error"");
                done.countDown();
            }

            @Override
            protected void onFullCloseMessage(WebSocketChannel channel, BufferedBinaryMessage message) throws IOException {
                message.getData().free();
                done.countDown();
            }
        });
        channel.resumeReceives();
        Assert.assertTrue(channel.isOpen());
        WebSockets.sendText(""Hello World"", channel, null);
        Thread.sleep(500);
        serverContainer.pause(null);
        try {
            Assert.assertTrue(done.await(10, TimeUnit.SECONDS));
            Assert.assertEquals(""Hello World"", message.get());
        } finally {
            serverContainer.resume();
        }
    }
"
"    @Test
    public void testConnectionClosedOnPause() throws Exception {
        final CountDownLatch done = new CountDownLatch(1);
        final AtomicReference<String> message = new AtomicReference<>();
        WebSocketChannel channel = WebSocketClient.connectionBuilder(DefaultServer.getWorker(), DefaultServer.getBufferPool(), new URI(DefaultServer.getDefaultServerURL() + ""/""))
                .connect().get();
        channel.getReceiveSetter().set(new ChannelListener<WebSocketChannel>() {
            @Override
            public void handleEvent(WebSocketChannel channel) {
                try {
                    StreamSourceFrameChannel res = channel.receive();
                    if(res == null) {
                        return;
                    }
                    if (res.getType() == WebSocketFrameType.CLOSE) {
                        message.set(""closed"");
                        done.countDown();
                    }
                    Channels.drain(res, Long.MAX_VALUE);
                } catch (IOException e) {
                    if(message.get() == null) {
                        e.printStackTrace();
                        message.set(""error"");
                        done.countDown();
                    }
                }
            }
"
"    @Test
    public void testRejectWhenSuspended() throws Exception {
        try {
            serverContainer.pause(null);
            WebSocketChannel channel = WebSocketClient.connectionBuilder(DefaultServer.getWorker(), DefaultServer.getBufferPool(), new URI(DefaultServer.getDefaultServerURL() + ""/""))
                    .connect().get();
            IoUtils.safeClose(channel);
            Assert.fail();
        } catch (UpgradeFailedException e) {
            //expected
        } finally {
            serverContainer.resume();
        }

    }
"
"    @Test
    public void testLongTextMessage() throws Exception {

        final String SEC_WEBSOCKET_EXTENSIONS = ""permessage-deflate; client_no_context_takeover; client_max_window_bits"";
        List<WebSocketExtension> extensionsList = WebSocketExtension.parse(SEC_WEBSOCKET_EXTENSIONS);

        final WebSocketClientNegotiation negotiation = new WebSocketClientNegotiation(null, extensionsList);

        Set<ExtensionHandshake> extensionHandshakes = new HashSet<>();
        extensionHandshakes.add(new PerMessageDeflateHandshake(true));

        final WebSocketChannel clientChannel = WebSocketClient.connect(DefaultServer.getWorker(), null, DefaultServer.getBufferPool(), OptionMap.EMPTY, new URI(DefaultServer.getDefaultServerURL()), WebSocketVersion.V13, negotiation, extensionHandshakes).get();

        final LinkedBlockingDeque<String> resultQueue  = new LinkedBlockingDeque<>();

        clientChannel.getReceiveSetter().set(new AbstractReceiveListener() {
            @Override
            protected void onFullTextMessage(WebSocketChannel channel, BufferedTextMessage message) throws IOException {
                String data = message.getData();
                // WebSocketLogger.ROOT_LOGGER.info(""onFullTextMessage() - Client - Received: "" + data.getBytes().length + "" bytes."");
                resultQueue.addLast(data);
            }

            @Override
            protected void onFullCloseMessage(WebSocketChannel channel, BufferedBinaryMessage message) throws IOException {
                message.getData().close();
                WebSocketLogger.ROOT_LOGGER.info(""onFullCloseMessage"");
            }

            @Override
            protected void onError(WebSocketChannel channel, Throwable error) {
                WebSocketLogger.ROOT_LOGGER.info(""onError"");
                super.onError(channel, error);
                error.printStackTrace();
                resultQueue.add(""FAILED "" + error);
            }

        });
        clientChannel.resumeReceives();

        int LONG_MSG = 125 * 1024;
        StringBuilder longMsg = new StringBuilder(LONG_MSG);

        for (int i = 0; i < LONG_MSG; i++) {
            longMsg.append(Integer.toString(i).charAt(0));
        }

        String message = longMsg.toString();
        for(int j = 0; j < MSG_COUNT; ++ j) {

            WebSockets.sendTextBlocking(message, clientChannel);
            String res = resultQueue.poll(10, TimeUnit.SECONDS);
            Assert.assertEquals(message, res);
        }

        clientChannel.sendClose();

    }
"
"    @Test
    public void testExtensionsHeaders() throws Exception {


        final String SEC_WEBSOCKET_EXTENSIONS = ""permessage-deflate; client_no_context_takeover; client_max_window_bits"";
        final String SEC_WEBSOCKET_EXTENSIONS_EXPECTED = ""[permessage-deflate; client_no_context_takeover]"";  // List format
        List<WebSocketExtension> extensions = WebSocketExtension.parse(SEC_WEBSOCKET_EXTENSIONS);

        final WebSocketClientNegotiation negotiation = new WebSocketClientNegotiation(null, extensions);

        Set<ExtensionHandshake> extensionHandshakes = new HashSet<>();
        extensionHandshakes.add(new PerMessageDeflateHandshake(true));

        final WebSocketChannel clientChannel = WebSocketClient.connect(DefaultServer.getWorker(), null, DefaultServer.getBufferPool(), OptionMap.EMPTY, new URI(DefaultServer.getDefaultServerURL()), WebSocketVersion.V13, negotiation, extensionHandshakes).get();

        final CountDownLatch latch = new CountDownLatch(1);
        final AtomicReference<String> result = new AtomicReference<>();

        clientChannel.getReceiveSetter().set(new AbstractReceiveListener() {
            @Override
            protected void onFullTextMessage(WebSocketChannel channel, BufferedTextMessage message) throws IOException {
                String data = message.getData();
                WebSocketLogger.ROOT_LOGGER.info(""onFullTextMessage - Client - Received: "" + data.getBytes().length + "" bytes . Data: "" + data);
                result.set(data);
                latch.countDown();
            }

            @Override
            protected void onFullCloseMessage(WebSocketChannel channel, BufferedBinaryMessage message) throws IOException {
                message.getData().close();
                WebSocketLogger.ROOT_LOGGER.info(""onFullCloseMessage"");
            }

            @Override
            protected void onError(WebSocketChannel channel, Throwable error) {
                WebSocketLogger.ROOT_LOGGER.info(""onError"");
                super.onError(channel, error);
                error.printStackTrace();
                latch.countDown();
            }

        });
        clientChannel.resumeReceives();

        StreamSinkFrameChannel sendChannel = clientChannel.send(WebSocketFrameType.TEXT);
        new StringWriteChannelListener(""Hello, World!"").setup(sendChannel);

        latch.await(10, TimeUnit.SECONDS);
        Assert.assertEquals(""Hello, World!"", result.get());
        clientChannel.sendClose();

        Assert.assertEquals(SEC_WEBSOCKET_EXTENSIONS_EXPECTED, debug.getResponseExtensions().toString());
    }
"
"    @Test
    public void testStringOnMessage() throws Exception {
        final byte[] payload = ""hello"".getBytes();
        final FutureResult latch = new FutureResult();

        WebSocketTestClient client = new WebSocketTestClient(WebSocketVersion.V13, new URI(""ws://"" + DefaultServer.getHostAddress(""default"") + "":"" + DefaultServer.getHostPort(""default"") + ""/ws/chat/Stuart""));
        client.connect();
        client.send(new TextWebSocketFrame(Unpooled.wrappedBuffer(payload)), new FrameChecker(TextWebSocketFrame.class, ""hello Stuart"".getBytes(), latch));
        latch.getIoFuture().get();
        client.destroy();
    }
"
"    @Test
    public void testRedirectHandling() throws Exception {
        AnnotatedClientEndpoint.reset();
        Session session = deployment.connectToServer(AnnotatedClientEndpoint.class, new URI(""ws://"" + DefaultServer.getHostAddress(""default"") + "":"" + DefaultServer.getHostPort(""default"") + ""/ws/redirect""));

        Assert.assertEquals(""hi Stuart (protocol=foo)"", AnnotatedClientEndpoint.message());

        session.close();
        Assert.assertEquals(""CLOSED"", AnnotatedClientEndpoint.message());
    }
"
"    @Test
    public void testWebSocketInRootContext() throws Exception {
        final byte[] payload = ""hello"".getBytes();
        final FutureResult latch = new FutureResult();

        WebSocketTestClient client = new WebSocketTestClient(WebSocketVersion.V13, new URI(""ws://"" + DefaultServer.getHostAddress(""default"") + "":"" + DefaultServer.getHostPort(""default"") + ""/ws""));
        client.connect();
        client.send(new TextWebSocketFrame(Unpooled.wrappedBuffer(payload)), new FrameChecker(TextWebSocketFrame.class, ""hello"".getBytes(), latch));
        latch.getIoFuture().get();
        client.destroy();
    }
"
"    @Test
    public void testAnnotatedClientEndpoint() throws Exception {
        AnnotatedClientEndpoint.reset();
        Session session = deployment.connectToServer(AnnotatedClientEndpoint.class, new URI(""ws://"" + DefaultServer.getHostAddress(""default"") + "":"" + DefaultServer.getHostPort(""default"") + ""/ws/chat/Bob""));

        Assert.assertEquals(""hi Bob (protocol=foo)"", AnnotatedClientEndpoint.message());

        session.close();
        Assert.assertEquals(""CLOSED"", AnnotatedClientEndpoint.message());
    }
"
"    @Test
    public void testIdleTimeout() throws Exception {
        AnnotatedClientEndpoint.reset();
        Session session = deployment.connectToServer(AnnotatedClientEndpoint.class, new URI(""ws://"" + DefaultServer.getHostAddress(""default"") + "":"" + DefaultServer.getHostPort(""default"") + ""/ws/chat/Bob""));

        Assert.assertEquals(""hi Bob (protocol=foo)"", AnnotatedClientEndpoint.message());

        session.close();
        Assert.assertEquals(""CLOSED"", AnnotatedClientEndpoint.message());
    }
"
"    @Test
    public void testCloseReason() throws Exception {
        MessageEndpoint.reset();

        Session session = deployment.connectToServer(AnnotatedClientEndpoint.class, new URI(""ws://"" + DefaultServer.getHostAddress(""default"") + "":"" + DefaultServer.getHostPort(""default"") + ""/ws/chat/Bob""));

        Assert.assertEquals(""hi Bob (protocol=foo)"", AnnotatedClientEndpoint.message());

        session.close(new CloseReason(CloseReason.CloseCodes.VIOLATED_POLICY, ""Foo!""));
        Assert.assertEquals(""CLOSED"", AnnotatedClientEndpoint.message());
        CloseReason cr = MessageEndpoint.getReason();
        Assert.assertEquals(CloseReason.CloseCodes.VIOLATED_POLICY.getCode(), cr.getCloseCode().getCode());
        Assert.assertEquals(""Foo!"", cr.getReasonPhrase());

    }
"
"    @Test
    public void testAnnotatedClientEndpointWithConfigurator() throws Exception {


        Session session = deployment.connectToServer(AnnotatedClientEndpointWithConfigurator.class, new URI(""ws://"" + DefaultServer.getHostAddress(""default"") + "":"" + DefaultServer.getHostPort(""default"") + ""/ws/chat/Bob""));

        Assert.assertEquals(""hi Bob (protocol=configured-proto)"", AnnotatedClientEndpointWithConfigurator.message());
        Assert.assertEquals(""foo, bar, configured-proto"", ClientConfigurator.sentSubProtocol);
        Assert.assertEquals(""configured-proto"", ClientConfigurator.receivedSubProtocol());

        session.close();
        Assert.assertEquals(""CLOSED"", AnnotatedClientEndpointWithConfigurator.message());
    }
"
"    @Test
    public void testErrorHandling() throws Exception {
        //make a sub class
        AnnotatedClientEndpoint c = new AnnotatedClientEndpoint() {

        };

        Session session = deployment.connectToServer(c, new URI(""ws://"" + DefaultServer.getHostAddress(""default"") + "":"" + DefaultServer.getHostPort(""default"") + ""/ws/error""));
        Assert.assertEquals(""hi"", ErrorEndpoint.getMessage());
        session.getAsyncRemote().sendText(""app-error"");
        Assert.assertEquals(""app-error"", ErrorEndpoint.getMessage());
        Assert.assertEquals(""ERROR: java.lang.RuntimeException"", ErrorEndpoint.getMessage());
        Assert.assertTrue(c.isOpen());

        session.getBasicRemote().sendText(""io-error"");
        Assert.assertEquals(""io-error"", ErrorEndpoint.getMessage());
        Assert.assertEquals(""ERROR: java.io.IOException"", ErrorEndpoint.getMessage());
        Assert.assertTrue(c.isOpen());
        ((UndertowSession)session).forceClose();
        Assert.assertEquals(""CLOSED"", ErrorEndpoint.getMessage());

    }
"
"    @Test
    public void testClientSideIdleTimeout() throws Exception {
        //make a sub class
        CountDownLatch latch = new CountDownLatch(1);
        CloseCountdownEndpoint c = new CloseCountdownEndpoint(latch);

        Session session = deployment.connectToServer(c, new URI(""ws://"" + DefaultServer.getHostAddress(""default"") + "":"" + DefaultServer.getHostPort(""default"") + ""/ws/chat/Bob""));
        session.setMaxIdleTimeout(100);
        Assert.assertTrue(latch.await(2000, TimeUnit.MILLISECONDS));
        Assert.assertFalse(session.isOpen());

    }
"
"    @Test
    public void testGenericMessageHandling() throws Exception {
        //make a sub class
        AnnotatedGenericClientEndpoint c = new AnnotatedGenericClientEndpoint() {

        };

        Session session = deployment.connectToServer(c, new URI(""ws://"" + DefaultServer.getHostAddress(""default"") + "":"" + DefaultServer.getHostPort(""default"") + ""/ws/error""));
        Assert.assertEquals(""hi"", ErrorEndpoint.getMessage());
        session.getAsyncRemote().sendText(""app-error"");
        Assert.assertEquals(""app-error"", ErrorEndpoint.getMessage());
        Assert.assertEquals(""ERROR: java.lang.RuntimeException"", ErrorEndpoint.getMessage());
        Assert.assertTrue(c.isOpen());

        session.getBasicRemote().sendText(""io-error"");
        Assert.assertEquals(""io-error"", ErrorEndpoint.getMessage());
        Assert.assertEquals(""ERROR: java.io.IOException"", ErrorEndpoint.getMessage());
        Assert.assertTrue(c.isOpen());
        ((UndertowSession)session).forceClose();
        Assert.assertEquals(""CLOSED"", ErrorEndpoint.getMessage());

    }
"
"    @Test
    public void testImplicitIntegerConversion() throws Exception {
        final byte[] payload = ""12"".getBytes();
        final FutureResult latch = new FutureResult();

        WebSocketTestClient client = new WebSocketTestClient(WebSocketVersion.V13, new URI(""ws://"" + DefaultServer.getHostAddress(""default"") + "":"" + DefaultServer.getHostPort(""default"") + ""/ws/increment/2""));
        client.connect();
        client.send(new TextWebSocketFrame(Unpooled.wrappedBuffer(payload)), new FrameChecker(TextWebSocketFrame.class, ""14"".getBytes(), latch));
        latch.getIoFuture().get();
        client.destroy();
    }
"
"    @Test
    public void testEncodingAndDecodingText() throws Exception {
        final byte[] payload = ""hello"".getBytes();
        final FutureResult latch = new FutureResult();

        WebSocketTestClient client = new WebSocketTestClient(WebSocketVersion.V13, new URI(""ws://"" + DefaultServer.getHostAddress(""default"") + "":"" + DefaultServer.getHostPort(""default"") + ""/ws/encoding/Stuart""));
        client.connect();
        client.send(new TextWebSocketFrame(Unpooled.wrappedBuffer(payload)), new FrameChecker(TextWebSocketFrame.class, ""hello Stuart"".getBytes(), latch));
        latch.getIoFuture().get();
        client.destroy();
    }
"
"    @Test
    public void testEncodingAndDecodingBinary() throws Exception {
        final byte[] payload = ""hello"".getBytes();
        final FutureResult latch = new FutureResult();

        WebSocketTestClient client = new WebSocketTestClient(WebSocketVersion.V13, new URI(""ws://"" + DefaultServer.getHostAddress(""default"") + "":"" + DefaultServer.getHostPort(""default"") + ""/ws/encoding/Stuart""));
        client.connect();
        client.send(new BinaryWebSocketFrame(Unpooled.wrappedBuffer(payload)), new FrameChecker(TextWebSocketFrame.class, ""hello Stuart"".getBytes(), latch));
        latch.getIoFuture().get();
        client.destroy();
    }
"
"    @Test
    public void testEncodingWithGenericSuperclass() throws Exception {
        final byte[] payload = ""hello"".getBytes();
        final FutureResult latch = new FutureResult();

        WebSocketTestClient client = new WebSocketTestClient(WebSocketVersion.V13, new URI(""ws://"" + DefaultServer.getHostAddress(""default"") + "":"" + DefaultServer.getHostPort(""default"") + ""/ws/encodingGenerics/Stuart""));
        client.connect();
        client.send(new TextWebSocketFrame(Unpooled.wrappedBuffer(payload)), new FrameChecker(TextWebSocketFrame.class, ""hello Stuart"".getBytes(), latch));
        latch.getIoFuture().get();
        client.destroy();
    }
"
"    @Test
    public void testRequestUri() throws Exception {
        final byte[] payload = ""hello"".getBytes();
        final FutureResult latch = new FutureResult();

        WebSocketTestClient client = new WebSocketTestClient(WebSocketVersion.V13, new URI(""ws://"" + DefaultServer.getHostAddress(""default"") + "":"" + DefaultServer.getHostPort(""default"") + ""/ws/request?a=b""));
        client.connect();
        client.send(new TextWebSocketFrame(Unpooled.wrappedBuffer(payload)), new FrameChecker(TextWebSocketFrame.class, ""/ws/request?a=b"".getBytes(), latch));
        latch.getIoFuture().get();
        client.destroy();
    }
"
"    @Test
    public void testTimeoutCloseReason() throws Exception {
        TimeoutEndpoint.reset();

        Session session = deployment.connectToServer(DoNothingEndpoint.class, new URI(""ws://"" + DefaultServer.getHostAddress(""default"") + "":"" + DefaultServer.getHostPort(""default"") + ""/ws/timeout""));

        Assert.assertEquals(CloseReason.CloseCodes.CLOSED_ABNORMALLY, TimeoutEndpoint.getReason().getCloseCode());
    }
"
"    @Test
    public void testThreadSafeSend() throws Exception {
        AnnotatedClientEndpoint.reset();
        Session session = deployment.connectToServer(AnnotatedClientEndpoint.class, new URI(""ws://"" + DefaultServer.getHostAddress(""default"") + "":"" + DefaultServer.getHostPort(""default"") + ""/ws/threads""));
        Set<String> expected = ThreadSafetyEndpoint.expected();
        long end = System.currentTimeMillis() + 10000;
        while (!expected.isEmpty() && System.currentTimeMillis() < end) {
            expected.remove(AnnotatedClientEndpoint.message());
        }
        session.close();
        Assert.assertEquals(0, expected.size());
    }
"
"    @Test
    public void testMessagesReceivedInOrder() throws Exception {
        stacks.clear();
        EchoSocket.receivedEchos = new FutureResult<>();
        final ClientEndpointConfig clientEndpointConfig = ClientEndpointConfig.Builder.create().build();
        final CountDownLatch done = new CountDownLatch(1);
        final AtomicReference<String> error = new AtomicReference<>();
        ContainerProvider.getWebSocketContainer()
                .connectToServer(new Endpoint() {
                                     @Override
                                     public void onOpen(final Session session, EndpointConfig endpointConfig) {

                                         try {
                                             RemoteEndpoint.Basic rem = session.getBasicRemote();
                                             List<String> messages = new ArrayList<>();
                                             for (int i = 0; i < MESSAGES; i++) {
                                                 byte[] data = new byte[2048];
                                                 (new Random()).nextBytes(data);
                                                 String crc = md5(data);
                                                 rem.sendBinary(ByteBuffer.wrap(data));
                                                 messages.add(crc);
                                             }

                                             List<String> received = EchoSocket.receivedEchos.getIoFuture().get();
                                             StringBuilder sb = new StringBuilder();
                                             boolean fail = false;
                                             for (int i = 0; i < messages.size(); i++) {
                                                 if (received.size() <= i) {
                                                     fail = true;
                                                     sb.append(i + "": should be "" + messages.get(i) + "" but is empty."");
                                                 } else {
                                                     if (!messages.get(i).equals(received.get(i))) {
                                                         fail = true;
                                                         sb.append(i + "": should be "" + messages.get(i) + "" but is "" + received.get(i) + "" (but found at "" + received.indexOf(messages.get(i)) + "")."");
                                                     }
                                                 }
                                             }
                                             if(fail) {
                                                 error.set(sb.toString());
                                             }
                                             done.countDown();

                                         } catch (Throwable t) {
                                             System.out.println(t);
                                         }
                                     }
"
"    @Test
    public void testErrorHandling() throws Exception {


        ServerWebSocketContainer builder = new ServerWebSocketContainer(TestClassIntrospector.INSTANCE, DefaultServer.getWorker(), DefaultServer.getBufferPool(), Collections.EMPTY_LIST, false, false);

        builder.addEndpoint(ServerEndpointConfig.Builder.create(ProgramaticErrorEndpoint.class, ""/"").configurator(new InstanceConfigurator(new ProgramaticErrorEndpoint())).build());
        deployServlet(builder);

        AnnotatedClientEndpoint c = new AnnotatedClientEndpoint();

        Session session = ContainerProvider.getWebSocketContainer().connectToServer(c, new URI(""ws://"" + DefaultServer.getHostAddress(""default"") + "":"" + DefaultServer.getHostPort(""default"") + ""/""));
        Assert.assertEquals(""hi"", ProgramaticErrorEndpoint.getMessage());
        session.getAsyncRemote().sendText(""app-error"");
        Assert.assertEquals(""app-error"", ProgramaticErrorEndpoint.getMessage());
        Assert.assertEquals(""ERROR: java.lang.RuntimeException"", ProgramaticErrorEndpoint.getMessage());
        Assert.assertTrue(c.isOpen());

        session.getBasicRemote().sendText(""io-error"");
        Assert.assertEquals(""io-error"", ProgramaticErrorEndpoint.getMessage());
        Assert.assertEquals(""ERROR: java.lang.RuntimeException"", ProgramaticErrorEndpoint.getMessage());
        Assert.assertTrue(c.isOpen());
        ((UndertowSession) session).forceClose();
        Assert.assertEquals(""CLOSED"", ProgramaticErrorEndpoint.getMessage());

    }
"
"    @Test
    public void testDynamicAnnotatedEndpoint() throws Exception {
        final byte[] payload = ""hello"".getBytes();
        final FutureResult latch = new FutureResult();

        WebSocketTestClient client = new WebSocketTestClient(WebSocketVersion.V13, new URI(""ws://"" + DefaultServer.getHostAddress(""default"") + "":"" + DefaultServer.getHostPort(""default"") + ""/ws/dynamicEchoEndpoint?annotated=true""));
        client.connect();
        client.send(new TextWebSocketFrame(Unpooled.wrappedBuffer(payload)), new FrameChecker(TextWebSocketFrame.class, ""opened:true /dynamicEchoEndpoint hello"".getBytes(), latch));
        latch.getIoFuture().get();
        client.destroy();
    }
"
"    @Test
    public void testDynamicProgramaticEndpoint() throws Exception {
        final byte[] payload = ""hello"".getBytes();
        final FutureResult latch = new FutureResult();

        WebSocketTestClient client = new WebSocketTestClient(WebSocketVersion.V13, new URI(""ws://"" + DefaultServer.getHostAddress(""default"") + "":"" + DefaultServer.getHostPort(""default"") + ""/ws/dynamicEchoEndpoint""));
        client.connect();
        client.send(new TextWebSocketFrame(Unpooled.wrappedBuffer(payload)), new FrameChecker(TextWebSocketFrame.class, ""/dynamicEchoEndpoint hello"".getBytes(), latch));
        latch.getIoFuture().get();
        client.destroy();
    }
"
"    @Test
    public void webSocketStringStressTestCase() throws Exception {
        List<CountDownLatch> latches = new ArrayList<>();
        for (int i = 0; i < NUM_THREADS; ++i) {
            final CountDownLatch latch = new CountDownLatch(1);
            latches.add(latch);
            final Session session = deployment.connectToServer(new Endpoint() {
                @Override
                public void onOpen(Session session, EndpointConfig config) {
                }
"
"    @Test
    public void websocketFragmentationStressTestCase() throws Exception {

        final ByteArrayOutputStream out = new ByteArrayOutputStream();
        final CountDownLatch done = new CountDownLatch(1);

        StringBuilder sb = new StringBuilder();
        for (int i = 0; i < 10000; ++i) {
            sb.append(""message "");
            sb.append(i);
        }
        String toSend = sb.toString();

        final Session session = defaultContainer.connectToServer(new Endpoint() {
            @Override
            public void onOpen(Session session, EndpointConfig config) {
                session.addMessageHandler(new MessageHandler.Partial<byte[]>() {
                    @Override
                    public void onMessage(byte[] bytes, boolean b) {
                        try {
                            out.write(bytes);
                        } catch (IOException e) {
                            e.printStackTrace();
                            done.countDown();
                        }
                        if (b) {
                            done.countDown();
                        }
                    }
"
"    @Test
    public void testExtractHandlerType() {

        Map<Class<?>, Boolean>  types = ClassUtils.getHandlerTypes(FinalIm.class);
        Assert.assertEquals(1, types.size());
        Assert.assertTrue(types.containsKey(ByteBuffer.class));

        types = ClassUtils.getHandlerTypes(ByteBufferFromSuperClassEncoder.class);
        Assert.assertEquals(1, types.size());
        Assert.assertTrue(types.containsKey(ByteBuffer.class));

        types = ClassUtils.getHandlerTypes(MessageHandlerImpl.class);
        Assert.assertEquals(1, types.size());
        Assert.assertTrue(types.containsKey(ByteBuffer.class));
        Assert.assertFalse(types.get(ByteBuffer.class));

        types = ClassUtils.getHandlerTypes(AsyncMessageHandlerImpl.class);
        Assert.assertEquals(1, types.size());
        Assert.assertTrue(types.containsKey(ByteBuffer.class));
        Assert.assertTrue(types.get(ByteBuffer.class));

        types = ClassUtils.getHandlerTypes(ComplexMessageHandlerImpl.class);
        Assert.assertEquals(2, types.size());
        Assert.assertTrue(types.containsKey(ByteBuffer.class));
        Assert.assertFalse(types.get(ByteBuffer.class));
        Assert.assertTrue(types.containsKey(String.class));
        Assert.assertTrue(types.get(String.class));
        Assert.assertFalse(types.containsKey(byte[].class));

    }
"
"    @Test
    public void testExtractEncoderType() {
        Class<?> clazz = ClassUtils.getEncoderType(BinaryEncoder.class);
        Assert.assertEquals(String.class, clazz);

        Class<?> clazz2 = ClassUtils.getEncoderType(TextEncoder.class);
        Assert.assertEquals(String.class, clazz2);

        Class<?> clazz3 = ClassUtils.getEncoderType(TextStreamEncoder.class);
        Assert.assertEquals(String.class, clazz3);

        Class<?> clazz4 = ClassUtils.getEncoderType(BinaryStreamEncoder.class);
        Assert.assertEquals(String.class, clazz4);
    }
"
"    @Test
    public void testAnnotatedClientEndpoint() throws Exception {
        AnnotatedClientReconnectEndpoint endpoint = new AnnotatedClientReconnectEndpoint();
        Session session = deployment.connectToServer(endpoint, new URI(""ws://"" + DefaultServer.getHostAddress(""default"") + "":"" + DefaultServer.getHostPort(""default"") + ""/ws/""));

        Assert.assertEquals(""OPEN"", endpoint.message());
        session.getBasicRemote().sendText(""hi"");
        Assert.assertEquals(""MESSAGE-ECHO-hi"", endpoint.message());
        session.getBasicRemote().sendText(""close"");
        Assert.assertEquals(""CLOSE"", endpoint.message());
        Assert.assertEquals(""OPEN"", endpoint.message());
        session.getBasicRemote().sendText(""hi"");
        Assert.assertEquals(""MESSAGE-ECHO-hi"", endpoint.message());
        session.getBasicRemote().sendText(""close"");
        Assert.assertEquals(""CLOSE"", endpoint.message());
        Assert.assertEquals(""OPEN"", endpoint.message());
        session.getBasicRemote().sendText(""hi"");
        Assert.assertEquals(""MESSAGE-ECHO-hi"", endpoint.message());
        session.getBasicRemote().sendText(""close"");
        Assert.assertEquals(""CLOSE"", endpoint.message());
        Assert.assertNull(endpoint.quickMessage());
        Assert.assertFalse(failed);
    }
"
"    @Test
    public void testSimpleBasic() throws Exception {
        //
        final UndertowClient client = createClient();

        final List<ClientResponse> responses = new CopyOnWriteArrayList<>();
        final CountDownLatch latch = new CountDownLatch(10);
        final ClientConnection connection = client.connect(ADDRESS, worker, DefaultServer.getBufferPool(), OptionMap.EMPTY).get();
        try {
            connection.getIoThread().execute(new Runnable() {
                @Override
                public void run() {
                    for (int i = 0; i < 10; i++) {
                        final ClientRequest request = new ClientRequest().setMethod(Methods.GET).setPath(MESSAGE);
                        request.getRequestHeaders().put(Headers.HOST, DefaultServer.getHostAddress());
                        connection.sendRequest(request, createClientCallback(responses, latch));
                    }
                }
"
"    @Test
    public void testSendPing() throws Exception {
        //
        final UndertowClient client = createClient();

        final List<ClientResponse> responses = new CopyOnWriteArrayList<>();
        final FutureResult<Boolean> result = new FutureResult<>();
        final CountDownLatch latch = new CountDownLatch(3);
        final ClientConnection connection = client.connect(ADDRESS, worker, DefaultServer.getBufferPool(), OptionMap.EMPTY).get();
        Assert.assertTrue(connection.isPingSupported());
        try {
            connection.getIoThread().execute(new Runnable() {
                @Override
                public void run() {
                        final ClientRequest request = new ClientRequest().setMethod(Methods.GET).setPath(MESSAGE);
                        request.getRequestHeaders().put(Headers.HOST, DefaultServer.getHostAddress());
                        connection.sendRequest(request, createClientCallback(responses, latch));
                        connection.sendPing(new ClientConnection.PingListener() {
                            @Override
                            public void acknowledged() {
                                result.setResult(true);
                                latch.countDown();
                            }
"
"    @Test
    public void testPostRequest() throws Exception {
        //
        final UndertowClient client = createClient();
        final String postMessage = ""This is a post request"";

        final List<String> responses = new CopyOnWriteArrayList<>();
        final CountDownLatch latch = new CountDownLatch(10);
        final ClientConnection connection = client.connect(ADDRESS, worker, DefaultServer.getBufferPool(), OptionMap.EMPTY).get();
        try {
            connection.getIoThread().execute(new Runnable() {
                @Override
                public void run() {
                    for (int i = 0; i < 10; i++) {
                        final ClientRequest request = new ClientRequest().setMethod(Methods.POST).setPath(POST);
                        request.getRequestHeaders().put(Headers.HOST, DefaultServer.getHostAddress());
                        request.getRequestHeaders().put(Headers.TRANSFER_ENCODING, ""chunked"");
                        connection.sendRequest(request, new ClientCallback<ClientExchange>() {
                            @Override
                            public void completed(ClientExchange result) {
                                new StringWriteChannelListener(postMessage).setup(result.getRequestChannel());
                                result.setResponseListener(new ClientCallback<ClientExchange>() {
                                    @Override
                                    public void completed(ClientExchange result) {
                                        new StringReadChannelListener(DefaultServer.getBufferPool()) {

                                            @Override
                                            protected void stringDone(String string) {
                                                responses.add(string);
                                                latch.countDown();
                                            }

                                            @Override
                                            protected void error(IOException e) {
                                                e.printStackTrace();
                                                latch.countDown();
                                            }
                                        }.setup(result.getResponseChannel());
                                    }
"
"    @Test
    public void testConnectionClose() throws Exception {
        //
        final UndertowClient client = createClient();

        final CountDownLatch latch = new CountDownLatch(1);
        final ClientConnection connection = client.connect(ADDRESS, worker, DefaultServer.getBufferPool(), OptionMap.EMPTY).get();
        try {
            ClientRequest request = new ClientRequest().setPath(MESSAGE).setMethod(Methods.GET);
            request.getRequestHeaders().put(Headers.HOST, DefaultServer.getHostAddress());
            final List<ClientResponse> responses = new CopyOnWriteArrayList<>();
            request.getRequestHeaders().add(Headers.CONNECTION, Headers.CLOSE.toString());
            connection.sendRequest(request, createClientCallback(responses, latch));
            latch.await();
            final ClientResponse response = responses.iterator().next();
            Assert.assertEquals(message, response.getAttachment(RESPONSE_BODY));
            Assert.assertEquals(false, connection.isOpen());
        } finally {
            IoUtils.safeClose(connection);
        }

    }
"
"    @Test
    public void testSimpleBasic() throws Exception {
        //
        final UndertowClient client = createClient();

        final List<ClientResponse> responses = new CopyOnWriteArrayList<>();
        final CountDownLatch latch = new CountDownLatch(10);
        final ClientConnection connection = client.connect(ADDRESS, worker, DefaultServer.getBufferPool(), OptionMap.EMPTY).get();
        try {
            connection.getIoThread().execute(new Runnable() {
                @Override
                public void run() {
                    for (int i = 0; i < 10; i++) {
                        final ClientRequest request = new ClientRequest().setMethod(Methods.GET).setPath(MESSAGE);
                        request.getRequestHeaders().put(Headers.HOST, DefaultServer.getHostAddress());
                        connection.sendRequest(request, createClientCallback(responses, latch));
                    }
                }
"
"    @Test
    public void testPostRequest() throws Exception {
        //
        final UndertowClient client = createClient();
        final String postMessage = ""This is a post request"";

        final List<String> responses = new CopyOnWriteArrayList<>();
        final CountDownLatch latch = new CountDownLatch(10);
        final ClientConnection connection = client.connect(ADDRESS, worker, DefaultServer.getBufferPool(), OptionMap.EMPTY).get();
        try {
            connection.getIoThread().execute(new Runnable() {
                @Override
                public void run() {
                    for (int i = 0; i < 10; i++) {
                        final ClientRequest request = new ClientRequest().setMethod(Methods.POST).setPath(POST);
                        request.getRequestHeaders().put(Headers.HOST, DefaultServer.getHostAddress());
                        request.getRequestHeaders().put(Headers.TRANSFER_ENCODING, ""chunked"");
                        connection.sendRequest(request, new ClientCallback<ClientExchange>() {
                            @Override
                            public void completed(ClientExchange result) {
                                new StringWriteChannelListener(postMessage).setup(result.getRequestChannel());
                                result.setResponseListener(new ClientCallback<ClientExchange>() {
                                    @Override
                                    public void completed(ClientExchange result) {
                                        new StringReadChannelListener(DefaultServer.getBufferPool()) {

                                            @Override
                                            protected void stringDone(String string) {
                                                responses.add(string);
                                                latch.countDown();
                                            }

                                            @Override
                                            protected void error(IOException e) {
                                                e.printStackTrace();
                                                latch.countDown();
                                            }
                                        }.setup(result.getResponseChannel());
                                    }
"
"    @Test
    public void testSsl() throws Exception {
        //
        final UndertowClient client = createClient();

        final List<ClientResponse> responses = new CopyOnWriteArrayList<>();
        final CountDownLatch latch = new CountDownLatch(10);
        DefaultServer.startSSLServer();
        SSLContext context = DefaultServer.getClientSSLContext();
        XnioSsl ssl = new UndertowXnioSsl(DefaultServer.getWorker().getXnio(), OptionMap.EMPTY, DefaultServer.SSL_BUFFER_POOL, context);

        final ClientConnection connection = client.connect(new URI(DefaultServer.getDefaultServerSSLAddress()), worker, ssl, DefaultServer.getBufferPool(), OptionMap.EMPTY).get();
        try {
            connection.getIoThread().execute(new Runnable() {
                @Override
                public void run() {
                    for (int i = 0; i < 10; i++) {
                        final ClientRequest request = new ClientRequest().setMethod(Methods.GET).setPath(MESSAGE);
                        request.getRequestHeaders().put(Headers.HOST, DefaultServer.getHostAddress());
                        connection.sendRequest(request, createClientCallback(responses, latch));
                    }
                }
"
"    @Test
    public void testConnectionClose() throws Exception {
        //
        final UndertowClient client = createClient();

        final CountDownLatch latch = new CountDownLatch(1);
        final ClientConnection connection = client.connect(ADDRESS, worker, DefaultServer.getBufferPool(), OptionMap.EMPTY).get();
        try {
            ClientRequest request = new ClientRequest().setPath(MESSAGE).setMethod(Methods.GET);
            request.getRequestHeaders().put(Headers.HOST, DefaultServer.getHostAddress());
            final List<ClientResponse> responses = new CopyOnWriteArrayList<>();
            request.getRequestHeaders().add(Headers.CONNECTION, Headers.CLOSE.toString());
            connection.sendRequest(request, createClientCallback(responses, latch));
            latch.await();
            final ClientResponse response = responses.iterator().next();
            Assert.assertEquals(message, response.getAttachment(RESPONSE_BODY));
            Assert.assertEquals(false, connection.isOpen());
        } finally {
            IoUtils.safeClose(connection);
        }

    }
"
"    @Test
    public void testMethodSplit() {
        byte[] in = DATA.getBytes();
        for (int i = 0; i < in.length - 4; ++i) {
            try {
                testResume(i, in);
            } catch (Throwable e) {
                throw new RuntimeException(""Test failed at split "" + i, e);
            }
        }
    }
"
"    @Test
    public void testOneCharacterAtATime() throws BadRequestException {
        byte[] in = DATA.getBytes();
        final ResponseParseState context = new ResponseParseState();
        HttpResponseBuilder result = new HttpResponseBuilder();
        ByteBuffer buffer = ByteBuffer.wrap(in);
        buffer.limit(1);
        while (context.state != ResponseParseState.PARSE_COMPLETE) {
            HttpResponseParser.INSTANCE.handle(buffer, context, result);
            buffer.limit(buffer.limit() + 1);
        }
        runAssertions(result, context);
    }
"
"    @Test
    public void testSimpleBasic() throws Exception {
        //
        final UndertowClient client = createClient();

        final List<ClientResponse> responses = new CopyOnWriteArrayList<>();
        final CountDownLatch latch = new CountDownLatch(10);
        final ClientConnection connection = client.connect(ADDRESS, worker, new UndertowXnioSsl(worker.getXnio(), OptionMap.EMPTY, DefaultServer.getClientSSLContext()), DefaultServer.getBufferPool(), OptionMap.create(UndertowOptions.ENABLE_HTTP2, true)).get();
        try {
            connection.getIoThread().execute(new Runnable() {
                @Override
                public void run() {
                    for (int i = 0; i < 10; i++) {
                        final ClientRequest request = new ClientRequest().setMethod(Methods.GET).setPath(MESSAGE);
                        request.getRequestHeaders().put(Headers.HOST, DefaultServer.getHostAddress());
                        connection.sendRequest(request, createClientCallback(responses, latch));
                    }
                }
"
"    @Test
    public void testPostRequest() throws Exception {
        //
        final UndertowClient client = createClient();
        final String postMessage = ""This is a post request"";

        final List<String> responses = new CopyOnWriteArrayList<>();
        final CountDownLatch latch = new CountDownLatch(10);
        final ClientConnection connection = client.connect(ADDRESS, worker, new UndertowXnioSsl(worker.getXnio(), OptionMap.EMPTY, DefaultServer.getClientSSLContext()), DefaultServer.getBufferPool(), OptionMap.create(UndertowOptions.ENABLE_HTTP2, true)).get();
        try {
            connection.getIoThread().execute(new Runnable() {
                @Override
                public void run() {
                    for (int i = 0; i < 10; i++) {
                        final ClientRequest request = new ClientRequest().setMethod(Methods.POST).setPath(POST);
                        request.getRequestHeaders().put(Headers.HOST, DefaultServer.getHostAddress());
                        request.getRequestHeaders().put(Headers.TRANSFER_ENCODING, ""chunked"");
                        connection.sendRequest(request, new ClientCallback<ClientExchange>() {
                            @Override
                            public void completed(ClientExchange result) {
                                new StringWriteChannelListener(postMessage).setup(result.getRequestChannel());
                                result.setResponseListener(new ClientCallback<ClientExchange>() {
                                    @Override
                                    public void completed(ClientExchange result) {
                                        new StringReadChannelListener(DefaultServer.getBufferPool()) {

                                            @Override
                                            protected void stringDone(String string) {
                                                responses.add(string);
                                                latch.countDown();
                                            }

                                            @Override
                                            protected void error(IOException e) {
                                                e.printStackTrace();
                                                latch.countDown();
                                            }
                                        }.setup(result.getResponseChannel());
                                    }
"
"    @Test
    public void testOrderShorterFirst() {
        HttpString a =  new HttpString(""a"");
        HttpString aa =  new HttpString(""aa"");
        Assert.assertEquals(-1, a.compareTo(aa));
    }
"
"    @Test
    public void testCompareShorterFirst() {
        HttpString accept =  new HttpString(Headers.ACCEPT_STRING);
        Assert.assertEquals(accept.compareTo(Headers.ACCEPT_CHARSET), Headers.ACCEPT.compareTo(Headers.ACCEPT_CHARSET));

        HttpString acceptCharset =  new HttpString(Headers.ACCEPT_CHARSET_STRING);
        Assert.assertEquals(acceptCharset.compareTo(Headers.ACCEPT), Headers.ACCEPT_CHARSET.compareTo(Headers.ACCEPT));
    }
"
"    @Test
    public void testCompare() {
        HttpString contentType =  new HttpString(Headers.CONTENT_TYPE_STRING);
        Assert.assertEquals(contentType.compareTo(Headers.COOKIE), Headers.CONTENT_TYPE.compareTo(Headers.COOKIE));

        HttpString cookie =  new HttpString(Headers.COOKIE_STRING);
        Assert.assertEquals(cookie.compareTo(Headers.CONTENT_TYPE), Headers.COOKIE.compareTo(Headers.CONTENT_TYPE));
    }
"
"    @Test
    public void testSerialization() throws IOException, ClassNotFoundException {
        ByteArrayOutputStream out = new ByteArrayOutputStream();
        ObjectOutputStream so = new ObjectOutputStream(out);
        HttpString testString = new HttpString(""test"");
        so.writeObject(testString);
        so.close();

        ObjectInputStream in = new ObjectInputStream(new ByteArrayInputStream(out.toByteArray()));
        Object res = in.readObject();
        Assert.assertEquals(testString, res);
    }
"
"    @Test
    public void testParseFirefoxDate() {

        String firefoxHeader = ""Mon, 31 Mar 2014 09:24:49 GMT"";
        Date firefoxDate = DateUtils.parseDate(firefoxHeader);

        Assert.assertNotNull(firefoxDate);

        Calendar calendar = Calendar.getInstance(TimeZone.getTimeZone(""GMT""));
        calendar.set(2014, Calendar.MARCH, 31, 9, 24, 49);
        calendar.set(Calendar.MILLISECOND, 0);

        Assert.assertEquals(calendar.getTime(), firefoxDate);


    }
"
"    @Test
    public void testParseChromeDate() {

        String chromeHeader = ""Mon, 31 Mar 2014 09:44:00 GMT"";
        Date chromeDate = DateUtils.parseDate(chromeHeader);

        Assert.assertNotNull(chromeDate);

        Calendar calendar = Calendar.getInstance(TimeZone.getTimeZone(""GMT""));
        calendar.set(2014, Calendar.MARCH, 31, 9, 44, 00);
        calendar.set(Calendar.MILLISECOND, 0);

        Assert.assertEquals(calendar.getTime(), chromeDate);

    }
"
"    @Test
    public void testParseIE9Date() {

        String ie9Header = ""Wed, 12 Feb 2014 04:43:29 GMT; length=142951"";

        Date ie9Date = DateUtils.parseDate(ie9Header);
        Assert.assertNotNull(ie9Date);

        Calendar calendar = Calendar.getInstance(TimeZone.getTimeZone(""GMT""));
        calendar.set(2014, Calendar.FEBRUARY, 12, 4, 43, 29);
        calendar.set(Calendar.MILLISECOND, 0);

        Assert.assertEquals(calendar.getTime(), ie9Date);

    }
"
"    @Test
    public void testPerformance() {

        String ie9Header = ""Wed, 12 Feb 2014 04:43:29 GMT; length=142951"";

        long timestamp = System.currentTimeMillis();
        for (int i=0; i < 1000; i++) {
            ie9Header.replaceAll("";.*$"", """");
        }
        long ts1 = System.currentTimeMillis() - timestamp;

        timestamp = System.currentTimeMillis();

        for (int i=0; i < 1000; i++) {
            int index = ie9Header.indexOf(';');
            final String trimmedDate = index >=0 ? ie9Header.substring(0, index) : ie9Header;
        }

        long ts2 = System.currentTimeMillis() - timestamp;

        Assert.assertTrue(ts2 < ts1);

    }
"
"    @Test
    public void testSubstringMap() {

        SubstringMap<Integer> paths = new SubstringMap<>();

        for (int count = 0; count < 10; ++count) {
            int seed = new Random().nextInt();

            Random random = new Random(seed);
            System.out.println(""Using Seed "" + seed);

            List<String> parts = new ArrayList<>();

            Set<String> keys = new HashSet<>();

            for (int i = 0; i < NUM_TEST_VALUES; ++i) {
                String s = null;
                do {
                    byte[] bytes = new byte[random.nextInt(30) + 5];
                    random.nextBytes(bytes);
                    s = FlexBase64.encodeString(bytes, false);
                } while (keys.contains(s));
                keys.add(s);
                parts.add(s);
                paths.put(s, i);
                Assert.assertEquals(Integer.valueOf(i), paths.get(s, s.length()).getValue());
                Assert.assertEquals(Integer.valueOf(i), paths.get(s + ""fooosdf"", s.length()).getValue());
                String missing = s + ""asdfdasfasf"";
                Assert.assertNull(paths.get(missing, missing.length()));
            }

            for (String k : paths.keys()) {
                Assert.assertTrue(keys.remove(k));
            }
            Assert.assertEquals(0, keys.size());

            for (int i = 0; i < NUM_TEST_VALUES; ++i) {
                String p = parts.get(i);
                Assert.assertEquals(Integer.valueOf(i), paths.get(p, p.length()).getValue());
                Assert.assertEquals(Integer.valueOf(i), paths.get(p + ""asdfdsafasfw"", p.length()).getValue());
            }
            for (int i = 0; i < NUM_TEST_VALUES; ++i) {
                Integer p = paths.remove(parts.get(i));
                Assert.assertEquals(Integer.valueOf(i), p);
            }
        }
    }
"
"    @Test
    public void testTokenExtraction() {

        Assert.assertEquals(""--xyz"", Headers.extractTokenFromHeader(""multipart/form-data; boundary=--xyz; param=abc"", ""boundary""));
    }
"
"    @Test
    public void testCanonicalization() {

        //these strings should not be touched
        Assert.assertSame(""a/b/c"", CanonicalPathUtils.canonicalize(""a/b/c""));
        Assert.assertSame(""a/b/c/"", CanonicalPathUtils.canonicalize(""a/b/c/""));
        Assert.assertSame(""aaaaa"", CanonicalPathUtils.canonicalize(""aaaaa""));

        //these strings should result in the same string being output
        Assert.assertEquals(""a./b"", CanonicalPathUtils.canonicalize(""a./b""));
        Assert.assertEquals(""a./.b"", CanonicalPathUtils.canonicalize(""a./.b""));

        //removing double slash

        Assert.assertEquals(""a/b"", CanonicalPathUtils.canonicalize(""a//b""));
        Assert.assertEquals(""a/b"", CanonicalPathUtils.canonicalize(""a///b""));
        Assert.assertEquals(""a/b"", CanonicalPathUtils.canonicalize(""a////b""));

        //removing /./
        Assert.assertEquals(""a/b"", CanonicalPathUtils.canonicalize(""a/./b""));
        Assert.assertEquals(""a/b"", CanonicalPathUtils.canonicalize(""a/././b""));
        Assert.assertEquals(""a/b/c"", CanonicalPathUtils.canonicalize(""a/./b/./c""));
        Assert.assertEquals(""a/b"", CanonicalPathUtils.canonicalize(""a/./././b""));
        Assert.assertEquals(""a/b/"", CanonicalPathUtils.canonicalize(""a/./././b/./""));
        Assert.assertEquals(""a/b"", CanonicalPathUtils.canonicalize(""a/./././b/.""));

        //dealing with /../
        Assert.assertEquals(""/b"", CanonicalPathUtils.canonicalize(""/a/../b""));
        Assert.assertEquals(""/b"", CanonicalPathUtils.canonicalize(""/a/../c/../e/../b""));
        Assert.assertEquals(""/b"", CanonicalPathUtils.canonicalize(""/a/c/../../b""));
        Assert.assertEquals(""/"", CanonicalPathUtils.canonicalize(""/a/../..""));
        Assert.assertEquals(""/foo"", CanonicalPathUtils.canonicalize(""/a/../../foo""));

        //preserve (single) trailing /
        Assert.assertEquals(""/a/"", CanonicalPathUtils.canonicalize(""/a/""));
        Assert.assertEquals(""/"", CanonicalPathUtils.canonicalize(""/""));
        Assert.assertEquals(""/bbb/a"", CanonicalPathUtils.canonicalize(""/cc/../bbb/a/.""));
        Assert.assertEquals(""/aaa/bbb/"", CanonicalPathUtils.canonicalize(""/aaa/bbb//////""));
    }
"
"    @Test
    public void testCanonicalizationBackslash() {

        //these strings should not be touched
        Assert.assertSame(""a\\b\\c"", CanonicalPathUtils.canonicalize(""a\\b\\c""));
        Assert.assertSame(""a\\b\\c\\"", CanonicalPathUtils.canonicalize(""a\\b\\c\\""));
        Assert.assertSame(""aaaaa"", CanonicalPathUtils.canonicalize(""aaaaa""));

        //these strings should result in the same string being output
        Assert.assertEquals(""a.\\b"", CanonicalPathUtils.canonicalize(""a.\\b""));
        Assert.assertEquals(""a.\\.b"", CanonicalPathUtils.canonicalize(""a.\\.b""));

        //removing double slash

        Assert.assertEquals(""a\\b"", CanonicalPathUtils.canonicalize(""a\\\\b""));
        Assert.assertEquals(""a\\b"", CanonicalPathUtils.canonicalize(""a\\\\\\b""));
        Assert.assertEquals(""a\\b"", CanonicalPathUtils.canonicalize(""a\\\\\\\\b""));

        //removing \.\
        Assert.assertEquals(""a\\b"", CanonicalPathUtils.canonicalize(""a\\.\\b""));
        Assert.assertEquals(""a\\b"", CanonicalPathUtils.canonicalize(""a\\.\\.\\b""));
        Assert.assertEquals(""a\\b\\c"", CanonicalPathUtils.canonicalize(""a\\.\\b\\.\\c""));
        Assert.assertEquals(""a\\b"", CanonicalPathUtils.canonicalize(""a\\.\\.\\.\\b""));
        Assert.assertEquals(""a\\b\\"", CanonicalPathUtils.canonicalize(""a\\.\\.\\.\\b\\.\\""));
        Assert.assertEquals(""a\\b"", CanonicalPathUtils.canonicalize(""a\\.\\.\\.\\b\\.""));

        //dealing with \..\
        Assert.assertEquals(""\\b"", CanonicalPathUtils.canonicalize(""\\a\\..\\b""));
        Assert.assertEquals(""\\b"", CanonicalPathUtils.canonicalize(""\\a\\..\\c\\..\\e\\..\\b""));
        Assert.assertEquals(""\\b"", CanonicalPathUtils.canonicalize(""\\a\\c\\..\\..\\b""));
        Assert.assertEquals(""/"", CanonicalPathUtils.canonicalize(""\\a\\..\\..""));
        Assert.assertEquals(""\\foo"", CanonicalPathUtils.canonicalize(""\\a\\..\\..\\foo""));

        //preserve (single) trailing \
        Assert.assertEquals(""\\a\\"", CanonicalPathUtils.canonicalize(""\\a\\""));
        Assert.assertEquals(""\\"", CanonicalPathUtils.canonicalize(""\\""));
        Assert.assertEquals(""\\bbb\\a"", CanonicalPathUtils.canonicalize(""\\cc\\..\\bbb\\a\\.""));
        Assert.assertEquals(""\\aaa\\bbb\\"", CanonicalPathUtils.canonicalize(""\\aaa\\bbb\\\\\\\\\\\\""));

        //test mixtures of both forward and back slash
        Assert.assertEquals(""/"", CanonicalPathUtils.canonicalize(""\\a/..\\./""));
        Assert.assertEquals(""\\a/"", CanonicalPathUtils.canonicalize(""\\a\\b\\..\\./""));
        Assert.assertEquals(""/a/b/c../d..\\"", CanonicalPathUtils.canonicalize(""/a/b/c../d..\\""));
        Assert.assertEquals(""/a/d\\"", CanonicalPathUtils.canonicalize(""/a/b/c/..\\../d\\.\\""));
    }
"
"    @Test
    public void testIpV4Address() throws IOException {
        InetAddress res = NetworkUtils.parseIpv4Address(""1.123.255.2"");
        Assert.assertTrue(res instanceof Inet4Address);
        Assert.assertEquals(1, res.getAddress()[0]);
        Assert.assertEquals(123, res.getAddress()[1]);
        Assert.assertEquals((byte)255, res.getAddress()[2]);
        Assert.assertEquals(2, res.getAddress()[3]);
        Assert.assertEquals(""/1.123.255.2"", res.toString());

        res = NetworkUtils.parseIpv4Address(""127.0.0.1"");
        Assert.assertTrue(res instanceof Inet4Address);
        Assert.assertEquals(127, res.getAddress()[0]);
        Assert.assertEquals(0, res.getAddress()[1]);
        Assert.assertEquals((byte)0, res.getAddress()[2]);
        Assert.assertEquals(1, res.getAddress()[3]);
        Assert.assertEquals(""/127.0.0.1"", res.toString());
    }
"
"    @Test(expected = IOException.class)
    public void testIpV4AddressWithLeadingZero() throws IOException {
        NetworkUtils.parseIpv4Address(""01.123.255.2"");
    }
"
"    @Test(expected = IOException.class)
    public void testIpV4AddressToSmall() throws IOException {
        NetworkUtils.parseIpv4Address(""01.123.255"");
    }
"
"    @Test(expected = IOException.class)
    public void testIpV4AddressToLarge() throws IOException {
        NetworkUtils.parseIpv4Address(""01.123.255.1.1"");
    }
"
"    @Test(expected = IOException.class)
    public void testIpV4AddressMultipleDots() throws IOException {
        NetworkUtils.parseIpv4Address(""1..255.2"");
    }
"
"    @Test(expected = IOException.class)
    public void testIpV4AddressMultipleDots2() throws IOException {
        NetworkUtils.parseIpv4Address(""1..3.255.2"");
    }
"
"    @Test(expected = IOException.class)
    public void testIpV4Hostname() throws IOException {
        NetworkUtils.parseIpv4Address(""localhost"");
    }
"
"    @Test(expected = IOException.class)
    public void testIpV4Hostname2() throws IOException {
        NetworkUtils.parseIpv4Address(""ff"");
    }
"
"    @Test(expected = IOException.class)
    public void testIpV4AddressStartsWithDot() throws IOException {
        NetworkUtils.parseIpv4Address("".1.123.255.2"");
    }
"
"    @Test
    public void testIpv6Address() throws IOException {
        String addressString = ""2001:1db8:100:3:6:ff00:42:8329"";
        InetAddress res = NetworkUtils.parseIpv6Address(addressString);
        Assert.assertTrue(res instanceof Inet6Address);

        int[] parts = {0x2001, 0x1db8, 0x100, 0x3, 0x6, 0xff00, 0x42, 0x8329};
        for(int i = 0 ; i < parts.length; ++i) {
            Assert.assertEquals(((byte)(parts[i]>>8)), res.getAddress()[i * 2]);
            Assert.assertEquals(((byte)(parts[i])), res.getAddress()[i * 2 + 1]);
        }
        Assert.assertEquals(""/"" + addressString, res.toString());

        addressString = ""2001:1db8:100::6:ff00:42:8329"";
        res = NetworkUtils.parseIpv6Address(addressString);
        Assert.assertTrue(res instanceof Inet6Address);

        parts = new int[]{0x2001, 0x1db8, 0x100, 0x0, 0x6, 0xff00, 0x42, 0x8329};
        for(int i = 0 ; i < parts.length; ++i) {
            Assert.assertEquals(((byte)(parts[i]>>8)), res.getAddress()[i * 2]);
            Assert.assertEquals(((byte)(parts[i])), res.getAddress()[i * 2 + 1]);
        }
        Assert.assertEquals(""/2001:1db8:100:0:6:ff00:42:8329"", res.toString());

        addressString = ""2001:1db8:100::ff00:42:8329"";
        res = NetworkUtils.parseIpv6Address(addressString);
        Assert.assertTrue(res instanceof Inet6Address);

        parts = new int[]{0x2001, 0x1db8, 0x100, 0x0, 0x0, 0xff00, 0x42, 0x8329};
        for(int i = 0 ; i < parts.length; ++i) {
            Assert.assertEquals(((byte)(parts[i]>>8)), res.getAddress()[i * 2]);
            Assert.assertEquals(((byte)(parts[i])), res.getAddress()[i * 2 + 1]);
        }
        Assert.assertEquals(""/2001:1db8:100:0:0:ff00:42:8329"", res.toString());


        addressString = ""::1"";
        res = NetworkUtils.parseIpv6Address(addressString);
        Assert.assertTrue(res instanceof Inet6Address);

        parts = new int[]{0, 0, 0, 0, 0, 0, 0, 0x1};
        for(int i = 0 ; i < parts.length; ++i) {
            Assert.assertEquals(((byte)(parts[i]>>8)), res.getAddress()[i * 2]);
            Assert.assertEquals(((byte)(parts[i])), res.getAddress()[i * 2 + 1]);
        }
        Assert.assertEquals(""/0:0:0:0:0:0:0:1"", res.toString());
    }
"
"    @Test(expected = IOException.class)
    public void testIpV6AddressWithLeadingZero() throws IOException {
        NetworkUtils.parseIpv6Address(""2001:1db8:100:03:6:ff00:42:8329"");
    }
"
"    @Test(expected = IOException.class)
    public void testIpV6AddressToSmall() throws IOException {
        NetworkUtils.parseIpv6Address(""2001:1db8:3:6:ff00:42:8329"");
    }
"
"    @Test(expected = IOException.class)
    public void testIpV6AddressToLarge() throws IOException {
        NetworkUtils.parseIpv6Address(""2001:1db8:100:3:6:7:ff00:42:8329"");
    }
"
"    @Test(expected = IOException.class)
    public void testIpV6AddressMultipleColons() throws IOException {
        NetworkUtils.parseIpv6Address(""2001:1db8:100::3:6:ff00:42:8329"");
    }
"
"    @Test(expected = IOException.class)
    public void testIpV6AddressMultipleColons2() throws IOException {
        NetworkUtils.parseIpv6Address(""2001::100::329"");
    }
"
"    @Test(expected = IOException.class)
    public void testIpV6Hostname() throws IOException {
        NetworkUtils.parseIpv6Address(""localhost"");
    }
"
"    @Test(expected = IOException.class)
    public void testIpV6Hostname2() throws IOException {
        NetworkUtils.parseIpv6Address(""ff"");
    }
"
"    @Test(expected = IOException.class)
    public void testIpV6AddressStartsWithColon() throws IOException {
        NetworkUtils.parseIpv6Address("":2001:1db8:100:3:6:ff00:42:8329"");
    }
"
"    @Test
    public void testCodeLookup() {
        Assert.assertEquals(""OK"", StatusCodes.getReason(StatusCodes.OK));
    }
"
"    @Test
    public void testCharsetParsing() {
        Assert.assertEquals(null, Headers.extractQuotedValueFromHeader(""text/html; other-data=\""charset=UTF-8\"""", ""charset""));
        Assert.assertEquals(null, Headers.extractQuotedValueFromHeader(""text/html;"", ""charset""));
        Assert.assertEquals(""UTF-8"", Headers.extractQuotedValueFromHeader(""text/html; charset=\""UTF-8\"""", ""charset""));
        Assert.assertEquals(""UTF-8"", Headers.extractQuotedValueFromHeader(""text/html; charset=UTF-8"", ""charset""));
        Assert.assertEquals(""UTF-8"", Headers.extractQuotedValueFromHeader(""text/html; charset=\""UTF-8\""; foo=bar"", ""charset""));
        Assert.assertEquals(""UTF-8"", Headers.extractQuotedValueFromHeader(""text/html; charset=UTF-8 foo=bar"", ""charset""));
        Assert.assertEquals(""UTF-8"", Headers.extractQuotedValueFromHeader(""text/html; badcharset=bad charset=UTF-8 foo=bar"", ""charset""));
        Assert.assertEquals(""UTF-8"", Headers.extractQuotedValueFromHeader(""text/html;charset=UTF-8"", ""charset""));
        Assert.assertEquals(""UTF-8"", Headers.extractQuotedValueFromHeader(""text/html;\tcharset=UTF-8"", ""charset""));
    }
"
"    @Test
    public void testSimplePrefixCase() {

        PathMatcher<String> pathMatcher = new PathMatcher<>();

        pathMatcher.addPrefixPath(""prefix"", ""response"");
        Assert.assertEquals(""response"", pathMatcher.getPrefixPath(""prefix""));
        Assert.assertEquals(""response"", pathMatcher.getPrefixPath(""/prefix""));
        Assert.assertEquals(""response"", pathMatcher.getPrefixPath(""/prefix/""));

        pathMatcher.addPrefixPath(""/prefix"", ""new response"");
        Assert.assertEquals(""new response"", pathMatcher.getPrefixPath(""prefix""));
        Assert.assertEquals(""new response"", pathMatcher.getPrefixPath(""/prefix""));
        Assert.assertEquals(""new response"", pathMatcher.getPrefixPath(""/prefix/""));

        pathMatcher.addPrefixPath(""/prefix/"", ""different response"");
        Assert.assertEquals(""different response"", pathMatcher.getPrefixPath(""prefix""));
        Assert.assertEquals(""different response"", pathMatcher.getPrefixPath(""/prefix""));
        Assert.assertEquals(""different response"", pathMatcher.getPrefixPath(""/prefix/""));

        pathMatcher.addPrefixPath(""/prefix//////////////////////"", ""last response"");
        Assert.assertEquals(""last response"", pathMatcher.getPrefixPath(""prefix""));
        Assert.assertEquals(""last response"", pathMatcher.getPrefixPath(""/prefix""));
        Assert.assertEquals(""last response"", pathMatcher.getPrefixPath(""/prefix/""));

        pathMatcher.clearPaths();
        Assert.assertNull(pathMatcher.getPrefixPath(""prefix""));
        Assert.assertNull(pathMatcher.getPrefixPath(""/prefix""));
        Assert.assertNull(pathMatcher.getPrefixPath(""/prefix/""));
    }
"
"    @Test
    public void testSimpleMatchCase() {

        PathMatcher<String> pathMatcher = new PathMatcher<>();

        pathMatcher.addPrefixPath(""prefix"", ""response"");
        Assert.assertEquals(""response"", pathMatcher.match(""/prefix"").getValue());
        Assert.assertEquals(""response"", pathMatcher.match(""/prefix/"").getValue());

        pathMatcher.addPrefixPath(""/prefix"", ""new response"");
        Assert.assertEquals(""new response"", pathMatcher.match(""/prefix"").getValue());
        Assert.assertEquals(""new response"", pathMatcher.match(""/prefix/"").getValue());

        pathMatcher.addPrefixPath(""/prefix/"", ""different response"");
        Assert.assertEquals(""different response"", pathMatcher.match(""/prefix"").getValue());
        Assert.assertEquals(""different response"", pathMatcher.match(""/prefix/"").getValue());

        pathMatcher.addPrefixPath(""/prefix//////////////////////"", ""last response"");
        Assert.assertEquals(""last response"", pathMatcher.match(""/prefix"").getValue());
        Assert.assertEquals(""last response"", pathMatcher.match(""/prefix/"").getValue());

        pathMatcher.clearPaths();
        Assert.assertNull(pathMatcher.match(""/prefix"").getValue());
        Assert.assertNull(pathMatcher.match(""/prefix/"").getValue());
    }
"
"    @Test
    public void testSimpleDefaultCase() {

        PathMatcher<String> pathMatcher = new PathMatcher<>();

        pathMatcher.addPrefixPath(""/"", ""default"");
        Assert.assertEquals(""default"", pathMatcher.getPrefixPath(""/""));
        Assert.assertEquals(""default"", pathMatcher.match(""/"").getValue());

        pathMatcher.addPrefixPath(""//////"", ""needs normalize default"");
        Assert.assertEquals(""needs normalize default"", pathMatcher.getPrefixPath(""/""));
        Assert.assertEquals(""needs normalize default"", pathMatcher.match(""/"").getValue());

        pathMatcher.clearPaths();
        Assert.assertNull(pathMatcher.getPrefixPath(""/""));
    }
"
"    @Test
    public void testDefaultFallthrough() {

        PathMatcher<String> pathMatcher = new PathMatcher<>(""default"");

        // check defaults
        Assert.assertEquals(""default"", pathMatcher.getPrefixPath(""/""));
        Assert.assertEquals(""default"", pathMatcher.match(""/"").getValue());

        // add a few items
        pathMatcher.addPrefixPath(""/test1"", ""test1"");
        pathMatcher.addPrefixPath(""/test2"", ""test2"");
        pathMatcher.addPrefixPath(""/test3"", ""test3"");
        pathMatcher.addPrefixPath(""/test4"", ""test4"");

        // check matching with no matches
        Assert.assertEquals(""default"", pathMatcher.match(""/adsfasdfdsaf"").getValue());
        Assert.assertEquals(""default"", pathMatcher.match(""/   "").getValue());
        Assert.assertEquals(""default"", pathMatcher.match(""/drooadfas"").getValue());
        Assert.assertEquals(""default"", pathMatcher.match(""/thing/thing"").getValue());
        Assert.assertEquals(""default"", pathMatcher.match("""").getValue());

        // check that matching actual matches still works
        Assert.assertEquals(""test1"", pathMatcher.match(""/test1"").getValue());
        Assert.assertEquals(""test2"", pathMatcher.match(""/test2"").getValue());
        Assert.assertEquals(""test3"", pathMatcher.match(""/test3"").getValue());
        Assert.assertEquals(""test4"", pathMatcher.match(""/test4"").getValue());
    }
"
"    @Test
    public void testDecodingWithEncodedAndDecodedSlashAndSlashDecodingDisabled() throws Exception {
        String url = ""http://localhost:3001/by-path/wild%20card/wild%28west%29/wild"" + spaceCode + ""wolf"";

        final String result = URLUtils.decode(url, Charset.defaultCharset().name(), false, new StringBuilder());
        assertEquals(""http://localhost:3001/by-path/wild card/wild(west)/wild"" + spaceCode + ""wolf"", result);
    }
"
"    @Test
    public void testDecodingURLMustNotMutateSpaceSymbolsCaseIfSpaceDecodingDisabled() throws Exception {
        final String url = ""http://localhost:3001/wild"" + spaceCode + ""west"";

        final String result = URLUtils.decode(url, Charset.defaultCharset().name(), false, new StringBuilder());
        assertEquals(url, result);
    }
"
"    @Test
    public void testIsAbsoluteUrlRecognizingAbsolutUrls() {
        assertTrue(URLUtils.isAbsoluteUrl(""https://some.valid.url:8080/path?query=val""));
        assertTrue(URLUtils.isAbsoluteUrl(""http://some.valid.url:8080/path?query=val""));
        assertTrue(URLUtils.isAbsoluteUrl(""http://some.valid.url""));
    }
"
"    @Test
    public void testIsAbsoluteUrlRecognizingAppUrls() {
        assertTrue(URLUtils.isAbsoluteUrl(""com.example.app:/oauth2redirect/example-provider""));
        assertTrue(URLUtils.isAbsoluteUrl(""com.example.app:/oauth2redirect/example-provider?query=val""));
    }
"
"    @Test
    public void testIsAbsoluteUrlRecognizingRelativeUrls() {
        assertFalse(URLUtils.isAbsoluteUrl(""relative""));
        assertFalse(URLUtils.isAbsoluteUrl(""relative/path""));
        assertFalse(URLUtils.isAbsoluteUrl(""relative/path?query=val""));
        assertFalse(URLUtils.isAbsoluteUrl(""/root/relative/path""));
    }
"
"    @Test
    public void testIsAbsoluteUrlRecognizingEmptyOrNullAsRelative() {
        assertFalse(URLUtils.isAbsoluteUrl(null));
        assertFalse(URLUtils.isAbsoluteUrl(""""));
    }
"
"    @Test
    public void testIsAbsoluteUrlIgnoresSyntaxErrorsAreNotAbsolute() {
        assertFalse(URLUtils.isAbsoluteUrl("":""));
    }
"
"    @Test
    public void testParsingSetCookieHeaderV0() {

        Cookie cookie = Cookies.parseSetCookieHeader(""CUSTOMER=WILE_E_COYOTE; path=/; expires=Wednesday, 09-Nov-99 23:12:40 GMT"");
        Assert.assertEquals(""CUSTOMER"", cookie.getName());
        Assert.assertEquals(""WILE_E_COYOTE"", cookie.getValue());
        Assert.assertEquals(""/"", cookie.getPath());
        Assert.assertEquals(date(1999, 11, 9, 23, 12, 40), cookie.getExpires());


        cookie = Cookies.parseSetCookieHeader(""SHIPPING=FEDEX; path=/foo; secure"");
        Assert.assertEquals(""SHIPPING"", cookie.getName());
        Assert.assertEquals(""FEDEX"", cookie.getValue());
        Assert.assertEquals(""/foo"", cookie.getPath());
        Assert.assertTrue(cookie.isSecure());

        cookie = Cookies.parseSetCookieHeader(""SHIPPING=FEDEX"");
        Assert.assertEquals(""SHIPPING"", cookie.getName());
        Assert.assertEquals(""FEDEX"", cookie.getValue());
    }
"
"    @Test
    public void testParsingSetCookieHeaderV1() {
        Cookie cookie = Cookies.parseSetCookieHeader(""Customer=\""WILE_E_COYOTE\""; Version=\""1\""; Path=\""/acme\"""");
        Assert.assertEquals(""Customer"", cookie.getName());
        Assert.assertEquals(""WILE_E_COYOTE"", cookie.getValue());
        Assert.assertEquals(""/acme"", cookie.getPath());
        Assert.assertEquals(1, cookie.getVersion());


        cookie = Cookies.parseSetCookieHeader(""SHIPPING=\""FEDEX\""; path=\""/foo\""; secure; Version=\""1\"";"");
        Assert.assertEquals(""SHIPPING"", cookie.getName());
        Assert.assertEquals(""FEDEX"", cookie.getValue());
        Assert.assertEquals(""/foo"", cookie.getPath());
        Assert.assertTrue(cookie.isSecure());
        Assert.assertEquals(1, cookie.getVersion());
    }
"
"    @Test
    public void testInvalidCookie() {
        Map<String, Cookie> cookies = Cookies.parseRequestCookies(1, false, Arrays.asList(""\""; CUSTOMER=WILE_E_COYOTE""));

        Assert.assertFalse(cookies.containsKey(""$Domain""));
        Assert.assertFalse(cookies.containsKey(""$Version""));
        Assert.assertFalse(cookies.containsKey(""$Path""));

        Cookie cookie = cookies.get(""CUSTOMER"");
        Assert.assertEquals(""CUSTOMER"", cookie.getName());
        Assert.assertEquals(""WILE_E_COYOTE"", cookie.getValue());

        cookies = Cookies.parseRequestCookies(1, false, Arrays.asList(""; CUSTOMER=WILE_E_COYOTE""));

        cookie = cookies.get(""CUSTOMER"");
        Assert.assertEquals(""CUSTOMER"", cookie.getName());
        Assert.assertEquals(""WILE_E_COYOTE"", cookie.getValue());

        cookies = Cookies.parseRequestCookies(1, false, Arrays.asList(""foobar; CUSTOMER=WILE_E_COYOTE""));

        cookie = cookies.get(""CUSTOMER"");
        Assert.assertEquals(""CUSTOMER"", cookie.getName());
        Assert.assertEquals(""WILE_E_COYOTE"", cookie.getValue());
    }
"
"    @Test
    public void testRequestCookieDomainPathVersion() {
        Map<String, Cookie> cookies = Cookies.parseRequestCookies(1, false, Arrays.asList(
                ""CUSTOMER=WILE_E_COYOTE; $Domain=LOONEY_TUNES; $Version=1; $Path=/""));

        Assert.assertFalse(cookies.containsKey(""$Domain""));
        Assert.assertFalse(cookies.containsKey(""$Version""));
        Assert.assertFalse(cookies.containsKey(""$Path""));

        Cookie cookie = cookies.get(""CUSTOMER"");
        Assert.assertEquals(""CUSTOMER"", cookie.getName());
        Assert.assertEquals(""WILE_E_COYOTE"", cookie.getValue());
        Assert.assertEquals(""LOONEY_TUNES"", cookie.getDomain());
        Assert.assertEquals(1, cookie.getVersion());
        Assert.assertEquals(""/"", cookie.getPath());
    }
"
"    @Test
    public void testMultipleRequestCookies() {
        Map<String, Cookie> cookies = Cookies.parseRequestCookies(2, false, Arrays.asList(
                ""CUSTOMER=WILE_E_COYOTE; $Domain=LOONEY_TUNES; $Version=1; $Path=/; SHIPPING=FEDEX""));

        Cookie cookie = cookies.get(""CUSTOMER"");
        Assert.assertEquals(""CUSTOMER"", cookie.getName());
        Assert.assertEquals(""WILE_E_COYOTE"", cookie.getValue());
        Assert.assertEquals(""LOONEY_TUNES"", cookie.getDomain());
        Assert.assertEquals(1, cookie.getVersion());
        Assert.assertEquals(""/"", cookie.getPath());

        cookie = cookies.get(""SHIPPING"");
        Assert.assertEquals(""SHIPPING"", cookie.getName());
        Assert.assertEquals(""FEDEX"", cookie.getValue());
        Assert.assertEquals(""LOONEY_TUNES"", cookie.getDomain());
        Assert.assertEquals(1, cookie.getVersion());
        Assert.assertEquals(""/"", cookie.getPath());
    }
"
"    @Test
    public void testEqualsInValueNotAllowed() {
        Map<String, Cookie> cookies = Cookies.parseRequestCookies(2, false, Arrays.asList(""CUSTOMER=WILE_E_COYOTE=THE_COYOTE; SHIPPING=FEDEX""));
        Cookie cookie = cookies.get(""CUSTOMER"");
        Assert.assertNotNull(cookie);
        Assert.assertEquals(""WILE_E_COYOTE"", cookie.getValue());
        cookie = cookies.get(""SHIPPING"");
        Assert.assertNotNull(cookie);
        Assert.assertEquals(""FEDEX"", cookie.getValue());
    }
"
"    @Test
    public void testEmptyCookieNames() {
        Map<String, Cookie> cookies = Cookies.parseRequestCookies(4, false, Arrays.asList(""=foo; CUSTOMER=WILE_E_COYOTE=THE_COYOTE; =foobar; SHIPPING=FEDEX; =bar""));
        Cookie cookie = cookies.get(""CUSTOMER"");
        Assert.assertNotNull(cookie);
        Assert.assertEquals(""WILE_E_COYOTE"", cookie.getValue());
        cookie = cookies.get(""SHIPPING"");
        Assert.assertNotNull(cookie);
        Assert.assertEquals(""FEDEX"", cookie.getValue());
        cookie = cookies.get("""");
        Assert.assertNotNull(cookie);
        Assert.assertEquals(""foo"", cookie.getValue());
    }
"
"    @Test
    public void testEqualsInValueAllowed() {
        Map<String, Cookie> cookies = Cookies.parseRequestCookies(1, true, Arrays.asList(""CUSTOMER=WILE_E_COYOTE=THE_COYOTE""));
        Cookie cookie = cookies.get(""CUSTOMER"");
        Assert.assertNotNull(cookie);
        Assert.assertEquals(""WILE_E_COYOTE=THE_COYOTE"", cookie.getValue());
    }
"
"    @Test
    public void testEqualsInValueAllowedInQuotedValue() {
        Map<String, Cookie> cookies = Cookies.parseRequestCookies(2, true, Arrays.asList(""CUSTOMER=\""WILE_E_COYOTE=THE_COYOTE\""; SHIPPING=FEDEX"" ));
        Assert.assertEquals(2, cookies.size());
        Cookie cookie = cookies.get(""CUSTOMER"");
        Assert.assertNotNull(cookie);
        Assert.assertEquals(""WILE_E_COYOTE=THE_COYOTE"", cookie.getValue());
        cookie = cookies.get(""SHIPPING"");
        Assert.assertNotNull(cookie);
        Assert.assertEquals(""FEDEX"", cookie.getValue());
    }
"
"    @Test
    public void testEqualsInValueNotAllowedInQuotedValue() {
        Map<String, Cookie> cookies = Cookies.parseRequestCookies(2, false, Arrays.asList(""CUSTOMER=\""WILE_E_COYOTE=THE_COYOTE\""; SHIPPING=FEDEX"" ));
        Assert.assertEquals(2, cookies.size());
        Cookie cookie = cookies.get(""CUSTOMER"");
        Assert.assertNotNull(cookie);
        Assert.assertEquals(""WILE_E_COYOTE=THE_COYOTE"", cookie.getValue());
        cookie = cookies.get(""SHIPPING"");
        Assert.assertNotNull(cookie);
        Assert.assertEquals(""FEDEX"", cookie.getValue());
    }
"
"    @Test
    public void testCommaSeparatedCookies() {
        Map<String, Cookie> cookies = Cookies.parseRequestCookies(2, false, Arrays.asList(""CUSTOMER=\""WILE_E_COYOTE\"", SHIPPING=FEDEX"" ), true);
        Assert.assertEquals(2, cookies.size());
        Cookie cookie = cookies.get(""CUSTOMER"");
        Assert.assertNotNull(cookie);
        Assert.assertEquals(""WILE_E_COYOTE"", cookie.getValue());
        cookie = cookies.get(""SHIPPING"");
        Assert.assertNotNull(cookie);
        Assert.assertEquals(""FEDEX"", cookie.getValue());

        //also make sure semi colon works as normal
        cookies = Cookies.parseRequestCookies(2, false, Arrays.asList(""CUSTOMER=\""WILE_E_COYOTE\""; SHIPPING=FEDEX"" ), true);
        Assert.assertEquals(2, cookies.size());
        cookie = cookies.get(""CUSTOMER"");
        Assert.assertNotNull(cookie);
        Assert.assertEquals(""WILE_E_COYOTE"", cookie.getValue());
        cookie = cookies.get(""SHIPPING"");
        Assert.assertNotNull(cookie);
        Assert.assertEquals(""FEDEX"", cookie.getValue());
    }
"
"    @Test
    public void testSimpleJSONObjectInRequestCookies() {
        Map<String, Cookie> cookies = Cookies.parseRequestCookies(2, true, Arrays.asList(
                ""CUSTOMER={\""v1\"":1, \""id\"":\""some_unique_id\"", \""c\"":\""http://www.google.com?q=love me\""};""
                + "" $Domain=LOONEY_TUNES; $Version=1; $Path=/; SHIPPING=FEDEX""));

        Cookie cookie = cookies.get(""CUSTOMER"");
        Assert.assertEquals(""CUSTOMER"", cookie.getName());
        Assert.assertEquals(""{\""v1\"":1, \""id\"":\""some_unique_id\"", \""c\"":\""http://www.google.com?q=love me\""}"",
               cookie.getValue());
        Assert.assertEquals(""LOONEY_TUNES"", cookie.getDomain());
        Assert.assertEquals(1, cookie.getVersion());
        Assert.assertEquals(""/"", cookie.getPath());

        cookie = cookies.get(""SHIPPING"");
        Assert.assertEquals(""SHIPPING"", cookie.getName());
        Assert.assertEquals(""FEDEX"", cookie.getValue());
        Assert.assertEquals(""LOONEY_TUNES"", cookie.getDomain());
        Assert.assertEquals(1, cookie.getVersion());
        Assert.assertEquals(""/"", cookie.getPath());
    }
"
"    @Test
    public void deleteOrderTest() {
        String orderId = null;
        //api.deleteOrder(orderId);
        
        // TODO: test validations
        
        
    }
"
"    @Test
    public void getInventoryTest() {
        //Map<String, Integer> response = api.getInventory();
        //assertNotNull(response);
        // TODO: test validations
        
        
    }
"
"    @Test
    public void getOrderByIdTest() {
        Long orderId = null;
        //Order response = api.getOrderById(orderId);
        //assertNotNull(response);
        // TODO: test validations
        
        
    }
"
"    @Test
    public void placeOrderTest() {
        Order body = null;
        //Order response = api.placeOrder(body);
        //assertNotNull(response);
        // TODO: test validations
        
        
    }
"
"    @Test
    public void addPetTest() {
        Pet body = null;
        //api.addPet(body);
        
        // TODO: test validations
        
        
    }
"
"    @Test
    public void deletePetTest() {
        Long petId = null;
        String apiKey = null;
        //api.deletePet(petId, apiKey);
        
        // TODO: test validations
        
        
    }
"
"    @Test
    public void findPetsByStatusTest() {
        List<String> status = null;
        //List<Pet> response = api.findPetsByStatus(status);
        //assertNotNull(response);
        // TODO: test validations
        
        
    }
"
"    @Test
    public void findPetsByTagsTest() {
        List<String> tags = null;
        //List<Pet> response = api.findPetsByTags(tags);
        //assertNotNull(response);
        // TODO: test validations
        
        
    }
"
"    @Test
    public void getPetByIdTest() {
        Long petId = null;
        //Pet response = api.getPetById(petId);
        //assertNotNull(response);
        // TODO: test validations
        
        
    }
"
"    @Test
    public void updatePetTest() {
        Pet body = null;
        //api.updatePet(body);
        
        // TODO: test validations
        
        
    }
"
"    @Test
    public void updatePetWithFormTest() {
        Long petId = null;
        String name = null;
        String status = null;
        //api.updatePetWithForm(petId, name, status);
        
        // TODO: test validations
        
        
    }
"
"    @Test
    public void uploadFileTest() {
        Long petId = null;
        String additionalMetadata = null;
        org.apache.cxf.jaxrs.ext.multipart.Attachment file = null;
        //ModelApiResponse response = api.uploadFile(petId, additionalMetadata, file);
        //assertNotNull(response);
        // TODO: test validations
        
        
    }
"
"    @Test
    public void createUserTest() {
        User body = null;
        //api.createUser(body);
        
        // TODO: test validations
        
        
    }
"
"    @Test
    public void createUsersWithArrayInputTest() {
        List<User> body = null;
        //api.createUsersWithArrayInput(body);
        
        // TODO: test validations
        
        
    }
"
"    @Test
    public void createUsersWithListInputTest() {
        List<User> body = null;
        //api.createUsersWithListInput(body);
        
        // TODO: test validations
        
        
    }
"
"    @Test
    public void deleteUserTest() {
        String username = null;
        //api.deleteUser(username);
        
        // TODO: test validations
        
        
    }
"
"    @Test
    public void getUserByNameTest() {
        String username = null;
        //User response = api.getUserByName(username);
        //assertNotNull(response);
        // TODO: test validations
        
        
    }
"
"    @Test
    public void loginUserTest() {
        String username = null;
        String password = null;
        //String response = api.loginUser(username, password);
        //assertNotNull(response);
        // TODO: test validations
        
        
    }
"
"    @Test
    public void logoutUserTest() {
        //api.logoutUser();
        
        // TODO: test validations
        
        
    }
"
"    @Test
    public void updateUserTest() {
        String username = null;
        User body = null;
        //api.updateUser(username, body);
        
        // TODO: test validations
        
        
    }
"
"    @Test
    public void testTypeHolderDefault() {
        // TODO: test TypeHolderDefault
    }
"
"    @Test
    public void stringItemTest() {
        // TODO: test stringItem
    }
"
"    @Test
    public void numberItemTest() {
        // TODO: test numberItem
    }
"
"    @Test
    public void integerItemTest() {
        // TODO: test integerItem
    }
"
"    @Test
    public void boolItemTest() {
        // TODO: test boolItem
    }
"
"    @Test
    public void arrayItemTest() {
        // TODO: test arrayItem
    }
"
"    @Test
    public void testOuterEnum() {
        // TODO: test OuterEnum
    }
"
"    @Test
    public void testClassModel() {
        // TODO: test ClassModel
    }
"
"    @Test
    public void propertyClassTest() {
        // TODO: test propertyClass
    }
"
"    @Test
    public void testArrayTest() {
        // TODO: test ArrayTest
    }
"
"    @Test
    public void arrayOfStringTest() {
        // TODO: test arrayOfString
    }
"
"    @Test
    public void arrayArrayOfIntegerTest() {
        // TODO: test arrayArrayOfInteger
    }
"
"    @Test
    public void arrayArrayOfModelTest() {
        // TODO: test arrayArrayOfModel
    }
"
"    @Test
    public void testCat() {
        // TODO: test Cat
    }
"
"    @Test
    public void classNameTest() {
        // TODO: test className
    }
"
"    @Test
    public void colorTest() {
        // TODO: test color
    }
"
"    @Test
    public void declawedTest() {
        // TODO: test declawed
    }
"
"    @Test
    public void testXmlItem() {
        // TODO: test XmlItem
    }
"
"    @Test
    public void attributeStringTest() {
        // TODO: test attributeString
    }
"
"    @Test
    public void attributeNumberTest() {
        // TODO: test attributeNumber
    }
"
"    @Test
    public void attributeIntegerTest() {
        // TODO: test attributeInteger
    }
"
"    @Test
    public void attributeBooleanTest() {
        // TODO: test attributeBoolean
    }
"
"    @Test
    public void wrappedArrayTest() {
        // TODO: test wrappedArray
    }
"
"    @Test
    public void nameStringTest() {
        // TODO: test nameString
    }
"
"    @Test
    public void nameNumberTest() {
        // TODO: test nameNumber
    }
"
"    @Test
    public void nameIntegerTest() {
        // TODO: test nameInteger
    }
"
"    @Test
    public void nameBooleanTest() {
        // TODO: test nameBoolean
    }
"
"    @Test
    public void nameArrayTest() {
        // TODO: test nameArray
    }
"
"    @Test
    public void nameWrappedArrayTest() {
        // TODO: test nameWrappedArray
    }
"
"    @Test
    public void prefixStringTest() {
        // TODO: test prefixString
    }
"
"    @Test
    public void prefixNumberTest() {
        // TODO: test prefixNumber
    }
"
"    @Test
    public void prefixIntegerTest() {
        // TODO: test prefixInteger
    }
"
"    @Test
    public void prefixBooleanTest() {
        // TODO: test prefixBoolean
    }
"
"    @Test
    public void prefixArrayTest() {
        // TODO: test prefixArray
    }
"
"    @Test
    public void prefixWrappedArrayTest() {
        // TODO: test prefixWrappedArray
    }
"
"    @Test
    public void namespaceStringTest() {
        // TODO: test namespaceString
    }
"
"    @Test
    public void namespaceNumberTest() {
        // TODO: test namespaceNumber
    }
"
"    @Test
    public void namespaceIntegerTest() {
        // TODO: test namespaceInteger
    }
"
"    @Test
    public void namespaceBooleanTest() {
        // TODO: test namespaceBoolean
    }
"
"    @Test
    public void namespaceArrayTest() {
        // TODO: test namespaceArray
    }
"
"    @Test
    public void namespaceWrappedArrayTest() {
        // TODO: test namespaceWrappedArray
    }
"
"    @Test
    public void prefixNamespaceStringTest() {
        // TODO: test prefixNamespaceString
    }
"
"    @Test
    public void prefixNamespaceNumberTest() {
        // TODO: test prefixNamespaceNumber
    }
"
"    @Test
    public void prefixNamespaceIntegerTest() {
        // TODO: test prefixNamespaceInteger
    }
"
"    @Test
    public void prefixNamespaceBooleanTest() {
        // TODO: test prefixNamespaceBoolean
    }
"
"    @Test
    public void prefixNamespaceArrayTest() {
        // TODO: test prefixNamespaceArray
    }
"
"    @Test
    public void prefixNamespaceWrappedArrayTest() {
        // TODO: test prefixNamespaceWrappedArray
    }
"
"    @Test
    public void testOuterComposite() {
        // TODO: test OuterComposite
    }
"
"    @Test
    public void myNumberTest() {
        // TODO: test myNumber
    }
"
"    @Test
    public void myStringTest() {
        // TODO: test myString
    }
"
"    @Test
    public void myBooleanTest() {
        // TODO: test myBoolean
    }
"
"    @Test
    public void testModelApiResponse() {
        // TODO: test ModelApiResponse
    }
"
"    @Test
    public void codeTest() {
        // TODO: test code
    }
"
"    @Test
    public void typeTest() {
        // TODO: test type
    }
"
"    @Test
    public void messageTest() {
        // TODO: test message
    }
"
"    @Test
    public void testCategory() {
        // TODO: test Category
    }
"
"    @Test
    public void idTest() {
        // TODO: test id
    }
"
"    @Test
    public void nameTest() {
        // TODO: test name
    }
"
"    @Test
    public void testName() {
        // TODO: test Name
    }
"
"    @Test
    public void nameTest() {
        // TODO: test name
    }
"
"    @Test
    public void snakeCaseTest() {
        // TODO: test snakeCase
    }
"
"    @Test
    public void propertyTest() {
        // TODO: test property
    }
"
"    @Test
    public void _123numberTest() {
        // TODO: test _123number
    }
"
"    @Test
    public void testAdditionalPropertiesBoolean() {
        // TODO: test AdditionalPropertiesBoolean
    }
"
"    @Test
    public void nameTest() {
        // TODO: test name
    }
"
"    @Test
    public void testCatAllOf() {
        // TODO: test CatAllOf
    }
"
"    @Test
    public void declawedTest() {
        // TODO: test declawed
    }
"
"    @Test
    public void testFileSchemaTestClass() {
        // TODO: test FileSchemaTestClass
    }
"
"    @Test
    public void fileTest() {
        // TODO: test file
    }
"
"    @Test
    public void filesTest() {
        // TODO: test files
    }
"
"    @Test
    public void testAdditionalPropertiesInteger() {
        // TODO: test AdditionalPropertiesInteger
    }
"
"    @Test
    public void nameTest() {
        // TODO: test name
    }
"
"    @Test
    public void testAdditionalPropertiesObject() {
        // TODO: test AdditionalPropertiesObject
    }
"
"    @Test
    public void nameTest() {
        // TODO: test name
    }
"
"    @Test
    public void testReadOnlyFirst() {
        // TODO: test ReadOnlyFirst
    }
"
"    @Test
    public void barTest() {
        // TODO: test bar
    }
"
"    @Test
    public void bazTest() {
        // TODO: test baz
    }
"
"    @Test
    public void testEnumTest() {
        // TODO: test EnumTest
    }
"
"    @Test
    public void enumStringTest() {
        // TODO: test enumString
    }
"
"    @Test
    public void enumStringRequiredTest() {
        // TODO: test enumStringRequired
    }
"
"	@Test
	public void testDefaultSettings() throws CoreException {
		connector = new SocketListenMultiConnector();
		Map<String, Connector.Argument> defaults = connector.getDefaultArguments();
		assertTrue(defaults.containsKey(""connectionLimit""));
		assertEquals(1, ((Connector.IntegerArgument) defaults.get(""connectionLimit"")).intValue());
	}
"
"	@Test
	public void testDefaultBehaviour() throws CoreException, InterruptedException {
		connector = new SocketListenMultiConnector();
		Map<String, String> arguments = new HashMap<>();
		arguments.put(""port"", Integer.toString(port));
		connector.connect(arguments, new NullProgressMonitor(), launch);
		Thread.sleep(200);

		assertTrue(""first connect should succeed"", connect());
		assertFalse(""second connect should fail"", connect());
	}
"
"	@Test
	public void testSingleConnectionBehaviour() throws CoreException, InterruptedException {
		connector = new SocketListenMultiConnector();
		Map<String, String> arguments = new HashMap<>();
		arguments.put(""port"", Integer.toString(port));
		arguments.put(""connectionLimit"", ""1"");
		connector.connect(arguments, new NullProgressMonitor(), launch);
		Thread.sleep(200);

		assertTrue(""first connect should succeed"", connect());
		assertFalse(""second connect should fail"", connect());
	}
"
"	@Test
	public void testTwoConnectionsBehaviour() throws CoreException, InterruptedException {
		connector = new SocketListenMultiConnector();
		Map<String, String> arguments = new HashMap<>();
		arguments.put(""port"", Integer.toString(port));
		arguments.put(""connectionLimit"", ""2"");
		connector.connect(arguments, new NullProgressMonitor(), launch);
		Thread.sleep(200);

		assertTrue(""first connect should succeed"", connect());
		assertTrue(""second connect should succeed"", connect());
	}
"
"	@Test
	public void testUnlimitedConnectionsBehaviour() throws CoreException, InterruptedException {
		connector = new SocketListenMultiConnector();
		Map<String, String> arguments = new HashMap<>();
		arguments.put(""port"", Integer.toString(port));
		arguments.put(""connectionLimit"", ""0"");
		connector.connect(arguments, new NullProgressMonitor(), launch);
		Thread.sleep(200);

		for (int i = 0; i < 10; i++) {
			assertTrue(""connection "" + i + "" should succeed"", connect());
		}
	}
"
"  @Test
  public void testCloudSdkNotConfigured() {
    Assert.assertEquals(""Deploy failed."", Messages.getString(""deploy.failed.error.message""));
  }
"
"  @Test
  public void testSpecifyVersionTooltip() {
    Assert.assertEquals(
        ""If checked, stops the previously running version when ""
        + ""deploying a new version that receives all traffic."",
        Messages.getString(""tooltip.stop.previous.version""));
  }
"
"  @Test
  public void testAutoSelectSingleAccount() {
    when(loginService.getAccounts()).thenReturn(oneAccountSet);
    deployPanel = createPanel(true /* requireValues */);
    assertThat(deployPanel.getSelectedCredential(), is(credential));

    // verify not in error
    IStatus status = getAccountSelectorValidationStatus();
    assertTrue(""account selector is in error: "" + status.getMessage(), status.isOK());

    assertThat(""auto-selected value should be propagated back to model"",
        deployPanel.model.getAccountEmail(), is(account1.getEmail()));
  }
"
"  @Test
  public void testAutoSelectSingleAccount_loadGcpProjects()
      throws ProjectRepositoryException, InterruptedException {
    when(loginService.getAccounts()).thenReturn(oneAccountSet);
    initializeProjectRepository();
    deployPanel = createPanel(true /* requireValues */);
    assertNotNull(deployPanel.latestGcpProjectQueryJob);
    deployPanel.latestGcpProjectQueryJob.join();

    Table projectTable = getProjectSelector().getViewer().getTable();
    assertThat(projectTable.getItemCount(), is(2));
  }
"
"  @Test
  public void testValidationMessageWhenNotSignedIn() {
    deployPanel = createPanel(true /* requireValues */);
    IStatus status = getAccountSelectorValidationStatus();
    assertThat(status.getMessage(), is(""Sign in to Google.""));
  }
"
"  @Test
  public void testValidationMessageWhenSignedIn() {
    // Return two accounts because the account selector will auto-select if there exists only one.
    when(loginService.getAccounts()).thenReturn(twoAccountSet);

    deployPanel = createPanel(true /* requireValues */);
    IStatus status = getAccountSelectorValidationStatus();
    assertThat(status.getMessage(), is(""Select an account.""));
  }
"
"  @Test
  public void testUncheckStopPreviousVersionButtonWhenDisabled() {
    deployPanel = createPanel(true /* requireValues */);

    Button promoteButton = getButtonWithText(""Promote the deployed version to receive all traffic"");
    Button stopButton = getButtonWithText(""Stop previous version"");
    SWTBotCheckBox promote = new SWTBotCheckBox(promoteButton);
    SWTBotCheckBox stop = new SWTBotCheckBox(stopButton);

    // Initially, everything is checked and enabled.
    assertTrue(promoteButton.getSelection());
    assertTrue(stopButton.getSelection());
    assertTrue(stopButton.getEnabled());

    promote.click();
    assertFalse(promoteButton.getSelection());
    assertFalse(stopButton.getSelection());
    assertFalse(stopButton.getEnabled());

    promote.click();
    assertTrue(promoteButton.getSelection());
    assertTrue(stopButton.getSelection());
    assertTrue(stopButton.getEnabled());

    stop.click();
    assertTrue(promoteButton.getSelection());
    assertFalse(stopButton.getSelection());
    assertTrue(stopButton.getEnabled());

    promote.click();
    assertFalse(promoteButton.getSelection());
    assertFalse(stopButton.getSelection());
    assertFalse(stopButton.getEnabled());

    promote.click();
    assertTrue(promoteButton.getSelection());
    assertFalse(stopButton.getSelection());
    assertTrue(stopButton.getEnabled());
  }
"
"  @Test
  public void testProjectSavedInPreferencesSelected()
      throws ProjectRepositoryException, InterruptedException, BackingStoreException {
    IEclipsePreferences node =
        new ProjectScope(project).getNode(DeployPreferences.PREFERENCE_STORE_QUALIFIER);
    try {
      node.put(""project.id"", ""projectId1"");
      node.put(""account.email"", EMAIL_1);
      initializeProjectRepository();
      when(loginService.getAccounts()).thenReturn(twoAccountSet);
      deployPanel = createPanel(true /* requireValues */);
      deployPanel.latestGcpProjectQueryJob.join();

      ProjectSelector projectSelector = getProjectSelector();
      IStructuredSelection selection = projectSelector.getViewer().getStructuredSelection();
      assertThat(selection.size(), is(1));
      assertThat(((GcpProject) selection.getFirstElement()).getId(), is(""projectId1""));
    } finally {
      node.clear();
    }
  }
"
"  @Test
  public void testProjectNotSelectedIsAnErrorWhenRequireValuesIsTrue() {
    deployPanel = createPanel(true /* requireValues */);
    assertThat(getProjectSelectionValidator().getSeverity(), is(IStatus.ERROR));
  }
"
"  @Test
  public void testProjectNotSelectedIsNotAnErrorWhenRequireValuesIsFalse() {
    deployPanel = createPanel(false /* requireValues */);
    assertThat(getProjectSelectionValidator().getSeverity(), is(IStatus.INFO));
  }
"
"  @Test
  public void testProjectsExistThenNoProjectNotFoundError()
      throws ProjectRepositoryException, InterruptedException {
    when(loginService.getAccounts()).thenReturn(oneAccountSet);
    initializeProjectRepository();
    deployPanel = createPanel(false /* requireValues */);
    selectAccount(account1);
    deployPanel.latestGcpProjectQueryJob.join();
    assertThat(getProjectSelectionValidator().getSeverity(), is(IStatus.OK));
  }
"
"  @Test
  public void testRefreshProjectsForSelectedCredential()
      throws ProjectRepositoryException, InterruptedException {
    when(loginService.getAccounts()).thenReturn(twoAccountSet);
    initializeProjectRepository();

    deployPanel = createPanel(false /* requireValues */);
    Table projectTable = getProjectSelector().getViewer().getTable();
    assertNull(deployPanel.latestGcpProjectQueryJob);
    assertThat(projectTable.getItemCount(), is(0));

    selectAccount(account1);
    assertNotNull(deployPanel.latestGcpProjectQueryJob);
    deployPanel.latestGcpProjectQueryJob.join();
    assertThat(projectTable.getItemCount(), is(2));
    assertThat(((GcpProject) projectTable.getItem(0).getData()).getId(), is(""projectId1""));
    assertThat(((GcpProject) projectTable.getItem(1).getData()).getId(), is(""projectId2""));
  }
"
"  @Test
  public void testRefreshProjectsForSelectedCredential_switchAccounts()
      throws ProjectRepositoryException, InterruptedException {
    when(loginService.getAccounts()).thenReturn(twoAccountSet);
    initializeProjectRepository();

    deployPanel = createPanel(false /* requireValues */);
    Table projectTable = getProjectSelector().getViewer().getTable();
    assertNull(deployPanel.latestGcpProjectQueryJob);
    assertThat(projectTable.getItemCount(), is(0));

    selectAccount(account1);
    Job jobForAccount1 = deployPanel.latestGcpProjectQueryJob;
    jobForAccount1.join();
    assertThat(projectTable.getItemCount(), is(2));

    selectAccount(account2);
    assertNotEquals(jobForAccount1, deployPanel.latestGcpProjectQueryJob);
    deployPanel.latestGcpProjectQueryJob.join();
    assertThat(projectTable.getItemCount(), is(1));
    assertThat(((GcpProject) projectTable.getItem(0).getData()).getId(), is(""projectId2""));
  }
"
"  @Test
  public void testNoProjectSelectedWhenSwitchingAccounts()
      throws ProjectRepositoryException, InterruptedException {
    when(loginService.getAccounts()).thenReturn(twoAccountSet);
    initializeProjectRepository();

    deployPanel = createPanel(false /* requireValues */);
    selectAccount(account1);
    deployPanel.latestGcpProjectQueryJob.join();

    Table projectTable = getProjectSelector().getViewer().getTable();
    assertThat(projectTable.getItemCount(), is(2));
    projectTable.setSelection(0);
    assertThat(projectTable.getSelectionCount(), is(1));

    selectAccount(account2);
    deployPanel.latestGcpProjectQueryJob.join();

    assertThat(projectTable.getItemCount(), is(1));
    assertThat(projectTable.getSelectionCount(), is(0));
  }
"
"  @Test
  public void testLimitedVisibility() {
    NodeList pages = getDocument().getElementsByTagName(""page"");
    Assert.assertEquals(2, pages.getLength());
    NodeList enabledWhen = getDocument().getElementsByTagName(""enabledWhen"");
    Assert.assertEquals(4, enabledWhen.getLength());
    NodeList tests = getDocument().getElementsByTagName(""test"");
    Assert.assertEquals(4, tests.getLength());
    NodeList adapts = getDocument().getElementsByTagName(""adapt"");
    Assert.assertEquals(4, adapts.getLength());

    for (int i = 0; i < enabledWhen.getLength(); i++) {
      Element element = (Element) enabledWhen.item(i);
      Node parent = element.getParentNode();
      assertThat(parent.getNodeName(), either(is(""page"")).or(is(""handler"")));
    }

    Element standardAdapt = (Element) adapts.item(0);
    verifyAdapt(standardAdapt, AppEngineStandardFacet.ID);
    Element flexAdapt = (Element) adapts.item(1);
    verifyAdapt(flexAdapt, AppEngineFlexFacet.ID);
  }
"
"  @Test(expected = NullPointerException.class)
  public void testNullCredential() {
    new GcpProjectQueryJob(null /* credential */, projectRepository, projectSelector,
        dataBindingContext, isLatestQueryJob);
  }
"
"  @Test
  public void testRun_setsProjects() throws InterruptedException, ProjectRepositoryException {
    queryJob.schedule();
    queryJob.join();

    verify(projectRepository).getProjects(credential);
    verify(isLatestQueryJob).apply(queryJob);
    verify(projectSelector).isDisposed();
    verify(projectSelector).setProjects(projects);
  }
"
"  @Test
  public void testRun_abandonIfDisposed() throws InterruptedException, ProjectRepositoryException {
    when(projectSelector.isDisposed()).thenReturn(true);

    queryJob.schedule();
    queryJob.join();

    verify(projectRepository).getProjects(credential);
    verify(projectSelector, never()).setProjects(projects);
  }
"
"  @Test
  public void testRun_abandonIfNotLatestJob()
      throws InterruptedException, ProjectRepositoryException {
    when(isLatestQueryJob.apply(queryJob)).thenReturn(false);

    queryJob.schedule();
    queryJob.join();

    verify(projectRepository).getProjects(credential);
    verify(projectSelector, never()).setProjects(projects);
  }
"
"  @Test
  public void testRun_abandonStaleJob() throws InterruptedException, ProjectRepositoryException {
    // Prepare another concurrent query job.
    Credential staleCredential = mock(Credential.class);

    List<GcpProject> anotherProjectList = mock(List.class);
    ProjectRepository projectRepository2 = mock(ProjectRepository.class);
    when(projectRepository2.getProjects(staleCredential)).thenReturn(anotherProjectList);

    Predicate<Job> notLatest = mock(Predicate.class);
    Job staleJob = new GcpProjectQueryJob(staleCredential, projectRepository2,
        projectSelector, dataBindingContext, notLatest);

    // This second job is stale, i.e., it was fired, but user has selected another credential.
    when(notLatest.apply(staleJob)).thenReturn(false);

    queryJob.schedule();
    queryJob.join();
    // Make the stale job complete even after ""queryJob"" finishes.
    staleJob.schedule();
    staleJob.join();

    verify(projectRepository).getProjects(credential);
    verify(projectRepository2).getProjects(staleCredential);

    verify(projectSelector).setProjects(projects);
    verify(projectSelector, never()).setProjects(anotherProjectList);
  }
"
"  @Test
  public void testGetHelpContextId() {
    assertNull(new BlankDeployPreferencesPanel(shellTestResource.getShell()).getHelpContextId());
  }
"
"  @Test
  public void testCorrectPanelIsShownForFacetedProject() {
    DeployPropertyPage page = new DeployPropertyPage(loginService, googleApiFactory);
    Shell parent = shellTestResource.getShell();
    page.setElement(getProject());
    page.createControl(parent);
    page.setVisible(true);
    Composite preferencePageComposite = (Composite) parent.getChildren()[0];
    for (Control control : preferencePageComposite.getChildren()) {
      if (control instanceof Composite) {
        Composite maybeDeployPageComposite = (Composite) control;
        Layout layout = maybeDeployPageComposite.getLayout();
        if (layout instanceof StackLayout) {
          StackLayout stackLayout = (StackLayout) layout;
          assertThat(stackLayout.topControl, instanceOf(getPanelClass()));
          return;
        }
      }
    }
    fail(""Did not find the deploy preferences panel"");
  }
"
"  @Test
  public void testGetHelpContextId() {
    IProject project = mock(IProject.class);
    when(project.getName()).thenReturn("""");
    StandardDeployPreferencesPanel panel = new StandardDeployPreferencesPanel(
        shellResource.getShell(), project, mock(IGoogleLoginService.class), mock(Runnable.class),
        false, mock(ProjectRepository.class));

    assertEquals(
        ""com.google.cloud.tools.eclipse.appengine.deploy.ui.DeployAppEngineStandardProjectContext"",
        panel.getHelpContextId());
  }
"
"  @Test
  public void testGetHelpContextId() {
    FlexDeployPreferencesPanel panel = createPanel(true /* requireValues */);

    assertEquals(
        ""com.google.cloud.tools.eclipse.appengine.deploy.ui.DeployAppEngineFlexProjectContext"",
        panel.getHelpContextId());
  }
"
"  @Test
  public void testDefaultAppYamlPathSet() {
    FlexDeployPreferencesPanel panel = createPanel(true /* requireValues */);

    Text appYamlField = findAppYamlField(panel);
    assertEquals(""src/main/appengine/app.yaml"", appYamlField.getText());
    assertTrue(getAppYamlPathValidationStatus(panel).isOK());
  }
"
"  @Test
  public void testAppYamlPathValidation_nonExistingAppYaml() {
    FlexDeployPreferencesPanel panel = createPanel(true /* requireValues */);

    Text appYamlField = findAppYamlField(panel);
    appYamlField.setText(""non/existing/app.yaml"");
    assertFalse(getAppYamlPathValidationStatus(panel).isOK());
  }
"
"  @Test
  public void testAppYamlPathValidation_noValidationIfRequireValuesIsFalse() {
    FlexDeployPreferencesPanel panel = createPanel(false /* requireValues */);

    Text appYamlField = findAppYamlField(panel);
    appYamlField.setText(""non/existing/app.yaml"");
    assertNull(getAppYamlPathValidationStatus(panel));
  }
"
"  @Test
  public void testAppYamlPathValidation_absolutePathWorks() {
    FlexDeployPreferencesPanel panel = createPanel(true /* requireValues */);
    Text appYamlField = findAppYamlField(panel);

    IPath absolutePath = project.getLocation().append(""src/main/appengine/app.yaml"");
    assertTrue(absolutePath.isAbsolute());

    appYamlField.setText(absolutePath.toString());
    assertTrue(getAppYamlPathValidationStatus(panel).isOK());
  }
"
"  @Test
  public void testFlexPricingLabel() {
    dialog.setBlockOnOpen(false);
    dialog.open();
    Composite dialogArea = (Composite) dialog.createDialogArea(shellResource.getShell());

    assertNotNull(findGcpPricingLink(dialogArea));
  }
"
"  @Test
  public void testContructor_nonAbsoluteBasePath() {
    try {
      when(appYamlPath.getValue()).thenReturn(""app.yaml"");
      new AppYamlValidator(new Path(""non/absolute/base/path""), appYamlPath);
      fail();
    } catch (IllegalArgumentException ex) {
      assertEquals(""basePath is not absolute."", ex.getMessage());
    }
  }
"
"  @Test
  public void testValidate_relativePathAndNoAppYaml() {
    when(appYamlPath.getValue()).thenReturn(""relative/path/app.yaml"");

    IStatus result = pathValidator.validate();
    assertEquals(IStatus.ERROR, result.getSeverity());
    assertEquals(""app.yaml does not exist."", result.getMessage());
  }
"
"  @Test
  public void testValidate_absolutePathAndNoAppYaml() {
    String absolutePath = basePath + ""/sub/directory/app.yaml"";
    when(appYamlPath.getValue()).thenReturn(absolutePath);

    IStatus result = pathValidator.validate();
    assertEquals(IStatus.ERROR, result.getSeverity());
    assertEquals(""app.yaml does not exist."", result.getMessage());
  }
"
"  @Test
  public void testValidate_relativePathAndInvalidFileName() {
    when(appYamlPath.getValue()).thenReturn(""relative/path/my-app.yaml"");

    IStatus result = pathValidator.validate();
    assertEquals(IStatus.ERROR, result.getSeverity());
    assertEquals(""File name is not app.yaml: ""
        + new Path(basePath + ""/relative/path/my-app.yaml"").toOSString(),
        result.getMessage());
  }
"
"  @Test
  public void testValidate_absolutePathInvalidFileName() {
    String absolutePath = basePath + ""/sub/directory/my-app.yaml"";
    when(appYamlPath.getValue()).thenReturn(absolutePath);

    IStatus result = pathValidator.validate();
    assertEquals(IStatus.ERROR, result.getSeverity());
    assertEquals(""File name is not app.yaml: ""
        + new Path(basePath + ""/sub/directory/my-app.yaml"").toOSString(),
        result.getMessage());
  }
"
"  @Test
  public void testValidate_relativePathNotFile() {
    createAppYamlAsDirectory(basePath);
    when(appYamlPath.getValue()).thenReturn(""app.yaml"");

    IStatus result = pathValidator.validate();
    assertEquals(IStatus.ERROR, result.getSeverity());
    assertEquals(""Not a file: "" + new Path(basePath + ""/app.yaml"").toOSString(),
        result.getMessage());
  }
"
"  @Test
  public void testValidate_absolutePathNotFile() {
    createAppYamlAsDirectory(basePath);

    String absolutePath = basePath + ""/app.yaml"";
    when(appYamlPath.getValue()).thenReturn(absolutePath);

    IStatus result = pathValidator.validate();
    assertEquals(IStatus.ERROR, result.getSeverity());
    assertEquals(""Not a file: "" + new Path(basePath + ""/app.yaml"").toOSString(),
        result.getMessage());
  }
"
"  @Test
  public void testValidate_relativePathWithAppYaml() throws IOException {
    createAppYamlFile(basePath + ""/some/directory"", ""runtime: java"");

    when(appYamlPath.getValue()).thenReturn(""some/directory/app.yaml"");
    IStatus result = pathValidator.validate();
    assertTrue(result.isOK());
  }
"
"  @Test
  public void testValidate_absolutePathWithAppYaml() throws IOException {
    File absolutePath = tempFolder.newFolder(""another"", ""folder"");
    File appYaml = createAppYamlFile(absolutePath.toString(), ""runtime: java"");

    when(appYamlPath.getValue()).thenReturn(appYaml.toString());
    IStatus result = pathValidator.validate();
    assertTrue(result.isOK());
  }
"
"  @Test
  public void testValidateRuntime_javaRuntime() throws IOException {
    File appYaml = createAppYamlFile(tempFolder.getRoot().toString(), ""runtime: java"");
    IStatus result = AppYamlValidator.validateRuntime(appYaml);
    assertTrue(result.isOK());
  }
"
"  @Test
  public void testValidateRuntime_malformedAppYaml() throws IOException {
    File appYaml = createAppYamlFile(tempFolder.getRoot().toString(), "": m a l f o r m e d !"");
    IStatus result = AppYamlValidator.validateRuntime(appYaml);
    assertEquals(IStatus.ERROR, result.getSeverity());
    assertEquals(""Malformed app.yaml."", result.getMessage());
  }
"
"  @Test
  public void testValidateRuntime_noRuntime() throws IOException {
    File appYaml = createAppYamlFile(tempFolder.getRoot().toString(), ""env: flex"");
    IStatus result = AppYamlValidator.validateRuntime(appYaml);
    assertEquals(IStatus.ERROR, result.getSeverity());
    assertEquals(""\""runtime: null\"" in app.yaml is not \""java\""."", result.getMessage());
  }
"
"  @Test
  public void testValidateRuntime_nullRuntime() throws IOException {
    File appYaml = createAppYamlFile(tempFolder.getRoot().toString(), ""runtime:"");
    IStatus result = AppYamlValidator.validateRuntime(appYaml);
    assertEquals(IStatus.ERROR, result.getSeverity());
    assertEquals(""\""runtime: null\"" in app.yaml is not \""java\""."", result.getMessage());
  }
"
"  @Test
  public void testValidateRuntime_notJavaRuntime() throws IOException {
    File appYaml = createAppYamlFile(tempFolder.getRoot().toString(), ""runtime: python"");
    IStatus result = AppYamlValidator.validateRuntime(appYaml);
    assertEquals(IStatus.ERROR, result.getSeverity());
    assertEquals(""\""runtime: python\"" in app.yaml is not \""java\""."", result.getMessage());
  }
"
"  @Test
  public void testValidateRuntime_customRuntime() throws IOException {
    File appYaml = createAppYamlFile(tempFolder.getRoot().toString(), ""runtime: custom"");
    IStatus result = AppYamlValidator.validateRuntime(appYaml);
    assertEquals(IStatus.ERROR, result.getSeverity());
    assertEquals(""\""runtime: custom\"" is not yet supported by Cloud Tools for Eclipse."",
        result.getMessage());
  }
"
"  @Test
  public void testValidateRuntime_ioException() {
    File nonExisting = new File(""/non/existing/file"");
    IStatus result = AppYamlValidator.validateRuntime(nonExisting);
    assertEquals(IStatus.ERROR, result.getSeverity());
    assertTrue(result.getMessage().startsWith(""Cannot read app.yaml:""));
  }
"
"  @Test
  public void testRun_projectHasNoApplication()
      throws ProjectRepositoryException, InterruptedException {
    when(projectRepository.getAppEngineApplication(credential, ""projectId""))
        .thenReturn(AppEngine.NO_APPENGINE_APPLICATION);
    assertNull(project.getAppEngine());

    queryJob.schedule();
    queryJob.join();

    verify(projectRepository).getAppEngineApplication(credential, ""projectId"");
    verify(isLatestQueryJob).apply(queryJob);
    verify(projectSelector).isDisposed();
    verify(projectSelection).isEmpty();
    verify(projectSelector).setStatusLink(EXPECTED_MESSAGE_WHEN_NO_APPLICATION, EXPECTED_LINK);

    assertEquals(AppEngine.NO_APPENGINE_APPLICATION, project.getAppEngine());
  }
"
"  @Test
  public void testRun_projectHasApplication()
      throws ProjectRepositoryException, InterruptedException {
    AppEngine appEngine = AppEngine.withId(""unique-id"");
    when(projectRepository.getAppEngineApplication(credential, ""projectId"")).thenReturn(appEngine);

    queryJob.schedule();
    queryJob.join();

    verify(isLatestQueryJob, never()).apply(queryJob);
    verify(projectSelector, never()).isDisposed();
    verify(projectSelector, never()).setStatusLink(anyString(), anyString());

    assertTrue(appEngine == project.getAppEngine());
  }
"
"  @Test
  public void testRun_queryError() throws ProjectRepositoryException, InterruptedException {
    when(projectRepository.getAppEngineApplication(credential, ""projectId""))
        .thenThrow(new ProjectRepositoryException(""testException""));

    queryJob.schedule();
    queryJob.join();

    verify(isLatestQueryJob).apply(queryJob);
    verify(projectSelector).isDisposed();
    verify(projectSelector).setStatusLink(EXPECTED_MESSAGE_WHEN_EXCEPTION, null);

    assertNull(project.getAppEngine());
  }
"
"  @Test
  public void testRun_abandonIfDisposed() throws InterruptedException, ProjectRepositoryException {
    when(projectSelector.isDisposed()).thenReturn(true);
    when(projectRepository.getAppEngineApplication(credential, ""projectId""))
        .thenReturn(AppEngine.NO_APPENGINE_APPLICATION);

    queryJob.schedule();
    queryJob.join();

    verify(projectSelector).isDisposed();
    verify(projectSelector, never()).setStatusLink(anyString(), anyString());
  }
"
"  @Test
  public void testRun_abandonIfNotLatestJob()
      throws InterruptedException, ProjectRepositoryException {
    when(isLatestQueryJob.apply(queryJob)).thenReturn(false);
    when(projectRepository.getAppEngineApplication(credential, ""projectId""))
        .thenReturn(AppEngine.NO_APPENGINE_APPLICATION);

    queryJob.schedule();
    queryJob.join();

    verify(isLatestQueryJob).apply(queryJob);
    verify(projectSelector, never()).setStatusLink(anyString(), anyString());
  }
"
"  @Test
  public void testRun_abandonIfProjectSelectorHasNoSelection()
      throws ProjectRepositoryException, InterruptedException {
    when(projectRepository.getAppEngineApplication(credential, ""projectId""))
        .thenReturn(AppEngine.NO_APPENGINE_APPLICATION);
    when(projectSelection.isEmpty()).thenReturn(true);

    queryJob.schedule();
    queryJob.join();

    verify(isLatestQueryJob).apply(queryJob);
    verify(projectSelector, never()).setStatusLink(anyString(), anyString());
  }
"
"  @Test
  public void testRun_abandonStaleJob() throws InterruptedException, ProjectRepositoryException {
    when(projectRepository.getAppEngineApplication(credential, ""projectId""))
        .thenReturn(AppEngine.NO_APPENGINE_APPLICATION);

    // Prepare another concurrent query job.
    Credential staleCredential = mock(Credential.class);

    GcpProject staleProject = new GcpProject(""name"", ""staleProjectId"");
    ProjectRepository projectRepository2 = mock(ProjectRepository.class);
    when(projectRepository2.getAppEngineApplication(staleCredential, ""staleProjectId""))
        .thenThrow(new ProjectRepositoryException(""testException""));

    Predicate<Job> notLatest = mock(Predicate.class);
    Job staleJob = new AppEngineApplicationQueryJob(staleProject, staleCredential,
        projectRepository2, projectSelector, EXPECTED_LINK, notLatest);

    // This second job is stale, i.e., it was fired, but user has selected another credential.
    when(notLatest.apply(staleJob)).thenReturn(false);

    queryJob.schedule();
    queryJob.join();
    // Make the stale job complete even after ""queryJob"" finishes.
    staleJob.schedule();
    staleJob.join();

    verify(projectRepository).getAppEngineApplication(credential, ""projectId"");
    verify(projectRepository2).getAppEngineApplication(staleCredential, ""staleProjectId"");

    verify(projectSelector).setStatusLink(EXPECTED_MESSAGE_WHEN_NO_APPLICATION, EXPECTED_LINK);
    verify(projectSelector, never()).setStatusLink(EXPECTED_MESSAGE_WHEN_EXCEPTION, null);
  }
"
"  @Test
  public void testConstructor_nonAbsoluteBasePath() {
    try {
      new RelativeFileFieldSetter(field, new Path(""non/absolute/base/path""), dialog);
      fail();
    } catch (IllegalArgumentException ex) {}
  }
"
"  @Test
  public void testFileDialogCanceled() {
    when(field.getText()).thenReturn("""");
    when(dialog.open()).thenReturn(null /* means canceled */);

    new RelativeFileFieldSetter(field, basePath, dialog).widgetSelected(event);
    verify(field, never()).setText(anyString());
  }
"
"  @Test
  public void testSetField() {
    when(field.getText()).thenReturn("""");
    when(dialog.open()).thenReturn(basePath + ""/sub/directory/app.yaml"");

    new RelativeFileFieldSetter(field, basePath, dialog).widgetSelected(event);
    verify(field).setText(""sub/directory/app.yaml"");
  }
"
"  @Test
  public void testSetField_userSuppliesPathOutsideBase() {
    when(field.getText()).thenReturn("""");
    when(dialog.open()).thenReturn(""/path/outside/base/app.yaml"");

    new RelativeFileFieldSetter(field, new Path(""/base/path""), dialog).widgetSelected(event);
    verify(field).setText(""../../path/outside/base/app.yaml"");
  }
"
"  @Test
  public void testFileDialogFilterSet_relativePathInField() {
    when(field.getText()).thenReturn(""src/main/appengine/app.yaml"");
    when(dialog.open()).thenReturn(null);

    new RelativeFileFieldSetter(field, basePath, dialog).widgetSelected(event);
    // ""basePath"" is the first physically existing directory.
    verify(dialog).setFilterPath(basePath.toString());

    basePath.append(""src"").toFile().mkdir();
    new RelativeFileFieldSetter(field, basePath, dialog).widgetSelected(event);
    verify(dialog).setFilterPath(basePath + ""/src"");
  }
"
"  @Test
  public void testFileDialogFilterSet_absolutePathInField() {
    when(field.getText()).thenReturn(basePath + ""/deploy/temp/app.yaml"");
    when(dialog.open()).thenReturn(null);

    new RelativeFileFieldSetter(field, basePath, dialog).widgetSelected(event);
    // ""basePath"" is the first physically existing directory.
    verify(dialog).setFilterPath(basePath.toString());

    basePath.append(""deploy"").toFile().mkdir();
    new RelativeFileFieldSetter(field, basePath, dialog).widgetSelected(event);
    verify(dialog).setFilterPath(basePath + ""/deploy"");
  }
"
"  @Test
  public void testSelectionChanged_emptySelection() {
    when(event.getSelection()).thenReturn(new StructuredSelection());
    listener.selectionChanged(event);
    verify(projectSelector).clearStatusLink();
  }
"
"  @Test
  public void testSelectionChanged_repositoryException()
      throws ProjectRepositoryException, InterruptedException {
    initSelectionAndAccountSelector();
    when(projectRepository.getAppEngineApplication(any(Credential.class), anyString()))
        .thenThrow(new ProjectRepositoryException(""testException""));

    listener.selectionChanged(event);
    listener.latestQueryJob.join();
    verify(projectSelector).clearStatusLink();  // Should clear initially.
    verify(projectSelector).setStatusLink(EXPECTED_MESSAGE_WHEN_EXCEPTION, null /* tooltip */);
  }
"
"  @Test
  public void testSelectionChanged_noAppEngineApplication()
      throws ProjectRepositoryException, InterruptedException {
    initSelectionAndAccountSelector();
    when(projectRepository.getAppEngineApplication(any(Credential.class), anyString()))
        .thenReturn(AppEngine.NO_APPENGINE_APPLICATION);

    listener.selectionChanged(event);
    listener.latestQueryJob.join();
    verify(projectSelector).clearStatusLink();  // Should clear initially.
    verify(projectSelector).setStatusLink(EXPECTED_MESSAGE_WHEN_NO_APPLICATION, EXPECTED_LINK);
  }
"
"  @Test
  public void testSelectionChanged_hasAppEngineApplication()
      throws ProjectRepositoryException, InterruptedException {
    initSelectionAndAccountSelector();
    when(projectRepository.getAppEngineApplication(any(Credential.class), anyString()))
        .thenReturn(AppEngine.withId(""id""));

    listener.selectionChanged(event);
    listener.latestQueryJob.join();
    verify(projectSelector).clearStatusLink();
  }
"
"  @Test
  public void testSelectionChanged_doNotRunQueryJobIfCached() throws ProjectRepositoryException {
    GcpProject gcpProject = new GcpProject(""projectName"", ""projectId"");
    initSelectionAndAccountSelector(gcpProject);
    gcpProject.setAppEngine(AppEngine.withId(""id""));

    listener.selectionChanged(event);
    assertNull(listener.latestQueryJob);
    verify(projectRepository, never()).getAppEngineApplication(any(Credential.class), anyString());
    verify(projectSelector).clearStatusLink();
  }
"
"  @Test
  public void testSelectionChanged_whenCachedResultIsNoAppEngineApplication()
      throws ProjectRepositoryException {
    GcpProject gcpProject = new GcpProject(""projectName"", ""projectId"");
    initSelectionAndAccountSelector(gcpProject);
    gcpProject.setAppEngine(AppEngine.NO_APPENGINE_APPLICATION);

    listener.selectionChanged(event);
    assertNull(listener.latestQueryJob);
    verify(projectRepository, never()).getAppEngineApplication(any(Credential.class), anyString());
    verify(projectSelector).setStatusLink(EXPECTED_MESSAGE_WHEN_NO_APPLICATION, EXPECTED_LINK);
  }
"
"  @Test
  public void testSelectionChanged_changeSelectedProject()
      throws ProjectRepositoryException, InterruptedException {
    when(projectRepository.getAppEngineApplication(any(Credential.class), eq(""oldProjectId"")))
        .thenThrow(new ProjectRepositoryException(""testException""));
    when(projectRepository.getAppEngineApplication(any(Credential.class), eq(""projectId"")))
        .thenReturn(AppEngine.NO_APPENGINE_APPLICATION);

    initSelectionAndAccountSelector(new GcpProject(""oldProjectName"", ""oldProjectId""));
    listener.selectionChanged(event);

    Job oldJob = listener.latestQueryJob;
    assertNotNull(oldJob);
    oldJob.join();

    initSelectionAndAccountSelector();
    listener.selectionChanged(event);

    Job newJob = listener.latestQueryJob;
    assertNotNull(newJob);
    assertNotEquals(oldJob, newJob);
    newJob.join();

    verify(projectRepository).getAppEngineApplication(any(Credential.class), eq(""oldProjectId""));
    verify(projectRepository).getAppEngineApplication(any(Credential.class), eq(""projectId""));
    verify(projectSelector).setStatusLink(EXPECTED_MESSAGE_WHEN_NO_APPLICATION, EXPECTED_LINK);
  }
"
"  @Test
  public void testUrlOpenErrorDialogTitle() {
    assertEquals(""Error"", Messages.getString(""openurllistener.error.title""));
  }
"
"  @Test
  public void testUrlOpenErrorDialogMessage() {
    assertEquals(""Could not open URL"", Messages.getString(""openurllistener.error.message""));
  }
"
"  @Test
  public void testInvalidUrlErrorMessage() {
    assertEquals(""Invalid URL: http://www.example.com"", 
        Messages.getString(""invalid.url"", ""http://www.example.com""));
  }
"
"  @Test
  public void testCreateRefreshIcon() {
    assertNotNull(SharedImages.REFRESH_IMAGE_DESCRIPTOR.createImage(shell.getDisplay()));
  }
"
"  @Test
  public void testExtensionPoint() {
    NodeList extensions = getDocument().getElementsByTagName(""extension"");
    assertEquals(1, extensions.getLength());
    Element extension = (Element) extensions.item(0);
    assertEquals(""org.eclipse.ui.commands"", extension.getAttribute(""point""));

    NodeList commandDefinitions = extension.getElementsByTagName(""command"");
    assertEquals(1, commandDefinitions.getLength());
    Element configExtension = (Element) commandDefinitions.item(0);
    assertEquals(OpenDropDownMenuHandler.class.getName(),
        configExtension.getAttribute(""defaultHandler""));
    assertEquals(""com.google.cloud.tools.eclipse.ui.util.showPopup"",
        configExtension.getAttribute(""id""));
  }
"
"  @Test
  public void testConvertFontToBold() {
    Label label = new Label(shellTestResource.getShell(), SWT.NONE);
    for (FontData fontData : label.getFont().getFontData()) {
      assertThat(fontData.getStyle(), is(not(SWT.BOLD)));
    }
    FontUtil.convertFontToBold(label);
    for (FontData fontData : label.getFont().getFontData()) {
      assertThat(fontData.getStyle(), is(SWT.BOLD));
    }
  }
"
"  @Test
  public void testConvertFontToItalic() {
    Label label = new Label(shellTestResource.getShell(), SWT.NONE);
    for (FontData fontData : label.getFont().getFontData()) {
      assertThat(fontData.getStyle(), is(not(SWT.ITALIC)));
    }
    FontUtil.convertFontToItalic(label);
    for (FontData fontData : label.getFont().getFontData()) {
      assertThat(fontData.getStyle(), is(SWT.ITALIC));
    }
  }
"
"  @Test
  public void testWidgetSelected_InvalidURI() {
    SelectionEvent selectionEvent = getEvent(INVALID_URI);

    new OpenUriSelectionListener(queryParameterProvider, errorHandler, browserSupport).widgetSelected(selectionEvent);
    verify(errorHandler).handle(captor.capture(), any(URI.class));
    assertThat(captor.getValue(), instanceOf(URISyntaxException.class));
  }
"
"  @Test
  public void testWidgetDefaultSelected_InvalidURI() {
    SelectionEvent selectionEvent = getEvent(INVALID_URI);

    new OpenUriSelectionListener(queryParameterProvider, errorHandler, browserSupport).widgetDefaultSelected(selectionEvent);
    verify(errorHandler).handle(captor.capture(), any(URI.class));
    assertThat(captor.getValue(), instanceOf(URISyntaxException.class));
  }
"
"  @Test
  public void testWidgetSelected_MalformedURL() {
    SelectionEvent selectionEvent = getEvent(MALFORMED_URL);

    new OpenUriSelectionListener(queryParameterProvider, errorHandler, browserSupport).widgetSelected(selectionEvent);
    verify(errorHandler).handle(captor.capture(), any(URI.class));
    assertThat(captor.getValue(), instanceOf(MalformedURLException.class));
  }
"
"  @Test
  public void testWidgetDefaultSelected_MalformedURL() {
    SelectionEvent selectionEvent = getEvent(MALFORMED_URL);

    new OpenUriSelectionListener(queryParameterProvider, errorHandler, browserSupport).widgetDefaultSelected(selectionEvent);
    verify(errorHandler).handle(captor.capture(), any(URI.class));
    assertThat(captor.getValue(), instanceOf(MalformedURLException.class));
  }
"
"  @Test
  public void testWidgetSelected_errorInvokingBrowser() throws PartInitException {
    SelectionEvent selectionEvent = getEvent(VALID_URI);
    doThrow(new PartInitException(""fake exception"")).when(browser).openURL(any(URL.class));

    new OpenUriSelectionListener(queryParameterProvider, errorHandler, browserSupport).widgetSelected(selectionEvent);
    verify(errorHandler).handle(captor.capture(), any(URI.class));
    assertThat(captor.getValue(), instanceOf(PartInitException.class));
  }
"
"  @Test
  public void testWidgetDefaultSelected_errorInvokingBrowser() throws PartInitException {
    SelectionEvent selectionEvent = getEvent(VALID_URI);
    doThrow(new PartInitException(""fake exception"")).when(browser).openURL(any(URL.class));

    new OpenUriSelectionListener(queryParameterProvider, errorHandler, browserSupport)
      .widgetDefaultSelected(selectionEvent);
    verify(errorHandler).handle(captor.capture(), any(URI.class));
    assertThat(captor.getValue(), instanceOf(PartInitException.class));
  }
"
"  @Test
  public void testWidgetSelected_successful() throws PartInitException, MalformedURLException {
    SelectionEvent selectionEvent = getEvent(VALID_URI);
    when(queryParameterProvider.getParameters()).thenReturn(Collections.singletonMap(URL_PARAM_PROJECT, PROJECT_ID));

    new OpenUriSelectionListener(queryParameterProvider, errorHandler, browserSupport).widgetSelected(selectionEvent);
    verify(errorHandler, never()).handle(any(Exception.class), any(URI.class));
    verify(browser).openURL(new URL(VALID_URI + ""?project="" + PROJECT_ID));
  }
"
"  @Test
  public void testWidgetDefaultSelected_successful() throws PartInitException, MalformedURLException {
    SelectionEvent selectionEvent = getEvent(VALID_URI);
    when(queryParameterProvider.getParameters()).thenReturn(Collections.singletonMap(URL_PARAM_PROJECT, PROJECT_ID));

    new OpenUriSelectionListener(queryParameterProvider, errorHandler, browserSupport)
      .widgetDefaultSelected(selectionEvent);
    verify(errorHandler, never()).handle(any(Exception.class), any(URI.class));
    verify(browser).openURL(new URL(VALID_URI + ""?project="" + PROJECT_ID));
  }
"
"  @Test
  public void testNegate() {
    assertTrue((Boolean) BooleanConverter.negate().convert(Boolean.FALSE));
    assertFalse((Boolean) BooleanConverter.negate().convert(Boolean.TRUE));
  }
"
"  @Test
  public void testValidation_nonStringInput() {
    IStatus status = validator.validate(new Object());
    assertThat(status.getSeverity(), is(IStatus.ERROR));
    assertThat(status.getMessage(), is(""Invalid bucket name""));
  }
"
"  @Test
  public void testValidation_emptyString() {
    assertThat(validator.validate("""").getSeverity(), is(IStatus.OK));
  }
"
"  @Test
  public void testValidation_upperCaseLetter() {
    IStatus status = validator.validate(""THISWOULDBEVALIDIFLOWERCASE"");
    assertThat(status.getSeverity(), is(IStatus.ERROR));
    assertThat(status.getMessage(), is(""Invalid bucket name: THISWOULDBEVALIDIFLOWERCASE""));
  }
"
"  @Test
  public void testValidation_startWithDot() {
    assertThat(validator.validate("".bucket"").getSeverity(), is(IStatus.ERROR));
  }
"
"  @Test
  public void testValidation_endWithDot() {
    assertThat(validator.validate(""bucket."").getSeverity(), is(IStatus.ERROR));
  }
"
"  @Test
  public void testValidation_startWithHyphen() {
    assertThat(validator.validate(""-bucket"").getSeverity(), is(IStatus.ERROR));
  }
"
"  @Test
  public void testValidation_endWithHyphen() {
    assertThat(validator.validate(""bucket-"").getSeverity(), is(IStatus.ERROR));
  }
"
"  @Test
  public void testValidation_startWithUnderscore() {
    assertThat(validator.validate(""_bucket"").getSeverity(), is(IStatus.ERROR));
  }
"
"  @Test
  public void testValidation_endWithUnderscore() {
    assertThat(validator.validate(""bucket_"").getSeverity(), is(IStatus.ERROR));
  }
"
"  @Test
  public void testValidation_maxLengthWithoutDot() {
    assertThat(validator.validate(LENGTH_63).getSeverity(), is(IStatus.OK));
  }
"
"  @Test
  public void testValidation_tooLongNameWithoutDot() {
    assertThat(validator.validate(LENGTH_63 + ""4"").getSeverity(), is(IStatus.ERROR));
  }
"
"  @Test
  public void testValidation_validNameWithDot() {
    assertThat(validator.validate(LENGTH_64_WITH_DOT).getSeverity(), is(IStatus.OK));
  }
"
"  @Test
  public void testValidation_tooLongNameWithDot() {
    assertThat(validator.validate(LENGTH_222 + ""9"").getSeverity(), is(IStatus.ERROR));
  }
"
"  @Test
  public void testValidation_maxLengthWithDot() {
    assertThat(validator.validate(LENGTH_222).getSeverity(), is(IStatus.OK));
  }
"

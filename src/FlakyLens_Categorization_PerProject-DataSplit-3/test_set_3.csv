id,project,test_name,full_code,label,category
2,neo4j_neo4j,RobustJobSchedulerWrapperTest.shouldBeAbleToCancelJob,"@Test
public void shouldBeAbleToCancelJob() throws Exception {
    RobustJobSchedulerWrapper robustWrapper = new RobustJobSchedulerWrapper(actualScheduler, log);
    AtomicInteger count = new AtomicInteger();
    JobHandle jobHandle = robustWrapper.scheduleRecurring(""JobName"", 1, count::incrementAndGet);
    assertEventually(""run count"", count::get, Matchers.greaterThanOrEqualTo(100), DEFAULT_TIMEOUT_MS, MILLISECONDS);
    robustWrapper.cancelAndWaitTermination(jobHandle);
    int finalCount = count.get();
    Thread.sleep(50);
    assertEquals(finalCount, count.get());
}",concurrency,1
12,Ericsson_ecchronos,TestRepairGroup.testGetPartialRepairTasks,"@Test
public void testGetPartialRepairTasks() {
    Node node = mockNode(""DC1"");
    Node node2 = mockNode(""DC1"");
    ImmutableList<LongTokenRange> vnodes = ImmutableList.of(new LongTokenRange(1, 2), new LongTokenRange(2, 3), new LongTokenRange(4, 5));
    ReplicaRepairGroup replicaRepairGroup = new ReplicaRepairGroup(ImmutableSet.of(node, node2), vnodes);
    RepairGroup repairGroup = builderFor(replicaRepairGroup).build(priority);
    Collection<RepairTask> tasks = repairGroup.getRepairTasks();
    assertThat(tasks.size()).isEqualTo(3);
    Set<LongTokenRange> repairTaskRanges = new HashSet<>();
    for (RepairTask repairTask : tasks) {
        assertThat(repairTask.getTokenRanges().size()).isEqualTo(1);
        LongTokenRange range = repairTask.getTokenRanges().iterator().next();
        repairTaskRanges.add(range);
        assertThat(repairTask.getReplicas()).containsExactlyInAnyOrder(node, node2);
        assertThat(repairTask.getTableReference()).isEqualTo(tableReference);
        assertThat(repairTask.getRepairConfiguration().getRepairParallelism()).isEqualTo(PARALLEL);
    }
    assertThat(repairTaskRanges).containsExactlyElementsOf(vnodes);
}",unordered collections,3
14,neo4j_neo4j,RobustJobSchedulerWrapperTest.recurringJobWithExceptionShouldKeepRunning,"@Test
public void recurringJobWithExceptionShouldKeepRunning() throws Exception
{
    RobustJobSchedulerWrapper robustWrapper = new RobustJobSchedulerWrapper( actualScheduler, log );
    AtomicInteger count = new AtomicInteger();
    IllegalStateException e = new IllegalStateException();
    int nRuns = 100;
    JobHandle jobHandle = robustWrapper.scheduleRecurring( ""JobName"", 1, () -> {
        if ( count.get() < nRuns )
        {
            count.incrementAndGet();
            throw e;
        }
    }
    );
    assertEventually( ""run count"", count::get, Matchers.equalTo( nRuns ), DEFAULT_TIMEOUT_MS , MILLISECONDS );
    robustWrapper.cancelAndWaitTermination( jobHandle );
    verify( log, timeout( DEFAULT_TIMEOUT_MS ).times( nRuns ) ).warn( ""Uncaught exception"", e );
}",concurrency,1
20,cdapio_cdap,WorkflowHttpHandlerTest.testWorkflowForkFailure,"@Test
public void testWorkflowForkFailure() throws Exception {
    Assert.assertEquals(200, deploy(WorkflowFailureInForkApp.class).getStatusLine().getStatusCode());
    Id.Application appId = Application.from(DEFAULT, NAME);
    Id.Workflow workflowId = Workflow.from(appId, NAME);
    Id.Program firstMRId = Program.from(appId, MAPREDUCE, FIRST_MAPREDUCE_NAME);
    Id.Program secondMRId = Program.from(appId, MAPREDUCE, SECOND_MAPREDUCE_NAME);
    String outputPath = new File(tmpFolder.newFolder(), ""output"").getAbsolutePath();
    File fileToSync = new File(tmpFolder.newFolder() + ""/sync.file"");
    File fileToWait = new File(tmpFolder.newFolder() + ""/wait.file"");
    startProgram(workflowId, ImmutableMap.of(""inputPath"", createInput(""testWorkflowForkFailureInput""), ""outputPath"", outputPath, ""sync.file"", fileToSync.getAbsolutePath(), ""wait.file"", fileToWait.getAbsolutePath(), (""mapreduce."" + WorkflowFailureInForkApp.SECOND_MAPREDUCE_NAME) + "".throw.exception"", ""true""));
    waitState(workflowId, RUNNING.name());
    waitState(workflowId, STOPPED.name());
    verifyProgramRuns(workflowId, ""failed"");
    List<RunRecord> mapReduceProgramRuns = getProgramRuns(firstMRId, KILLED.name());
    Assert.assertEquals(1, mapReduceProgramRuns.size());
    mapReduceProgramRuns = getProgramRuns(secondMRId, FAILED.name());
    Assert.assertEquals(1, mapReduceProgramRuns.size());
}",concurrency,1
22,apache_pulsar,TopicReaderTest.testMultiReaderIsAbleToSeekWithTimeOnMiddleOfTopic,"@Test
public void testMultiReaderIsAbleToSeekWithTimeOnMiddleOfTopic() throws Exception {
    final String topicName = ""persistent"";
    final int numOfMessage = 10;
    final int halfMessages = numOfMessage / 2;
    admin.topics().createPartitionedTopic(topicName, 3);
    Producer<byte[]> producer = pulsarClient.newProducer().topic(topicName).create();
    long l = System.currentTimeMillis();
    for (int i = 0; i < numOfMessage; i++) {
        producer.send(String.format(""msg num %d"", i).getBytes());
    }
    Reader<byte[]> reader = pulsarClient.newReader().topic(topicName).startMessageId(earliest).create();
    int plusTime = (halfMessages + 1) * 100;
    reader.seek(l + plusTime);
    Set<String> messageSet = Sets.newHashSet();
    for (int i = halfMessages + 1; i < numOfMessage; i++) {
        Message<byte[]> message = reader.readNext();
        String receivedMessage = new String(message.getData());
        Assert.assertTrue(messageSet.add(receivedMessage), ""Received duplicate message "" + receivedMessage);
    }
    reader.close();
    producer.close();
}",time,2
23,fabiomaffioletti_jsondoc,JSONDocApiAuthBuilderTest.testApiAuthToken,"@Test
public void testApiAuthToken() {
    ApiDoc apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>>newHashSet(Controller.class), URI).iterator().next();
    Assert.assertEquals(""TOKEN"", apiDoc.getAuth().getType());
    Assert.assertEquals("""", apiDoc.getAuth().getScheme());
    Assert.assertEquals(""abc"", apiDoc.getAuth().getTesttokens().iterator().next());
    for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
        if (apiMethodDoc.getPath().contains(""/inherit"")) {
            Assert.assertEquals(""TOKEN"", apiMethodDoc.getAuth().getType());
            Assert.assertEquals("""", apiMethodDoc.getAuth().getScheme());
            Assert.assertEquals(""abc"", apiMethodDoc.getAuth().getTesttokens().iterator().next());
        }
        if (apiMethodDoc.getPath().contains(""/override"")) {
            Assert.assertEquals(""TOKEN"", apiMethodDoc.getAuth().getType());
            Assert.assertEquals(""Bearer"", apiMethodDoc.getAuth().getScheme());
            Assert.assertEquals(""xyz"", apiMethodDoc.getAuth().getTesttokens().iterator().next());
        }
    }
}",unordered collections,3
27,apache_ozone,TestSCMUpdateServiceGrpcServer.testClientUpdateWithDelayedRevoke,"@Test
public void testClientUpdateWithDelayedRevoke() throws Exception {
    OzoneConfiguration conf = new OzoneConfiguration();
    SCMUpdateServiceGrpcServer server = new SCMUpdateServiceGrpcServer(getUpdateServiceConfig(conf), mockCRLStore);
    ClientCRLStore clientCRLStore = new ClientCRLStore();
    SCMUpdateClientConfiguration updateClientConfiguration = conf.getObject(SCMUpdateClientConfiguration.class);
    updateClientConfiguration.setClientCrlCheckInterval(Duration.ofSeconds(2));
    conf.setFromObject(updateClientConfiguration);
    SCMUpdateServiceGrpcClient client = new SCMUpdateServiceGrpcClient(""localhost"", conf, clientCRLStore);
    server.start();
    client.start();
    try {
        List<BigInteger> certIds = new ArrayList<>();
        for (int i = 0; i < 10; i++) {
            BigInteger certId = mockCRLStore.issueCert();
            certIds.add(certId);
        }
        revokeCertNow(certIds.get(0));
        server.notifyCrlUpdate();
        GenericTestUtils.waitFor(() -> client.getUpdateCount() == 1, 100, 2000);
        Assert.assertEquals(1, client.getUpdateCount());
        Assert.assertEquals(0, client.getErrorCount());
        revokeCert(certIds.get(5), Instant.now().plus(Duration.ofSeconds(5)));
        server.notifyCrlUpdate();
        GenericTestUtils.waitFor(() -> client.getUpdateCount() > 1, 100, 2000);
        Assert.assertEquals(2, client.getUpdateCount());
        Assert.assertEquals(0, client.getErrorCount());
        Assert.assertEquals(1, client.getClientCRLStore().getPendingCrlIds().size());
        GenericTestUtils.waitFor(() -> client.getPendingCrlRemoveCount() == 1, 100, 20000);
        Assert.assertTrue(client.getClientCRLStore().getPendingCrlIds().isEmpty());
    } catch (Exception e) {
        e.printStackTrace();
    } finally {
        client.stop(true);
        server.stop();
    }
}",async wait,0
30,apache_pulsar,ZKSessionTest.testReacquireLocksAfterSessionLost,"@Test
public void testReacquireLocksAfterSessionLost() throws Exception {
    @Cleanup
    MetadataStoreExtended store = MetadataStoreExtended.create(zks.getConnectionString(), MetadataStoreConfig.builder().sessionTimeoutMillis(2000).build());
    BlockingQueue<SessionEvent> sessionEvents = new LinkedBlockingQueue<>();
    store.registerSessionListener(sessionEvents::add);
    @Cleanup
    CoordinationService coordinationService = new CoordinationServiceImpl(store);
    @Cleanup
    LockManager<String> lm1 = coordinationService.getLockManager(String.class);
    String path = newKey();
    ResourceLock<String> lock = lm1.acquireLock(path, ""value-1"").join();
    zks.expireSession(((ZKMetadataStore) (store)).getZkSessionId());
    SessionEvent e = sessionEvents.poll(5, TimeUnit.SECONDS);
    assertEquals(e, ConnectionLost);
    e = sessionEvents.poll(10, TimeUnit.SECONDS);
    assertEquals(e, SessionLost);
    e = sessionEvents.poll(10, TimeUnit.SECONDS);
    assertEquals(e, Reconnected);
    e = sessionEvents.poll(10, TimeUnit.SECONDS);
    assertEquals(e, SessionReestablished);
    Awaitility.await().untilAsserted(() -> {
        assertFalse(lock.getLockExpiredFuture().isDone());
    });
    assertTrue(store.get(path).join().isPresent());
}",async wait,0
36,apache_pulsar,BacklogQuotaManagerTest.testConsumerBacklogEvictionTimeQuotaWithEmptyLedger,"@Test
public void testConsumerBacklogEvictionTimeQuotaWithEmptyLedger() throws Exception {
    assertEquals(admin.namespaces().getBacklogQuotaMap(""prop/ns-quota""), Maps.newHashMap());
    admin.namespaces().setBacklogQuota(""prop/ns-quota"", BacklogQuota.builder().limitTime(TIME_TO_CHECK_BACKLOG_QUOTA).retentionPolicy(consumer_backlog_eviction).build(), message_age);
    PulsarClient client = PulsarClient.builder().serviceUrl(adminUrl.toString()).statsInterval(0, TimeUnit.SECONDS).build();
    final String topic = ""persistent"";
    final String subName = ""c1"";
    Consumer<byte[]> consumer = client.newConsumer().topic(topic).subscriptionName(subName).subscribe();
    Producer<byte[]> producer = createProducer(client, topic);
    producer.send(new byte[1024]);
    consumer.receive();
    admin.topics().unload(topic);
    PersistentTopicInternalStats internalStats = admin.topics().getInternalStats(topic);
    assertEquals(internalStats.ledgers.size(), 2);
    assertEquals(internalStats.ledgers.get(1).entries, 0);
    TopicStats stats = admin.topics().getStats(topic);
    assertEquals(stats.getSubscriptions().get(subName).getMsgBacklog(), 1);
    TimeUnit.SECONDS.sleep(TIME_TO_CHECK_BACKLOG_QUOTA);
    Awaitility.await().pollInterval(Duration.ofSeconds(1)).atMost(Duration.ofSeconds(TIME_TO_CHECK_BACKLOG_QUOTA)).untilAsserted(() -> {
        rolloverStats();
        PersistentTopicInternalStats latestInternalStats = admin.topics().getInternalStats(topic);
        assertEquals(latestInternalStats.ledgers.size(), 2);
        assertEquals(latestInternalStats.ledgers.get(1).entries, 0);
        TopicStats latestStats = admin.topics().getStats(topic);
        assertEquals(latestStats.getSubscriptions().get(subName).getMsgBacklog(), 0);
    });
    client.close();
}",async wait,0
37,apache_pulsar,InactiveTopicDeleteTest.testTopicLevelInActiveTopicApi,"@Test
public void testTopicLevelInActiveTopicApi() throws Exception {
    super.resetConfig();
    conf.setSystemTopicEnabled(true);
    conf.setTopicLevelPoliciesEnabled(true);
    super.baseSetup();
    Thread.sleep(2000);
    final String topicName = ""persistent://prop/ns-abc/testMaxInactiveDuration-"" + UUID.randomUUID().toString();
    admin.topics().createPartitionedTopic(topicName, 3);
    InactiveTopicPolicies inactiveTopicPolicies = admin.topics().getInactiveTopicPolicies(topicName);
    assertNull(inactiveTopicPolicies);
    InactiveTopicPolicies policies = new InactiveTopicPolicies();
    policies.setDeleteWhileInactive(true);
    policies.setInactiveTopicDeleteMode(InactiveTopicDeleteMode.delete_when_no_subscriptions);
    policies.setMaxInactiveDurationSeconds(10);
    admin.topics().setInactiveTopicPolicies(topicName, policies);
    for (int i = 0; i < 50; i++) {
        if (admin.topics().getInactiveTopicPolicies(topicName) != null) {
            break;
        }
        Thread.sleep(100);
    }
    assertEquals(admin.topics().getInactiveTopicPolicies(topicName), policies);
    admin.topics().removeInactiveTopicPolicies(topicName);
    for (int i = 0; i < 50; i++) {
        if (admin.topics().getInactiveTopicPolicies(topicName) == null) {
            break;
        }
        Thread.sleep(100);
    }
    assertNull(admin.topics().getInactiveTopicPolicies(topicName));
    super.internalCleanup();
}",async wait,0
41,Ericsson_ecchronos,TestRepairTask.testRepairSuccessfully,"@Test
public void testRepairSuccessfully() throws InterruptedException {
    Collection<LongTokenRange> ranges = new ArrayList<>();
    LongTokenRange range1 = new LongTokenRange(1, 2);
    LongTokenRange range2 = new LongTokenRange(3, 4);
    ranges.add(range1);
    ranges.add(range2);
    final RepairTask repairTask = new RepairTask.Builder().withJMXProxyFactory(jmxProxyFactory).withTableReference(myTableReference).withTokenRanges(ranges).withTableRepairMetrics(myTableRepairMetrics).withRepairHistory(repairHistory).withJobId(jobId).withReplicas(participants).build();
    CountDownLatch cdl = startRepair(repairTask, false);
    Notification notification = new Notification(""progress"", ""repair:1"", 0, getRepairMessage(range1));
    notification.setUserData(getNotificationData(PROGRESS.ordinal(), 1, 2));
    proxy.notify(notification);
    notification = new Notification(""progress"", ""repair:1"", 1, getRepairMessage(range2));
    notification.setUserData(getNotificationData(PROGRESS.ordinal(), 2, 2));
    proxy.notify(notification);
    notification = new Notification(""progress"", ""repair:1"", 2, ""Done with repair"");
    notification.setUserData(getNotificationData(COMPLETE.ordinal(), 2, 2));
    proxy.notify(notification);
    cdl.await();
    assertThat(repairTask.getUnknownRanges()).isNull();
    assertThat(repairTask.getCompletedRanges()).containsExactlyElementsOf(ranges);
    assertThat(proxy.myOptions.get(RANGES_KEY)).isNotEmpty();
    verify(myTableRepairMetrics).repairTiming(eq(TABLE_REFERENCE), anyLong(), any(TimeUnit.class), eq(true));
    verify(repairSessions.get(range1)).start();
    verify(repairSessions.get(range2)).start();
    verify(repairSessions.get(range1)).finish(eq(SUCCESS));
    verify(repairSessions.get(range2)).finish(eq(SUCCESS));
}",unordered collections,3
43,spring-projects_spring-data-envers,5637994be37747e82b2d6d5b34555e2bee791fe6.testWithRevisions,"@Test
public void testWithRevisions() {
    Country de = new Country();
    de.code = ""de"";
    de.name = ""Deutschland"";
    countryRepository.save(de);
    de.name = ""Germany"";
    countryRepository.save(de);
    Revisions<Integer, Country> revisions = countryRepository.findRevisions(de.id);
    assertThat(revisions).hasSize(2);
    Iterator<Revision<Integer, Country>> iterator = revisions.iterator();
    Integer firstRevisionNumber = iterator.next().getRevisionNumber().get();
    Integer secondRevisionNumber = iterator.next().getRevisionNumber().get();
    assertThat(countryRepository.findRevision(de.id, firstRevisionNumber).get().getEntity().name)
    .isEqualTo(""Deutschland"");
    assertThat(countryRepository.findRevision(de.id, secondRevisionNumber).get().getEntity().name).isEqualTo(""Germany"");
}",test order dependency,4
46,networknt_json-schema-validator,CollectorContextTest.testCollectorContextWithKeyword,"@Test
public void testCollectorContextWithKeyword() throws Exception {
    ValidationResult validationResult = validate(""{\""test-property1\"":\""sample1\"",\""test-property2\"":\""sample2\""}"");
    Assertions.assertEquals(0, validationResult.getValidationMessages().size());
    List<String> contextValues = ((List<String>) (validationResult.getCollectorContext().get(SAMPLE_COLLECTOR)));
    Assertions.assertEquals(0, validationResult.getValidationMessages().size());
    Assertions.assertEquals(2, contextValues.size());
    Assertions.assertEquals(contextValues.get(0), ""actual_value_added_to_context1"");
    Assertions.assertEquals(contextValues.get(1), ""actual_value_added_to_context2"");
}",unordered collections,3
48,cdapio_cdap,PreviewDataPipelineTest.testLogicalTypePreviewRun,"@Test
public void testLogicalTypePreviewRun(Engine engine) throws Exception {
    PreviewManager previewManager = getPreviewManager();
    String sourceTableName = ""singleInput"";
    String sinkTableName = ""singleOutput"";
    Schema schema = Schema.recordOf(
    ""testRecord"",
    Schema.Field.of(""name"", Schema.of(Schema.Type.STRING)),
    Schema.Field.of(""date"", Schema.of(Schema.LogicalType.DATE)),
    Schema.Field.of(""ts"", Schema.of(Schema.LogicalType.TIMESTAMP_MILLIS))
    );
    ETLBatchConfig etlConfig = ETLBatchConfig.builder()
    .addStage(new ETLStage(""source"", MockSource.getPlugin(sourceTableName, schema)))
    .addStage(new ETLStage(""transform"", IdentityTransform.getPlugin()))
    .addStage(new ETLStage(""sink"", MockSink.getPlugin(sinkTableName)))
    .addConnection(""source"", ""transform"")
    .addConnection(""transform"", ""sink"")
    .setEngine(engine)
    .setNumOfRecordsPreview(100)
    .build();
    PreviewConfig previewConfig = new PreviewConfig(SmartWorkflow.NAME, ProgramType.WORKFLOW,
    Collections.<String, String>emptyMap(), 10);
    addDatasetInstance(Table.class.getName(), sourceTableName,
    DatasetProperties.of(ImmutableMap.of(""schema"", schema.toString())));
    DataSetManager<Table> inputManager = getDataset(NamespaceId.DEFAULT.dataset(sourceTableName));
    ZonedDateTime expectedMillis = ZonedDateTime.of(2018, 11, 11, 11, 11, 11, 123 * 1000 * 1000,
    ZoneId.ofOffset(""UTC"", ZoneOffset.UTC));
    StructuredRecord recordSamuel = StructuredRecord.builder(schema).set(""name"", ""samuel"")
    .setDate(""date"", LocalDate.of(2002, 11, 18)).setTimestamp(""ts"", expectedMillis).build();
    StructuredRecord recordBob = StructuredRecord.builder(schema).set(""name"", ""bob"")
    .setDate(""date"", LocalDate.of(2003, 11, 18)).setTimestamp(""ts"", expectedMillis).build();
    MockSource.writeInput(inputManager, ImmutableList.of(recordSamuel, recordBob));
    AppRequest<ETLBatchConfig> appRequest = new AppRequest<>(APP_ARTIFACT_RANGE, etlConfig, previewConfig);
    ApplicationId previewId = previewManager.start(NamespaceId.DEFAULT, appRequest);
    Tasks.waitFor(PreviewStatus.Status.COMPLETED, new Callable<PreviewStatus.Status>() {
        @Override
        public PreviewStatus.Status call() throws Exception {
            PreviewStatus status = previewManager.getStatus(previewId);
            return status == null ? null : status.getStatus();
        }
    }, 5, TimeUnit.MINUTES);
    checkPreviewStore(previewManager, previewId, ""source"", 2);
    List<JsonElement> data = previewManager.getData(previewId, ""source"").get(DATA_TRACER_PROPERTY);
    StructuredRecord actualRecordSamuel = GSON.fromJson(data.get(0), StructuredRecord.class);
    Assert.assertEquals(actualRecordSamuel.get(""date""), ""2002-11-18"");
    Assert.assertEquals(actualRecordSamuel.get(""ts""), ""2018-11-11T11:11:11.123Z[UTC]"");
    StructuredRecord actualRecordBob = GSON.fromJson(data.get(1), StructuredRecord.class);
    Assert.assertEquals(actualRecordBob.get(""date""), ""2003-11-18"");
    Assert.assertEquals(actualRecordBob.get(""ts""), ""2018-11-11T11:11:11.123Z[UTC]"");
    checkPreviewStore(previewManager, previewId, ""transform"", 2);
    checkPreviewStore(previewManager, previewId, ""sink"", 2);
    validateMetric(2, previewId, ""source.records.in"", previewManager);
    validateMetric(2, previewId, ""source.records.out"", previewManager);
    validateMetric(2, previewId, ""transform.records.in"", previewManager);
    validateMetric(2, previewId, ""transform.records.out"", previewManager);
    validateMetric(2, previewId, ""sink.records.out"", previewManager);
    validateMetric(2, previewId, ""sink.records.in"", previewManager);
    DataSetManager<Table> sinkManager = getDataset(sinkTableName);
    Assert.assertNull(sinkManager.get());
    deleteDatasetInstance(NamespaceId.DEFAULT.dataset(sourceTableName));
    Assert.assertNotNull(previewManager.getRunId(previewId));
}",time,2
64,apache_ignite,testFlowNoConflictsWithClients,"@Test
public void testFlowNoConflictsWithClients() throws Exception {
    startComputation(0, stopFlag0);
    if (!tcpDiscovery())
    return;
    startComputation(1, stopFlag1);
    startComputation(2, stopFlag2);
    startComputation(3, stopFlag3);
    startComputation(4, stopFlag4);
    final Set<Integer> deafClientObservedIds = new ConcurrentHashSet<>();
    startListening(5, true, deafClientObservedIds);
    final Set<Integer> regClientObservedIds = new ConcurrentHashSet<>();
    startListening(6, false, regClientObservedIds);
    START_LATCH.countDown();
    Thread killer = new Thread(new ServerNodeKiller());
    Thread resurrection = new Thread(new ServerNodeResurrection());
    killer.setName(""node-killer-thread"");
    killer.start();
    resurrection.setName(""node-resurrection-thread"");
    resurrection.start();
    while (!updatesQueue.isEmpty())
    Thread.sleep(1000);
    killer.interrupt();
    resurrection.interrupt();
}",concurrency,1
77,apache_kylin,CoordinatorTest.testReassignFailOnStopAndSync,"@Test
public void testReassignFailOnStopAndSync() throws IOException {
    ReceiverAdminClient receiverAdminClient = mockReceiverClientFailOnStopAndSync();
    coordinator = new Coordinator(metadataStore, receiverAdminClient);
    Map<Integer, List<Partition>> preAssignMap = metadataStore.getAssignmentsByCube(cubeName).getAssignments();
    Map<Integer, List<Partition>> newAssignMap = new HashMap<>();
    newAssignMap.put(1, Lists.newArrayList(p1, p2, p3));
    newAssignMap.put(2, Lists.newArrayList(p4, p5));
    newAssignMap.put(3, Lists.newArrayList(p6));
    CubeAssignment preAssigment = new CubeAssignment(cube.getName(), preAssignMap);
    CubeAssignment newAssigment = new CubeAssignment(cube.getName(), newAssignMap);
    try {
        coordinator.doReassign(cube, preAssigment, newAssigment);
    } catch (ClusterStateException rune) {
        assertSame(ROLLBACK_FAILED, rune.getClusterState());
        assertSame(STOP_AND_SNYC, rune.getTransactionStep());
        System.out.println(rune.getMessage());
        throw rune;
    }
}",unordered collections,3
78,apache_pulsar,testTransactionMetaStoreAssignAndFailover,"@Test
public void testTransactionMetaStoreAssignAndFailover() throws IOException, InterruptedException {
    int transactionMetaStoreCount = 0;
    for (PulsarService pulsarService : pulsarServices) {
        transactionMetaStoreCount += pulsarService.getTransactionMetadataStoreService().getStores().size();
    }
    Assert.assertEquals(transactionMetaStoreCount, 16);
    PulsarService crashedMetaStore = null;
    for (int i = pulsarServices.length - 1; i >= 0; i--) {
        if (pulsarServices[i].getTransactionMetadataStoreService().getStores().size() > 0) {
            crashedMetaStore = pulsarServices[i];
            break;
        }
    }
    Assert.assertNotNull(crashedMetaStore);
    List<PulsarService> services = new ArrayList<>(pulsarServices.length - 1);
    for (PulsarService pulsarService : pulsarServices) {
        if (pulsarService != crashedMetaStore) {
            services.add(pulsarService);
        }
    }
    pulsarServices = new PulsarService[pulsarServices.length - 1];
    for (int i = 0; i < services.size(); i++) {
        pulsarServices[i] = services.get(i);
    }
    crashedMetaStore.close();
    Thread.sleep(3000);
    transactionMetaStoreCount = 0;
    for (PulsarService pulsarService : pulsarServices) {
        transactionMetaStoreCount += pulsarService.getTransactionMetadataStoreService().getStores().size();
    }
    Assert.assertEquals(transactionMetaStoreCount, 16);
    transactionCoordinatorClient.close();
}",async wait,0
81,apache_dubbo,737f7a7ea67832d7f17517326fb2491d0a086dd7.testEmptyByteArrayForEmptyInput,"@Test
public void testEmptyByteArrayForEmptyInput() throws IOException {
    this.fstObjectInput = new FstObjectInput(new ByteArrayInputStream("""".getBytes()));
    byte[] bytes = fstObjectInput.readBytes();
    assertThat(bytes.length, is(0));
}",test order dependency,4
92,apache_pulsar,KeySharedSubscriptionTest.testRemoveFirstConsumer,"@Test
public void testRemoveFirstConsumer() throws Exception {
    this.conf.setSubscriptionKeySharedEnable(true);
    String topic = ""testReadAheadWhenAddingConsumers-"" + UUID.randomUUID();
    @Cleanup
    Producer<Integer> producer = createProducer(topic, false);
    @Cleanup
    Consumer<Integer> c1 = pulsarClient.newConsumer(INT32).topic(topic).subscriptionName(""key_shared"").subscriptionType(Key_Shared).receiverQueueSize(10).consumerName(""c1"").subscribe();
    for (int i = 0; i < 10; i++) {
        producer.newMessage().key(String.valueOf(random.nextInt(NUMBER_OF_KEYS))).value(i).send();
    }
    @Cleanup
    Consumer<Integer> c2 = pulsarClient.newConsumer(INT32).topic(topic).subscriptionName(""key_shared"").subscriptionType(Key_Shared).receiverQueueSize(10).consumerName(""c2"").subscribe();
    for (int i = 10; i < 20; i++) {
        producer.newMessage().key(String.valueOf(random.nextInt(NUMBER_OF_KEYS))).value(i).send();
    }
    assertNull(c2.receive(100, TimeUnit.MILLISECONDS));
    c1.close();
    for (int i = 0; i < 20; i++) {
        Message<Integer> msg = c2.receive();
        assertEquals(msg.getValue().intValue(), i);
        c2.acknowledge(msg);
    }
}",async wait,0
94,apache_pulsar,PersistentFailoverE2ETest.testSimpleConsumerEventsWithoutPartition,"@Test
public void testSimpleConsumerEventsWithoutPartition() throws Exception {
    final String topicName = ""persistent"";
    final String subName = ""sub1"";
    final int numMsgs = 100;
    TestConsumerStateEventListener listener1 = new TestConsumerStateEventListener();
    TestConsumerStateEventListener listener2 = new TestConsumerStateEventListener();
    ConsumerBuilder<byte[]> consumerBuilder = pulsarClient.newConsumer().topic(topicName).subscriptionName(subName).acknowledgmentGroupTime(0, TimeUnit.SECONDS).subscriptionType(Failover);
    ConsumerBuilder<byte[]> consumerBulder1 = consumerBuilder.clone().consumerName(""1"").consumerEventListener(listener1).acknowledgmentGroupTime(0, TimeUnit.SECONDS);
    Consumer<byte[]> consumer1 = consumerBulder1.subscribe();
    Consumer<byte[]> consumer2 = consumerBuilder.clone().consumerName(""2"").consumerEventListener(listener2).subscribe();
    verifyConsumerActive(listener1, -1);
    verifyConsumerInactive(listener2, -1);
    PersistentTopic topicRef = ((PersistentTopic) (pulsar.getBrokerService().getTopicReference(topicName).get()));
    PersistentSubscription subRef = topicRef.getSubscription(subName);
    assertNotNull(topicRef);
    assertNotNull(subRef);
    assertTrue(subRef.getDispatcher().isConsumerConnected());
    assertEquals(subRef.getDispatcher().getType(), Failover);
    List<CompletableFuture<MessageId>> futures = Lists.newArrayListWithCapacity(numMsgs);
    Producer<byte[]> producer = pulsarClient.newProducer().topic(topicName).enableBatching(false).messageRoutingMode(SinglePartition).create();
    for (int i = 0; i < numMsgs; i++) {
        String message = ""my-message-"" + i;
        futures.add(producer.sendAsync(message.getBytes()));
    }
    FutureUtil.waitForAll(futures).get();
    futures.clear();
    rolloverPerIntervalStats();
    assertEquals(subRef.getNumberOfEntriesInBacklog(), numMsgs);
    Thread.sleep(ASYNC_EVENT_COMPLETION_WAIT);
    Message<byte[]> msg = null;
    Assert.assertNull(consumer2.receive(1, TimeUnit.SECONDS));
    for (int i = 0; i < numMsgs; i++) {
        msg = consumer1.receive(1, TimeUnit.SECONDS);
        Assert.assertNotNull(msg);
        Assert.assertEquals(new String(msg.getData()), ""my-message-"" + i);
        consumer1.acknowledge(msg);
    }
    rolloverPerIntervalStats();
    Thread.sleep(ASYNC_EVENT_COMPLETION_WAIT);
    assertEquals(subRef.getNumberOfEntriesInBacklog(), 0);
    for (int i = 0; i < numMsgs; i++) {
        String message = ""my-message-"" + i;
        futures.add(producer.sendAsync(message.getBytes()));
    }
    FutureUtil.waitForAll(futures).get();
    futures.clear();
    for (int i = 0; i < 5; i++) {
        msg = consumer1.receive(1, TimeUnit.SECONDS);
        Assert.assertNotNull(msg);
        Assert.assertEquals(new String(msg.getData()), ""my-message-"" + i);
        consumer1.acknowledge(msg);
    }
    for (int i = 5; i < 10; i++) {
        msg = consumer1.receive(1, TimeUnit.SECONDS);
        Assert.assertNotNull(msg);
        Assert.assertEquals(new String(msg.getData()), ""my-message-"" + i);
    }
    consumer1.close();
    Thread.sleep(CONSUMER_ADD_OR_REMOVE_WAIT_TIME);
    verifyConsumerActive(listener2, -1);
    verifyConsumerNotReceiveAnyStateChanges(listener1);
    for (int i = 5; i < numMsgs; i++) {
        msg = consumer2.receive(1, TimeUnit.SECONDS);
        Assert.assertNotNull(msg);
        Assert.assertEquals(new String(msg.getData()), ""my-message-"" + i);
        consumer2.acknowledge(msg);
    }
    Assert.assertNull(consumer2.receive(1, TimeUnit.SECONDS));
    rolloverPerIntervalStats();
    Thread.sleep(ASYNC_EVENT_COMPLETION_WAIT);
    assertEquals(subRef.getNumberOfEntriesInBacklog(), 0);
    for (int i = 0; i < numMsgs; i++) {
        String message = ""my-message-"" + i;
        futures.add(producer.sendAsync(message.getBytes()));
    }
    FutureUtil.waitForAll(futures).get();
    futures.clear();
    for (int i = 0; i < 5; i++) {
        msg = consumer2.receive(1, TimeUnit.SECONDS);
        Assert.assertNotNull(msg);
        Assert.assertEquals(new String(msg.getData()), ""my-message-"" + i);
        consumer2.acknowledge(msg);
    }
    consumer1 = consumerBulder1.subscribe();
    Thread.sleep(CONSUMER_ADD_OR_REMOVE_WAIT_TIME);
    for (int i = 5; i < numMsgs; i++) {
        msg = consumer1.receive(1, TimeUnit.SECONDS);
        Assert.assertNotNull(msg);
        Assert.assertEquals(new String(msg.getData()), ""my-message-"" + i);
        consumer1.acknowledge(msg);
    }
    Assert.assertNull(consumer1.receive(1, TimeUnit.SECONDS));
    rolloverPerIntervalStats();
    Thread.sleep(ASYNC_EVENT_COMPLETION_WAIT);
    assertEquals(subRef.getNumberOfEntriesInBacklog(), 0);
    for (int i = 0; i < numMsgs; i++) {
        String message = ""my-message-"" + i;
        futures.add(producer.sendAsync(message.getBytes()));
    }
    FutureUtil.waitForAll(futures).get();
    futures.clear();
    for (int i = 0; i < 5; i++) {
        msg = consumer1.receive(1, TimeUnit.SECONDS);
        Assert.assertNotNull(msg);
        Assert.assertEquals(new String(msg.getData()), ""my-message-"" + i);
        consumer1.acknowledge(msg);
    }
    TestConsumerStateEventListener listener3 = new TestConsumerStateEventListener();
    Consumer<byte[]> consumer3 = consumerBuilder.clone().consumerName(""3"").consumerEventListener(listener3).subscribe();
    Thread.sleep(CONSUMER_ADD_OR_REMOVE_WAIT_TIME);
    verifyConsumerInactive(listener3, -1);
    Assert.assertNull(consumer3.receive(1, TimeUnit.SECONDS));
    for (int i = 5; i < numMsgs; i++) {
        msg = consumer1.receive(1, TimeUnit.SECONDS);
        Assert.assertNotNull(msg);
        Assert.assertEquals(new String(msg.getData()), ""my-message-"" + i);
        consumer1.acknowledge(msg);
    }
    rolloverPerIntervalStats();
    Thread.sleep(ASYNC_EVENT_COMPLETION_WAIT);
    assertEquals(subRef.getNumberOfEntriesInBacklog(), 0);
    try {
        consumer1.unsubscribe();
        fail(""should fail"");
    } catch (PulsarClientException e) {
    }
    consumer1.close();
    Thread.sleep(CONSUMER_ADD_OR_REMOVE_WAIT_TIME);
    consumer2.close();
    Thread.sleep(CONSUMER_ADD_OR_REMOVE_WAIT_TIME);
    try {
        consumer3.unsubscribe();
    } catch (PulsarClientException e) {
        fail(""Should not fail"", e);
    }
    Thread.sleep(ASYNC_EVENT_COMPLETION_WAIT);
    subRef = topicRef.getSubscription(subName);
    assertNull(subRef);
    producer.close();
    consumer3.close();
    admin.topics().delete(topicName);
}",concurrency,1
98,cdapio_cdap,ProgramLifecycleHttpHandlerTest.testStartProgramWithDisabledRuntimeArgs,"@Test
public void testStartProgramWithDisabledRuntimeArgs() throws Exception {
    ProfileId profileId = new NamespaceId(TEST_NAMESPACE1).profile(""MyProfile"");
    Profile profile = new Profile(""MyProfile"", Profile.NATIVE.getLabel(), Profile.NATIVE.getDescription(),Profile.NATIVE.getScope(), Profile.NATIVE.getProvisioner());
    putProfile(profileId, profile, 200);
    disableProfile(profileId, 200);
    deploy(AppWithWorkflow.class, 200, Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE1);
    ProgramId programId = new NamespaceId(TEST_NAMESPACE1).app(APP_WITH_WORKFLOW_APP_ID).workflow(APP_WITH_WORKFLOW_WORKFLOW_NAME);
    Assert.assertEquals(STOPPED, getProgramStatus(programId));
    startProgram(programId, Collections.singletonMap(SystemArguments.PROFILE_NAME, profileId.getScopedName()), 409);
    Assert.assertEquals(STOPPED, getProgramStatus(programId));
    startProgram(programId, Collections.singletonMap(SystemArguments.PROFILE_NAME, ProfileId.NATIVE.getScopedName()),200);
    waitState(programId, STOPPED);
}",async wait,0
99,stanfordnlp_CoreNLP,DirectedMultiGraphTest.testConnectedComponents,"@Test
public void testConnectedComponents() {
    System.out.println(""graph is "" + graph.toString());
    List<Set<Integer>> ccs = graph.getConnectedComponents();
    for (Set<Integer> cc : ccs) {
        System.out.println(""Connected component: "" + cc);
    }
    assertEquals(ccs.size(), 4);
    assertEquals(CollectionUtils.sorted(ccs.get(0)), Arrays.asList(1, 2, 3, 4));
}",unordered collections,3
101,apache_avro,testRecordWithJsr310LogicalTypes,"@Test
public void testRecordWithJsr310LogicalTypes() throws IOException {
    TestRecordWithJsr310LogicalTypes record = new TestRecordWithJsr310LogicalTypes(
    true,
    34,
    35L,
    3.14F,
    3019.34,
    null,
    java.time.LocalDate.now(),
    java.time.LocalTime.now().truncatedTo(ChronoUnit.MILLIS),
    java.time.Instant.now().truncatedTo(ChronoUnit.MILLIS),
    new BigDecimal(123.45f).setScale(2, BigDecimal.ROUND_HALF_DOWN)
    );
    File data = write(TestRecordWithJsr310LogicalTypes.getClassSchema(), record);
    List<TestRecordWithJsr310LogicalTypes> actual = read(
    TestRecordWithJsr310LogicalTypes.getClassSchema(), data);
    Assert.assertEquals(""Should match written record"", record, actual.get(0));
}",time,2
109,apache_pulsar,ServerCnxTest.testDuplicateConcurrentSubscribeCommand,"@Test
public void testDuplicateConcurrentSubscribeCommand() throws Exception {
    resetChannel();
    setChannelConnected();
    CompletableFuture<Topic> delayFuture = new CompletableFuture<>();
    doReturn(delayFuture).when(brokerService).getOrCreateTopic(any(String.class));
    ByteBuf clientCommand =
    Commands.newSubscribe(successTopicName, successSubName, 1, 1, Exclusive, 0, ""test"", 0);
    channel.writeInbound(clientCommand);
    clientCommand =
    Commands.newSubscribe(successTopicName, successSubName, 1, 1, Exclusive, 0, ""test"", 0);
    channel.writeInbound(clientCommand);
    Object response = getResponse();
    assertTrue(response instanceof CommandError, ""Response is not CommandError but "" + response);
    CommandError error = ((CommandError) (response));
    assertEquals(error.getError(), ServiceNotReady);
    channel.finish();
}",concurrency,1
116,apache_kylin,CoordinatorTest.testReassignFailOnStartNew,"@Test
public void testReassignFailOnStartNew() throws IOException {
    ReceiverAdminClient receiverAdminClient = mockReceiverClientFailOnStartNewComsumer();
    coordinator = new Coordinator(metadataStore, receiverAdminClient);
    Map<Integer, List<Partition>> preAssignMap = metadataStore.getAssignmentsByCube(cubeName).getAssignments();
    Map<Integer, List<Partition>> newAssignMap = new HashMap<>();
    newAssignMap.put(1, Lists.newArrayList(p1, p2, p3));
    newAssignMap.put(2, Lists.newArrayList(p4, p5));
    newAssignMap.put(3, Lists.newArrayList(p6));
    CubeAssignment preAssigment = new CubeAssignment(cube.getName(), preAssignMap);
    CubeAssignment newAssigment = new CubeAssignment(cube.getName(), newAssignMap);
    try {
        coordinator.doReassign(cube, preAssigment, newAssigment);
    } catch (ClusterStateException rune) {
        assertSame(ROLLBACK_FAILED, rune.getClusterState());
        assertSame(START_NEW, rune.getTransactionStep());
        System.out.println(rune.getMessage());
        throw rune;
    }
}",unordered collections,3
128,neo4j_neo4j,createdWorkerThreadsShouldContainConnectorName,"@Test
public void createdWorkerThreadsShouldContainConnectorName() throws Exception
{
    AtomicInteger processNextBatchCount = new AtomicInteger();
    AtomicReference<Thread> poolThread = new AtomicReference<>();
    AtomicReference<String> poolThreadName = new AtomicReference<>();
    String id = UUID.randomUUID().toString();
    BoltConnection connection = newConnection( id );
    when( connection.processNextBatch() ).thenAnswer( inv ->
    {
        poolThread.set( Thread.currentThread() );
        poolThreadName.set( Thread.currentThread().getName() );
        processNextBatchCount.incrementAndGet();
        return true;
    } );
    boltScheduler.start();
    boltScheduler.created( connection );
    boltScheduler.enqueued( connection, Jobs.noop() );
    Predicates.await( () -> processNextBatchCount.get() > 0, 1, MINUTES );
    assertThat( poolThread.get().getName(), not( equalTo( poolThreadName.get() ) ) );
    assertThat( poolThread.get().getName(), containsString( String.format( ""[%s]"", CONNECTOR_KEY ) ) );
    assertThat( poolThread.get().getName(), not( containsString( String.format( ""[%s]"", connection.remoteAddress() ) ) ) );
}",concurrency,1
133,ONSdigital_rm-collection-exercise-service,SampleSummaryServiceTest.testActivateSamples,"@Test
public void testActivateSamples() throws Exception {
    UUID collectionExerciseId = UUID.randomUUID();
    UUID surveyId = UUID.randomUUID();
    UUID sampleSummaryId = UUID.randomUUID();
    SampleLink sampleLink = new SampleLink();
    sampleLink.setSampleSummaryId(sampleSummaryId);
    sampleLink.setCollectionExerciseId(collectionExerciseId);
    List<SampleLink> sampleLinks = new ArrayList<>();
    sampleLinks.add(sampleLink);
    CollectionExercise collectionExercise = new CollectionExercise();
    collectionExercise.setId(collectionExerciseId);
    collectionExercise.setSurveyId(surveyId);
    Event event = new Event();
    event.setTimestamp(new Timestamp(System.currentTimeMillis()));
    when(collectionExerciseRepository.findOneById(collectionExerciseId)).thenReturn(collectionExercise);
    when(sampleLinkRepository.findByCollectionExerciseId(collectionExerciseId)).thenReturn(sampleLinks);
    when(eventRepository.findOneByCollectionExerciseAndTag(collectionExercise, go_live.name())).thenReturn(event);
    sampleSummaryService.activateSamples(collectionExerciseId);
    sampleSummaryService.sampleSummaryValidated(true, collectionExerciseId);
    sampleSummaryService.sampleSummaryDistributed(true, collectionExerciseId);
    verify(collectionExerciseRepository, times(3)).findOneById(collectionExerciseId);
    verify(sampleSummaryActivationPublisher, times(1)).sendSampleSummaryActivation(collectionExerciseId, sampleSummaryId, surveyId);
    verify(collectionExerciseService, times(1)).transitionCollectionExercise(collectionExercise, EXECUTE);
    verify(collectionExerciseService, times(1)).transitionCollectionExercise(collectionExercise, VALIDATE);
    verify(collectionExerciseService, times(1)).transitionCollectionExercise(collectionExercise, EXECUTION_COMPLETE);
    verify(collectionExerciseService, times(1)).transitionCollectionExercise(collectionExercise, GO_LIVE);
}",time,2
139,cdapio_cdap,WorkflowClientTestRun.testWorkflowClient,"@Test
public void testWorkflowClient() throws Exception {
    String outputPath = new File(tmpFolder.newFolder(), ""output"").getAbsolutePath();
    Map<String, String> runtimeArgs = ImmutableMap.of(""inputPath"", createInput(""input""),
    ""outputPath"", outputPath);
    Id.Workflow workflowId = Id.Workflow.from(appId, AppWithWorkflow.SampleWorkflow.NAME);
    programClient.start(workflowId, false, runtimeArgs);
    programClient.waitForStatus(workflowId, ""STOPPED"", 60, TimeUnit.SECONDS);
    List<RunRecord> workflowRuns = programClient.getProgramRuns(workflowId, ProgramRunStatus.COMPLETED.name(), 0,
    Long.MAX_VALUE, 10);
    Assert.assertEquals(1, workflowRuns.size());
    Id.Run workflowRunId = new Id.Run(workflowId, workflowRuns.get(0).getPid());
    try {
        workflowClient.getWorkflowToken(new Id.Run(Id.Workflow.from(appId, ""random""), workflowRunId.getId()));
        Assert.fail(""Should not find a workflow token for a non-existing workflow"");
    } catch (NotFoundException expected) {
    }
    try {
        workflowClient.getWorkflowToken(new Id.Run(workflowId, RunIds.generate().getId()));
        Assert.fail(""Should not find a workflow token for a random run id"");
    } catch (NotFoundException expected) {
    }
    WorkflowTokenDetail workflowToken = workflowClient.getWorkflowToken(workflowRunId);
    Assert.assertEquals(3, workflowToken.getTokenData().size());
    workflowToken = workflowClient.getWorkflowToken(workflowRunId, WorkflowToken.Scope.SYSTEM);
    Assert.assertTrue(workflowToken.getTokenData().size() > 0);
    workflowToken = workflowClient.getWorkflowToken(workflowRunId, ""start_time"");
    Map<String, List<WorkflowTokenDetail.NodeValueDetail>> tokenData = workflowToken.getTokenData();
    Assert.assertEquals(AppWithWorkflow.WordCountMapReduce.NAME, tokenData.get(""start_time"").get(0).getNode());
    Assert.assertTrue(Long.parseLong(tokenData.get(""start_time"").get(0).getValue()) < System.currentTimeMillis());
    workflowToken = workflowClient.getWorkflowToken(workflowRunId, WorkflowToken.Scope.USER, ""action_type"");
    tokenData = workflowToken.getTokenData();
    Assert.assertEquals(AppWithWorkflow.WordCountMapReduce.NAME, tokenData.get(""action_type"").get(0).getNode());
    Assert.assertEquals(""MapReduce"", tokenData.get(""action_type"").get(0).getValue());
    String nodeName = AppWithWorkflow.SampleWorkflow.firstActionName;
    WorkflowTokenNodeDetail workflowTokenAtNode =
    workflowClient.getWorkflowTokenAtNode(workflowRunId, nodeName);
    Assert.assertEquals(AppWithWorkflow.DummyAction.TOKEN_VALUE,
    workflowTokenAtNode.getTokenDataAtNode().get(AppWithWorkflow.DummyAction.TOKEN_KEY));
    workflowTokenAtNode = workflowClient.getWorkflowTokenAtNode(workflowRunId, nodeName, WorkflowToken.Scope.SYSTEM);
    Assert.assertEquals(0, workflowTokenAtNode.getTokenDataAtNode().size());
    workflowTokenAtNode = workflowClient.getWorkflowTokenAtNode(workflowRunId, nodeName,
    AppWithWorkflow.DummyAction.TOKEN_KEY);
    Assert.assertEquals(AppWithWorkflow.DummyAction.TOKEN_VALUE,
    workflowTokenAtNode.getTokenDataAtNode().get(AppWithWorkflow.DummyAction.TOKEN_KEY));
    String reduceOutputRecordsCounter = ""org.apache.hadoop.mapreduce.TaskCounter.REDUCE_OUTPUT_RECORDS"";
    workflowTokenAtNode = workflowClient.getWorkflowTokenAtNode(workflowRunId, AppWithWorkflow.WordCountMapReduce.NAME,
    WorkflowToken.Scope.SYSTEM, reduceOutputRecordsCounter);
    Assert.assertEquals(6, Integer.parseInt(workflowTokenAtNode.getTokenDataAtNode().get(reduceOutputRecordsCounter)));
}",async wait,0
144,spring-cloud_spring-cloud-config,GiteePropertyPathNotificationExtractorTests.giteeSample,"@Test
public void giteeSample() throws Exception {
    Map<String, Object> value = new ObjectMapper().readValue(new ClassPathResource(""pathsamples/gitee.json"").getInputStream(), new TypeReference<Map<String, Object>>() {});
    this.headers.set(""x-git-oschina-event"", ""Push Hook"");
    PropertyPathNotification extracted = this.extractor.extract(this.headers, value);
    assertThat(extracted).isNotNull();
    assertThat(extracted.getPaths()[0]).isEqualTo(""d.txt"");
}",unordered collections,3
152,apache_dubbo,737f7a7ea67832d7f17517326fb2491d0a086dd7.testListAllPort,"@Test
public void testListAllPort() throws RemotingException {
    String result = port.telnet(null, """");
    assertEquals(""20887"", result);
}",test order dependency,4
154,cdapio_cdap,TestFrameworkTestRun.testWorkerInstances,"@Test
public void testWorkerInstances() throws Exception {
    ApplicationManager applicationManager = deployApplication(testSpace, AppUsingGetServiceURL.class);
    WorkerManager workerManager = applicationManager.getWorkerManager(PINGING_WORKER).start();
    workerManager.waitForStatus(true);
    workerInstancesCheck(workerManager, 5);
    workerManager.setInstances(10);
    workerInstancesCheck(workerManager, 10);
    workerManager.setInstances(2);
    workerInstancesCheck(workerManager, 2);
    workerManager.setInstances(2);
    workerInstancesCheck(workerManager, 2);
    WorkerManager lifecycleWorkerManager = applicationManager.getWorkerManager(LIFECYCLE_WORKER).start();
    lifecycleWorkerManager.waitForStatus(true);
    lifecycleWorkerManager.setInstances(5);
    workerInstancesCheck(lifecycleWorkerManager, 5);
    for (int i = 0; i < 5; i++) {
        kvTableKeyCheck(testSpace, WORKER_INSTANCES_DATASET, Bytes.toBytes(String.format(""init.%d"", i)));
    }
    lifecycleWorkerManager.stop();
    lifecycleWorkerManager.waitForStatus(false);
    if (workerManager.isRunning()) {
        workerManager.stop();
    }
    workerManager.waitForStatus(false);
    workerInstancesCheck(lifecycleWorkerManager, 5);
    workerInstancesCheck(workerManager, 2);
    assertWorkerDatasetWrites(Bytes.toBytes(""init""), Bytes.stopKeyForPrefix(Bytes.toBytes(""init.2"")), 3, 3);
    assertWorkerDatasetWrites(Bytes.toBytes(""init.3""), Bytes.stopKeyForPrefix(Bytes.toBytes(""init"")), 2, 5);
    byte[] startRow = Bytes.toBytes(""stop"");
    assertWorkerDatasetWrites(startRow, Bytes.stopKeyForPrefix(startRow), 5, 5);
}",async wait,0
163,pushtorefresh_storio,NotifyAboutChangesTest.notifyAboutChangesConcurrently,"@Test
public void notifyAboutChangesConcurrently() {
    final int numberOfThreads = 100;
    final TestSubscriber<Changes> testSubscriber = new TestSubscriber<Changes>();
    final Set<String> tables = new HashSet<String>();
    final List<Changes> expectedChanges = new ArrayList<Changes>();
    for (int i = 0; i < numberOfThreads; i++) {
        final String table = ""test_table"" + i;
        tables.add(table);
        expectedChanges.add(Changes.newInstance(table));
    }
    storIOSQLite.observeChanges(LATEST).subscribe(testSubscriber);
    final CountDownLatch startAllThreadsLock = new CountDownLatch(1);
    for (int i = 0; i < numberOfThreads; i++) {
        final int finalI = i;
        new Thread(new Runnable() {
            @Override
            public void run() {
                try {
                    startAllThreadsLock.await();
                } catch (InterruptedException e) {
                    throw new RuntimeException(e);
                }
                storIOSQLite.lowLevel().notifyAboutChanges(Changes.newInstance(""test_table"" + finalI));
            }
        }).start();
    }
    startAllThreadsLock.countDown();
    final long startTime = SystemClock.elapsedRealtime();
    while ((testSubscriber.valueCount() != tables.size()) && ((SystemClock.elapsedRealtime() - startTime) < 20000)) {
        Thread.yield();
    }
    testSubscriber.assertNoErrors();
    testSubscriber.assertValueCount(expectedChanges.size());
    assertThat(expectedChanges.containsAll(testSubscriber.values())).isTrue();
}",concurrency,1
164,apache_ignite,GridCacheRebalancingWithAsyncClearingTest.testCorrectRebalancingCurrentlyRentingPartitions,"@Test
public void testCorrectRebalancingCurrentlyRentingPartitions() throws Exception {
    IgniteEx ignite = ((IgniteEx) (startGrids(3)));
    ignite.cluster().active(true);
    final int keysCnt = SF.applyLB(300000, 10000);
    try (final IgniteDataStreamer<Integer, Integer> ds = ignite.dataStreamer(CACHE_NAME)) {
        log.info(""Writing initial data..."");
        ds.allowOverwrite(true);
        for (int k = 1; k <= keysCnt; k++) {
            ds.addData(k, k);
            if ((k % 10000) == 0) {
                log.info((""Written "" + k) + "" entities."");
            }
        }
        log.info(""Writing initial data finished."");
    }
    startGrid(3);
    resetBaselineTopology();
    stopGrid(3);
    resetBaselineTopology();
    stopGrid(1);
    startGrid(1);
    awaitPartitionMapExchange();
    for (int k = 1; k <= keysCnt; k++) {
        Integer val = ((Integer) (ignite.cache(CACHE_NAME).get(k)));
        Assert.assertNotNull((""Value for "" + k) + "" is null"", val);
        Assert.assertEquals(((""Check failed for "" + k) + "" = "") + val, k, ((int) (val)));
    }
}",concurrency,1
167,neo4j_neo4j,RaftMessageProcessingMetricTest.shouldBeAbleToUpdateAllMessageTypes,"@Test
public void shouldBeAbleToUpdateAllMessageTypes() throws Throwable
{
    int durationNanos = 5;
    for ( RaftMessages.Type type : RaftMessages.Type.values() )
    {
        metric.updateTimer( type, Duration.ofNanos( durationNanos ) );
        assertEquals( 1, metric.timer( type ).getCount() );
        assertEquals( durationNanos, metric.timer( type ).getSnapshot().getMean(), 0 );
    }
    assertEquals( RaftMessages.Type.values().length, metric.timer().getCount() );
    assertEquals( 0, metric.timer().getSnapshot().getMean(), durationNanos );
}",time,2
170,apache_pulsar,AntiAffinityNamespaceGroupTest.testBrokerSelectionForAntiAffinityGroup,"@Test
public void testBrokerSelectionForAntiAffinityGroup() throws Exception {
    final String broker1 = primaryHost;
    final String broker2 = secondaryHost;
    final String cluster = pulsar1.getConfiguration().getClusterName();
    final String tenant = ""tenant-"" + UUID.randomUUID().toString();
    final String namespace1 = ((tenant + ""/"") + cluster) + ""/ns1"";
    final String namespace2 = ((tenant + ""/"") + cluster) + ""/ns2"";
    final String namespaceAntiAffinityGroup = ""group"";
    FailureDomain domain1 = new FailureDomain();
    domain1.brokers = Sets.newHashSet(broker1);
    admin1.clusters().createFailureDomain(cluster, ""domain1"", domain1);
    FailureDomain domain2 = new FailureDomain();
    domain2.brokers = Sets.newHashSet(broker2);
    admin1.clusters().createFailureDomain(cluster, ""domain2"", domain2);
    admin1.tenants().createTenant(tenant, new TenantInfo(null, Sets.newHashSet(cluster)));
    admin1.namespaces().createNamespace(namespace1);
    admin1.namespaces().createNamespace(namespace2);
    admin1.namespaces().setNamespaceAntiAffinityGroup(namespace1, namespaceAntiAffinityGroup);
    admin1.namespaces().setNamespaceAntiAffinityGroup(namespace2, namespaceAntiAffinityGroup);
    for (int i = 0; i < 5; i++) {
        if ((!isLoadManagerUpdatedDomainCache(primaryLoadManager)) || (!isLoadManagerUpdatedDomainCache(secondaryLoadManager))) {
            Thread.sleep(200);
        } else {
            break;
        }
    }
    assertTrue(isLoadManagerUpdatedDomainCache(primaryLoadManager));
    assertTrue(isLoadManagerUpdatedDomainCache(secondaryLoadManager));
    ServiceUnitId serviceUnit1 = makeBundle(tenant, cluster, ""ns1"");
    String selectedBroker1 = primaryLoadManager.selectBrokerForAssignment(serviceUnit1).get();
    ServiceUnitId serviceUnit2 = makeBundle(tenant, cluster, ""ns2"");
    String selectedBroker2 = primaryLoadManager.selectBrokerForAssignment(serviceUnit2).get();
    assertNotEquals(selectedBroker1, selectedBroker2);
}",concurrency,1
176,apache_avro,TestTraceCollection.testRecursingTrace,"@Test
public void testRecursingTrace() throws Exception {
    TracePluginConfiguration conf = new TracePluginConfiguration();
    conf.traceProb = 1.0;
    conf.port = 51010;
    conf.clientPort = 12346;
    TracePlugin aPlugin = new TracePlugin(conf);
    conf.port = 51011;
    conf.clientPort = 12347;
    TracePlugin bPlugin = new TracePlugin(conf);
    conf.port = 51012;
    conf.clientPort = 12348;
    TracePlugin cPlugin = new TracePlugin(conf);
    conf.port = 51013;
    conf.clientPort = 12349;
    TracePlugin dPlugin = new TracePlugin(conf);
    Responder bRes = new RecursingResponder(TestBasicTracing.advancedProtocol, bPlugin);
    bRes.addRPCPlugin(bPlugin);
    HttpServer server1 = new HttpServer(bRes, 21005);
    server1.start();
    Responder cRes = new EndpointResponder(TestBasicTracing.advancedProtocol);
    cRes.addRPCPlugin(cPlugin);
    HttpServer server2 = new HttpServer(cRes, 21006);
    server2.start();
    Responder dRes = new EndpointResponder(TestBasicTracing.advancedProtocol);
    dRes.addRPCPlugin(dPlugin);
    HttpServer server3 = new HttpServer(dRes, 21007);
    server3.start();
    HttpTransceiver trans = new HttpTransceiver(new URL(""http:www.example.com""));
    GenericRequestor r = new GenericRequestor(TestBasicTracing.advancedProtocol, trans);
    r.addRPCPlugin(aPlugin);
    GenericRecord params = new GenericData.Record(advancedProtocol.getMessages().get(""w"").getRequest());
    params.put(""req"", 1);
    for (int i = 0; i < 40; i++) {
        r.request(""w"", params);
    }
    List<Span> allSpans = new ArrayList<Span>();
    allSpans.addAll(aPlugin.storage.getAllSpans());
    allSpans.addAll(bPlugin.storage.getAllSpans());
    allSpans.addAll(cPlugin.storage.getAllSpans());
    allSpans.addAll(dPlugin.storage.getAllSpans());
    SpanAggregationResults results = SpanAggregator.getFullSpans(allSpans);
    assertEquals(0, results.incompleteSpans.size());
    List<Span> merged = results.completeSpans;
    List<Trace> traces = SpanAggregator.getTraces(merged).traces;
    assertEquals(40, traces.size());
    TraceCollection collection = new TraceCollection(traces.get(0));
    for (Trace t : traces) {
        collection.addTrace(t);
    }
    server1.close();
    server2.close();
    server3.close();
    aPlugin.httpServer.close();
    aPlugin.clientFacingServer.stop();
    bPlugin.httpServer.close();
    bPlugin.clientFacingServer.stop();
    cPlugin.httpServer.close();
    cPlugin.clientFacingServer.stop();
    dPlugin.httpServer.close();
    dPlugin.clientFacingServer.stop();
}",async wait,0
177,apache_pulsar,NamespacesTest.testSubscribeRate,"@Test
public void testSubscribeRate() throws Exception {
    SubscribeRate subscribeRate = new SubscribeRate(1, 5);
    String namespace = ""my-tenants/my-namespace"";
    admin.tenants().createTenant(""my-tenants"", new TenantInfoImpl(Sets.newHashSet(), Sets.newHashSet(testLocalCluster)));
    admin.namespaces().createNamespace(namespace, Sets.newHashSet(testLocalCluster));
    admin.namespaces().setSubscribeRate(namespace, subscribeRate);
    assertEquals(subscribeRate, admin.namespaces().getSubscribeRate(namespace));
    String topicName = ((""persistent""));
    admin.topics().createPartitionedTopic(topicName, 2);
    pulsar.getConfiguration().setAuthorizationEnabled(false);
    Consumer<?> consumer = pulsarClient.newConsumer().topic(topicName).subscriptionType(Shared).subscriptionName(""subscribe-rate"").subscribe();
    assertTrue(consumer.isConnected());
    pulsarClient.updateServiceUrl(lookupUrl.toString());
    Awaitility.await().untilAsserted(() -> assertFalse(consumer.isConnected()));
    Thread.sleep(6000L);
    pulsarClient.updateServiceUrl(lookupUrl.toString());
    assertTrue(consumer.isConnected());
    subscribeRate = new SubscribeRate(0, 10);
    admin.namespaces().setSubscribeRate(namespace, subscribeRate);
    pulsarClient.updateServiceUrl(lookupUrl.toString());
    Awaitility.await().untilAsserted(() -> assertTrue(consumer.isConnected()));
    pulsar.getConfiguration().setAuthorizationEnabled(true);
    admin.topics().deletePartitionedTopic(topicName, true);
    admin.namespaces().deleteNamespace(namespace);
    admin.tenants().deleteTenant(""my-tenants"");
}",async wait,0
181,spring-projects_spring-data-couchbase,MappingCouchbaseConverterTests.writesAndReadsCustomFieldsConvertedClass,"@Test
void writesAndReadsCustomFieldsConvertedClass() {
    List<Object> converters = new ArrayList<>();
    converters.add(BigDecimalToStringConverter.INSTANCE);
    converters.add(StringToBigDecimalConverter.INSTANCE);
    CustomConversions customConversions = new CouchbaseCustomConversions(converters);
    converter.setCustomConversions(customConversions);
    converter.afterPropertiesSet();
    ((CouchbaseMappingContext) (converter.getMappingContext())).setSimpleTypeHolder(customConversions.getSimpleTypeHolder());
    CouchbaseDocument converted = new CouchbaseDocument();
    final String valueStr = ""12.345"";
    final BigDecimal value = new BigDecimal(valueStr);
    final String value2Str = ""0.6789"";
    final BigDecimal value2 = new BigDecimal(value2Str);
    List<BigDecimal> listOfValues = new ArrayList<>();
    listOfValues.add(value);
    listOfValues.add(value2);
    Map<String, BigDecimal> mapOfValues = new HashMap<>();
    mapOfValues.put(""val1"", value);
    mapOfValues.put(""val2"", value2);
    CustomFieldsEntity entity = new CustomFieldsEntity(value, listOfValues, mapOfValues);
    converter.write(entity, converted);
    CouchbaseDocument source = new CouchbaseDocument();
    source.put(""_class"", CustomFieldsEntity.class.getName());
    source.put(""decimalValue"", valueStr);
    CouchbaseList listOfValuesDoc = new CouchbaseList();
    listOfValuesDoc.put(valueStr);
    listOfValuesDoc.put(value2Str);
    source.put(""listOfDecimalValues"", listOfValuesDoc);
    CouchbaseDocument mapOfValuesDoc = new CouchbaseDocument();
    mapOfValuesDoc.put(""val1"", valueStr);
    mapOfValuesDoc.put(""val2"", value2Str);
    source.put(""mapOfDecimalValues"", mapOfValuesDoc);
    assertThat(valueStr).isEqualTo(((CouchbaseList) (converted.getContent().get(""listOfDecimalValues""))).get(0));
    assertThat(value2Str).isEqualTo(((CouchbaseList) (converted.getContent().get(""listOfDecimalValues""))).get(1));
    assertThat(converted.export().toString()).isEqualTo(source.export().toString());
    CustomFieldsEntity readConverted = converter.read(CustomFieldsEntity.class, source);
    assertThat(readConverted.value).isEqualTo(value);
    assertThat(readConverted.listOfValues.get(0)).isEqualTo(listOfValues.get(0));
    assertThat(readConverted.listOfValues.get(1)).isEqualTo(listOfValues.get(1));
    assertThat(readConverted.mapOfValues.get(""val1"")).isEqualTo(mapOfValues.get(""val1""));
    assertThat(readConverted.mapOfValues.get(""val2"")).isEqualTo(mapOfValues.get(""val2""));
}",unordered collections,3
184,apache_pulsar,testTopicLevelInactivePolicyUpdateAndClean,"@Test
public void testTopicLevelInactivePolicyUpdateAndClean() throws Exception {
    super.resetConfig();
    conf.setSystemTopicEnabled(true);
    conf.setTopicLevelPoliciesEnabled(true);
    conf.setBrokerDeleteInactiveTopicsEnabled(true);
    conf.setBrokerDeleteInactiveTopicsMaxInactiveDurationSeconds(1000);
    conf.setBrokerDeleteInactiveTopicsMode(delete_when_no_subscriptions);
    InactiveTopicPolicies defaultPolicy = new InactiveTopicPolicies(InactiveTopicDeleteMode.delete_when_no_subscriptions, 1000, true);
    super.baseSetup();
    Thread.sleep(2000);
    final String namespace = ""prop/ns-abc"";
    final String topic = ""persistent"";
    final String topic2 = ""persistent"";
    final String topic3 = ""persistent"";
    List<String> topics = Arrays.asList(topic, topic2, topic3);
    for (String tp : topics) {
        admin.topics().createNonPartitionedTopic(tp);
    }
    InactiveTopicPolicies inactiveTopicPolicies = new InactiveTopicPolicies(InactiveTopicDeleteMode.delete_when_no_subscriptions, 1, true);
    admin.topics().setInactiveTopicPolicies(topic, inactiveTopicPolicies);
    inactiveTopicPolicies.setInactiveTopicDeleteMode(delete_when_subscriptions_caught_up);
    admin.topics().setInactiveTopicPolicies(topic2, inactiveTopicPolicies);
    inactiveTopicPolicies.setInactiveTopicDeleteMode(delete_when_no_subscriptions);
    admin.topics().setInactiveTopicPolicies(topic3, inactiveTopicPolicies);
    for (int i = 0; i < 50; i++) {
        if (admin.topics().getInactiveTopicPolicies(topic) != null) {
            break;
        }
        Thread.sleep(100);
    }
    InactiveTopicPolicies policies = ((PersistentTopic) (pulsar.getBrokerService().getTopic(topic, false).get().get())).inactiveTopicPolicies;
    Assert.assertTrue(policies.isDeleteWhileInactive());
    assertEquals(policies.getInactiveTopicDeleteMode(), delete_when_no_subscriptions);
    assertEquals(policies.getMaxInactiveDurationSeconds(), 1);
    assertEquals(policies, admin.topics().getInactiveTopicPolicies(topic));
    admin.topics().removeInactiveTopicPolicies(topic);
    for (int i = 0; i < 50; i++) {
        if (admin.topics().getInactiveTopicPolicies(topic) == null) {
            break;
        }
        Thread.sleep(100);
    }
    assertEquals(((PersistentTopic) (pulsar.getBrokerService().getTopic(topic, false).get().get())).inactiveTopicPolicies, defaultPolicy);
    policies = ((PersistentTopic) (pulsar.getBrokerService().getTopic(topic2, false).get().get())).inactiveTopicPolicies;
    Assert.assertTrue(policies.isDeleteWhileInactive());
    assertEquals(policies.getInactiveTopicDeleteMode(), delete_when_subscriptions_caught_up);
    assertEquals(policies.getMaxInactiveDurationSeconds(), 1);
    assertEquals(policies, admin.topics().getInactiveTopicPolicies(topic2));
    inactiveTopicPolicies.setMaxInactiveDurationSeconds(999);
    admin.namespaces().setInactiveTopicPolicies(namespace, inactiveTopicPolicies);
    Thread.sleep(1000);
    admin.topics().removeInactiveTopicPolicies(topic2);
    for (int i = 0; i < 50; i++) {
        if (admin.topics().getInactiveTopicPolicies(topic2) == null) {
            break;
        }
        Thread.sleep(100);
    }
    InactiveTopicPolicies nsPolicies = ((PersistentTopic) (pulsar.getBrokerService().getTopic(topic2, false).get().get())).inactiveTopicPolicies;
    assertEquals(nsPolicies.getMaxInactiveDurationSeconds(), 999);
    super.internalCleanup();
}",async wait,0
187,apache_pulsar,ManagedLedgerTest.testMaximumRolloverTime,"@Test
public void testMaximumRolloverTime() throws Exception {
    ManagedLedgerConfig conf = new ManagedLedgerConfig();
    conf.setMaxEntriesPerLedger(5);
    conf.setMinimumRolloverTime(1, SECONDS);
    conf.setMaximumRolloverTime(1, SECONDS);
    ManagedLedgerImpl ledger = ((ManagedLedgerImpl) (factory.open(""my_test_maxtime_ledger"", conf)));
    ledger.openCursor(""c1"");
    ledger.addEntry(""data"".getBytes());
    ledger.addEntry(""data"".getBytes());
    assertEquals(ledger.getLedgersInfoAsList().size(), 1);
    Thread.sleep(2000);
    ledger.addEntry(""data"".getBytes());
    ledger.addEntry(""data"".getBytes());
    assertEquals(ledger.getLedgersInfoAsList().size(), 2);
}",async wait,0
190,apache_dubbo,737f7a7ea67832d7f17517326fb2491d0a086dd7.testChangeServiceNotExport,"@Test
public void testChangeServiceNotExport() throws RemotingException {
    String result = change.telnet(mockChannel, ""demo"");
    assertEquals(""No such service demo"", result);
}",test order dependency,4
191,apache_pulsar,DiscoveryServiceTest.testBrokerDiscoveryRoundRobin,"@Test
public void testBrokerDiscoveryRoundRobin() throws Exception {
    addBrokerToZk(5);
    String prevUrl = null;
    for (int i = 0; i < 10; i++) {
        String current = service.getDiscoveryProvider().nextBroker().getPulsarServiceUrl();
        assertNotEquals(prevUrl, current);
        prevUrl = current;
    }
}",async wait,0
197,apache_pulsar,PrometheusMetricsTest.testPerTopicStats,"@Test
public void testPerTopicStats() throws Exception {
    String randSeed = randomName(16);
    System.out.println(""The randSeed of testPerTopicStats() is: "" + randSeed);
    Producer<byte[]> p1 = pulsarClient.newProducer().topic(""persistent://my-property/use/"" + randSeed + ""/my-topic1"").create();
    Producer<byte[]> p2 = pulsarClient.newProducer().topic(""persistent://my-property/use/"" + randSeed + ""/my-topic2"").create();
    for (int i = 0; i < 10; i++) {
        String message = ""my-message-"" + i;
        p1.send(message.getBytes());
        p2.send(message.getBytes());
    }
    ByteArrayOutputStream statsOut = new ByteArrayOutputStream();
    PrometheusMetricsGenerator.generate(pulsar, true, false, statsOut);
    String metricsStr = new String(statsOut.toByteArray());
    Multimap<String, Metric> metrics = parseMetrics(metricsStr);
    metrics.entries().forEach(e -> {
        System.out.println(e.getKey() + "": "" + e.getValue());
    });
    List<Metric> cm = (List<Metric>) metrics.get(""pulsar_storage_write_latency_le_1"");
    List<Metric> matchingMetrics = cm.stream().filter(t -> t.tags.containsValue(""my-property/use/"" + randSeed)).collect(Collectors.toList());
    int positionOfTopic1;
    int positionOfTopic2;
    if(cm.get(0).tags.get(""topic"").equals(""persistent://my-property/use/"" + randSeed + ""/my-topic1"")) {
        positionOfTopic1 = 0;
        positionOfTopic2 = 1;
    } else {
        positionOfTopic2 = 0;
        positionOfTopic1 = 1;
    }
    matchingMetrics = cm.stream().filter(t -> t.tags.containsValue(""my-property/use/"" + randSeed)).collect(Collectors.toList());
    if(matchingMetrics.size() > 2){
        System.out.println(""matchingMetrics.size() > 2 in testPerTopicStats(). First check. Debug entries: "");
        matchingMetrics.forEach(t -> t.tags.entrySet().forEach(kv -> System.out.println(kv.getKey() + "":""  + kv.getValue())));
    }
    assertEquals(matchingMetrics.size(), 2);
    assertEquals(matchingMetrics.get(positionOfTopic2).tags.get(""topic""), ""persistent://my-property/use/"" + randSeed + ""/my-topic2"");
    assertEquals(matchingMetrics.get(positionOfTopic2).tags.get(""namespace""), ""my-property/use/"" + randSeed);
    assertEquals(matchingMetrics.get(positionOfTopic1).tags.get(""topic""), ""persistent://my-property/use/"" + randSeed + ""/my-topic1"");
    assertEquals(matchingMetrics.get(positionOfTopic1).tags.get(""namespace""), ""my-property/use/"" + randSeed);
    cm = (List<Metric>) metrics.get(""pulsar_producers_count"");
    if(cm.get(1).tags.get(""topic"").equals(""persistent://my-property/use/"" + randSeed + ""/my-topic1"")) {
        positionOfTopic1 = 1;
        positionOfTopic2 = 2;
    } else {
        positionOfTopic2 = 1;
        positionOfTopic1 = 2;
    }
    matchingMetrics = cm.stream().filter(t -> t.tags.containsValue(""my-property/use/"" + randSeed)).collect(Collectors.toList());
    if(matchingMetrics.size() > 2){
        System.out.println(""matchingMetrics.size() > 2 in testPerTopicStats(). Second check. Debug entries: "");
        matchingMetrics.forEach(t -> t.tags.entrySet().forEach(kv -> System.out.println(kv.getKey() + "":""  + kv.getValue())));
    }
    assertEquals(matchingMetrics.size(), 2);
    assertEquals(matchingMetrics.get(positionOfTopic2).tags.get(""topic""), ""persistent://my-property/use/"" + randSeed + ""/my-topic2"");
    assertEquals(matchingMetrics.get(positionOfTopic2).tags.get(""namespace""), ""my-property/use/"" + randSeed);
    assertEquals(matchingMetrics.get(positionOfTopic1).tags.get(""topic""), ""persistent://my-property/use/"" + randSeed + ""/my-topic1"");
    assertEquals(matchingMetrics.get(positionOfTopic1).tags.get(""namespace""), ""my-property/use/"" + randSeed);
    cm = (List<Metric>) metrics.get(""topic_load_times_count"");
    if(cm.size() > 1){
        System.out.println(""matchingMetrics.size() > 2 in testPerTopicStats(). Third check. Debug entries: "");
        cm.forEach(t -> t.tags.entrySet().forEach(kv -> System.out.println(kv.getKey() + "":""  + kv.getValue())));
    }
    assertEquals(cm.size(), 1);
    assertEquals(cm.get(0).tags.get(""cluster""), ""test"");
    cm = (List<Metric>) metrics.get(""pulsar_in_bytes_total"");
    if(cm.get(0).tags.get(""topic"").equals(""persistent://my-property/use/"" + randSeed + ""/my-topic1"")) {
        positionOfTopic1 = 0;
        positionOfTopic2 = 1;
    } else {
        positionOfTopic2 = 0;
        positionOfTopic1 = 1;
    }
    matchingMetrics = cm.stream().filter(t -> t.tags.containsValue(""my-property/use/"" + randSeed)).collect(Collectors.toList());
    if(matchingMetrics.size() > 2){
        System.out.println(""matchingMetrics.size() > 2 in testPerTopicStats(). Fourth check. Debug entries: "");
        matchingMetrics.forEach(t -> t.tags.entrySet().forEach(kv -> System.out.println(kv.getKey() + "":""  + kv.getValue())));
    }
    assertEquals(matchingMetrics.size(), 2);
    assertEquals(matchingMetrics.get(positionOfTopic2).tags.get(""topic""), ""persistent://my-property/use/"" + randSeed + ""/my-topic2"");
    assertEquals(matchingMetrics.get(positionOfTopic2).tags.get(""namespace""), ""my-property/use/"" + randSeed);
    assertEquals(matchingMetrics.get(positionOfTopic1).tags.get(""topic""), ""persistent://my-property/use/"" + randSeed + ""/my-topic1"");
    assertEquals(matchingMetrics.get(positionOfTopic1).tags.get(""namespace""), ""my-property/use/"" + randSeed);
    cm = (List<Metric>) metrics.get(""pulsar_in_messages_total"");
    if(cm.get(0).tags.get(""topic"").equals(""persistent://my-property/use/"" + randSeed + ""/my-topic1"")) {
        positionOfTopic1 = 0;
        positionOfTopic2 = 1;
    } else {
        positionOfTopic2 = 0;
        positionOfTopic1 = 1;
    }
    matchingMetrics = cm.stream().filter(t -> t.tags.containsValue(""my-property/use/"" + randSeed)).collect(Collectors.toList());
    if(matchingMetrics.size() > 2){
        System.out.println(""matchingMetrics.size() > 2 in testPerTopicStats(). Fifth check. Debug entries: "");
        matchingMetrics.forEach(t -> t.tags.entrySet().forEach(kv -> System.out.println(kv.getKey() + "":""  + kv.getValue())));
    }
    assertEquals(matchingMetrics.size(), 2);
    assertEquals(matchingMetrics.get(positionOfTopic2).tags.get(""topic""), ""persistent://my-property/use/"" + randSeed + ""/my-topic2"");
    assertEquals(matchingMetrics.get(positionOfTopic2).tags.get(""namespace""), ""my-property/use/"" + randSeed);
    assertEquals(matchingMetrics.get(positionOfTopic1).tags.get(""topic""), ""persistent://my-property/use/"" + randSeed + ""/my-topic1"");
    assertEquals(matchingMetrics.get(positionOfTopic1).tags.get(""namespace""), ""my-property/use/"" + randSeed);
    p1.close();
    p2.close();
}",concurrency,1
202,cdapio_cdap,ProvisioningServiceTest.testCancelDeprovision,"@Test
public void testCancelDeprovision() throws Exception {
    ProvisionerInfo provisionerInfo = new MockProvisioner.PropertyBuilder().waitDelete(1, TimeUnit.MINUTES).build();
    TaskFields taskFields = testProvision(ProvisioningOp.Status.CREATED, provisionerInfo);
    Runnable task = Transactionals.execute(transactional, dsContext -> {
        return provisioningService.deprovision(taskFields.programRunId, dsContext);
    });
    task.run();
    Assert.assertTrue(provisioningService.cancelDeprovisionTask(taskFields.programRunId).isPresent());
    ProvisioningTaskKey taskKey = new ProvisioningTaskKey(taskFields.programRunId, ProvisioningOp.Type.DEPROVISION);
    waitForExpectedProvisioningState(taskKey, ProvisioningOp.Status.CANCELLED);
}",test order dependency,4
207,apache_pulsar,testAsyncFunction,"@Test
public void testAsyncFunction() throws Exception {
    InstanceConfig instanceConfig = new InstanceConfig();
    Function<String, CompletableFuture<String>> function = (input, context) -> {
        log.info(""input string: {}"", input);
        CompletableFuture<String> result  = new CompletableFuture<>();
        Executors.newCachedThreadPool().submit(() -> {
            try {
                Thread.sleep(500);
                result.complete(String.format(""%s-lambda"", input));
            } catch (Exception e) {
                result.completeExceptionally(e);
            }
        });
        return result;
    };
    JavaInstance instance = new JavaInstance(
    mock(ContextImpl.class),
    function,
    instanceConfig);
    String testString = ""ABC123"";
    CompletableFuture<JavaExecutionResult> result = instance.handleMessage(mock(Record.class), testString);
    assertNotNull(result.get().getResult());
    assertEquals(new String(testString + ""-lambda""), result.get().getResult());
    instance.close();
}",concurrency,1
211,apache_pulsar,testLedgerReachMaximumRolloverTime,"@Test
public void testLedgerReachMaximumRolloverTime() throws Exception {
    ManagedLedgerConfig config = new ManagedLedgerConfig();
    config.setMinimumRolloverTime(1, TimeUnit.MILLISECONDS);
    config.setMaximumRolloverTime(1, TimeUnit.SECONDS);
    ManagedLedger ml = factory.open(""ledger-reach-maximum-rollover-time"", config);
    long firstLedgerId = ml.addEntry(""test"".getBytes()).getLedgerId();
    Awaitility.await()
    .atMost(1100, TimeUnit.MILLISECONDS)
    .pollInterval(100, TimeUnit.MILLISECONDS)
    .until(() -> firstLedgerId != ml.addEntry(""test"".getBytes()).getLedgerId());
}",async wait,0
214,cdapio_cdap,MetadataSubscriberServiceTest.testSubscriber,"@Test
public void testSubscriber() throws InterruptedException, ExecutionException, TimeoutException {
    LineageWriter lineageWriter = getInjector().getInstance(MessagingLineageWriter.class);
    ProgramRunId run1 = service1.run(RunIds.generate());
    lineageWriter.addAccess(run1, dataset1, AccessType.READ);
    lineageWriter.addAccess(run1, dataset2, AccessType.WRITE);
    LineageStoreReader lineageReader = getInjector().getInstance(LineageStoreReader.class);
    ProgramRunId run1 = service1.run(RunIds.generate());
    Set<NamespacedEntityId> entities = lineageReader.getEntitiesForRun(run1);
    Assert.assertTrue(entities.isEmpty());
    LineageWriter lineageWriter = getInjector().getInstance(MessagingLineageWriter.class);
    lineageWriter.addAccess(run1, dataset1, AccessType.READ);
    lineageWriter.addAccess(run1, dataset2, AccessType.WRITE);
    FieldLineageWriter fieldLineageWriter = getInjector().getInstance(MessagingLineageWriter.class);
    ProgramRunId spark1Run1 = spark1.run(RunIds.generate(100));
    ReadOperation read = new ReadOperation(""read"", ""some read"", EndPoint.of(""ns"", ""endpoint1""), ""offset"", ""body"");
    TransformOperation parse = new TransformOperation(""parse"", ""parse body"",
    Collections.singletonList(InputField.of(""read"", ""body"")),
    ""name"", ""address"");
    WriteOperation write = new WriteOperation(""write"", ""write data"", EndPoint.of(""ns"", ""endpoint2""),
    Arrays.asList(InputField.of(""read"", ""offset""),
    InputField.of(""parse"", ""name""),
    InputField.of(""parse"", ""address"")));
    List<Operation> operations = new ArrayList<>();
    operations.add(read);
    operations.add(write);
    operations.add(parse);
    FieldLineageInfo info1 = new FieldLineageInfo(operations);
    fieldLineageWriter.write(spark1Run1, info1);
    ProgramRunId spark1Run2 = spark1.run(RunIds.generate(200));
    fieldLineageWriter.write(spark1Run2, info1);
    List<Operation> operations2 = new ArrayList<>();
    operations2.add(read);
    operations2.add(parse);
    TransformOperation normalize = new TransformOperation(""normalize"", ""normalize address"",
    Collections.singletonList(InputField.of(""parse"", ""address"")),
    ""address"");
    operations2.add(normalize);
    WriteOperation anotherWrite = new WriteOperation(""anotherwrite"", ""write data"", EndPoint.of(""ns"", ""endpoint2""),
    Arrays.asList(InputField.of(""read"", ""offset""),
    InputField.of(""parse"", ""name""),
    InputField.of(""normalize"", ""address"")));
    operations2.add(anotherWrite);
    FieldLineageInfo info2 = new FieldLineageInfo(operations2);
    ProgramRunId spark1Run3 = spark1.run(RunIds.generate(300));
    fieldLineageWriter.write(spark1Run3, info2);
    UsageWriter usageWriter = getInjector().getInstance(MessagingUsageWriter.class);
    usageWriter.register(spark1, dataset1);
    usageWriter.registerAll(Collections.singleton(spark1), dataset3);
    Set<NamespacedEntityId> expectedLineage = new HashSet<>(Arrays.asList(run1.getParent(), dataset1, dataset2));
    Tasks.waitFor(true, () -> expectedLineage.equals(lineageReader.getEntitiesForRun(run1)),
    10, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);
    Assert.assertTrue(lineageReader.getRelations(spark1, 0L, Long.MAX_VALUE, x -> true).isEmpty());
    FieldLineageReader fieldLineageReader = getInjector().getInstance(FieldLineageReader.class);
    Set<Operation> expectedOperations = new HashSet<>();
    expectedOperations.add(read);
    expectedOperations.add(anotherWrite);
    List<ProgramRunOperations> expected = new ArrayList<>();
    expected.add(new ProgramRunOperations(Collections.singleton(spark1Run3), expectedOperations));
    expectedOperations = new HashSet<>();
    expectedOperations.add(read);
    expectedOperations.add(write);
    expected.add(new ProgramRunOperations(new HashSet<>(Arrays.asList(spark1Run1, spark1Run2)),
    expectedOperations));
    EndPointField endPointField = new EndPointField(EndPoint.of(""ns"", ""endpoint2""), ""offset"");
    Tasks.waitFor(expected, () -> fieldLineageReader.getIncomingOperations(endPointField, 1L, Long.MAX_VALUE - 1),
    10, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);
    Set<EntityId> expectedUsage = new HashSet<>(Arrays.asList(dataset1, dataset3));
    UsageRegistry usageRegistry = getInjector().getInstance(UsageRegistry.class);
    Tasks.waitFor(true, () -> expectedUsage.equals(usageRegistry.getDatasets(spark1)),
    10, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);
}",async wait,0
215,neo4j_neo4j,shouldPickANewServerToWriteToOnLeaderSwitch,"@Test
public void shouldPickANewServerToWriteToOnLeaderSwitch() throws Throwable
{
    cluster = clusterRule.withNumberOfEdgeMembers( 0 ).startCluster();
    CoreClusterMember leader = cluster.awaitLeader();
    CountDownLatch startTheLeaderSwitching = new CountDownLatch( 1 );
    Thread thread = new Thread( () ->
    {
        try
        {
            startTheLeaderSwitching.await();
            CoreClusterMember theLeader = cluster.awaitLeader();
            switchLeader( theLeader );
        }
        catch ( TimeoutException | InterruptedException e )
        {
        }
    } );
    thread.start();
    Config config = Config.build().withLogging( new JULogging( Level.OFF ) ).toConfig();
    try ( Driver driver = GraphDatabase
    .driver( leader.routingURI(), AuthTokens.basic( ""neo4j"", ""neo4j"" ), config ) )
    {
        boolean success = false;
        Set<BoltServerAddress> seenAddresses = new HashSet<>();
        long deadline = System.currentTimeMillis() + (30 * 1000);
        while ( !success )
        {
            if ( System.currentTimeMillis() > deadline )
            {
                fail( ""Failed to write to the new leader in time"" );
            }
            try ( Session session = driver.session( AccessMode.WRITE ) )
            {
                startTheLeaderSwitching.countDown();
                BoltServerAddress boltServerAddress = ((RoutingNetworkSession) session).address();
                seenAddresses.add( boltServerAddress );
                session.run( ""CREATE (p:Person)"" );
                success = seenAddresses.size() >= 2;
            }
            catch ( Exception e )
            {
                Thread.sleep( 100 );
            }
        }
    }
    finally
    {
        thread.join();
    }
}",concurrency,1
229,apache_pulsar,AvroSchemaTest.testNotAllowNullSchema,"@Test
public void testNotAllowNullSchema() {
    AvroSchema<Foo> avroSchema = AvroSchema.of(SchemaDefinition.<Foo>builder().withPojo(Foo.class).withAlwaysAllowNull(false).build());
    assertEquals(avroSchema.getSchemaInfo().getType(), AVRO);
    Schema.Parser parser = new Schema.Parser();
    String schemaJson = new String(avroSchema.getSchemaInfo().getSchema());
    assertEquals(schemaJson, SCHEMA_AVRO_NOT_ALLOW_NULL);
    Schema schema = parser.parse(schemaJson);
    for (String fieldName : FOO_FIELDS) {
        Schema.Field field = schema.getField(fieldName);
        Assert.assertNotNull(field);
        if (field.name().equals(""field4"")) {
            Assert.assertNotNull(field.schema().getTypes().get(1).getField(""field1""));
        }
        if (field.name().equals(""fieldUnableNull"")) {
            Assert.assertNotNull(field.schema().getType());
        }
    }
}",unordered collections,3
238,apache_pulsar,ReplicatorTest.testReplicatorProducerName,"@Test
public void testReplicatorProducerName() throws Exception {
    log.info(""--- Starting ReplicatorTest::testReplicatorProducerName ---"");
    final String topicName = BrokerTestUtil.newUniqueName(""persistent"");
    final TopicName dest = TopicName.get(topicName);
    @Cleanup
    MessageProducer producer1 = new MessageProducer(url1, dest);
    Awaitility.await().untilAsserted(() -> {
        assertTrue(pulsar2.getBrokerService().getTopicReference(topicName).isPresent());
    });
    Optional<Topic> topic = pulsar2.getBrokerService().getTopicReference(topicName);
    assertTrue(topic.isPresent());
    Set<String> remoteClusters = topic.get().getProducers().values().stream().map(Producer::getRemoteCluster).collect(Collectors.toSet());
    assertTrue(remoteClusters.contains(""r1""));
}",async wait,0
245,doanduyhai_Achilles,TestEntityWithStaticAnnotations.should_insert_using_static_strategy_an_consistency_level,"@Test
public void should_insert_using_static_strategy_an_consistency_level() throws Exception {
    final long id = RandomUtils.nextLong(0L, Long.MAX_VALUE);
    scriptExecutor.executeScriptTemplate(""EntityWithStaticAnnotations/insert_single_row.cql"", ImmutableMap.of(""id"", id));
    final EntityWithStaticAnnotations entity = new EntityWithStaticAnnotations(id, ""new_val"", null);
    final CassandraLogAsserter logAsserter = new CassandraLogAsserter();
    logAsserter.prepareLogLevelForDriverConnection();
    manager.crud().insert(entity).usingTimeToLive(1000).execute();
    Row actual = session.execute(""SELECT * FROM entity_static_annotations WHERE partition_key = "" + id).one();
    assertThat(actual).isNotNull();
    assertThat(actual.getString(""value"")).isEqualTo(""new_val"");
    assertThat(actual.getString(""\""overRiden\"""")).isEqualTo(""overriden_val"");
    logAsserter.assertConsistencyLevels(LOCAL_ONE);
}",async wait,0
248,apache_ignite,SystemCacheNotConfiguredTest.test,"@Test
public void test() throws Exception {
    captureErr();
    new Thread(this::startServer).start();
    Ignite client = startGrid(getConfiguration(""client"").setClientMode(true));
    IgniteServices services = client.services();
    SimpleService srvc = services.serviceProxy(""service"", SimpleService.class, false);
    Thread.sleep(1000);
    srvc.isWorking();
    assertFalse(getErr().contains(""Cache is not configured:""));
}",async wait,0
253,neo4j_neo4j,RobustJobSchedulerWrapperTest.recurringJobWithErrorShouldStop,"@Test
public void recurringJobWithErrorShouldStop() throws Exception
{
    RobustJobSchedulerWrapper robustWrapper = new RobustJobSchedulerWrapper( actualScheduler, log );
    AtomicInteger count = new AtomicInteger();
    Error e = new Error();
    JobHandle jobHandle = robustWrapper.scheduleRecurring( ""JobName"", 1, () ->{
        count.incrementAndGet();
        throw e;
    }
    );
    Thread.sleep( 50 );
    assertEventually( ""run count"", count::get, Matchers.equalTo( 1 ), DEFAULT_TIMEOUT_MS , MILLISECONDS );
    robustWrapper.cancelAndWaitTermination( jobHandle );
    verify( log, timeout( DEFAULT_TIMEOUT_MS ).times( 1 ) ).error( ""Uncaught error rethrown"", e );
}",concurrency,1
259,microsoft_botbuilder-java,AdditionalPropertiesSerializerTests.canSerializeAdditionalProperties,"@Test
public void canSerializeAdditionalProperties() throws Exception {
    Foo foo = new Foo();
    foo.bar = ""hello.world"";
    foo.baz = new ArrayList<>();
    foo.baz.add(""hello"");
    foo.baz.add(""hello.world"");
    foo.qux = new HashMap<>();
    foo.qux.put(""hello"", ""world"");
    foo.qux.put(""a.b"", ""c.d"");
    foo.qux.put(""bar.a"", ""ttyy"");
    foo.qux.put(""bar.b"", ""uuzz"");
    foo.additionalProperties = new HashMap<>();
    foo.additionalProperties.put(""bar"", ""baz"");
    foo.additionalProperties.put(""a.b"", ""c.d"");
    foo.additionalProperties.put(""properties.bar"", ""barbar"");
    String serialized = new JacksonAdapter().serialize(foo);
    Assert.assertEquals(""{\""$type\"":\""foo\"",\""properties\"":{\""bar\"":\""hello.world\"",\""props\"":{\""baz\"":[\""hello\"",\""hello.world\""],\""q\"":{\""qux\"":{\""hello\"":\""world\"",\""a.b\"":\""c.d\"",\""bar.b\"":\""uuzz\"",\""bar.a\"":\""ttyy\""}}}},\""bar\"":\""baz\"",\""a.b\"":\""c.d\"",\""properties.bar\"":\""barbar\""}"", serialized);
}",unordered collections,3
262,cdapio_cdap,WorkflowHttpHandlerTest.testWorkflowTokenPut,"@Test
public void testWorkflowTokenPut() throws Exception {
    Assert.assertEquals(200, deploy(WorkflowTokenTestPutApp.class).getStatusLine().getStatusCode());
    Id.Application appId = Id.Application.from(Id.Namespace.DEFAULT, WorkflowTokenTestPutApp.NAME);
    Id.Workflow workflowId = Id.Workflow.from(appId, WorkflowTokenTestPutApp.WorkflowTokenTestPut.NAME);
    Id.Program mapReduceId = Id.Program.from(appId, ProgramType.MAPREDUCE, WorkflowTokenTestPutApp.RecordCounter.NAME);
    Id.Program sparkId = Id.Program.from(appId, ProgramType.SPARK, WorkflowTokenTestPutApp.SparkTestApp.NAME);
    String outputPath = new File(tmpFolder.newFolder(), ""output"").getAbsolutePath();
    startProgram(workflowId, ImmutableMap.of(""inputPath"", createInputForRecordVerification(""firstInput""),
    ""outputPath"", outputPath, ""put.in.mapper.initialize"", ""true""));
    waitState(workflowId, ProgramRunStatus.RUNNING.name());
    waitState(workflowId, ""STOPPED"");
    List<RunRecord> workflowProgramRuns = getProgramRuns(workflowId, ProgramRunStatus.FAILED.name());
    Assert.assertEquals(1, workflowProgramRuns.size());
    List<RunRecord> mapReduceProgramRuns = getProgramRuns(mapReduceId, ProgramRunStatus.FAILED.name());
    Assert.assertEquals(1, mapReduceProgramRuns.size());
    outputPath = new File(tmpFolder.newFolder(), ""output"").getAbsolutePath();
    startProgram(workflowId, ImmutableMap.of(""inputPath"", createInputForRecordVerification(""secondInput""),
    ""outputPath"", outputPath, ""put.in.map"", ""true""));
    waitState(workflowId, ProgramRunStatus.RUNNING.name());
    waitState(workflowId, ""STOPPED"");
    workflowProgramRuns = getProgramRuns(workflowId, ProgramRunStatus.FAILED.name());
    Assert.assertEquals(2, workflowProgramRuns.size());
    mapReduceProgramRuns = getProgramRuns(mapReduceId, ProgramRunStatus.FAILED.name());
    Assert.assertEquals(2, mapReduceProgramRuns.size());
    outputPath = new File(tmpFolder.newFolder(), ""output"").getAbsolutePath();
    startProgram(workflowId, ImmutableMap.of(""inputPath"", createInputForRecordVerification(""thirdInput""),
    ""outputPath"", outputPath, ""put.in.reducer.initialize"", ""true""));
    waitState(workflowId, ProgramRunStatus.RUNNING.name());
    waitState(workflowId, ""STOPPED"");
    workflowProgramRuns = getProgramRuns(workflowId, ProgramRunStatus.FAILED.name());
    Assert.assertEquals(3, workflowProgramRuns.size());
    mapReduceProgramRuns = getProgramRuns(mapReduceId, ProgramRunStatus.FAILED.name());
    Assert.assertEquals(3, mapReduceProgramRuns.size());
    outputPath = new File(tmpFolder.newFolder(), ""output"").getAbsolutePath();
    startProgram(workflowId, ImmutableMap.of(""inputPath"", createInputForRecordVerification(""fourthInput""),
    ""outputPath"", outputPath, ""put.in.reduce"", ""true""));
    waitState(workflowId, ProgramRunStatus.RUNNING.name());
    waitState(workflowId, ""STOPPED"");
    workflowProgramRuns = getProgramRuns(workflowId, ProgramRunStatus.FAILED.name());
    Assert.assertEquals(4, workflowProgramRuns.size());
    mapReduceProgramRuns = getProgramRuns(mapReduceId, ProgramRunStatus.FAILED.name());
    Assert.assertEquals(4, mapReduceProgramRuns.size());
    outputPath = new File(tmpFolder.newFolder(), ""output"").getAbsolutePath();
    startProgram(workflowId, ImmutableMap.of(""inputPath"", createInputForRecordVerification(""fifthInput""),
    ""outputPath"", outputPath, ""closurePutToken"", ""true""));
    waitState(workflowId, ProgramRunStatus.RUNNING.name());
    waitState(workflowId, ""STOPPED"");
    workflowProgramRuns = getProgramRuns(workflowId, ProgramRunStatus.FAILED.name());
    Assert.assertEquals(5, workflowProgramRuns.size());
    mapReduceProgramRuns = getProgramRuns(mapReduceId, ProgramRunStatus.COMPLETED.name());
    Assert.assertEquals(1, mapReduceProgramRuns.size());
    List<RunRecord> sparkProgramRuns = getProgramRuns(sparkId, ProgramRunStatus.FAILED.name());
    Assert.assertEquals(1, sparkProgramRuns.size());
    outputPath = new File(tmpFolder.newFolder(), ""output"").getAbsolutePath();
    startProgram(workflowId, ImmutableMap.of(""inputPath"", createInputForRecordVerification(""sixthInput""),
    ""outputPath"", outputPath));
    waitState(workflowId, ProgramRunStatus.RUNNING.name());
    waitState(workflowId, ""STOPPED"");
    workflowProgramRuns = getProgramRuns(workflowId, ProgramRunStatus.COMPLETED.name());
    Assert.assertEquals(1, workflowProgramRuns.size());
    workflowProgramRuns = getProgramRuns(sparkId, ProgramRunStatus.COMPLETED.name());
    Assert.assertEquals(1, workflowProgramRuns.size());
}",async wait,0
265,apache_hive,testCleanup,"@Test
public void testCleanup() throws Exception {
    ObjectStore objStore = new ObjectStore();
    objStore.setConf(metaStore.getConf());
    objStore.deleteRuntimeStats(0);
    objStore.addRuntimeStat(createStat(1));
    Thread.sleep(2000);
    objStore.addRuntimeStat(createStat(2));
    int deleted = objStore.deleteRuntimeStats(1);
    int deleted = objStore.deleteRuntimeStats(5);
    assertEquals(1, deleted);
    List<RuntimeStat> all = getRuntimeStats();
    assertEquals(1, all.size());
    assertEquals(2, all.get(0).getWeight());
}",async wait,0
283,Ericsson_ecchronos,TestRepairTask.testPartialRepair,"@Test
public void testPartialRepair() throws InterruptedException {
    Collection<LongTokenRange> ranges = new ArrayList<>();
    LongTokenRange range1 = new LongTokenRange(1, 2);
    LongTokenRange range2 = new LongTokenRange(3, 4);
    ranges.add(range1);
    ranges.add(range2);
    final RepairTask repairTask = new RepairTask.Builder().withJMXProxyFactory(jmxProxyFactory).withTableReference(myTableReference).withTokenRanges(ranges).withTableRepairMetrics(myTableRepairMetrics).withRepairHistory(repairHistory).withJobId(jobId).withReplicas(participants).build();
    CountDownLatch cdl = startRepair(repairTask, false);
    Notification notification = new Notification(""progress"", ""repair:1"", 0, getRepairMessage(range1));
    notification.setUserData(getNotificationData(PROGRESS.ordinal(), 1, 2));
    proxy.notify(notification);
    notification = new Notification(""progress"", ""repair:1"", 1, getRepairMessage(range2));
    notification.setUserData(getNotificationData(PROGRESS.ordinal(), 2, 2));
    proxy.notify(notification);
    notification = new Notification(""progress"", ""repair:1"", 2, ""Done with repair"");
    notification.setUserData(getNotificationData(COMPLETE.ordinal(), 2, 2));
    proxy.notify(notification);
    cdl.await();
    assertThat(repairTask.getUnknownRanges()).isNull();
    assertThat(repairTask.getCompletedRanges()).containsExactlyElementsOf(ranges);
    assertThat(proxy.myOptions.get(RANGES_KEY)).isNotEmpty();
    verify(myTableRepairMetrics).repairTiming(eq(TABLE_REFERENCE), anyLong(), any(TimeUnit.class), eq(true));
    verify(repairSessions.get(range1)).start();
    verify(repairSessions.get(range2)).start();
    verify(repairSessions.get(range1)).finish(eq(SUCCESS));
    verify(repairSessions.get(range2)).finish(eq(SUCCESS));
}",unordered collections,3
285,neo4j_neo4j,shouldBuildUpGracefullyUntilReachedMinPoolSize,"@Test
public void shouldBuildUpGracefullyUntilReachedMinPoolSize() throws InterruptedException
{
    StatefulMonitor stateMonitor = new StatefulMonitor();
    FakeClock clock = new FakeClock();
    final LinkedQueuePool<Object> pool = getLinkedQueuePool( stateMonitor, clock, 5 );
    ExecutorService executor = Executors.newCachedThreadPool();
    List<FlyweightHolder<Object>> flyweightHolders = acquireFromPool( pool, 5, executor );
    executor.shutdown();
    for ( FlyweightHolder<Object> flyweightHolder : flyweightHolders )
    {
        flyweightHolder.release();
    }
    executor.awaitTermination( 10, TimeUnit.SECONDS );
    assertEquals( -1, stateMonitor.currentPeakSize.get() );
    assertEquals( -1, stateMonitor.targetSize.get() );
    assertEquals( 0, stateMonitor.disposed.get() );
}",concurrency,1
288,apache_pulsar,MessageIdTest.producerSendAsync,"@Test
@Test(timeOut = 10000)
public void producerSendAsync() throws PulsarClientException {
    String key = ""producerSendAsync"";
    final String topicName = ""persistent://prop/cluster/namespace/topic-"" + key;
    final String subscriptionName = ""my-subscription-"" + key;
    final String messagePredicate = ""my-message-"" + key + ""-"";
    final int numberOfMessages = 30;
    Producer<byte[]> producer = pulsarClient.newProducer().topic(topicName)
    .enableBatching(false)
    .messageRoutingMode(MessageRoutingMode.SinglePartition)
    .create();
    Consumer<byte[]> consumer = pulsarClient.newConsumer().topic(topicName).subscriptionName(subscriptionName)
    .subscribe();
    Set<MessageId> messageIds = new HashSet<>();
    List<Future<MessageId>> futures = new ArrayList<>();
    for (int i = 0; i < numberOfMessages; i++) {
        String message = messagePredicate + i;
        futures.add(producer.sendAsync(message.getBytes()));
    }
    MessageIdImpl previousMessageId = null;
    for (Future<MessageId> f : futures) {
        try {
            MessageIdImpl currentMessageId = (MessageIdImpl) f.get();
            if (previousMessageId != null) {
                Assert.assertTrue(currentMessageId.compareTo(previousMessageId) > 0,
                ""Message Ids should be in ascending order"");
            }
            messageIds.add(currentMessageId);
            previousMessageId = currentMessageId;
        } catch (Exception e) {
            Assert.fail(""Failed to publish message, Exception: "" + e.getMessage());
        }
    }
    log.info(""Message IDs = "" + messageIds);
    Assert.assertEquals(messageIds.size(), numberOfMessages, ""Not all messages published successfully"");
    for (int i = 0; i < numberOfMessages; i++) {
        Message<byte[]> message = consumer.receive();
        Assert.assertEquals(new String(message.getData()), messagePredicate + i);
        MessageId messageId = message.getMessageId();
        Assert.assertTrue(messageIds.remove(messageId), ""Failed to receive message"");
    }
    log.info(""Message IDs = "" + messageIds);
    Assert.assertEquals(messageIds.size(), 0, ""Not all messages received successfully"");
    consumer.unsubscribe();
}",async wait,0
293,neo4j_neo4j,schema.IndexPopulationIT.shutdownDatabaseDuringIndexPopulations,"@Test
public void shutdownDatabaseDuringIndexPopulations() {
    AssertableLogProvider assertableLogProvider = new AssertableLogProvider(true);
    File storeDir = directory.directory(""shutdownDbTest"");
    Label testLabel = Label.label(""testLabel"");
    String propertyName = ""testProperty"";
    GraphDatabaseService shutDownDb = new TestGraphDatabaseFactory().setInternalLogProvider(assertableLogProvider).newEmbeddedDatabase(storeDir);
    prePopulateDatabase(shutDownDb, testLabel, propertyName);
    try (final Transaction transaction = shutDownDb.beginTx()) {
        shutDownDb.schema().indexFor(testLabel).on(propertyName).create();
        transaction.success();
    }
    shutDownDb.shutdown();
    assertableLogProvider.assertNone(AssertableLogProvider.inLog(IndexPopulationJob.class).anyError());
}",concurrency,1
294,realm_realm-java,executeTransactionAsync_callbacksShouldBeClearedBeforeCalling,"@Test
public void executeTransactionAsync_callbacksShouldBeClearedBeforeCalling() {
    final AtomicInteger callbackCounter = new AtomicInteger(0);
    final Realm foregroundRealm = looperThread.getRealm();
    foregroundRealm.setAutoRefresh(false);
    foregroundRealm.executeTransactionAsync(new Realm.Transaction() {
        @Override
        public void execute(Realm realm) {
            realm.createObject(AllTypes.class);
        }
    }, new Realm.Transaction.OnSuccess() {
        @Override
        public void onSuccess() {
            assertEquals(0, callbackCounter.getAndIncrement());
            foregroundRealm.beginTransaction();
            foregroundRealm.createObject(AllTypes.class);
            foregroundRealm.commitTransaction();
        }
    });
    foregroundRealm.executeTransactionAsync(new Realm.Transaction() {
        @Override
        public void execute(Realm realm) {
            realm.createObject(AllTypes.class);
            looperThread.postRunnableDelayed(new Runnable() {
                @Override
                public void run() {
                    foregroundRealm.sharedRealm.refresh();
                    foregroundRealm.setAutoRefresh(true);
                }
            }, 50);
        }
    }, new Realm.Transaction.OnSuccess() {
        @Override
        public void onSuccess() {
            assertEquals(1, callbackCounter.getAndIncrement());
            looperThread.testComplete();
        }
    });
}",async wait,0
296,apache_pulsar,MessageIdTest.testChecksumReconnection,"@Test
public void testChecksumReconnection() throws Exception {
    final String topicName = ""persistent"";
    ProducerImpl<byte[]> prod = ((ProducerImpl<byte[]>) (pulsarClient.newProducer().topic(topicName).enableBatching(false).messageRoutingMode(SinglePartition).create()));
    ProducerImpl<byte[]> producer = spy(prod);
    doReturn(producer.brokerChecksumSupportedVersion() + 1).when(producer).brokerChecksumSupportedVersion();
    doAnswer(( invocationOnMock) -> prod.getState()).when(producer).getState();
    doAnswer(( invocationOnMock) -> prod.getClientCnx()).when(producer).getClientCnx();
    doAnswer(( invocationOnMock) -> prod.cnx()).when(producer).cnx();
    Consumer<byte[]> consumer = pulsarClient.newConsumer().topic(topicName).subscriptionName(""my-sub"").subscribe();
    stopBroker();
    ((PulsarClientImpl) (pulsarClient)).timer().stop();
    ClientCnx mockClientCnx = spy(new ClientCnx(new ClientConfigurationData(), ((PulsarClientImpl) (pulsarClient)).eventLoopGroup()));
    doReturn(producer.brokerChecksumSupportedVersion() - 1).when(mockClientCnx).getRemoteEndpointProtocolVersion();
    prod.setClientCnx(mockClientCnx);
    CompletableFuture<MessageId> future1 = producer.sendAsync(""message-1"".getBytes());
    byte[] a2 = ""message-2"".getBytes();
    TypedMessageBuilder<byte[]> msg2 = producer.newMessage().value(a2);
    CompletableFuture<MessageId> future2 = msg2.sendAsync();
    ((TypedMessageBuilderImpl<byte[]>) (msg2)).getContent().put(a2.length - 1, ((byte) ('3')));
    prod.setClientCnx(null);
    startBroker();
    prod.grabCnx();
    try {
        future1.get(10, TimeUnit.SECONDS);
        future2.get(10, TimeUnit.SECONDS);
    } catch (Exception e) {
        e.printStackTrace();
        fail(""Broker shouldn't verify checksum for corrupted message and it shouldn't fail"");
    }
    ((ConsumerImpl<byte[]>) (consumer)).grabCnx();
    Message<byte[]> msg = consumer.receive(1, TimeUnit.SECONDS);
    assertEquals(new String(msg.getData()), ""message-1"");
    msg = consumer.receive(1, TimeUnit.SECONDS);
    assertEquals(new String(msg.getData()), ""message-3"");
}",async wait,0
305,apache_ignite,IgnitePdsThreadInterruptionTest.testInterruptsOnLFSRead,"@Test
public void testInterruptsOnLFSRead() throws Exception {
    final Ignite ignite = startGrid();
    ignite.active(true);
    final int valLen = 8192;
    final byte[] payload = new byte[valLen];
    final int maxKey = 10000;
    Thread[] workers = new Thread[THREADS_CNT];
    final IgniteCache<Object, Object> cache = ignite.cache(CACHE_NAME);
    for (int i = 0; i < maxKey; i++) {
        cache.put(i, payload);
    }
    final AtomicReference<Throwable> fail = new AtomicReference<>();
    Runnable clo = new Runnable() {
        @Override
        public void run() {
            cache.get(ThreadLocalRandom.current().nextInt(maxKey / 5));
        }
    };
    for (int i = 0; i < workers.length; i++) {
        workers[i] = new Thread(clo);
        workers[i].setName(""reader-"" + i);
        workers[i].setUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler() {
            @Override
            public void uncaughtException(Thread t, Throwable e) {
                fail.compareAndSet(null, e);
            }
        });
    }
    for (Thread worker : workers) {
        worker.start();
    }
    for (int i = 0; i < (workers.length / 2); i++) {
        workers[i].interrupt();
    }
    Thread.sleep(3000);
    stop = true;
    for (Thread worker : workers) {
        worker.join();
    }
    Throwable t = fail.get();
    assertNull(t);
    int verifiedKeys = 0;
    for (int i = 0; i < maxKey; i++) {
        byte[] val = ((byte[]) (cache.get(i)));
        if (val != null) {
            assertEquals(""Illegal length"", valLen, val.length);
            verifiedKeys++;
        }
    }
}",concurrency,1
306,apache_pulsar,shouldTerminateWhenFutureIsCancelled,"@Test
public void shouldTerminateWhenFutureIsCancelled() throws InterruptedException {
    GracefulExecutorServicesShutdown shutdown = GracefulExecutorServicesShutdown.initiate();
    shutdown.timeout(Duration.ofMillis(15000));
    ExecutorService executorService = mock(ExecutorService.class);
    when(executorService.isShutdown()).thenReturn(true);
    AtomicBoolean terminated = new AtomicBoolean();
    AtomicBoolean awaitTerminationInterrupted = new AtomicBoolean();
    when(executorService.isTerminated()).thenAnswer(invocation -> terminated.get());
    when(executorService.awaitTermination(anyLong(), any())).thenAnswer(invocation  -> {
        long timeout = invocation.getArgument(0);
        TimeUnit unit = invocation.getArgument(1);
        try {
            Thread.sleep(unit.toMillis(timeout));
        } catch (InterruptedException e) {
            awaitTerminationInterrupted.set(true);
            Thread.currentThread().interrupt();
            throw e;
        }
        throw new IllegalStateException(""Thread.sleep should have been interrupted"");
    });
    when(executorService.shutdownNow()).thenAnswer(invocation -> {
        terminated.set(true);
        return null;
    });
    shutdown.shutdown(executorService);
    CompletableFuture<Void> future = shutdown.handle();
    future.cancel(false);
    Awaitility.await().untilAsserted(() -> assertTrue(awaitTerminationInterrupted.get(),
    ""awaitTermination should have been interrupted""));
    verify(executorService, times(1)).awaitTermination(anyLong(), any());
    verify(executorService, times(1)).shutdownNow();
}",concurrency,1
322,triplea-game_triplea,close,"@Test
void close() throws Exception {
    when(webSocketClient.getConnection()).thenReturn(webSocket);
    when(webSocketClient.isOpen()).thenReturn(true);
    webSocketConnection.close();
    Thread.sleep(10);
    verify(webSocket).close();
}",async wait,0
325,cdapio_cdap,MetadataHttpHandlerTestRun.testSystemMetadataRetrieval,"@Test
public void testSystemMetadataRetrieval() throws Exception {
    appClient.deploy(DEFAULT, createAppJarFile(AllProgramsApp.class));
    Id.Stream streamId = Stream.from(DEFAULT, STREAM_NAME);
    Set<String> streamSystemTags = getTags(streamId, SYSTEM);
    Assert.assertEquals(ImmutableSet.of(STREAM_NAME), streamSystemTags);
    Map<String, String> streamSystemProperties = getProperties(streamId, SYSTEM);
    final String creationTime = ""creation-time"";
    String description = ""description"";
    String schema = ""schema"";
    String ttl = ""ttl"";
    Assert.assertTrue(""Expected creation time to exist but it does not"", streamSystemProperties.containsKey(creationTime));
    long createTime = Long.parseLong(streamSystemProperties.get(creationTime));
    Assert.assertTrue(""Stream create time should be within the last hour - "" + createTime, createTime > (System.currentTimeMillis() - TimeUnit.HOURS.toMillis(1)));
    Assert.assertEquals(ImmutableMap.of(schema, Schema.recordOf(""stringBody"", Field.of(""body"", Schema.of(STRING))).toString(), ttl, String.valueOf(Long.MAX_VALUE), description, ""test stream"", creationTime, String.valueOf(createTime)), streamSystemProperties);
    long newTtl = 100000L;
    streamClient.setStreamProperties(streamId, new StreamProperties(newTtl, null, null));
    streamSystemProperties = getProperties(streamId, SYSTEM);
    Assert.assertEquals(ImmutableMap.of(schema, Schema.recordOf(""stringBody"", Field.of(""body"", Schema.of(STRING))).toString(), ttl, String.valueOf(newTtl * 1000), description, ""test stream"", creationTime, String.valueOf(createTime)), streamSystemProperties);
    Set<MetadataRecord> streamSystemMetadata = getMetadata(streamId, SYSTEM);
    Assert.assertEquals(ImmutableSet.of(new MetadataRecord(streamId, MetadataScope.SYSTEM, streamSystemProperties, streamSystemTags)), streamSystemMetadata);
    Id.Stream.View view = View.from(streamId, ""view"");
    Schema viewSchema = Schema.recordOf(""record"", Field.of(""viewBody"", Schema.nullableOf(Schema.of(BYTES))));
    streamViewClient.createOrUpdate(view, new ViewSpecification(new FormatSpecification(""format"", viewSchema)));
    Set<String> viewSystemTags = getTags(view, SYSTEM);
    Assert.assertEquals(ImmutableSet.of(""view"", STREAM_NAME), viewSystemTags);
    Map<String, String> viewSystemProperties = getProperties(view, SYSTEM);
    Assert.assertEquals(viewSchema.toString(), viewSystemProperties.get(schema));
    ImmutableSet<String> viewUserTags = ImmutableSet.of(""viewTag"");
    addTags(view, viewUserTags);
    Assert.assertEquals(ImmutableSet.of(new MetadataRecord(view, MetadataScope.USER, ImmutableMap.<String, String>of(), viewUserTags), new MetadataRecord(view, MetadataScope.SYSTEM, viewSystemProperties, viewSystemTags)), getMetadata(view));
    Id.DatasetInstance datasetInstance = DatasetInstance.from(DEFAULT, DATASET_NAME);
    Set<String> dsSystemTags = getTags(datasetInstance, SYSTEM);
    Assert.assertEquals(ImmutableSet.of(DATASET_NAME, BATCH_TAG, EXPLORE_TAG), dsSystemTags);
    Map<String, String> dsSystemProperties = getProperties(datasetInstance, SYSTEM);
    Assert.assertTrue(""Expected creation time to exist but it does not"", dsSystemProperties.containsKey(creationTime));
    createTime = Long.parseLong(dsSystemProperties.get(creationTime));
    Assert.assertTrue(""Dataset create time should be within the last hour - "" + createTime, createTime > (System.currentTimeMillis() - TimeUnit.HOURS.toMillis(1)));
    Assert.assertEquals(ImmutableMap.of(""type"", KeyValueTable.class.getName(), description, ""test dataset"", creationTime, String.valueOf(createTime)), dsSystemProperties);
    datasetClient.update(datasetInstance, ImmutableMap.of(PROPERTY_TTL, ""100000""));
    dsSystemProperties = getProperties(datasetInstance, SYSTEM);
    Assert.assertEquals(ImmutableMap.of(""type"", KeyValueTable.class.getName(), description, ""test dataset"", ttl, ""100000"", creationTime, String.valueOf(createTime)), dsSystemProperties);
    Id.Artifact artifactId = getArtifactId();
    Assert.assertEquals(ImmutableSet.of(new MetadataRecord(artifactId, MetadataScope.SYSTEM, ImmutableMap.<String, String>of(), ImmutableSet.of(AllProgramsApp.class.getSimpleName()))), getMetadata(artifactId, SYSTEM));
    Id.Application app = Application.from(DEFAULT, NAME);
    Assert.assertEquals(ImmutableMap.builder().put((FLOW.getPrettyName() + MetadataDataset.KEYVALUE_SEPARATOR) + NoOpFlow.NAME, NAME).put((MAPREDUCE.getPrettyName() + MetadataDataset.KEYVALUE_SEPARATOR) + NoOpMR.NAME, NAME).put((MAPREDUCE.getPrettyName() + MetadataDataset.KEYVALUE_SEPARATOR) + NoOpMR2.NAME, NAME).put((SERVICE.getPrettyName() + MetadataDataset.KEYVALUE_SEPARATOR) + NoOpService.NAME, NAME).put((SPARK.getPrettyName() + MetadataDataset.KEYVALUE_SEPARATOR) + NoOpSpark.NAME, NAME).put((WORKER.getPrettyName() + MetadataDataset.KEYVALUE_SEPARATOR) + NoOpWorker.NAME, NAME).put((WORKFLOW.getPrettyName() + MetadataDataset.KEYVALUE_SEPARATOR) + NoOpWorkflow.NAME, NAME).put((""schedule"" + MetadataDataset.KEYVALUE_SEPARATOR) + AllProgramsApp.SCHEDULE_NAME, (AllProgramsApp.SCHEDULE_NAME + MetadataDataset.KEYVALUE_SEPARATOR) + AllProgramsApp.SCHEDULE_DESCRIPTION).build(), getProperties(app, SYSTEM));
    Assert.assertEquals(ImmutableSet.of(AllProgramsApp.class.getSimpleName(), NAME), getTags(app, SYSTEM));
    assertProgramSystemMetadata(Program.from(app, FLOW, NAME), ""Realtime"");
    assertProgramSystemMetadata(Program.from(app, WORKER, NAME), ""Realtime"");
    assertProgramSystemMetadata(Program.from(app, SERVICE, NAME), ""Realtime"");
    assertProgramSystemMetadata(Program.from(app, MAPREDUCE, NAME), ""Batch"");
    assertProgramSystemMetadata(Program.from(app, SPARK, NAME), ""Batch"");
    assertProgramSystemMetadata(Program.from(app, WORKFLOW, NAME), ""Batch"");
}",async wait,0
328,spring-projects_spring-data-couchbase,MappingCouchbaseConverterTests.writesAndReadsClassContainingCustomConvertedObjects,"@Test
void writesAndReadsClassContainingCustomConvertedObjects() {
    List<Object> converters = new ArrayList<>();
    converters.add(BigDecimalToStringConverter.INSTANCE);
    converters.add(StringToBigDecimalConverter.INSTANCE);
    CustomConversions customConversions = new CouchbaseCustomConversions(converters);
    converter.setCustomConversions(customConversions);
    converter.afterPropertiesSet();
    ((CouchbaseMappingContext) (converter.getMappingContext())).setSimpleTypeHolder(customConversions.getSimpleTypeHolder());
    CouchbaseDocument converted = new CouchbaseDocument();
    final String weightStr = ""12.34"";
    final BigDecimal weight = new BigDecimal(weightStr);
    final CustomObject addy = new CustomObject(weight);
    List<CustomObject> listOfObjects = new ArrayList<>();
    listOfObjects.add(addy);
    Map<String, CustomObject> mapOfObjects = new HashMap<>();
    mapOfObjects.put(""obj0"", addy);
    mapOfObjects.put(""obj1"", addy);
    CustomObjectEntity entity = new CustomObjectEntity(addy, listOfObjects, mapOfObjects);
    converter.write(entity, converted);
    CouchbaseDocument source = new CouchbaseDocument();
    source.put(""_class"", CustomObjectEntity.class.getName());
    CouchbaseDocument objectDoc = new CouchbaseDocument();
    objectDoc.put(""weight"", weightStr);
    source.put(""object"", objectDoc);
    CouchbaseList listOfObjectsDoc = new CouchbaseList();
    listOfObjectsDoc.put(objectDoc);
    source.put(""listOfObjects"", listOfObjectsDoc);
    CouchbaseDocument mapOfObjectsDoc = new CouchbaseDocument();
    mapOfObjectsDoc.put(""obj0"", objectDoc);
    mapOfObjectsDoc.put(""obj1"", objectDoc);
    source.put(""mapOfObjects"", mapOfObjectsDoc);
    assertThat(converted.export().toString()).isEqualTo(source.export().toString());
    CustomObjectEntity readConverted = converter.read(CustomObjectEntity.class, source);
    assertThat(readConverted.object.weight).isEqualTo(addy.weight);
    assertThat(readConverted.listOfObjects.get(0).weight).isEqualTo(listOfObjects.get(0).weight);
    assertThat(readConverted.mapOfObjects.get(""obj0"").weight).isEqualTo(mapOfObjects.get(""obj0"").weight);
    assertThat(readConverted.mapOfObjects.get(""obj1"").weight).isEqualTo(mapOfObjects.get(""obj1"").weight);
}",unordered collections,3
335,epimorphics_appbase,TestMonitor.testMonitor,"@Test
public void testMonitor() throws IOException, InterruptedException {
    monitor.setScanInterval(5);
    assertTrue(monitor.getEntries().isEmpty());
    File fooFile = touchFile(""foo"", ""foo1"");
    Thread.sleep(MONITOR_CHECK_DELAY);
    Collection<TestInstance> entries = monitor.getEntries();
    assertEquals(1, entries.size());
    TestInstance[] entryArray = new TestInstance[1];
    entryArray = entries.toArray(entryArray);
    TestInstance fooInst = entryArray[0];
    assertEquals(""foo1"", fooInst.getMessage());
    touchFile(""bar"", ""bar1"");
    Thread.sleep(MONITOR_CHECK_DELAY);
    entries = monitor.getEntries();
    assertEquals(2, entries.size());
    TestInstance fooCheck = monitor.get(""foo"");
    TestUtil.testArray(entryNames(entries), new String[]{ ""foo1"", ""bar1"" });
    assertEquals(fooCheck, fooInst);
    touchFile(""foo"", ""foo2"");
    Thread.sleep(MONITOR_CHECK_DELAY);
    entries = monitor.getEntries();
    assertEquals(2, entries.size());
    TestUtil.testArray(entryNames(entries), new String[]{ ""foo2"", ""bar1"" });
    fooCheck = monitor.get(""foo"");
    assertNotSame(fooInst, fooCheck);
    assertEquals(""foo2"", fooCheck.getMessage());
    fooFile.delete();
    Thread.sleep(MONITOR_CHECK_DELAY);
    entries = monitor.getEntries();
    assertEquals(1, entries.size());
    TestUtil.testArray(entryNames(entries), new String[]{ ""bar1"" });
}",async wait,0
340,apache_dubbo,737f7a7ea67832d7f17517326fb2491d0a086dd7.testListDetail,"@Test
public void testListDetail() throws RemotingException {
    String result = port.telnet(null, ""-l"");
    assertEquals(""dubbo://127.0.0.1:20887"", result);
}",test order dependency,4
341,open-telemetry_opentelemetry-java-instrumentation,OpenTelemetryAppenderConfigTest.logWithExtras,"@Test
void logWithExtras() {
    Instant start = Instant.now();
    List<LogData> logDataList = logExporter.getFinishedLogItems();
    assertThat(logDataList).hasSize(1);
    LogData logData = logDataList.get(0);
    assertThat(logData.getResource()).isEqualTo(resource);
    assertThat(logData.getInstrumentationLibraryInfo()).isEqualTo(instrumentationLibraryInfo);
    assertThat(logData.getBody().asString()).isEqualTo(""log message 1"");
    assertThat(logData.getEpochNanos()).isGreaterThanOrEqualTo(TimeUnit.MILLISECONDS.toNanos(start.toEpochMilli())).isLessThanOrEqualTo(TimeUnit.MILLISECONDS.toNanos(Instant.now().toEpochMilli()));
    assertThat(logData.getSeverity()).isEqualTo(INFO);
    assertThat(logData.getSeverityText()).isEqualTo(""INFO"");
    assertThat(logData.getAttributes().size()).isEqualTo(3);
    assertThat(logData.getAttributes().get(EXCEPTION_TYPE)).isEqualTo(IllegalStateException.class.getName());
    assertThat(logData.getAttributes().get(EXCEPTION_MESSAGE)).isEqualTo(""Error!"");
    assertThat(logData.getAttributes().get(EXCEPTION_STACKTRACE)).contains(""logWithExtras"");
}",time,2
342,cdapio_cdap,TestFrameworkTestRun.testAppWithServices,"@Test
public void testAppWithServices() throws Exception {
    ApplicationManager applicationManager = deployApplication(AppWithServices.class);
    LOG.info(""Deployed."");
    ServiceManager serviceManager = applicationManager.getServiceManager(AppWithServices.SERVICE_NAME).start();
    serviceManager.waitForStatus(true);
    LOG.info(""Service Started"");
    URL serviceURL = serviceManager.getServiceURL(15, TimeUnit.SECONDS);
    Assert.assertNotNull(serviceURL);
    URL url = new URL(serviceURL, ""ping2"");
    HttpRequest request = HttpRequest.get(url).build();
    HttpResponse response = HttpRequests.execute(request);
    Assert.assertEquals(200, response.getResponseCode());
    url = new URL(serviceURL, ""failure"");
    request = HttpRequest.get(url).build();
    response = HttpRequests.execute(request);
    Assert.assertEquals(500, response.getResponseCode());
    Assert.assertTrue(response.getResponseBodyAsString().contains(""Exception""));
    url = new URL(serviceURL, ""verifyClassLoader"");
    request = HttpRequest.get(url).build();
    response = HttpRequests.execute(request);
    Assert.assertEquals(200, response.getResponseCode());
    RuntimeMetrics serviceMetrics = serviceManager.getMetrics();
    serviceMetrics.waitForinput(3, 5, TimeUnit.SECONDS);
    Assert.assertEquals(3, serviceMetrics.getInput());
    Assert.assertEquals(2, serviceMetrics.getProcessed());
    Assert.assertEquals(1, serviceMetrics.getException());
    RuntimeMetrics handlerMetrics = getMetricsManager().getServiceHandlerMetrics(Id.Namespace.DEFAULT.getId(),
    AppWithServices.APP_NAME,
    AppWithServices.SERVICE_NAME,
    AppWithServices.SERVICE_NAME);
    handlerMetrics.waitForinput(3, 5, TimeUnit.SECONDS);
    Assert.assertEquals(3, handlerMetrics.getInput());
    Assert.assertEquals(2, handlerMetrics.getProcessed());
    Assert.assertEquals(1, handlerMetrics.getException());
    LOG.info(""DatasetUpdateService Started"");
    Map<String, String> args
    = ImmutableMap.of(AppWithServices.WRITE_VALUE_RUN_KEY, AppWithServices.DATASET_TEST_VALUE,
    AppWithServices.WRITE_VALUE_STOP_KEY, AppWithServices.DATASET_TEST_VALUE_STOP);
    ServiceManager datasetWorkerServiceManager = applicationManager
    .getServiceManager(AppWithServices.DATASET_WORKER_SERVICE_NAME).start(args);
    WorkerManager datasetWorker =
    applicationManager.getWorkerManager(AppWithServices.DATASET_UPDATE_WORKER).start(args);
    datasetWorkerServiceManager.waitForStatus(true);
    ServiceManager noopManager = applicationManager.getServiceManager(""NoOpService"").start();
    serviceManager.waitForStatus(true, 2, 1);
    String result = callServiceGet(noopManager.getServiceURL(), ""ping/"" + AppWithServices.DATASET_TEST_KEY);
    String decodedResult = new Gson().fromJson(result, String.class);
    Assert.assertEquals(AppWithServices.DATASET_TEST_VALUE, decodedResult);
    handlerMetrics = getMetricsManager().getServiceHandlerMetrics(Id.Namespace.DEFAULT.getId(),
    AppWithServices.APP_NAME,
    ""NoOpService"",
    ""NoOpHandler"");
    handlerMetrics.waitForinput(1, 5, TimeUnit.SECONDS);
    Assert.assertEquals(1, handlerMetrics.getInput());
    Assert.assertEquals(1, handlerMetrics.getProcessed());
    Assert.assertEquals(0, handlerMetrics.getException());
    String path = String.format(""discover/%s/%s"",
    AppWithServices.APP_NAME, AppWithServices.DATASET_WORKER_SERVICE_NAME);
    url = new URL(serviceURL, path);
    request = HttpRequest.get(url).build();
    response = HttpRequests.execute(request);
    Assert.assertEquals(200, response.getResponseCode());
    datasetWorker.stop();
    datasetWorkerServiceManager.stop();
    datasetWorkerServiceManager.waitForStatus(false);
    LOG.info(""DatasetUpdateService Stopped"");
    serviceManager.stop();
    serviceManager.waitForStatus(false);
    LOG.info(""ServerService Stopped"");
    result = callServiceGet(noopManager.getServiceURL(), ""ping/"" + AppWithServices.DATASET_TEST_KEY_STOP);
    decodedResult = new Gson().fromJson(result, String.class);
    Assert.assertEquals(AppWithServices.DATASET_TEST_VALUE_STOP, decodedResult);
    result = callServiceGet(noopManager.getServiceURL(), ""ping/"" + AppWithServices.DATASET_TEST_KEY_STOP_2);
    decodedResult = new Gson().fromJson(result, String.class);
    Assert.assertEquals(AppWithServices.DATASET_TEST_VALUE_STOP_2, decodedResult);
}",concurrency,1
356,cdapio_cdap,MetricsQueryTestRun.testingUserServiceGaugeMetrics,"@Test
public void testingUserServiceGaugeMetrics() throws Exception {
    MetricsCollector collector =
    collectionService.getCollector(getUserServiceContext(Constants.DEFAULT_NAMESPACE, ""WordCount"", ""CounterService"",
    ""CountRunnable""));
    collector.increment(""gmetric"", 1);
    collector.gauge(""gmetric"", 10);
    collector.increment(""gmetric"", 1);
    TimeUnit.SECONDS.sleep(1);
    collector.gauge(""gmetric"", 10);
    TimeUnit.SECONDS.sleep(2);
    String runnableRequest =
    ""/system/apps/WordCount/services/CounterService/runnables/CountRunnable/gmetric?aggregate=true"";
    String serviceRequest =
    ""/system/apps/WordCount/services/CounterService/gmetric?aggregate=true"";
    testSingleMetric(runnableRequest, 10);
    testSingleMetric(serviceRequest, 10);
}",async wait,0
358,OryxProject_oryx,ALSServingInputProducerIT.testALSInputProducer,"@Test
public void testALSInputProducer() throws Exception {
    Map<String, Object> overlayConfig = new HashMap<>();
    overlayConfig.put(""oryx.serving.application-resources"", ""\""com.cloudera.oryx.app.serving,com.cloudera.oryx.app.serving.als\"""");
    overlayConfig.put(""oryx.serving.model-manager-class"", ALSServingModelManager.class.getName());
    Config config = ConfigUtils.overlayOn(overlayConfig, getConfig());
    startMessaging();
    startServer(config);
    @SuppressWarnings(""unchecked"")
    TopicProducer<String, String> inputProducer = ((TopicProducer<String, String>) (getServingLayer().getContext().getServletContext().getAttribute(INPUT_PRODUCER_KEY)));
    String[] inputs = new String[]{ ""abc,123,1.5"", ""xyz,234,-0.5"", ""AB,10,0"" };
    List<Pair<String, String>> keyMessages;
    try (final CloseableIterator<Pair<String, String>> data = new ConsumeData(INPUT_TOPIC, getZKPort()).iterator()) {
        log.info(""Starting consumer thread"");
        ConsumeTopicRunnable consumeInput = new ConsumeTopicRunnable(data);
        new Thread(consumeInput).start();
        Thread.sleep(3000);
        for (String input : inputs) {
            inputProducer.send("""", input);
        }
        Thread.sleep(1000);
        keyMessages = consumeInput.getKeyMessages();
    }
    for (int i = 0; i < keyMessages.size(); i++) {
        Pair<String, String> keyMessage = keyMessages.get(i);
        assertEquals("""", keyMessage.getFirst());
        assertEquals(inputs[i], keyMessage.getSecond());
    }
    assertEquals(inputs.length, keyMessages.size());
}",async wait,0
369,apache_pulsar,socketTest,"@Test
public void socketTest() throws Exception {
    URI consumeUri = URI.create(CONSUME_URI);
    URI produceUri = URI.create(PRODUCE_URI);
    WebSocketClient consumeClient = new WebSocketClient();
    SimpleConsumerSocket consumeSocket = new SimpleConsumerSocket();
    WebSocketClient produceClient = new WebSocketClient();
    SimpleProducerSocket produceSocket = new SimpleProducerSocket();
    try {
        consumeClient.start();
        ClientUpgradeRequest consumeRequest = new ClientUpgradeRequest();
        Future<Session> consumerFuture = consumeClient.connect(consumeSocket, consumeUri, consumeRequest);
        log.info(""Connecting to : {}"", consumeUri);
        ClientUpgradeRequest produceRequest = new ClientUpgradeRequest();
        produceClient.start();
        Future<Session> producerFuture = produceClient.connect(produceSocket, produceUri, produceRequest);
        Thread.sleep(1000);
        Assert.assertTrue(consumerFuture.get().isOpen());
        Assert.assertTrue(producerFuture.get().isOpen());
        consumeSocket.awaitClose(1, TimeUnit.SECONDS);
        produceSocket.awaitClose(1, TimeUnit.SECONDS);
        Assert.assertTrue(produceSocket.getBuffer().size() > 0);
        Assert.assertEquals(produceSocket.getBuffer(), consumeSocket.getBuffer());
    } finally {
        try {
            consumeClient.stop();
            produceClient.stop();
        } catch (Exception e) {
            log.error(e.getMessage());
        }
    }
}",async wait,0
13832,neo4j_neo4j,LoggingResourcePoolMonitorTest.testUpdatedCurrentPeakSizeLogsOnlyOnChange,"    @Test
    public void testUpdatedCurrentPeakSizeLogsOnlyOnChange() throws Exception
    {
        StringLogger logger = mock( StringLogger.class );
        LoggingResourcePoolMonitor monitor = new LoggingResourcePoolMonitor( logger );

        monitor.updatedCurrentPeakSize( 10 );
        verify( logger, times( 1 ) ).debug( anyString() );

        monitor.updatedCurrentPeakSize( 10 );
        verify( logger, times( 1 ) ).debug( anyString() );

        monitor.updatedCurrentPeakSize( 11 );
        verify( logger, times( 2 ) ).debug( anyString() );
    }
",non-flaky,5
13833,neo4j_neo4j,LoggingResourcePoolMonitorTest.testUpdatedTargetSizeOnlyOnChange,"    @Test
    public void testUpdatedTargetSizeOnlyOnChange() throws Exception
    {
        StringLogger logger = mock( StringLogger.class );
        LoggingResourcePoolMonitor monitor = new LoggingResourcePoolMonitor( logger );

        monitor.updatedTargetSize( 10 );
        verify( logger, times( 1 ) ).debug( anyString() );

        monitor.updatedTargetSize( 10 );
        verify( logger, times( 1 ) ).debug( anyString() );

        monitor.updatedTargetSize( 11 );
        verify( logger, times( 2 ) ).debug( anyString() );
    }
",non-flaky,5
13834,neo4j_neo4j,ResponsePackerTest.obligation,"    @Test
    public void shouldHaveFixedTargetTransactionIdEvenIfLastTransactionIdIsMoving() throws Exception
    {
        // GIVEN
        LogicalTransactionStore transactionStore = mock( LogicalTransactionStore.class );
        long lastAppliedTransactionId = 5L;
        IOCursor<CommittedTransactionRepresentation> endlessCursor = new EndlessCursor( lastAppliedTransactionId+1 );
        when( transactionStore.getTransactions( anyLong() ) ).thenReturn( endlessCursor );
        final long targetTransactionId = 8L;
        final TransactionIdStore transactionIdStore = new DeadSimpleTransactionIdStore( targetTransactionId, 0 );
        ResponsePacker packer = new ResponsePacker( transactionStore, transactionIdStore,
                singletonProvider( new StoreId() ) );

        // WHEN
        Response<Object> response = packer.packTransactionStreamResponse( requestContextStartingAt( 5L ), null );
        final AtomicLong nextExpectedVisit = new AtomicLong( lastAppliedTransactionId );
        response.accept( new Response.Handler()
        {
            @Override
            public void obligation( long txId ) throws IOException
            {
                fail( ""Should not be called"" );
            }
",non-flaky,5
13835,neo4j_neo4j,StoreCopyClientTest.shouldCopyStoreFilesAcrossIfACancellationRequestHappensAfterTheTempStoreHasBeenRecovered,"    @Test
    public void shouldCopyStoreFilesAcrossIfACancellationRequestHappensAfterTheTempStoreHasBeenRecovered()
            throws IOException
",non-flaky,5
13836,neo4j_neo4j,StoreCopyClientTest.shouldEndUpWithAnEmptyStoreIfCancellationRequestIssuedJustBeforeRecoveryTakesPlace,"    @Test
    public void shouldEndUpWithAnEmptyStoreIfCancellationRequestIssuedJustBeforeRecoveryTakesPlace()
            throws IOException
",non-flaky,5
13837,neo4j_neo4j,TransactionCommittingResponseUnpackerTest.testStopShouldAllowTransactionsToCompleteCommitAndApply,"    @Test
    public void testStopShouldAllowTransactionsToCompleteCommitAndApply() throws Throwable
    {
        // Given

        // Handcrafted deep mocks, otherwise the dependency resolution throws ClassCastExceptions
        DependencyResolver dependencyResolver = mock( DependencyResolver.class );
        TransactionIdStore txIdStore = mock( TransactionIdStore.class );

        when( dependencyResolver.resolveDependency( TransactionIdStore.class ) ).thenReturn( txIdStore );

        TransactionAppender appender = mockedTransactionAppender();
        LogicalTransactionStore logicalTransactionStore = mock( LogicalTransactionStore.class );
        when( logicalTransactionStore.getAppender() ).thenReturn( appender );
        when( dependencyResolver.resolveDependency( LogicalTransactionStore.class ) )
                .thenReturn( logicalTransactionStore );

        when( dependencyResolver.resolveDependency( TransactionRepresentationStoreApplier.class ) )
                .thenReturn( mock( TransactionRepresentationStoreApplier.class ) );
        LogFile logFile = mock( LogFile.class );
        when( dependencyResolver.resolveDependency( LogFile.class ) ).thenReturn( logFile );
        LogRotation logRotation = mock(LogRotation.class);
        when( dependencyResolver.resolveDependency( LogRotation.class ) ).thenReturn( logRotation );

        setUpIndexUpdatesValidatorMocking( dependencyResolver );

          /*
           * The tx handler is called on every transaction applied after setting its id to committing
           * but before setting it to applied. We use this to stop the unpacker in the middle of the
           * process.
           */
        StoppingTxHandler stoppingTxHandler = new StoppingTxHandler();

        int maxBatchSize = 10;
        TransactionCommittingResponseUnpacker unpacker = new TransactionCommittingResponseUnpacker(
                dependencyResolver, maxBatchSize );
        stoppingTxHandler.setUnpacker( unpacker );

        // When
        unpacker.start();
        long committingTransactionId = BASE_TX_ID + 1;
        DummyTransactionResponse response = new DummyTransactionResponse( committingTransactionId, 1, appender, maxBatchSize );
        unpacker.unpackResponse( response, stoppingTxHandler );

        // Then
        // we can't verify transactionCommitted since that's part of the TransactionAppender, which we have mocked
        verify( txIdStore, times( 1 ) ).transactionClosed( committingTransactionId );
        verify( appender, times( 1 ) ).append( any( TransactionRepresentation.class ), anyLong() );
        verify( appender, times( 1 ) ).force();
        verify( logRotation, times( 1 ) ).rotateLogIfNeeded( logAppendEvent );

        // Then
          // The txhandler has stopped the unpacker. It should not allow any more transactions to go through
        try
        {
            unpacker.unpackResponse( mock( Response.class ), stoppingTxHandler );
            fail( ""A stopped transaction unpacker should not allow transactions to be applied"" );
        }
        catch( IllegalStateException e)
        {
            // good
        }
        verifyNoMoreInteractions( txIdStore );
        verifyNoMoreInteractions( appender );
    }
",non-flaky,5
13838,neo4j_neo4j,TransactionCommittingResponseUnpackerTest.shouldApplyQueuedTransactionsIfMany,"    @Test
    public void shouldApplyQueuedTransactionsIfMany() throws Throwable
    {
        // GIVEN
        DependencyResolver dependencyResolver = mock( DependencyResolver.class );
        TransactionIdStore txIdStore = mock( TransactionIdStore.class );

        when( dependencyResolver.resolveDependency( TransactionIdStore.class ) ).thenReturn( txIdStore );

        TransactionAppender appender = mockedTransactionAppender();
        LogicalTransactionStore logicalTransactionStore = mock( LogicalTransactionStore.class );
        when( logicalTransactionStore.getAppender() ).thenReturn( appender );
        when( dependencyResolver.resolveDependency( LogicalTransactionStore.class ) )
                .thenReturn( logicalTransactionStore );

        when( dependencyResolver.resolveDependency( TransactionRepresentationStoreApplier.class ) )
                .thenReturn( mock( TransactionRepresentationStoreApplier.class ) );

        setUpIndexUpdatesValidatorMocking( dependencyResolver );

        LogFile logFile = mock( LogFile.class );
        when( dependencyResolver.resolveDependency( LogFile.class ) ).thenReturn( logFile );

        LogRotation logRotation = mock(LogRotation.class);
        when( dependencyResolver.resolveDependency( LogRotation.class ) ).thenReturn( logRotation );

        int maxBatchSize = 3;
        TransactionCommittingResponseUnpacker unpacker = new TransactionCommittingResponseUnpacker(
                dependencyResolver, maxBatchSize );
        unpacker.start();

        // WHEN/THEN
        int txCount = maxBatchSize * 2 - 1;
        unpacker.unpackResponse( new DummyTransactionResponse( 2, txCount, appender, maxBatchSize ), NO_OP_TX_HANDLER );

        // and THEN
        verify( appender, times( txCount ) ).append( any( TransactionRepresentation.class ), anyLong() );
        verify( appender, times( 2 ) ).force();
        verify( logRotation, times( 2 ) ).rotateLogIfNeeded( logAppendEvent );
    }
",non-flaky,5
13839,neo4j_neo4j,TransactionCommittingResponseUnpackerTest.shouldAwaitTransactionObligationsToBeFulfilled,"    @Test
    public void shouldAwaitTransactionObligationsToBeFulfilled() throws Throwable
    {
        // GIVEN
        DependencyResolver dependencyResolver = mock( DependencyResolver.class );

        TransactionIdStore txIdStore = mock( TransactionIdStore.class );
        when( dependencyResolver.resolveDependency( TransactionIdStore.class ) ).thenReturn( txIdStore );

        TransactionAppender appender = mock( TransactionAppender.class );
        LogicalTransactionStore logicalTransactionStore = mock( LogicalTransactionStore.class );
        when( logicalTransactionStore.getAppender() ).thenReturn( appender );
        when( dependencyResolver.resolveDependency( LogicalTransactionStore.class ) )
                .thenReturn( logicalTransactionStore );

        when( dependencyResolver.resolveDependency( TransactionRepresentationStoreApplier.class ) )
                .thenReturn( mock( TransactionRepresentationStoreApplier.class ) );
        TransactionObligationFulfiller obligationFulfiller = mock( TransactionObligationFulfiller.class );
        when( dependencyResolver.resolveDependency( TransactionObligationFulfiller.class ) )
                .thenReturn( obligationFulfiller );
        final TransactionCommittingResponseUnpacker unpacker = new TransactionCommittingResponseUnpacker(
                dependencyResolver );
        unpacker.start();

        // WHEN
        unpacker.unpackResponse( new DummyObligationResponse( 4 ), NO_OP_TX_HANDLER );

        // THEN
        verify( obligationFulfiller, times( 1 ) ).fulfill( 4l );
    }
",non-flaky,5
13840,neo4j_neo4j,TransactionCommittingResponseUnpackerTest.shouldIssueKernelPanicInCaseOfFailureToAppendOrApply,"    @Test
    public void shouldIssueKernelPanicInCaseOfFailureToAppendOrApply() throws Throwable
    {
        // GIVEN
        DependencyResolver dependencyResolver = mock( DependencyResolver.class );

        TransactionIdStore txIdStore = mock( TransactionIdStore.class );
        when( dependencyResolver.resolveDependency( TransactionIdStore.class ) ).thenReturn( txIdStore );

        TransactionAppender appender = mock( TransactionAppender.class );
        LogicalTransactionStore logicalTransactionStore = mock( LogicalTransactionStore.class );
        when( logicalTransactionStore.getAppender() ).thenReturn( appender );
        when( dependencyResolver.resolveDependency( LogicalTransactionStore.class ) )
                .thenReturn( logicalTransactionStore );

        when( dependencyResolver.resolveDependency( TransactionRepresentationStoreApplier.class ) )
                .thenReturn( mock( TransactionRepresentationStoreApplier.class ) );
        TransactionObligationFulfiller obligationFulfiller = mock( TransactionObligationFulfiller.class );
        when( dependencyResolver.resolveDependency( TransactionObligationFulfiller.class ) )
                .thenReturn( obligationFulfiller );
        LogFile logFile = mock( LogFile.class );
        when( dependencyResolver.resolveDependency( LogFile.class ) ).thenReturn( logFile );
        KernelHealth kernelHealth = mock( KernelHealth.class );
        when( dependencyResolver.resolveDependency( KernelHealth.class ) ).thenReturn( kernelHealth );
        LogRotation logRotation = mock(LogRotation.class);
        when( dependencyResolver.resolveDependency( LogRotation.class ) ).thenReturn( logRotation );
        final TransactionCommittingResponseUnpacker unpacker = new TransactionCommittingResponseUnpacker(
                dependencyResolver );
        unpacker.start();

        // WHEN failing to append one or more transactions from a transaction stream response
        IOException failure = new IOException( ""Expected failure"" );
        doThrow( failure ).when( appender ).append( any( TransactionRepresentation.class ), anyLong() );
        try
        {
            unpacker.unpackResponse(
                    new DummyTransactionResponse( BASE_TX_ID+1, 1, appender, 10 ), NO_OP_TX_HANDLER );
            fail( ""Should have failed"" );
        }
        catch ( IOException e )
        {
            assertThat( e.getMessage(), containsString( failure.getMessage() ) );
            verify( kernelHealth ).panic( failure );
        }
    }
",non-flaky,5
13841,neo4j_neo4j,TransactionCommittingResponseUnpackerTest.shouldNotApplyTransactionIfIndexUpdatesValidationFails,"    @Test
    public void shouldNotApplyTransactionIfIndexUpdatesValidationFails() throws Throwable
    {
        // Given
        DependencyResolver resolver = mock( DependencyResolver.class );

        when( resolver.resolveDependency( LogFile.class ) ).thenReturn( mock( LogFile.class ) );
        when( resolver.resolveDependency( LogRotation.class ) ).thenReturn( mock( LogRotation.class ) );
        when( resolver.resolveDependency( TransactionIdStore.class ) ).thenReturn( mock( TransactionIdStore.class ) );
        KernelHealth kernelHealth = mock( KernelHealth.class );
        when( resolver.resolveDependency( KernelHealth.class ) ).thenReturn( kernelHealth );
        LogicalTransactionStore txStore = mock( LogicalTransactionStore.class );
        TransactionAppender appender = mockedTransactionAppender();
        when( txStore.getAppender() ).thenReturn( appender );
        when( resolver.resolveDependency( LogicalTransactionStore.class ) ).thenReturn( txStore );
        TransactionRepresentationStoreApplier storeApplier = mock( TransactionRepresentationStoreApplier.class );
        when( resolver.resolveDependency( TransactionRepresentationStoreApplier.class ) ).thenReturn( storeApplier );

        IndexUpdatesValidator validator = mock( IndexUpdatesValidator.class );
        IOException error = new IOException( ""error"" );
        when( validator.validate( any( TransactionRepresentation.class ), eq( TransactionApplicationMode.EXTERNAL ) ) )
                .thenThrow( error );
        when( resolver.resolveDependency( IndexUpdatesValidator.class ) ).thenReturn( validator );

        TransactionCommittingResponseUnpacker unpacker = new TransactionCommittingResponseUnpacker( resolver );
        unpacker.start();

        Response<?> response = new DummyTransactionResponse( BASE_TX_ID + 1, 1, appender, 10 );

        // When
        try
        {
            unpacker.unpackResponse( response, NO_OP_TX_HANDLER );
            fail( ""Should have thrown "" + IOException.class.getSimpleName() );
        }
        catch ( IOException e )
        {
            assertSame( error, e );
        }

        // Then
        verifyZeroInteractions( storeApplier );
        verify( kernelHealth ).panic( error );
    }
",non-flaky,5
13842,neo4j_neo4j,TransactionCommittingResponseUnpackerTest.shouldNotMarkTransactionsAsCommittedIfAppenderClosed,"    @Test
    public void shouldNotMarkTransactionsAsCommittedIfAppenderClosed() throws Throwable
    {
        // GIVEN an unpacker with close-to-real dependencies injected
        DependencyResolver resolver = mock( DependencyResolver.class );
        // (we don't want this FS in every test in this class, so just don't use EFSR)
        FileSystemAbstraction fs = cleanup.add( new EphemeralFileSystemAbstraction() );
        File directory = new File( ""dir"" );
        fs.mkdirs( directory );
        PhysicalLogFiles logFiles = new PhysicalLogFiles( directory, fs );
        TransactionIdStore transactionIdStore = spy( new DeadSimpleTransactionIdStore() );
        LogVersionRepository logVersionRepository = mock( LogVersionRepository.class );
        TransactionMetadataCache transactionMetadataCache = new TransactionMetadataCache( 10, 10 );
        LogFile logFile = life.add( new PhysicalLogFile( fs, logFiles, 1_000, transactionIdStore,
                logVersionRepository, new PhysicalLogFile.Monitor.Adapter(), transactionMetadataCache ) );
        KernelHealth health = mock( KernelHealth.class );
        LogRotation logRotation = LogRotation.NO_ROTATION;
        LogicalTransactionStore logicalTransactionStore = life.add( new PhysicalLogicalTransactionStore( logFile,
                logRotation, transactionMetadataCache, transactionIdStore, IdOrderingQueue.BYPASS,
                health, true ) );
        IndexUpdatesValidator indexUpdatesValidator = mock( IndexUpdatesValidator.class );
        when( indexUpdatesValidator.validate( any( TransactionRepresentation.class ),
                any( TransactionApplicationMode.class ) ) ).thenReturn( ValidatedIndexUpdates.NONE );
        life.start();
        TransactionAppender appender = logicalTransactionStore.getAppender();
        when( resolver.resolveDependency( LogicalTransactionStore.class ) ).thenReturn( logicalTransactionStore );
        when( resolver.resolveDependency( IndexUpdatesValidator.class ) ).thenReturn( indexUpdatesValidator );
        when( resolver.resolveDependency( TransactionIdStore.class ) ).thenReturn( transactionIdStore );
        when( resolver.resolveDependency( TransactionObligationFulfiller.class ) ).thenReturn( null );
        when( resolver.resolveDependency( LogFile.class ) ).thenReturn( logFile );
        when( resolver.resolveDependency( LogRotation.class ) ).thenReturn( logRotation );
        when( resolver.resolveDependency( KernelHealth.class ) ).thenReturn( health );
        when( resolver.resolveDependency( TransactionObligationFulfiller.class ) ).thenThrow(
                new IllegalArgumentException() );
        TransactionCommittingResponseUnpacker unpacker = new TransactionCommittingResponseUnpacker( resolver );
        unpacker.start();

        // and a closed logFile/appender
        life.shutdown();

        // WHEN packing up a transaction response
        try
        {
            unpacker.unpackResponse( new DummyTransactionResponse( BASE_TX_ID+1, 1, appender, 5 ), NO_OP_TX_HANDLER );
            fail( ""Should have failed"" );
        }
        catch ( Exception e )
        {
            // THEN apart from failing we don't want any committed/closed calls to TransactionIdStore
            verify( transactionIdStore, times( 0 ) ).transactionCommitted( anyLong(), anyLong() );
            verify( transactionIdStore, times( 0 ) ).transactionClosed( anyLong() );
        }
    }
",non-flaky,5
13843,neo4j_neo4j,ResourcePoolTest.shouldNotReuseBrokenInstances,"    @Test
    public void shouldNotReuseBrokenInstances() throws Exception
    {
        ResourcePool<Something> pool = new ResourcePool<Something>( 5 )
        {
            @Override
            protected Something create()
            {
                return new Something();
            }

            @Override
            protected boolean isAlive( Something resource )
            {
                return !resource.closed;
            }
        };

        Something somethingFirst = pool.acquire();
        somethingFirst.doStuff();
        pool.release();

        Something something = pool.acquire();
        assertEquals( somethingFirst, something );
        something.doStuff();
        something.close();
        pool.release();

        Something somethingElse = pool.acquire();
        assertFalse( something == somethingElse );
        somethingElse.doStuff();
    }
",non-flaky,5
13844,neo4j_neo4j,ResourcePoolTest.shouldTimeoutGracefully,"    @Test
    public void shouldTimeoutGracefully() throws InterruptedException
    {
        FakeClock clock = new FakeClock();

        ResourcePool.CheckStrategy timeStrategy = new ResourcePool.CheckStrategy.TimeoutCheckStrategy( 100, clock );

        while ( clock.currentTimeMillis() <= 100 )
        {
            assertFalse( timeStrategy.shouldCheck() );
            clock.forward( 10, TimeUnit.MILLISECONDS );
        }

        assertTrue( timeStrategy.shouldCheck() );

        clock.forward( 1, TimeUnit.MILLISECONDS );
        assertFalse( timeStrategy.shouldCheck() );
    }
",non-flaky,5
13845,neo4j_neo4j,ResourcePoolTest.shouldBuildUpGracefullyUntilReachedMinPoolSize,"    @Test
    public void shouldBuildUpGracefullyUntilReachedMinPoolSize() throws InterruptedException
    {
        // GIVEN
        StatefulMonitor stateMonitor = new StatefulMonitor();
        FakeClock clock = new FakeClock();
        final ResourcePool<Something> pool = getResourcePool( stateMonitor, clock, 5 );

        // WHEN
        acquireFromPool( pool, 5 );

        // THEN
        assertEquals( -1, stateMonitor.currentPeakSize.get() );
        assertEquals( -1, stateMonitor.targetSize.get() ); // that means the target size was not updated
        assertEquals( 0, stateMonitor.disposed.get() ); // no disposed happened, since the count to update is 10
    }
",non-flaky,5
13846,neo4j_neo4j,ResourcePoolTest.shouldBuildUpGracefullyWhilePassingMinPoolSizeBeforeTimerRings,"    @Test
    public void shouldBuildUpGracefullyWhilePassingMinPoolSizeBeforeTimerRings() throws InterruptedException
    {
        // GIVEN
        StatefulMonitor stateMonitor = new StatefulMonitor();
        FakeClock clock = new FakeClock();
        final ResourcePool<Something> pool = getResourcePool( stateMonitor, clock, 5 );

        // WHEN
        acquireFromPool( pool, 15 );

        // THEN
        assertEquals( -1, stateMonitor.currentPeakSize.get() );
        assertEquals( 15, stateMonitor.created.get() );
        assertEquals( -1, stateMonitor.targetSize.get() );
        assertEquals( 0, stateMonitor.disposed.get() );
    }
",non-flaky,5
13847,neo4j_neo4j,ResourcePoolTest.shouldUpdateTargetSizeWhenSpikesOccur,"    @Test
    public void shouldUpdateTargetSizeWhenSpikesOccur() throws Exception
    {
        // given
        final int MIN_SIZE = 5;
        final int MAX_SIZE = 10;

        StatefulMonitor stateMonitor = new StatefulMonitor();
        FakeClock clock = new FakeClock();
        final ResourcePool<Something> pool = getResourcePool( stateMonitor, clock, MIN_SIZE );

        // when
        List<ResourceHolder> holders = acquireFromPool( pool, MAX_SIZE );
        clock.forward( 110, TimeUnit.MILLISECONDS );
        holders.addAll( acquireFromPool( pool, 1 ) ); // Needed to trigger the alarm

        // then
        assertEquals( MAX_SIZE + 1, stateMonitor.currentPeakSize.get() );
        // We have not released anything, so targetSize will not be reduced
        assertEquals( MAX_SIZE + 1, stateMonitor.targetSize.get() ); // + 1 from the acquire

        for ( ResourceHolder holder : holders )
        {
            holder.end();
        }
    }
",non-flaky,5
13848,neo4j_neo4j,ResourcePoolTest.shouldKeepSmallPeakAndNeverDisposeIfAcquireAndReleaseContinuously,"    @Test
    public void shouldKeepSmallPeakAndNeverDisposeIfAcquireAndReleaseContinuously() throws Exception
    {
        // given
        final int MIN_SIZE = 1;

        StatefulMonitor stateMonitor = new StatefulMonitor();
        FakeClock clock = new FakeClock();
        final ResourcePool<Something> pool = getResourcePool( stateMonitor, clock, MIN_SIZE );

        // when
        for ( int i = 0; i < 200; i++ )
        {
            List<ResourceHolder> newOnes = acquireFromPool( pool, 1 );
            CountDownLatch release = new CountDownLatch( newOnes.size() );
            for ( ResourceHolder newOne : newOnes )
            {
                newOne.release( release );
            }
            release.await();
        }

        // then
        assertEquals( -1, stateMonitor.currentPeakSize.get() ); // no alarm has rung, -1 is the default
        assertEquals( 1, stateMonitor.created.get() );
        assertEquals( 0, stateMonitor.disposed.get() ); // we should always be below min size, so 0 dispose calls
    }
",non-flaky,5
13849,neo4j_neo4j,ResourcePoolTest.shouldSlowlyReduceTheNumberOfResourcesInThePoolWhenResourcesAreReleased,"    @Test
    public void shouldSlowlyReduceTheNumberOfResourcesInThePoolWhenResourcesAreReleased() throws Exception
    {
        // given
        final int MIN_SIZE = 50;
        final int MAX_SIZE = 200;

        StatefulMonitor stateMonitor = new StatefulMonitor();
        FakeClock clock = new FakeClock();
        final ResourcePool<Something> pool = getResourcePool( stateMonitor, clock, MIN_SIZE );
        List<ResourceHolder> holders = new LinkedList<ResourceHolder>();

        buildAPeakOfAcquiredResourcesAndTriggerAlarmWithSideEffects( MAX_SIZE, clock, pool, holders );

        // when
        // After the peak, stay below MIN_SIZE concurrent usage, using up all already present resources.
        clock.forward( 110, TimeUnit.MILLISECONDS );
        for ( int i = 0; i < MAX_SIZE; i++ )
        {
            acquireFromPool( pool, 1 ).get( 0 ).release();
        }

        // then

        // currentPeakSize must have reset from the latest alarm to MIN_SIZE.
        assertEquals( 1, stateMonitor.currentPeakSize.get() ); // Alarm
        // targetSize must be set to MIN_SIZE since currentPeakSize was that 2 alarms ago and didn't increase
        assertEquals( MIN_SIZE, stateMonitor.targetSize.get() );
        // Only pooled resources must be used, disposing what is in excess
        // +1 for the alarm from buildAPeakOfAcquiredResourcesAndTriggerAlarmWithSideEffects
        assertEquals( MAX_SIZE - MIN_SIZE + 1, stateMonitor.disposed.get() );
    }
",non-flaky,5
13850,neo4j_neo4j,ResourcePoolTest.shouldMaintainPoolAtHighWatermarkWhenConcurrentUsagePassesMinSize,"    @Test
    public void shouldMaintainPoolAtHighWatermarkWhenConcurrentUsagePassesMinSize() throws Exception
    {
        // given
        final int MIN_SIZE = 50;
        final int MAX_SIZE = 200;
        final int MID_SIZE = 90;

        StatefulMonitor stateMonitor = new StatefulMonitor();
        FakeClock clock = new FakeClock();
        final ResourcePool<Something> pool = getResourcePool( stateMonitor, clock, MIN_SIZE );
        List<ResourceHolder> holders = new LinkedList<ResourceHolder>();

        buildAPeakOfAcquiredResourcesAndTriggerAlarmWithSideEffects( MAX_SIZE, clock, pool, holders );

        // when
        // After the peak, stay at MID_SIZE concurrent usage, using up all already present resources in the process
        // but also keeping the high watermark above the MIN_SIZE
        clock.forward( 110, TimeUnit.MILLISECONDS );
        // Requires some rounds to happen, since there is constant racing between releasing and acquiring which does
        // not always result in reaping of resources, as there is reuse
        for ( int i = 0; i < 10; i++ )
        {
            // The latch is necessary to reduce races between batches
            CountDownLatch release = new CountDownLatch( MID_SIZE );
            for ( ResourceHolder holder : acquireFromPool( pool, MID_SIZE ) )
            {
                holder.release( release );
            }
            release.await();
            clock.forward( 110, TimeUnit.MILLISECONDS );
        }

        // then
        // currentPeakSize should be at MID_SIZE
        assertEquals( MID_SIZE, stateMonitor.currentPeakSize.get() );
        // target size too
        assertEquals( MID_SIZE, stateMonitor.targetSize.get() );
        // only the excess from the MAX_SIZE down to mid size must have been disposed
        // +1 for the alarm from buildAPeakOfAcquiredResourcesAndTriggerAlarmWithSideEffects
        assertEquals( MAX_SIZE - MID_SIZE + 1, stateMonitor.disposed.get() );
    }
",non-flaky,5
13851,neo4j_neo4j,ResourcePoolTest.shouldReclaimAndRecreateWhenLullBetweenSpikesOccurs,"    @Test
    public void shouldReclaimAndRecreateWhenLullBetweenSpikesOccurs() throws Exception
    {
        // given
        final int MIN_SIZE = 50;
        final int BELOW_MIN_SIZE = MIN_SIZE / 5;
        final int MAX_SIZE = 200;

        StatefulMonitor stateMonitor = new StatefulMonitor();
        FakeClock clock = new FakeClock();
        final ResourcePool<Something> pool = getResourcePool( stateMonitor, clock, MIN_SIZE );
        List<ResourceHolder> holders = new LinkedList<ResourceHolder>();

        buildAPeakOfAcquiredResourcesAndTriggerAlarmWithSideEffects( MAX_SIZE, clock, pool, holders );

        // when
        // After the peak, stay well below concurrent usage, using up all already present resources in the process
        clock.forward( 110, TimeUnit.MILLISECONDS );
        // Requires some rounds to happen, since there is constant racing between releasing and acquiring which does
        // not always result in reaping of resources, as there is reuse
        for ( int i = 0; i < 30; i++ )
        {
            // The latch is necessary to reduce races between batches
            CountDownLatch release = new CountDownLatch( BELOW_MIN_SIZE );
            for ( ResourceHolder holder : acquireFromPool( pool, BELOW_MIN_SIZE ) )
            {
                holder.release( release );
            }
            release.await();
            clock.forward( 110, TimeUnit.MILLISECONDS );
        }

        // then
        // currentPeakSize should be at MIN_SIZE / 5
        assertEquals( BELOW_MIN_SIZE, stateMonitor.currentPeakSize.get() );
        // target size should remain at MIN_SIZE
        assertEquals( MIN_SIZE, stateMonitor.targetSize.get() );
        // only the excess from the MAX_SIZE down to min size must have been disposed
        // +1 for the alarm from buildAPeakOfAcquiredResourcesAndTriggerAlarmWithSideEffects
        assertEquals( MAX_SIZE - MIN_SIZE + 1, stateMonitor.disposed.get() );

        stateMonitor.created.set( 0 );
        stateMonitor.disposed.set( 0 );

        // when
        // After the lull, recreate a peak
        buildAPeakOfAcquiredResourcesAndTriggerAlarmWithSideEffects( MAX_SIZE, clock, pool, holders );

        // then
        assertEquals( MAX_SIZE - MIN_SIZE + 1, stateMonitor.created.get() );
        assertEquals( 0, stateMonitor.disposed.get() );

    }
",non-flaky,5
13852,neo4j_neo4j,ServerTest.shouldSendExceptionBackToClientOnInvalidChecksum,"    @Test
    public void shouldSendExceptionBackToClientOnInvalidChecksum() throws Exception
    {
        // Given
        Server<Object, Object> server = newServer( checksumVerifier );
        RequestContext ctx = new RequestContext( 0, 1, 0, 1, 12 );

        doThrow(new IllegalStateException(""123"")).when(checksumVerifier).assertMatch( anyLong(), anyLong() );

        // When
        try
        {
            server.messageReceived( channelCtx( channel ), message( reqType, ctx, channel, EMPTY_SERIALIZER ) );
            fail(""Should have failed."");
        }
        catch(IllegalStateException e)
        {
            // Expected
        }

        // Then
        try
        {
            protocol.deserializeResponse( channel.asBlockingReadHandler(), ByteBuffer.allocateDirect( 1024 ), 1,
                    VOID_DESERIALIZER, mock( ResourceReleaser.class ) );
            fail(""Should have failed."");
        }
        catch(IllegalStateException e)
        {
            assertThat(e.getMessage(), equalTo(""123""));
        }

    }
",non-flaky,5
13853,neo4j_neo4j,TestCommunication.clientGetResponseFromServerViaComLayer,"    @Test
    public void clientGetResponseFromServerViaComLayer() throws Throwable
    {
        MadeUpServerImplementation serverImplementation = new MadeUpServerImplementation( storeIdToUse );
        MadeUpServer server = builder.server( serverImplementation );
        MadeUpClient client = builder.client();
        addToLifeAndStart( server, client );

        int value1 = 10;
        int value2 = 5;
        Response<Integer> response = client.multiply( 10, 5 );
        waitUntilResponseHasBeenWritten( server, 1000 );
        assertEquals( (Integer) (value1 * value2), response.response() );
        assertTrue( serverImplementation.gotCalled() );
        assertTrue( server.responseHasBeenWritten() );
    }
",non-flaky,5
13854,neo4j_neo4j,TestCommunication.makeSureClientStoreIdsMustMatch,"    @Test(expected = MismatchingStoreIdException.class)
    public void makeSureClientStoreIdsMustMatch() throws Throwable
    {
        MadeUpServer server = builder.server();
        MadeUpClient client = builder.storeId( new StoreId( 10, 10, 10, 10 ) ).client();
        addToLifeAndStart( server, client );

        client.multiply( 1, 2 );
    }
",non-flaky,5
13855,neo4j_neo4j,TestCommunication.makeSureServerStoreIdsMustMatch,"    @Test(expected = MismatchingStoreIdException.class)
    public void makeSureServerStoreIdsMustMatch() throws Throwable
    {
        MadeUpServer server = builder.storeId( new StoreId( 10, 10, 10, 10 ) ).server();
        MadeUpClient client = builder.client();
        addToLifeAndStart( server, client );

        client.multiply( 1, 2 );
    }
",non-flaky,5
13856,neo4j_neo4j,TestCommunication.makeSureClientCanStreamBigData,"    @Test
    public void makeSureClientCanStreamBigData() throws Throwable
    {
        MadeUpServer server = builder.server();
        MadeUpClient client = builder.client();
        addToLifeAndStart( server, client );


        client.fetchDataStream( new ToAssertionWriter(), FRAME_LENGTH * 3 );
    }
",non-flaky,5
13857,neo4j_neo4j,TestCommunication.fetchDataStream,"    @Test
    public void clientThrowsServerSideErrorMidwayThroughStreaming() throws Throwable
    {
        final String failureMessage = ""Just failing"";
        MadeUpServerImplementation serverImplementation = new MadeUpServerImplementation( storeIdToUse )
        {
            @Override
            public Response<Void> fetchDataStream( MadeUpWriter writer, int dataSize )
            {
                writer.write( new FailingByteChannel( dataSize, failureMessage ) );
                return new TransactionStreamResponse<>( null, storeIdToUse, TransactionStream.EMPTY,
                        ResourceReleaser.NO_OP );
            }
",non-flaky,5
13858,neo4j_neo4j,TestCommunication.communicateBetweenJvms,"    @Test
    public void communicateBetweenJvms() throws Throwable
    {
        ServerInterface server = builder.serverInOtherJvm();
        server.awaitStarted();
        MadeUpClient client = builder.port( MadeUpServerProcess.PORT ).client();
        life.add( client );
        life.start();

        assertEquals( (Integer) (9 * 5), client.multiply( 9, 5 ).response() );
        client.fetchDataStream( new ToAssertionWriter(), 1024 * 1024 * 3 );

        server.shutdown();
    }
",non-flaky,5
13859,neo4j_neo4j,TestCommunication.throwingServerSideExceptionBackToClient,"    @Test
    public void throwingServerSideExceptionBackToClient() throws Throwable
    {
        MadeUpServer server = builder.server();
        MadeUpClient client = builder.client();
        addToLifeAndStart( server, client );

        String exceptionMessage = ""The message"";
        try
        {
            client.throwException( exceptionMessage );
            fail( ""Should have thrown "" + MadeUpException.class.getSimpleName() );
        }
        catch ( MadeUpException e )
        {   // Good
            assertEquals( exceptionMessage, e.getMessage() );
        }
    }
",non-flaky,5
13860,neo4j_neo4j,TestCommunication.applicationProtocolVersionsMustMatch,"    @Test
    public void applicationProtocolVersionsMustMatch() throws Throwable
    {
        MadeUpServer server = builder.applicationProtocolVersion( (byte) (APPLICATION_PROTOCOL_VERSION + 1) ).server();
        MadeUpClient client = builder.client();
        addToLifeAndStart( server, client );

        try
        {
            client.multiply( 10, 20 );
            fail( ""Shouldn't be able to communicate with different application protocol versions"" );
        }
        catch ( IllegalProtocolVersionException e )
        { /* Good */ }
    }
",non-flaky,5
13861,neo4j_neo4j,TestCommunication.applicationProtocolVersionsMustMatchMultiJvm,"    @Test
    public void applicationProtocolVersionsMustMatchMultiJvm() throws Throwable
    {
        ServerInterface server = builder.applicationProtocolVersion( (byte) (APPLICATION_PROTOCOL_VERSION + 1) )
                                        .serverInOtherJvm();
        server.awaitStarted();
        MadeUpClient client = builder.port( MadeUpServerProcess.PORT ).client();
        life.add( client );
        life.start();

        try
        {
            client.multiply( 10, 20 );
            fail( ""Shouldn't be able to communicate with different application protocol versions"" );
        }
        catch ( IllegalProtocolVersionException e )
        { /* Good */ }

        server.shutdown();
    }
",non-flaky,5
13862,neo4j_neo4j,TestCommunication.internalProtocolVersionsMustMatch,"    @Test
    public void internalProtocolVersionsMustMatch() throws Throwable
    {
        MadeUpServer server = builder.internalProtocolVersion( (byte) 1 ).server();
        MadeUpClient client = builder.internalProtocolVersion( (byte) 2 ).client();
        addToLifeAndStart( server, client );

        try
        {
            client.multiply( 10, 20 );
            fail( ""Shouldn't be able to communicate with different application protocol versions"" );
        }
        catch ( IllegalProtocolVersionException e )
        { /* Good */ }
    }
",non-flaky,5
13863,neo4j_neo4j,TestCommunication.internalProtocolVersionsMustMatchMultiJvm,"    @Test
    public void internalProtocolVersionsMustMatchMultiJvm() throws Throwable
    {
        ServerInterface server = builder.internalProtocolVersion( (byte) 1 ).serverInOtherJvm();
        server.awaitStarted();
        MadeUpClient client = builder.port( MadeUpServerProcess.PORT ).internalProtocolVersion( (byte) 2 ).client();
        life.add( client );
        life.start();

        try
        {
            client.multiply( 10, 20 );
            fail( ""Shouldn't be able to communicate with different application protocol versions"" );
        }
        catch ( IllegalProtocolVersionException e )
        { /* Good */ }

        server.shutdown();
    }
",non-flaky,5
13864,neo4j_neo4j,TestCommunication.serverStopsStreamingToDeadClient,"    @Test
    public void serverStopsStreamingToDeadClient() throws Throwable
    {
        MadeUpServer server = builder.server();
        MadeUpClient client = builder.client();
        addToLifeAndStart( server, client );

        int failAtSize = FRAME_LENGTH / 2;
        ClientCrashingWriter writer = new ClientCrashingWriter( client, failAtSize );
        try
        {
            client.fetchDataStream( writer, FRAME_LENGTH * 10 );
            assertTrue( writer.getSizeRead() >= failAtSize );
            fail( ""Should fail in the middle"" );
        }
        catch ( ComException e )
        {   // Expected
        }
        assertTrue( writer.getSizeRead() >= failAtSize );

        long maxWaitUntil = System.currentTimeMillis() + 2 * 1000;
        while ( !server.responseFailureEncountered() && System.currentTimeMillis() < maxWaitUntil )
        {
            yield();
        }
        assertTrue( ""Failure writing the response should have been encountered"", server.responseFailureEncountered() );
        assertFalse( ""Response shouldn't have been successful"", server.responseHasBeenWritten() );
    }
",non-flaky,5
13865,neo4j_neo4j,TestCommunication.assertMatch,"    @Test
    public void serverContextVerificationCanThrowException() throws Throwable
    {
        final String failureMessage = ""I'm failing"";
        TxChecksumVerifier failingVerifier = new TxChecksumVerifier()
        {
            @Override
            public void assertMatch( long txId, long checksum )
            {
                throw new FailingException( failureMessage );
            }
",non-flaky,5
13866,neo4j_neo4j,TestCommunication.clientCanReadChunkSizeBiggerThanItsOwn,"    @Test
    public void clientCanReadChunkSizeBiggerThanItsOwn() throws Throwable
    {   // Given that frameLength is the same for both client and server.
        int serverChunkSize = 20000;
        int clientChunkSize = serverChunkSize / 10;
        MadeUpServer server = builder.chunkSize( serverChunkSize ).server();
        MadeUpClient client = builder.chunkSize( clientChunkSize ).client();

        addToLifeAndStart( server, client );

        // Tell server to stream data occupying roughly two chunks. The chunks
        // from server are 10 times bigger than the clients chunk size.
        client.fetchDataStream( new ToAssertionWriter(), serverChunkSize * 2 );
    }
",non-flaky,5
13867,neo4j_neo4j,TestCommunication.serverCanReadChunkSizeBiggerThanItsOwn,"    @Test
    public void serverCanReadChunkSizeBiggerThanItsOwn() throws Throwable
    {   // Given that frameLength is the same for both client and server.
        int serverChunkSize = 1000;
        int clientChunkSize = serverChunkSize * 10;
        MadeUpServer server = builder.chunkSize( serverChunkSize ).server();
        MadeUpClient client = builder.chunkSize( clientChunkSize ).client();

        addToLifeAndStart( server, client );

        // Tell server to stream data occupying roughly two chunks. The chunks
        // from server are 10 times bigger than the clients chunk size.
        client.sendDataStream( new DataProducer( clientChunkSize * 2 ) );
    }
",non-flaky,5
13868,neo4j_neo4j,TestCommunication.impossibleToHaveBiggerChunkSizeThanFrameSize,"    @Test
    public void impossibleToHaveBiggerChunkSizeThanFrameSize() throws Throwable
    {
        Builder myBuilder = builder.chunkSize( MadeUpServer.FRAME_LENGTH + 10 );
        try
        {
            myBuilder.server().start();
            fail( ""Shouldn't be possible"" );
        }
        catch ( IllegalArgumentException e )
        {   // Good
        }

        try
        {
            myBuilder.client();
            fail( ""Shouldn't be possible"" );
        }
        catch ( IllegalArgumentException e )
        {   // Good
        }
    }
",non-flaky,5
13869,neo4j_neo4j,TestCommunication.answer,"    @Test
    public void clientShouldUseHandlersToHandleComExceptions()
    {
        // Given
        final String comExceptionMessage = ""The ComException"";

        MadeUpCommunicationInterface communication = mock( MadeUpCommunicationInterface.class, new Answer<Response<?>>()
        {
            @Override
            public Response<?> answer( InvocationOnMock _ ) throws ComException
            {
                throw new ComException( comExceptionMessage );
            }
",non-flaky,5
13870,neo4j_neo4j,TestCommunication.masterResponseShouldBeUnpackedIfRequestTypeRequires,"    @Test
    public void masterResponseShouldBeUnpackedIfRequestTypeRequires() throws IOException
    {
        // Given
        ResponseUnpacker responseUnpacker = mock( ResponseUnpacker.class );
        MadeUpClient client = builder.clientWith( responseUnpacker );
        addToLifeAndStart( builder.server(), client );

        // When
        client.multiply( 42, 42 );

        // Then
        ArgumentCaptor<Response> captor = ArgumentCaptor.forClass( Response.class );
        verify( responseUnpacker ).unpackResponse( captor.capture(), any( TxHandler.class ) );
        assertEquals( storeIdToUse, captor.getValue().getStoreId() );
        assertEquals( 42 * 42, captor.getValue().response() );
    }
",non-flaky,5
13871,neo4j_neo4j,TestCommunication.masterResponseShouldNotBeUnpackedIfRequestTypeDoesNotRequire,"    @Test
    public void masterResponseShouldNotBeUnpackedIfRequestTypeDoesNotRequire()
    {
        // Given
        ResponseUnpacker responseUnpacker = mock( ResponseUnpacker.class );
        MadeUpClient client = builder.clientWith( responseUnpacker );
        addToLifeAndStart( builder.server(), client );

        // When
        client.sendDataStream( new KnownDataByteChannel( 100 ) );

        // Then
        verifyZeroInteractions( responseUnpacker );
    }
",non-flaky,5
13872,neo4j_neo4j,TestCommunication.shouldStreamBackTransactions,"    @Test
    public void shouldStreamBackTransactions() throws Exception
    {
        // GIVEN
        int value = 11, txCount = 3;
        life.add( builder.server() );
        MadeUpClient client = life.add( builder.client() );
        life.start();
        Response<Integer> respone = client.streamBackTransactions( value, txCount );
        TransactionStreamVerifyingResponseHandler handler = new TransactionStreamVerifyingResponseHandler( txCount );

        // WHEN
        respone.accept( handler );
        int responseValue = respone.response();

        // THEN
        assertEquals( value, responseValue );
        assertEquals( txCount, handler.expectedTxId );
    }
",non-flaky,5
13873,neo4j_neo4j,TestCommunication.shouldAdhereToTransactionObligations,"    @Test
    public void shouldAdhereToTransactionObligations() throws Exception
    {
        // GIVEN
        int value = 15;
        long desiredObligation = 8;
        life.add( builder.server() );
        MadeUpClient client = life.add( builder.client() );
        life.start();
        Response<Integer> respone = client.informAboutTransactionObligations( value, desiredObligation );
        TransactionObligationVerifyingResponseHandler handler = new TransactionObligationVerifyingResponseHandler();

        // WHEN
        respone.accept( handler );
        int responseValue = respone.response();

        // THEN
        assertEquals( value, responseValue );
        assertEquals( desiredObligation, handler.obligationTxId );
    }
",non-flaky,5
13874,neo4j_neo4j,ServerUtilTest.shouldIgnoreLogicalLogsWhenCopyingFilesForBackup,"    @Test
    public void shouldIgnoreLogicalLogsWhenCopyingFilesForBackup() throws IOException
    {
        // given
        final FileSystemAbstraction fs = new StubFileSystemAbstraction();

        XaDataSource dataSource = mock( XaDataSource.class );

        FileResourceIterator storeFiles = new FileResourceIterator( fs, testDirectory, ""neostore.nodestore.db"" );
        FileResourceIterator logicalLogs = new FileResourceIterator( fs, testDirectory,
        PhysicalLogFile.DEFAULT_NAME + PhysicalLogFile.DEFAULT_VERSION_SUFFIX + ""0"" );

        when( dataSource.listStoreFiles() ).thenReturn( storeFiles );
        when( dataSource.listLogicalLogs() ).thenReturn( logicalLogs );
        when( dataSource.getBranchId() ).thenReturn( ""branch"".getBytes() );
        when( dataSource.getName() ).thenReturn( ""branch"" );

        XaContainer xaContainer = mock( XaContainer.class );
        when( dataSource.getXaContainer() ).thenReturn( xaContainer );

        XaLogicalLog xaLogicalLog = mock( XaLogicalLog.class );

        when( xaContainer.getLogicalLog() ).thenReturn( xaLogicalLog );

        XaResourceManager xaResourceManager = mock( XaResourceManager.class );
        when( xaContainer.getResourceManager() ).thenReturn( xaResourceManager );

        XaDataSourceManager dsManager = new XaDataSourceManager( StringLogger.DEV_NULL );
        dsManager.registerDataSource( dataSource );

        KernelPanicEventGenerator kernelPanicEventGenerator = mock( KernelPanicEventGenerator.class );
        StoreWriter storeWriter = mock( StoreWriter.class );

        // when
        ServerUtil.rotateLogsAndStreamStoreFiles( testDirectory.absolutePath(), dsManager, kernelPanicEventGenerator,
                StringLogger.DEV_NULL, false, storeWriter, fs, StoreCopyMonitor.NONE );

        // then
        verify( storeWriter ).write( eq( ""neostore.nodestore.db"" ), any( ReadableByteChannel.class ),
                any( ByteBuffer.class ), any( Boolean.class ) );
        verify( storeWriter, never() ).write( eq( PhysicalLogFile.DEFAULT_NAME + PhysicalLogFile.DEFAULT_VERSION_SUFFIX + ""0"" ), any( ReadableByteChannel.class ),
                any( ByteBuffer.class ), any( Boolean.class ) );

    }
",non-flaky,5
13875,neo4j_neo4j,ServerUtilTest.shouldCopyLogicalLogFile,"    @Test
    public void shouldCopyLogicalLogFile() throws IOException
    {
        // given
        final FileSystemAbstraction fs = new StubFileSystemAbstraction();

        XaDataSource dataSource = mock( XaDataSource.class );

        FileResourceIterator storeFiles = new FileResourceIterator( fs, testDirectory );
        FileResourceIterator logicalLogs = new FileResourceIterator( fs, testDirectory, PhysicalLogFile.DEFAULT_NAME + PhysicalLogFile.DEFAULT_VERSION_SUFFIX + ""0"" );

        when( dataSource.listStoreFiles() ).thenReturn( storeFiles );
        when( dataSource.listLogicalLogs() ).thenReturn( logicalLogs );
        when( dataSource.getBranchId() ).thenReturn( ""branch"".getBytes() );
        when( dataSource.getName() ).thenReturn( ""branch"" );

        XaContainer xaContainer = mock( XaContainer.class );
        when( dataSource.getXaContainer() ).thenReturn( xaContainer );

        XaLogicalLog xaLogicalLog = mock( XaLogicalLog.class );

        when( xaContainer.getLogicalLog() ).thenReturn( xaLogicalLog );

        XaResourceManager xaResourceManager = mock( XaResourceManager.class );
        when( xaContainer.getResourceManager() ).thenReturn( xaResourceManager );

        XaDataSourceManager dsManager = new XaDataSourceManager( StringLogger.DEV_NULL );
        dsManager.registerDataSource( dataSource );

        KernelPanicEventGenerator kernelPanicEventGenerator = mock( KernelPanicEventGenerator.class );
        StoreWriter storeWriter = mock( StoreWriter.class );

        // when
        ServerUtil.rotateLogsAndStreamStoreFiles( testDirectory.absolutePath(), dsManager, kernelPanicEventGenerator,
                StringLogger.DEV_NULL, true, storeWriter, fs, StoreCopyMonitor.NONE );

        // then
        verify( storeWriter ).write( eq( PhysicalLogFile.DEFAULT_NAME + PhysicalLogFile.DEFAULT_VERSION_SUFFIX + ""0"" ), any( ReadableByteChannel.class ),
                any( ByteBuffer.class ), any( Boolean.class ) );
    }
",non-flaky,5
13876,neo4j_neo4j,ServerUtilTest.shouldNotThrowFileNotFoundExceptionWhenTryingToCopyAMissingLogicalLogFile,"    @Test
    public void shouldNotThrowFileNotFoundExceptionWhenTryingToCopyAMissingLogicalLogFile() throws IOException
    {
        // given
        final FileSystemAbstraction fs = new StubFileSystemAbstraction();

        XaDataSource dataSource = mock( XaDataSource.class );

        FileResourceIterator storeFiles = new FileResourceIterator( fs, testDirectory, ""neostore.nodestore.db"" );

        FileResourceIterator logicalLogs = new FileResourceIterator( fs, testDirectory,
        PhysicalLogFile.DEFAULT_NAME + PhysicalLogFile.DEFAULT_VERSION_SUFFIX + ""0"" );
        logicalLogs.deleteBeforeCopy( PhysicalLogFile.DEFAULT_NAME + PhysicalLogFile.DEFAULT_VERSION_SUFFIX + ""0"" );

        when( dataSource.listStoreFiles() ).thenReturn( storeFiles );
        when( dataSource.listLogicalLogs() ).thenReturn( logicalLogs );

        when( dataSource.getBranchId() ).thenReturn( ""branch"".getBytes() );
        when( dataSource.getName() ).thenReturn( ""branch"" );

        XaContainer xaContainer = mock( XaContainer.class );
        when( dataSource.getXaContainer() ).thenReturn( xaContainer );

        XaResourceManager xaResourceManager = mock( XaResourceManager.class );
        when( xaContainer.getResourceManager() ).thenReturn( xaResourceManager );

        XaDataSourceManager dsManager = new XaDataSourceManager( StringLogger.DEV_NULL );
        dsManager.registerDataSource( dataSource );

        KernelPanicEventGenerator kernelPanicEventGenerator = mock( KernelPanicEventGenerator.class );
        StoreWriter storeWriter = mock( StoreWriter.class );

        // when
        ServerUtil.rotateLogsAndStreamStoreFiles( testDirectory.absolutePath(), dsManager, kernelPanicEventGenerator,
                StringLogger.DEV_NULL, true, storeWriter, fs, StoreCopyMonitor.NONE );

        // then
        verify( storeWriter ).write( eq( ""neostore.nodestore.db"" ), any( ReadableByteChannel.class ),
                any( ByteBuffer.class ), any( Boolean.class ) );
    }
",non-flaky,5
13877,neo4j_neo4j,ServerUtilTest.shouldThrowFileNotFoundExceptionWhenTryingToCopyAStoreFileWhichDoesNotExist,"    @Test
    public void shouldThrowFileNotFoundExceptionWhenTryingToCopyAStoreFileWhichDoesNotExist() throws IOException
    {
        // given
        final FileSystemAbstraction fs = new StubFileSystemAbstraction();

        XaDataSource dataSource = mock( XaDataSource.class );

        FileResourceIterator storeFiles = new FileResourceIterator( fs, testDirectory, ""neostore.nodestore.db"" );
        storeFiles.deleteBeforeCopy( ""neostore.nodestore.db"" );

        FileResourceIterator logicalLogs = new FileResourceIterator( fs, testDirectory );

        when( dataSource.listStoreFiles() ).thenReturn( storeFiles );
        when( dataSource.listLogicalLogs() ).thenReturn( logicalLogs );


        when( dataSource.getBranchId() ).thenReturn( ""branch"".getBytes() );
        when( dataSource.getName() ).thenReturn( ""branch"" );

        XaContainer xaContainer = mock( XaContainer.class );
        when( dataSource.getXaContainer() ).thenReturn( xaContainer );

        XaResourceManager xaResourceManager = mock( XaResourceManager.class );
        when( xaContainer.getResourceManager() ).thenReturn( xaResourceManager );

        XaDataSourceManager dsManager = new XaDataSourceManager( StringLogger.DEV_NULL );
        dsManager.registerDataSource( dataSource );

        KernelPanicEventGenerator kernelPanicEventGenerator = mock( KernelPanicEventGenerator.class );
        StoreWriter storeWriter = mock( StoreWriter.class );

        // when
        try
        {
            ServerUtil.rotateLogsAndStreamStoreFiles( testDirectory.absolutePath(), dsManager,
                    kernelPanicEventGenerator,
                    StringLogger.DEV_NULL, true, storeWriter, fs, StoreCopyMonitor.NONE );
            fail( ""should have thrown exception"" );
        }
        catch ( ServerFailureException e )
        {
            // then
            assertEquals( java.io.FileNotFoundException.class, e.getCause().getClass() );
        }
    }
",non-flaky,5
13878,neo4j_neo4j,TestSlaveContext.assertSimilarity,"    @Test
    public void assertSimilarity()
    {
        // Different machine ids
        assertFalse( new RequestContext( 1234, 1, 2, 0, 0 ).equals( new RequestContext( 1234, 2, 2, 0, 0 ) ) );

        // Different event identifiers
        assertFalse( new RequestContext( 1234, 1, 10, 0, 0 ).equals( new RequestContext( 1234, 1, 20, 0, 0 ) ) );

        // Different session ids
        assertFalse( new RequestContext( 1001, 1, 5, 0, 0 ).equals( new RequestContext( 1101, 1, 5, 0, 0 ) ) );

        // Same everything
        assertEquals( new RequestContext( 12345, 4, 9, 0, 0 ), new RequestContext( 12345, 4, 9, 0, 0 ) );
    }
",non-flaky,5
13879,neo4j_neo4j,ProtocolTest.shouldSerializeAndDeserializeTransactionRepresentation,"    @Test
    public void shouldSerializeAndDeserializeTransactionRepresentation() throws Exception
    {
        // GIVEN
        PhysicalTransactionRepresentation transaction = new PhysicalTransactionRepresentation( justOneNode() );
        byte[] additionalHeader = ""extra"".getBytes();
        int masterId = 1, authorId = 2;
        long timeStarted = 12345, lastTxWhenStarted = 12, timeCommitted = timeStarted+10;
        transaction.setHeader( additionalHeader, masterId, authorId, timeStarted, lastTxWhenStarted, timeCommitted, -1 );
        Protocol.TransactionSerializer serializer = new Protocol.TransactionSerializer( transaction );
        ChannelBuffer buffer = new ChannelBufferWrapper( new InMemoryLogChannel() );

        // WHEN serializing the transaction
        serializer.write( buffer );

        // THEN deserializing the same transaction should yield the same data.
        // ... remember that this deserializer doesn't read the data source name string. Read it manually here
        assertEquals( NeoStoreDataSource.DEFAULT_DATA_SOURCE_NAME, Protocol.readString( buffer ) );
        TransactionRepresentation readTransaction = Protocol.TRANSACTION_REPRESENTATION_DESERIALIZER.read(
                buffer, ByteBuffer.allocate( 1000 ) );
        assertArrayEquals( additionalHeader, readTransaction.additionalHeader() );
        assertEquals( masterId, readTransaction.getMasterId() );
        assertEquals( authorId, readTransaction.getAuthorId() );
        assertEquals( timeStarted, readTransaction.getTimeStarted() );
        assertEquals( lastTxWhenStarted, readTransaction.getLatestCommittedTxWhenStarted() );
        assertEquals( timeCommitted, readTransaction.getTimeCommitted() );
    }
",non-flaky,5
13880,neo4j_neo4j,StoreMigratorFrom20IT.shouldMigrate,"    @Test
    public void shouldMigrate() throws IOException, ConsistencyCheckIncompleteException
    {
        // WHEN
        upgrader( new StoreMigrator( monitor, fs, DevNullLoggingService.DEV_NULL ) )
                .migrateIfNeeded(
                find20FormatStoreDirectory( storeDir.directory() ), schemaIndexProvider, pageCache );

        // THEN
        assertEquals( 100, monitor.eventSize() );
        assertTrue( monitor.isStarted() );
        assertTrue( monitor.isFinished() );

        GraphDatabaseService database = new GraphDatabaseFactory().newEmbeddedDatabase( storeDir.absolutePath() );
        try
        {
            verifyDatabaseContents( database );
        }
        finally
        {
            // CLEANUP
            database.shutdown();
        }

        try ( NeoStore neoStore = storeFactory.newNeoStore( true ) )
        {
            verifyNeoStore( neoStore );
        }
        assertConsistentStore( storeDir.directory() );
    }
",non-flaky,5
13881,neo4j_neo4j,StoreMigratorFrom20IT.shouldMigrateCluster,"    @Test
    public void shouldMigrateCluster() throws Throwable
    {
        // Given
        File legacyStoreDir = find20FormatStoreDirectory( storeDir.directory() );

        // When
        upgrader( new StoreMigrator( monitor, fs, DevNullLoggingService.DEV_NULL ) ).migrateIfNeeded(
                legacyStoreDir, schemaIndexProvider, pageCache );
        ClusterManager.ManagedCluster cluster = buildClusterWithMasterDirIn( fs, legacyStoreDir, life );
        cluster.await( allSeesAllAsAvailable() );
        cluster.sync();

        // Then
        HighlyAvailableGraphDatabase slave1 = cluster.getAnySlave();
        verifySlaveContents( slave1 );
        verifySlaveContents( cluster.getAnySlave( slave1 ) );
        verifyDatabaseContents( cluster.getMaster() );
    }
",non-flaky,5
13882,neo4j_neo4j,StoreMigratorFrom21IT.mustMendDuplicatePropertiesWhenUpgradingFromVersion21,"    @Test
    public void mustMendDuplicatePropertiesWhenUpgradingFromVersion21() throws Exception
    {
        // The rules:
        // If an index is present, all duplicates should be removed and the property set to the value in the index
        // If an index is not present, the property should be set to the value of the last duplicate in the property
        // chain, all duplicates except the first should be removed
        // If an index is not present, the first property in the duplicate chain should be kept for the users
        // benefit, moved to a special property value, `__DUPLICATE_<propkey>`
        //
        // This is the broken store that we are upgrading:
        //
        //   (#0:Label { keyA: ""actual"", keyA: ""phony!"", keyA: ""phony!"" })
        //   (#1 { keyA: ""actual"", keyA: ""actual"", keyA: ""actual"" })
        //   (#2:Label { keyA: ""real1"", keyA: ""phony"", keyA: ""phony"", keyD: ""real2"", keyD: ""phony"", keyD: ""phony"" })
        //   (#3 { keyA: ""real1"", keyA: ""phony"", keyA: ""phony"", keyD: ""real2"", keyD: ""phony"", keyD: ""phony"" })
        //   (#4 { keyA: ""actual"", keyB: ""actual"", keyC: ""actual"" })
        //   (#0)-[#0:REL { keyA: ""actual"", keyA: ""actual"", keyA: ""actual"" }]->(#1)
        //   (#0)-[#1:REL { keyA: ""real1"", keyA: ""phony"", keyA: ""phony"",
        //                  keyD: ""real2"", keyE: ""phony"", keyF: ""phony"" }]->(#1)
        //   (#2)-[#2:REL { keyA: ""actual"", keyB: ""actual"", keyC: ""actual"" }]->(#0)
        //
        // And this is what we want to end up with, after upgrading:
        //
        //   (#0:Label { keyA: ""actual"" })
        //   (#1 { keyA: ""actual"", __DUPLICATE_keyA: ""actual"" })
        //   (#2:Label { keyA: ""real1"", keyD: ""real2"" })
        //   (#3 { keyA: ""real1"", __DUPLICATE_keyA_1: ""real1"", __DUPLICATE_keyA_2: ""real1"",
        //         keyD: ""real2"", __DUPLICATE_keyD_1: ""real2"", __DUPLICATE_keyD_2: ""real2"" })
        //   (#4 { keyA: ""actual"", keyB: ""actual"", keyC: ""actual"" })
        //   (#0)-[#0:REL { keyA: ""actual"", __DUPLICATE_keyA: ""actual"" }]->(#1)
        //   (#0)-[#1:REL { keyA: ""real1"", __DUPLICATE_keyA_1: ""real1"", __DUPLICATE_keyA_2: ""real1"",
        //                  keyD: ""real2"", __DUPLICATE_keyD_1: ""real2"", __DUPLICATE_keyD_2: ""real2"" }]->(#1)
        //   (#2)-[#2:REL { keyA: ""actual"", keyB: ""actual"", keyC: ""actual"" }]->(#0)

        File dir = MigrationTestUtils.find21FormatStoreDirectoryWithDuplicateProperties( storeDir.directory() );

        GraphDatabaseBuilder builder =
                new GraphDatabaseFactory().newEmbeddedDatabaseBuilder( dir.getAbsolutePath() ).setConfig(
                        GraphDatabaseSettings.allow_store_upgrade, ""true"" );
        GraphDatabaseService database = builder.newGraphDatabase();
        database.shutdown();
        ConsistencyCheckService service = new ConsistencyCheckService();

        ConsistencyCheckService.Result result = service.runFullConsistencyCheck(
                dir.getAbsolutePath(), new Config(), ProgressMonitorFactory.NONE, StringLogger.SYSTEM );
        assertTrue( result.isSuccessful() );

        database = builder.newGraphDatabase();
        // Upgrade is now completed. Verify the contents:
        DependencyResolver dependencyResolver = ((GraphDatabaseAPI) database).getDependencyResolver();
        NeoStoreProvider provider = dependencyResolver.resolveDependency( NeoStoreProvider.class );
        NeoStore store = provider.evaluate();
        NodeStore nodeStore = store.getNodeStore();
        RelationshipStore relStore = store.getRelationshipStore();
        PropertyStore propertyStore = store.getPropertyStore();

        // Verify that the properties appear correct to the outside world:
        try ( Transaction ignore = database.beginTx() )
        {
            verifyPropertiesEqual( database.getNodeById( 0 ),
                    Pair.of( ""keyA"", ""actual"" ) );
            verifyPropertiesEqual( database.getNodeById( 1 ),
                    Pair.of( ""keyA"", ""actual"" ),
                    Pair.of( ""__DUPLICATE_keyA_1"", ""actual"" ),
                    Pair.of( ""__DUPLICATE_keyA_2"", ""actual"" ));
            verifyPropertiesEqual( database.getNodeById( 2 ),
                    Pair.of( ""keyA"", ""real1"" ),
                    Pair.of( ""keyD"", ""real2"" ) );
            verifyPropertiesEqual( database.getNodeById( 3 ),
                    Pair.of( ""keyA"", ""real1"" ),
                    Pair.of( ""__DUPLICATE_keyA_1"", ""real1"" ),
                    Pair.of( ""__DUPLICATE_keyA_2"", ""real1"" ),
                    Pair.of( ""keyD"", ""real2"" ),
                    Pair.of( ""__DUPLICATE_keyD_1"", ""real2"" ),
                    Pair.of( ""__DUPLICATE_keyD_2"", ""real2"" ) );
            verifyPropertiesEqual( database.getNodeById( 4 ),
                    Pair.of( ""keyA"", ""actual"" ),
                    Pair.of( ""keyB"", ""actual"" ),
                    Pair.of( ""keyC"", ""actual"" ) );
            verifyPropertiesEqual( database.getRelationshipById( 0 ),
                    Pair.of( ""keyA"", ""actual"" ),
                    Pair.of( ""__DUPLICATE_keyA_1"", ""actual"" ),
                    Pair.of( ""__DUPLICATE_keyA_2"", ""actual"" ));
            verifyPropertiesEqual( database.getRelationshipById( 1 ),
                    Pair.of( ""keyA"", ""real1"" ),
                    Pair.of( ""__DUPLICATE_keyA_1"", ""real1"" ),
                    Pair.of( ""__DUPLICATE_keyA_2"", ""real1"" ),
                    Pair.of( ""keyD"", ""real2"" ),
                    Pair.of( ""__DUPLICATE_keyD_1"", ""real2"" ),
                    Pair.of( ""__DUPLICATE_keyD_2"", ""real2"" ) );
            verifyPropertiesEqual( database.getRelationshipById( 2 ),
                    Pair.of( ""keyA"", ""actual"" ),
                    Pair.of( ""keyB"", ""actual"" ),
                    Pair.of( ""keyC"", ""actual"" ) );
        }

        // Verify that there are no two properties on the entities, that have the same key:
        // (This is important because the verification above cannot tell if we have two keys with the same value)
        verifyNoDuplicatePropertyKeys( propertyStore, nodeStore.getRecord( 0 ).getNextProp() );
        verifyNoDuplicatePropertyKeys( propertyStore, nodeStore.getRecord( 1 ).getNextProp() );
        verifyNoDuplicatePropertyKeys( propertyStore, nodeStore.getRecord( 2 ).getNextProp() );
        verifyNoDuplicatePropertyKeys( propertyStore, relStore.getRecord( 0 ).getNextProp() );
        verifyNoDuplicatePropertyKeys( propertyStore, relStore.getRecord( 1 ).getNextProp() );

        database.shutdown();
    }
",non-flaky,5
13883,neo4j_neo4j,StoreMigratorFrom19IT.shouldMigrate,"    @Test
    public void shouldMigrate() throws IOException, ConsistencyCheckIncompleteException
    {
        // GIVEN
        File legacyStoreDir = find19FormatHugeStoreDirectory( storeDir.directory() );

        // WHEN
        newStoreUpgrader().migrateIfNeeded( legacyStoreDir, schemaIndexProvider, pageCache );

        // THEN
        assertEquals( 100, monitor.eventSize() );
        assertTrue( monitor.isStarted() );
        assertTrue( monitor.isFinished() );

        GraphDatabaseService database = new GraphDatabaseFactory().newEmbeddedDatabase( storeDir.absolutePath() );

        try
        {
            verifyDatabaseContents( database );
        }
        finally
        {
            // CLEANUP
            database.shutdown();
        }

        try ( NeoStore neoStore = storeFactory.newNeoStore( true ) )
        {
            verifyNeoStore( neoStore );
        }

        assertConsistentStore( storeDir.directory() );
    }
",non-flaky,5
13884,neo4j_neo4j,StoreMigratorFrom19IT.shouldMigrateCluster,"    @Test
    public void shouldMigrateCluster() throws Throwable
    {
        // Given
        File legacyStoreDir = find19FormatHugeStoreDirectory( storeDir.directory() );

        // When
        newStoreUpgrader().migrateIfNeeded( legacyStoreDir, schemaIndexProvider, pageCache );

        ClusterManager.ManagedCluster cluster = buildClusterWithMasterDirIn( fs, legacyStoreDir, life );
        cluster.await( allSeesAllAsAvailable() );
        cluster.sync();

        // Then
        HighlyAvailableGraphDatabase slave1 = cluster.getAnySlave();
        verifySlaveContents( slave1 );
        verifySlaveContents( cluster.getAnySlave( slave1 ) );
        verifyDatabaseContents( cluster.getMaster() );
    }
",non-flaky,5
13885,neo4j_neo4j,StoreMigratorFrom19IT.shouldDeduplicateUniquePropertyIndexKeys,"    @Test
    public void shouldDeduplicateUniquePropertyIndexKeys() throws Exception
    {
        // GIVEN
        // a store that contains two nodes with property ""name"" of which there are two key tokens
        File legacyStoreDir = find19FormatStoreDirectory( storeDir.directory() );

        // WHEN
        // upgrading that store, the two key tokens for ""name"" should be merged

        newStoreUpgrader().migrateIfNeeded( storeDir.directory(), schemaIndexProvider, pageCache );

        // THEN
        // verify that the ""name"" property for both the involved nodes
        GraphDatabaseService db = new GraphDatabaseFactory().newEmbeddedDatabase( storeDir.absolutePath() );
        try
        {
            Node nodeA = getNodeWithName( db, ""A"" );
            assertThat( nodeA, inTx( db, hasProperty( ""name"" ).withValue( ""A"" ) ) );

            Node nodeB = getNodeWithName( db, ""B"" );
            assertThat( nodeB, inTx( db, hasProperty( ""name"" ).withValue( ""B"" ) ) );

            Node nodeC = getNodeWithName( db, ""C"" );
            assertThat( nodeC, inTx( db, hasProperty( ""name"" ).withValue( ""C"" ) ) );
            assertThat( nodeC, inTx( db, hasProperty( ""other"" ).withValue( ""a value"" ) ) );
            assertThat( nodeC, inTx( db, hasProperty( ""third"" ).withValue( ""something"" ) ) );
        }
        finally
        {
            db.shutdown();
        }

        // THEN
        // verify that there are no duplicate keys in the store
        try ( PropertyKeyTokenStore tokenStore = storeFactory.newPropertyKeyTokenStore() )
        {
            Token[] tokens = tokenStore.getTokens( MAX_VALUE );
            assertNoDuplicates( tokens );
        }

        assertConsistentStore( storeDir.directory() );
    }
",non-flaky,5
13886,neo4j_neo4j,DataGeneratorTest.shouldGenerateNodesAndRelationshipsWithProperties,"    @Test
    public void shouldGenerateNodesAndRelationshipsWithProperties() throws Exception
    {
        // given
        Configuration.Builder config = Configuration.builder();
        config.setValue( DataGenerator.node_count, 5 );
        config.setValue( DataGenerator.relationships, asList( new RelationshipSpec( ""FOO"", 1 ),
                                                              new RelationshipSpec( ""BAR"", 2 ) ) );
        config.setValue( DataGenerator.node_properties,
                asList( new PropertySpec( PropertyGenerator.STRING, 2 ) ) );
        config.setValue( DataGenerator.relationship_properties,
                asList( new PropertySpec( PropertyGenerator.STRING, 1 ) ) );

        DataGenerator generator = new DataGenerator( config.build() );

        BatchInserter batchInserter = mock( BatchInserter.class );

        // when
        generator.generateData( batchInserter );

        // then
        verify( batchInserter, times( 5 ) ).createNode( argThat( hasSize( 2 ) ) );
        verify( batchInserter, times( 5 ) ).createRelationship( anyLong(), anyLong(), argThat( hasName( ""FOO"" ) ),
                                                                argThat( hasSize( 1 ) ) );
        verify( batchInserter, times( 10 ) )
                .createRelationship( anyLong(), anyLong(), argThat( hasName( ""BAR"" ) ), argThat( hasSize( 1 ) ) );
        verifyNoMoreInteractions( batchInserter );
    }
",non-flaky,5
13887,neo4j_neo4j,JmxDocTest.dumpJmxInfo,"    @Test
    public void dumpJmxInfo() throws Exception
    {
        List<Triplet<String, String, String>> beanItems = new ArrayList<>();
        AsciiDocListGenerator listGenerator = new AsciiDocListGenerator( ""jmx-list"", ""MBeans exposed by Neo4j"", false );

        MBeanServer mBeanServer = ManagementFactory.getPlatformMBeanServer();
        SortedMap<String, ObjectName> neo4jBeans = new TreeMap<String, ObjectName>(
                String.CASE_INSENSITIVE_ORDER );

        for ( String query : QUERIES )
        {
            Set<ObjectInstance> beans = mBeanServer.queryMBeans(
                    new ObjectName( query ), null );
            for ( ObjectInstance bean : beans )
            {
                ObjectName objectName = bean.getObjectName();
                String name = objectName.getKeyProperty( BEAN_NAME );
                if ( EXCLUDES.contains( name ) )
                {
                    continue;
                }
                String name0 = objectName.getKeyProperty( BEAN_NAME0 );
                if ( name0 != null )
                {
                    name += '/' + name0;
                }
                neo4jBeans.put( name, bean.getObjectName() );
            }

        }
        assertEquals( ""Sanity checking the number of beans found;"",
                EXPECTED_NUMBER_OF_BEANS, neo4jBeans.size() );
        for ( Map.Entry<String, ObjectName> beanEntry : neo4jBeans.entrySet() )
        {
            ObjectName objectName = beanEntry.getValue();
            String name = beanEntry.getKey();
            Set<ObjectInstance> mBeans = mBeanServer.queryMBeans( objectName,
                    null );
            if ( mBeans.size() != 1 )
            {
                throw new IllegalStateException( ""Unexpected size [""
                        + mBeans.size()
                        + ""] of query result for [""
                        + objectName + ""]."" );
            }
            ObjectInstance bean = mBeans.iterator()
                    .next();
            MBeanInfo info = mBeanServer.getMBeanInfo( objectName );
            String description = info.getDescription()
                    .replace( '\n', ' ' );

            String id = getId( name );
            beanItems.add( Triplet.of( id, name, description ) );

            writeDetailsToFile( id, objectName, bean, info, description );
        }
        Writer fw = null;
        try
        {
            fw = AsciiDocGenerator.getFW( ""target/docs/ops"", ""JMX List"" );
            fw.write( listGenerator.generateListAndTableCombo( beanItems ) );
        }
        finally
        {
            if ( fw != null )
            {
                fw.close();
            }
        }
    }
",non-flaky,5
13888,neo4j_neo4j,HaBeanIT.canGetHaBean,"    @Test
    public void canGetHaBean() throws Throwable
    {
        startCluster( 1 );
        HighAvailability ha = ha( cluster.getMaster() );
        assertNotNull( ""could not get ha bean"", ha );
        assertMasterInformation( ha );
    }
",non-flaky,5
13889,neo4j_neo4j,HaBeanIT.testLatestTxInfoIsCorrect,"    @Test
    public void testLatestTxInfoIsCorrect() throws Throwable
    {
        startCluster( 1 );
        HighlyAvailableGraphDatabase db = cluster.getMaster();
        HighAvailability masterHa = ha( db );
        long lastCommitted = masterHa.getLastCommittedTxId();
        try ( Transaction tx = db.beginTx() )
        {
            db.createNode();
            tx.success();
        }
        assertEquals( lastCommitted + 1, masterHa.getLastCommittedTxId() );
    }
",non-flaky,5
13890,neo4j_neo4j,HaBeanIT.testUpdatePullWorksAndUpdatesLastUpdateTime,"    @Test
    public void testUpdatePullWorksAndUpdatesLastUpdateTime() throws Throwable
    {
        startCluster( 2 );
        HighlyAvailableGraphDatabase master = cluster.getMaster();
        HighlyAvailableGraphDatabase slave = cluster.getAnySlave();
        Transaction tx = master.beginTx();
        master.createNode();
        tx.success();
        tx.finish();
        HighAvailability slaveBean = ha( slave );
        DateFormat format = new SimpleDateFormat( ""yyyy-MM-DD kk:mm:ss.SSSZZZZ"" );
        // To begin with, no updates
        slaveBean.update();
        long timeUpdated = format.parse( slaveBean.getLastUpdateTime() ).getTime();
        assertTrue( timeUpdated > 0 );
    }
",non-flaky,5
13891,neo4j_neo4j,HaBeanIT.testAfterGentleMasterSwitchClusterInfoIsCorrect,"    @Test
    public void testAfterGentleMasterSwitchClusterInfoIsCorrect() throws Throwable
    {
        startCluster( 3 );
        RepairKit masterShutdown = cluster.shutdown( cluster.getMaster() );
        cluster.await( ClusterManager.masterAvailable() );
        cluster.await( ClusterManager.masterSeesSlavesAsAvailable( 1 ) );
        for ( HighlyAvailableGraphDatabase db : cluster.getAllMembers() )
        {
            assertEquals( 2, ha( db ).getInstancesInCluster().length );
        }
        masterShutdown.repair();
        cluster.await( ClusterManager.allSeesAllAsAvailable() );
        for ( HighlyAvailableGraphDatabase db : cluster.getAllMembers() )
        {
            HighAvailability bean = ha( db );

            assertEquals( 3, bean.getInstancesInCluster().length );
            for ( ClusterMemberInfo info : bean.getInstancesInCluster() )
            {
                assertTrue( ""every instance should be available"", info.isAvailable() );
                assertTrue( ""every instances should have at least one role"", info.getRoles().length > 0 );
                if ( HighAvailabilityModeSwitcher.MASTER.equals( info.getRoles()[0] ) )
                {
                    assertEquals( ""coordinator should be master"",
                            HighAvailabilityModeSwitcher.MASTER, info.getHaRole() );
                }
                else
                {
                    assertEquals( ""Either master or slave, no other way"",
                            HighAvailabilityModeSwitcher.SLAVE, info.getRoles()[0] );
                    assertEquals( ""instance "" + info.getInstanceId() + "" is cluster slave but HA master"",
                            HighAvailabilityModeSwitcher.SLAVE, info.getHaRole() );
                }
                for ( String uri : info.getUris() )
                {
                    assertTrue( ""roles should contain URIs"",
                            uri.startsWith( ""ha://"" ) || uri.startsWith( ""backup://"" ) );
                }
            }
        }
    }
",non-flaky,5
13892,neo4j_neo4j,HaBeanIT.testAfterHardMasterSwitchClusterInfoIsCorrect,"    @Test
    public void testAfterHardMasterSwitchClusterInfoIsCorrect() throws Throwable
    {
        startCluster( 3 );
        RepairKit masterShutdown = cluster.fail( cluster.getMaster() );
        cluster.await( ClusterManager.masterAvailable() );
        cluster.await( ClusterManager.masterSeesSlavesAsAvailable( 1 ) );
        for ( HighlyAvailableGraphDatabase db : cluster.getAllMembers() )
        {
            if ( db.getInstanceState() == HighAvailabilityMemberState.PENDING )
            {
                continue;
            }
            // Instance that was hard killed will still be in the cluster
            assertEquals( 3, ha( db ).getInstancesInCluster().length );
        }
        masterShutdown.repair();
        cluster.await( ClusterManager.masterAvailable() );
        cluster.await( ClusterManager.masterSeesSlavesAsAvailable( 2 ) );
        for ( HighlyAvailableGraphDatabase db : cluster.getAllMembers() )
        {
            int mastersFound = 0;
            HighAvailability bean = ha( db );

            assertEquals( 3, bean.getInstancesInCluster().length );
            for ( ClusterMemberInfo info : bean.getInstancesInCluster() )
            {
                assertTrue( bean.getInstanceId() + "": every instance should be available: "" + info.getInstanceId(),
                        info.isAvailable() );
                for ( String role : info.getRoles() )
                {
                    if (role.equals( HighAvailabilityModeSwitcher.MASTER ))
                    {
                        mastersFound++;
                    }
                }
            }
            assertEquals( 1, mastersFound );
        }
    }
",non-flaky,5
13893,neo4j_neo4j,HaBeanIT.canGetBranchedStoreBean,"    @Test
    public void canGetBranchedStoreBean() throws Throwable
    {
        startCluster( 1 );
        BranchedStore bs = beans( cluster.getMaster() ).getBranchedStoreBean();
        assertNotNull( ""could not get branched store bean"", bs );
        assertEquals( ""no branched stores for new db"", 0,
                bs.getBranchedStores().length );
    }
",non-flaky,5
13894,neo4j_neo4j,HaBeanIT.joinedInstanceShowsUpAsSlave,"    @Test
    public void joinedInstanceShowsUpAsSlave() throws Throwable
    {
        startCluster( 2 );
        ClusterMemberInfo[] instancesInCluster = ha( cluster.getMaster() ).getInstancesInCluster();
        assertEquals( 2, instancesInCluster.length );
        ClusterMemberInfo[] secondInstancesInCluster = ha( cluster.getAnySlave() ).getInstancesInCluster();
        assertEquals( 2, secondInstancesInCluster.length );
        assertMasterAndSlaveInformation( instancesInCluster );
        assertMasterAndSlaveInformation( secondInstancesInCluster );
    }
",non-flaky,5
13895,neo4j_neo4j,HaBeanIT.leftInstanceDisappearsFromMemberList,"    @Test
    public void leftInstanceDisappearsFromMemberList() throws Throwable
    {
        // Start the second db and make sure it's visible in the member list.
        // Then shut it down to see if it disappears from the member list again.
        startCluster( 3 );
        assertEquals( 3, ha( cluster.getAnySlave() ).getInstancesInCluster().length );
        cluster.shutdown( cluster.getAnySlave() );

        cluster.await( masterSeesMembers( 2 ) );

        assertEquals( 2, ha( cluster.getMaster() ).getInstancesInCluster().length );
        assertMasterInformation( ha( cluster.getMaster() ) );
    }
",non-flaky,5
13896,neo4j_neo4j,HaBeanIT.failedMemberIsStillInMemberListAlthoughFailed,"    @Test
    public void failedMemberIsStillInMemberListAlthoughFailed() throws Throwable
    {
        startCluster( 3 );
        assertEquals( 3, ha( cluster.getAnySlave() ).getInstancesInCluster().length );

        // Fail the instance
        HighlyAvailableGraphDatabase failedDb = cluster.getAnySlave();
        RepairKit dbFailure = cluster.fail( failedDb );
        await( ha( cluster.getMaster() ), dbAlive( false ) );
        await( ha( cluster.getAnySlave( failedDb )), dbAlive( false ) );

        // Repair the failure and come back
        dbFailure.repair();
        for ( HighlyAvailableGraphDatabase db : cluster.getAllMembers() )
        {
            await( ha( db ), dbAvailability( true ) );
            await( ha( db ), dbAlive( true ) );
        }
    }
",non-flaky,5
13897,neo4j_neo4j,BackupHaIT.makeSureBackupCanBePerformedFromClusterWithDefaultName,"    @Test
    public void makeSureBackupCanBePerformedFromClusterWithDefaultName() throws Throwable
    {
        testBackupFromCluster( null );
    }
",non-flaky,5
13898,neo4j_neo4j,BackupHaIT.makeSureBackupCanBePerformedFromWronglyNamedCluster,"    @Test
    public void makeSureBackupCanBePerformedFromWronglyNamedCluster() throws Throwable
    {
        assertEquals( 0, runBackupToolFromOtherJvmToGetExitCode(
                backupArguments( ""localhost:4445"", BACKUP_PATH.getPath(), ""non.existent"" ) ) );
    }
",non-flaky,5
13899,neo4j_neo4j,BackupHaIT.makeSureBackupCanBeRestored,"    @Test
    public void makeSureBackupCanBeRestored() throws Throwable
    {
        // Run backup
        assertEquals( 0, runBackupToolFromOtherJvmToGetExitCode( backupArguments( ""localhost:4445"",
                BACKUP_PATH.getPath(), null ) ) );

        // Add some new data
        DbRepresentation changedData = createSomeData( cluster.getMaster() );

        stopCluster();

        cleanData();

        copyBackup();

        startCluster();

        // Verify that old data is back
        assertThat( changedData.equals( DbRepresentation.of( cluster.getMaster() ) ), equalTo(false) );
    }
",non-flaky,5
13900,neo4j_neo4j,BackupHaIT.makeSureBackupCanBePerformedFromAnyInstance,"    @Test
    public void makeSureBackupCanBePerformedFromAnyInstance() throws Throwable
    {
        Integer[] backupPorts = {4445, 4446, 4447};

        for ( Integer port : backupPorts )
        {
            // Run backup
            assertEquals( 0, runBackupToolFromOtherJvmToGetExitCode( backupArguments( ""localhost:"" + port,
                    BACKUP_PATH.getPath(), null ) ) );

            // Add some new data
            DbRepresentation changedData = createSomeData( cluster.getMaster() );

            stopCluster();

            cleanData();

            copyBackup();

            startCluster();

            // Verify that old data is back
            assertThat( changedData.equals( DbRepresentation.of( cluster.getMaster() ) ), equalTo(false) );
        }
    }
",non-flaky,5
13901,neo4j_neo4j,TestClientThreadIsolation.run,"    @Test
    public void testTransactionsPulled() throws Exception
    {
        final HighlyAvailableGraphDatabase master =
                (HighlyAvailableGraphDatabase) new TestHighlyAvailableGraphDatabaseFactory().
                newHighlyAvailableDatabaseBuilder( TargetDirectory.forTest( TestClientThreadIsolation.class ).cleanDirectory(
                        ""master"" ).getAbsolutePath() ).
                setConfig( ClusterSettings.server_id, ""1"" ).
                newGraphDatabase();

        final HighlyAvailableGraphDatabase slave1 =
                (HighlyAvailableGraphDatabase) new TestHighlyAvailableGraphDatabaseFactory().
                newHighlyAvailableDatabaseBuilder( TargetDirectory.forTest( TestClientThreadIsolation.class ).cleanDirectory(
                        ""slave1"" ).getAbsolutePath() ).
                setConfig( ClusterSettings.cluster_server, ""127.0.0.1:5002"" ).
                setConfig( ClusterSettings.initial_hosts, ""127.0.0.1:5001"" ).
                setConfig( ClusterSettings.server_id, ""2"" ).
                setConfig( HaSettings.max_concurrent_channels_per_slave, ""2"" ).
                setConfig( HaSettings.ha_server, ""127.0.0.1:8001"" ).
                newGraphDatabase();

        Transaction masterTx = master.beginTx();
        master.createNode().createRelationshipTo( master.createNode(),
                DynamicRelationshipType.withName( ""master"" ) ).setProperty(
                ""largeArray"", new int[20000] );
        masterTx.success();
        masterTx.finish();

        Thread thread1 = new Thread( new Runnable()
        {
            public void run()
            {
                // TODO Figure out how to do this
//                Master masterClient = slave1.getBroker().getMaster().first();
//                Response<Integer> response = masterClient.createRelationshipType(
//                        slave1.getSlaveContext( 10 ), ""name"" );
//                slave1.receive( response ); // will be suspended here
//                response.close();
            }
",non-flaky,5
13902,neo4j_neo4j,TestPullUpdatesApplied.leftCluster,"    @Test
    public void testUpdatesAreWrittenToLogBeforeBeingAppliedToStore() throws Exception
    {
        int master = getCurrentMaster();
        addNode( master );
        int toKill = (master + 1) % dbs.length;
        HighlyAvailableGraphDatabase dbToKill = dbs[toKill];

        final CountDownLatch latch1 = new CountDownLatch( 1 );

        final HighlyAvailableGraphDatabase masterDb = dbs[master];
        masterDb.getDependencyResolver().resolveDependency( ClusterClient.class ).addClusterListener(
                new ClusterListener.Adapter()
                {
                    @Override
                    public void leftCluster( InstanceId instanceId, URI member )
                    {
                        latch1.countDown();
                        masterDb.getDependencyResolver().resolveDependency( ClusterClient.class )
                                .removeClusterListener( this );
                    }
",non-flaky,5
13903,neo4j_neo4j,RollingUpgradeIT.doRollingUpgradeFromPreviousVersionWithMasterLast,"    @Test
    public void doRollingUpgradeFromPreviousVersionWithMasterLast() throws Throwable
    {
        /* High level scenario:
         * 1   Have a cluster of 3 instances running <old version>
         * 1.1 Download a <old version> package
         * 1.2 Unpack the <old version> package
         * 1.4 Assembly classpath and start 3 JVMs running <old version>
         * 1.5 Create some data in the cluster
         * 2   Go over each one restarting into <this version>
         * 2.1 Grab a JVM and kill it
         * 2.2 Start that db inside this test JVM, which will run <this version>
         * 2.3 Perform a write transaction to the current master and see that it picks it up
         * 2.4 Perform a write transaction to to this instance and see that master picks it up
         * 3   Make sure the cluster functions after each one has been restarted
         * 3.1 Do basic transactions on master/slaves.
         * 3.2 Do a master switch
         * 3.3 Restart one slave
         * 3.4 Take down the instances and do consistency check */

        try
        {
            startOldVersionCluster();
            rollOverToNewVersion();
            shutdownAndDoConsistencyChecks();
        }
        catch ( Throwable e )
        {
            e.printStackTrace();
            throw e;
        }
    }
",non-flaky,5
13904,neo4j_neo4j,MasterClientTest.newClientsShouldNotIgnoreStoreIdDifferences,"    @Test(expected = MismatchingStoreIdException.class)
    public void newClientsShouldNotIgnoreStoreIdDifferences() throws Throwable
    {
        // Given
        MasterImpl.SPI masterImplSPI = MasterImplTest.mockedSpi( new StoreId( 1, 2, 3, 4 ) );
        when( masterImplSPI.getTransactionChecksum( anyLong() ) ).thenReturn( 5L );

        cleanupRule.add( newMasterServer( masterImplSPI ) );

        StoreId storeId = new StoreId( 5, 6, 7, 8 );
        MasterClient214 masterClient214 = cleanupRule.add( newMasterClient214( storeId ) );

        // When
        masterClient214.handshake( 1, storeId );
    }
",non-flaky,5
13905,neo4j_neo4j,MasterClientTest.clientShouldReadAndApplyTransactionLogsOnNewLockSessionRequest,"    @Test
    public void clientShouldReadAndApplyTransactionLogsOnNewLockSessionRequest() throws Throwable
    {
        // Given
        MasterImpl master = spy( newMasterImpl( mockMasterImplSpiWith( StoreId.DEFAULT ) ) );
        doReturn( voidResponseWithTransactionLogs() ).when( master ).newLockSession( any( RequestContext.class ) );

        cleanupRule.add( newMasterServer( master ) );

        DependencyResolver resolver = mock( DependencyResolver.class );
        LogicalTransactionStore txStore = mock( LogicalTransactionStore.class );
        TransactionRepresentationStoreApplier txApplier = mock( TransactionRepresentationStoreApplier.class );
        TransactionIdStore txIdStore = mock( TransactionIdStore.class );
        TransactionAppender txAppender = mock( TransactionAppender.class );
        when( txAppender.append( any( TransactionRepresentation.class ), anyLong() ) )
                .thenReturn( mock( Commitment.class ) );
        LogFile logFile = mock( LogFile.class );

        when( resolver.resolveDependency( LogicalTransactionStore.class ) ).thenReturn( txStore );
        when( resolver.resolveDependency( TransactionRepresentationStoreApplier.class ) ).thenReturn( txApplier );
        when( resolver.resolveDependency( TransactionIdStore.class ) ).thenReturn( txIdStore );
        when( resolver.resolveDependency( LogFile.class ) ).thenReturn( logFile );
        when( resolver.resolveDependency( LogRotation.class ) ).thenReturn( mock(LogRotation.class) );
        when( txStore.getAppender() ).thenReturn( txAppender );
        IndexUpdatesValidator indexUpdatesValidator = mock( IndexUpdatesValidator.class );
        when( indexUpdatesValidator.validate( any( TransactionRepresentation.class ),
                any( TransactionApplicationMode.class ) ) ).thenReturn( ValidatedIndexUpdates.NONE );
        when( resolver.resolveDependency( IndexUpdatesValidator.class ) ).thenReturn( indexUpdatesValidator );

        ResponseUnpacker unpacker = initAndStart( new TransactionCommittingResponseUnpacker( resolver ) );

        MasterClient masterClient = cleanupRule.add( newMasterClient214( StoreId.DEFAULT, unpacker ) );

        // When
        masterClient.newLockSession( new RequestContext( 1, 2, 3, 4, 5 ) );

        // Then
        verify( txAppender, times( TX_LOG_COUNT ) ).append( any( TransactionRepresentation.class ), anyLong() );
        // we can't verify transactionCommitted since that's part of the TransactionAppender, which we have mocked
        verify( txApplier, times( TX_LOG_COUNT ) )
                .apply( any( TransactionRepresentation.class ), any( ValidatedIndexUpdates.class ),
                        any( LockGroup.class ), anyLong(), any( TransactionApplicationMode.class ) );
        verify( txIdStore, times( TX_LOG_COUNT ) ).transactionClosed( anyLong() );
    }
",non-flaky,5
13906,neo4j_neo4j,TestClusterIndexDeletion.givenClusterWithCreatedIndexWhenDeleteIndexOnMasterThenIndexIsDeletedOnSlave,"    @Test
    public void givenClusterWithCreatedIndexWhenDeleteIndexOnMasterThenIndexIsDeletedOnSlave() throws Throwable
    {
        ClusterManager clusterManager =
            new ClusterManager( fromXml( getClass().getResource( ""/threeinstances.xml"" ).toURI() ),
                TargetDirectory.forTest( getClass() ).cleanDirectory( ""testCluster"" ),
                MapUtil.stringMap( HaSettings.ha_server.name(), "":6001-6005"",
                        HaSettings.tx_push_factor.name(), ""2"" ));
        try
        {
            // Given
            clusterManager.start();

            clusterManager.getDefaultCluster().await( ClusterManager.allSeesAllAsAvailable() );

            GraphDatabaseAPI master = clusterManager.getDefaultCluster().getMaster();
            try ( Transaction tx = master.beginTx() )
            {
                master.index().forNodes( ""Test"" );
                tx.success();
            }

            HighlyAvailableGraphDatabase aSlave = clusterManager.getDefaultCluster().getAnySlave();
            try ( Transaction tx = aSlave.beginTx() )
            {
                assertThat( aSlave.index().existsForNodes( ""Test"" ), equalTo( true ) );
                tx.success();
            }

            // When
            try ( Transaction tx = master.beginTx() )
            {
                master.index().forNodes( ""Test"" ).delete();
                tx.success();
            }

            // Then
            HighlyAvailableGraphDatabase anotherSlave = clusterManager.getDefaultCluster().getAnySlave();
            try ( Transaction tx = anotherSlave.beginTx() )
            {
                assertThat( anotherSlave.index().existsForNodes( ""Test"" ), equalTo( false ) );
                tx.success();
            }
        }
        finally
        {
            clusterManager.stop();
        }
    }
",non-flaky,5
13907,neo4j_neo4j,PullStormIT.run,"    @Test
    public void testPullStorm() throws Throwable
    {
        // given

        ClusterManager clusterManager = new ClusterManager( ClusterManager.clusterWithAdditionalArbiters( 2, 1 ),
                testDirectory.directory(),
                stringMap( HaSettings.pull_interval.name(), ""0"",
                           HaSettings.tx_push_factor.name(), ""1"") );

        clusterManager.start();

        try
        {
            ClusterManager.ManagedCluster cluster = clusterManager.getDefaultCluster();
            cluster.await( ClusterManager.masterAvailable(  ) );
            cluster.await( ClusterManager.masterSeesSlavesAsAvailable( 1 ) );

            // Create data
            final HighlyAvailableGraphDatabase master = cluster.getMaster();
            {
                Transaction tx = master.beginTx();
                for ( int i = 0; i < 1000; i++ )
                {
                    master.createNode().setProperty( ""foo"", ""bar"" );
                }
                tx.success();
                tx.finish();
            }

            // Slave goes down
            HighlyAvailableGraphDatabase slave = cluster.getAnySlave();
            ClusterManager.RepairKit repairKit = cluster.fail( slave );

            // Create more data
            for ( int i = 0; i < 1000; i++ )
            {
                {
                    Transaction tx = master.beginTx();
                    for ( int j = 0; j < 1000; j++ )
                    {
                        master.createNode().setProperty( ""foo"", ""bar"" );
                        master.createNode().setProperty( ""foo"", ""bar"" );
                    }
                    tx.success();
                    tx.finish();
                }
            }

            // Slave comes back online
            repairKit.repair();

            cluster.await( ClusterManager.masterSeesSlavesAsAvailable( 1 ) );

            // when

            // Create 20 concurrent transactions
            System.out.println( ""Pull storm"" );
            ExecutorService executor = Executors.newFixedThreadPool( 20 );
            for ( int i = 0; i < 20; i++ )
            {
                executor.submit( new Runnable()
                {
                    @Override
                    public void run()
                    {
                        Transaction tx = master.beginTx();
                        master.createNode().setProperty( ""foo"", ""bar"" );
                        tx.success();
                        tx.finish(); // This should cause lots of concurrent calls to pullUpdate()
                    }
",non-flaky,5
13908,neo4j_neo4j,TestBlockLogBuffer.onlyOneNonFullBlock,"    @Test
    public void onlyOneNonFullBlock() throws IOException
    {
        byte[] bytes = new byte[255];
        ChannelBuffer wrappedBuffer = ChannelBuffers.wrappedBuffer( bytes );
        wrappedBuffer.resetWriterIndex();
        BlockLogBuffer buffer = new BlockLogBuffer( wrappedBuffer, new Monitors().newMonitor( ByteCounterMonitor.class ) );

        byte byteValue = 5;
        int intValue = 1234;
        long longValue = 574853;
        float floatValue = 304985.5f;
        double doubleValue = 48493.22d;
        final byte[] bytesValue = new byte[] { 1, 5, 2, 6, 3 };
        buffer.put( byteValue );
        buffer.putInt( intValue );
        buffer.putLong( longValue );
        buffer.putFloat( floatValue );
        buffer.putDouble( doubleValue );
        buffer.put( bytesValue, bytesValue.length );
        buffer.close();

        ByteBuffer verificationBuffer = ByteBuffer.wrap( bytes );
        assertEquals( 30, verificationBuffer.get() );
        assertEquals( byteValue, verificationBuffer.get() );
        assertEquals( intValue, verificationBuffer.getInt() );
        assertEquals( longValue, verificationBuffer.getLong() );
        assertEquals( floatValue, verificationBuffer.getFloat(), 0.0 );
        assertEquals( doubleValue, verificationBuffer.getDouble(), 0.0 );
        byte[] actualBytes = new byte[bytesValue.length];
        verificationBuffer.get( actualBytes );
        assertThat( actualBytes, new ArrayMatches<byte[]>( bytesValue ) );
    }
",non-flaky,5
13909,neo4j_neo4j,TestBlockLogBuffer.readSmallPortions,"    @Test
    public void readSmallPortions() throws IOException
    {
        byte[] bytes = new byte[255];
        ChannelBuffer wrappedBuffer = ChannelBuffers.wrappedBuffer( bytes );
        wrappedBuffer.resetWriterIndex();
        BlockLogBuffer buffer = new BlockLogBuffer( wrappedBuffer, new Monitors().newMonitor( ByteCounterMonitor.class ) );

        byte byteValue = 5;
        int intValue = 1234;
        long longValue = 574853;
        buffer.put( byteValue );
        buffer.putInt( intValue );
        buffer.putLong( longValue );
        buffer.close();

        ReadableByteChannel reader = new BlockLogReader( wrappedBuffer );
        ByteBuffer verificationBuffer = ByteBuffer.wrap( new byte[1] );
        reader.read( verificationBuffer );
        verificationBuffer.flip();
        assertEquals( byteValue, verificationBuffer.get() );
        verificationBuffer = ByteBuffer.wrap( new byte[4] );
        reader.read( verificationBuffer );
        verificationBuffer.flip();
        assertEquals( intValue, verificationBuffer.getInt() );
        verificationBuffer = ByteBuffer.wrap( new byte[8] );
        reader.read( verificationBuffer );
        verificationBuffer.flip();
        assertEquals( longValue, verificationBuffer.getLong() );
    }
",non-flaky,5
13910,neo4j_neo4j,TestBlockLogBuffer.readOnlyOneNonFullBlock,"    @Test
    public void readOnlyOneNonFullBlock() throws IOException
    {
        byte[] bytes = new byte[255];
        ChannelBuffer wrappedBuffer = ChannelBuffers.wrappedBuffer( bytes );
        wrappedBuffer.resetWriterIndex();
        BlockLogBuffer buffer = new BlockLogBuffer( wrappedBuffer, new Monitors().newMonitor( ByteCounterMonitor.class ) );

        byte byteValue = 5;
        int intValue = 1234;
        long longValue = 574853;
        float floatValue = 304985.5f;
        double doubleValue = 48493.22d;
        final byte[] bytesValue = new byte[] { 1, 5, 2, 6, 3 };
        buffer.put( byteValue );
        buffer.putInt( intValue );
        buffer.putLong( longValue );
        buffer.putFloat( floatValue );
        buffer.putDouble( doubleValue );
        buffer.put( bytesValue, bytesValue.length );
        buffer.close();

        ReadableByteChannel reader = new BlockLogReader( wrappedBuffer );
        ByteBuffer verificationBuffer = ByteBuffer.wrap( new byte[1000] );
        reader.read( verificationBuffer );
        verificationBuffer.flip();
        assertEquals( byteValue, verificationBuffer.get() );
        assertEquals( intValue, verificationBuffer.getInt() );
        assertEquals( longValue, verificationBuffer.getLong() );
        assertEquals( floatValue, verificationBuffer.getFloat(), 0.0 );
        assertEquals( doubleValue, verificationBuffer.getDouble(), 0.0 );
        byte[] actualBytes = new byte[bytesValue.length];
        verificationBuffer.get( actualBytes );
        assertThat( actualBytes, new ArrayMatches<byte[]>( bytesValue ) );
    }
",non-flaky,5
13911,neo4j_neo4j,TestBlockLogBuffer.onlyOneFullBlock,"    @Test
    public void onlyOneFullBlock() throws Exception
    {
        byte[] bytes = new byte[256];
        ChannelBuffer wrappedBuffer = ChannelBuffers.wrappedBuffer( bytes );
        wrappedBuffer.resetWriterIndex();
        BlockLogBuffer buffer = new BlockLogBuffer( wrappedBuffer, new Monitors().newMonitor( ByteCounterMonitor.class ) );

        byte[] bytesValue = new byte[255];
        bytesValue[0] = 1;
        bytesValue[254] = -1;
        buffer.put( bytesValue, bytesValue.length );
        buffer.close();

        ByteBuffer verificationBuffer = ByteBuffer.wrap( bytes );
        assertEquals( (byte) 255, verificationBuffer.get() );
        byte[] actualBytes = new byte[bytesValue.length];
        verificationBuffer.get( actualBytes );
        assertThat( actualBytes, new ArrayMatches<byte[]>( bytesValue ) );
    }
",non-flaky,5
13912,neo4j_neo4j,TestBlockLogBuffer.readOnlyOneFullBlock,"    @Test
    public void readOnlyOneFullBlock() throws Exception
    {
        byte[] bytes = new byte[256];
        ChannelBuffer wrappedBuffer = ChannelBuffers.wrappedBuffer( bytes );
        wrappedBuffer.resetWriterIndex();
        BlockLogBuffer buffer = new BlockLogBuffer( wrappedBuffer, new Monitors().newMonitor( ByteCounterMonitor.class ) );

        byte[] bytesValue = new byte[255];
        bytesValue[0] = 1;
        bytesValue[254] = -1;
        buffer.put( bytesValue, bytesValue.length );
        buffer.close();

        ReadableByteChannel reader = new BlockLogReader( wrappedBuffer );
        ByteBuffer verificationBuffer = ByteBuffer.wrap( new byte[1000] );
        reader.read( verificationBuffer );
        verificationBuffer.flip();
        byte[] actualBytes = new byte[bytesValue.length];
        verificationBuffer.get( actualBytes );
        assertThat( actualBytes, new ArrayMatches<byte[]>( bytesValue ) );
    }
",non-flaky,5
13913,neo4j_neo4j,TestBlockLogBuffer.canWriteLargestAtomAfterFillingBuffer,"    @Test
    public void canWriteLargestAtomAfterFillingBuffer() throws Exception
    {
        byte[] bytes = new byte[300];
        ChannelBuffer wrappedBuffer = ChannelBuffers.wrappedBuffer( bytes );
        wrappedBuffer.resetWriterIndex();
        BlockLogBuffer buffer = new BlockLogBuffer( wrappedBuffer, new Monitors().newMonitor( ByteCounterMonitor.class ) );

        byte[] bytesValue = new byte[255];
        bytesValue[0] = 1;
        bytesValue[254] = -1;
        long longValue = 123456;
        buffer.put( bytesValue, bytesValue.length );
        buffer.putLong( longValue );
        buffer.close();

        ByteBuffer verificationBuffer = ByteBuffer.wrap( bytes );
        assertEquals( (byte) 0, verificationBuffer.get() );
        byte[] actualBytes = new byte[bytesValue.length];
        verificationBuffer.get( actualBytes );
        assertThat( actualBytes, new ArrayMatches<byte[]>( bytesValue ) );
        assertEquals( (byte) 8, verificationBuffer.get() );
        assertEquals( longValue, verificationBuffer.getLong() );
    }
",non-flaky,5
13914,neo4j_neo4j,TestBlockLogBuffer.canWriteReallyLargeByteArray,"    @Test
    public void canWriteReallyLargeByteArray() throws Exception
    {
        byte[] bytes = new byte[650];
        ChannelBuffer wrappedBuffer = ChannelBuffers.wrappedBuffer( bytes );
        wrappedBuffer.resetWriterIndex();
        BlockLogBuffer buffer = new BlockLogBuffer( wrappedBuffer, new Monitors().newMonitor( ByteCounterMonitor.class ) );

        byte[] bytesValue = new byte[600];
        bytesValue[1] = 1;
        bytesValue[99] = 2;
        bytesValue[199] = 3;
        bytesValue[299] = 4;
        bytesValue[399] = 5;
        bytesValue[499] = 6;
        bytesValue[599] = 7;
        buffer.put( bytesValue, bytesValue.length );
        buffer.close();

        byte[] actual;
        ByteBuffer verificationBuffer = ByteBuffer.wrap( bytes );
        assertEquals( (byte) 0, verificationBuffer.get() );
        actual = new byte[255];
        verificationBuffer.get( actual );
        assertThat( actual, new ArrayMatches<byte[]>( Arrays.copyOfRange( bytesValue, 0, 255 ) ) );
        assertEquals( (byte) 0, verificationBuffer.get() );
        actual = new byte[255];
        verificationBuffer.get( actual );
        assertThat( actual, new ArrayMatches<byte[]>( Arrays.copyOfRange( bytesValue, 255, 510 ) ) );
        assertEquals( (byte) 90, verificationBuffer.get() );
        actual = new byte[90];
        verificationBuffer.get( actual );
        assertThat( actual, new ArrayMatches<byte[]>( Arrays.copyOfRange( bytesValue, 510, 600 ) ) );
    }
",non-flaky,5
13915,neo4j_neo4j,TestBlockLogBuffer.canReaderReallyLargeByteArray,"    @Test
    public void canReaderReallyLargeByteArray() throws Exception
    {
        byte[] bytes = new byte[650];
        ChannelBuffer wrappedBuffer = ChannelBuffers.wrappedBuffer( bytes );
        wrappedBuffer.resetWriterIndex();
        BlockLogBuffer buffer = new BlockLogBuffer( wrappedBuffer, new Monitors().newMonitor( ByteCounterMonitor.class ) );

        byte[] bytesValue = new byte[600];
        bytesValue[1] = 1;
        bytesValue[99] = 2;
        bytesValue[199] = 3;
        bytesValue[299] = 4;
        bytesValue[399] = 5;
        bytesValue[499] = 6;
        bytesValue[599] = 7;
        buffer.put( bytesValue, bytesValue.length );
        buffer.close();

        byte[] actual;
        BlockLogReader reader = new BlockLogReader( wrappedBuffer );
        ByteBuffer verificationBuffer = ByteBuffer.wrap( new byte[1000] );
        reader.read( verificationBuffer );
        verificationBuffer.flip();
        actual = new byte[255];
        verificationBuffer.get( actual );
        assertThat( actual, new ArrayMatches<byte[]>( Arrays.copyOfRange( bytesValue, 0, 255 ) ) );
        actual = new byte[255];
        verificationBuffer.get( actual );
        assertThat( actual, new ArrayMatches<byte[]>( Arrays.copyOfRange( bytesValue, 255, 510 ) ) );
        actual = new byte[90];
        verificationBuffer.get( actual );
        assertThat( actual, new ArrayMatches<byte[]>( Arrays.copyOfRange( bytesValue, 510, 600 ) ) );
    }
",non-flaky,5
13916,neo4j_neo4j,HaCacheIT.shouldUpdateSlaveCacheWhenRemovingRelationshipGroupFromDenseNode,"    @Test
    public void shouldUpdateSlaveCacheWhenRemovingRelationshipGroupFromDenseNode() throws Throwable
    {
        ClusterManager manager = new ClusterManager( clusterOfSize( 3 ), root.directory(),
                                                     stringMap( tx_push_factor.name(), ""2"",
                                                                cache_type.name(), ""strong"",
                                                                dense_node_threshold.name(), """" + DENSE_NODE ) );
        try
        {
            // given
            manager.start();
            ClusterManager.ManagedCluster cluster = manager.getDefaultCluster();
            cluster.await( ClusterManager.masterAvailable() );
            cluster.await( ClusterManager.masterSeesAllSlavesAsAvailable() );
            HighlyAvailableGraphDatabase master = cluster.getMaster();
            long nodeId; // a dense node
            try ( Transaction tx = master.beginTx() )
            {
                Node node = master.createNode();
                for ( int i = 0; i < DENSE_NODE; i++ )
                {
                    node.createRelationshipTo( master.createNode(), withName( ""FOO"" ) );
                }
                master.createNode().createRelationshipTo( node, withName( ""BAR"" ) );

                tx.success();
                nodeId = node.getId();
            }
            // fully cache node on all instances
            int count = 0;
            for ( HighlyAvailableGraphDatabase db : cluster.getAllMembers() )
            {
                try ( Transaction tx = db.beginTx() )
                {
                    int these = count( db.getNodeById( nodeId ).getRelationships() );
                    assertTrue( String.format( ""expected=%s, count here=%s"", count, these ),
                                these != 0 && (count == 0 || these == count) );
                    count = these;
                    tx.success();
                }
            }

            // when
            try ( Transaction tx = master.beginTx() )
            {
                for ( Relationship relationship : master.getNodeById( nodeId ).getRelationships( withName( ""BAR"" ) ) )
                {
                    relationship.delete();
                }
                tx.success();
            }

            // then
            HighlyAvailableGraphDatabase slave = cluster.getAnySlave();
            try ( Transaction tx = slave.beginTx() )
            {
                List<String> relationships = new ArrayList<>();
                for ( Relationship relationship : slave.getNodeById( nodeId ).getRelationships() )
                {
                    relationships.add( String.format( ""(%d)-[%d:%s]->(%d)"",
                                                      relationship.getStartNode().getId(),
                                                      relationship.getId(), relationship.getType().name(),
                                                      relationship.getEndNode().getId() ) );
                }
                assertEquals( joinLines( relationships ), count - 1, relationships.size() );
                assertEquals( count - 1, count( slave.getNodeById( nodeId ).getRelationships() ) );

                tx.success();
            }
        }
        finally
        {
            manager.shutdown();
        }
    }
",non-flaky,5
13917,neo4j_neo4j,MultipleClusterTest.runTwoClusters,"    @Test
    public void runTwoClusters() throws Throwable
    {
        File root = TargetDirectory.forTest( getClass() ).cleanDirectory( ""cluster"" );

        ClusterManager clusterManager = new ClusterManager(
                fromXml( getClass().getResource( ""/twoclustertest.xml"" ).toURI() ), root, MapUtil.stringMap() );

        try
        {
            clusterManager.start();
            ManagedCluster cluster1 = clusterManager.getCluster( ""neo4j.ha"" );

            long cluster1NodeId;
            {
                GraphDatabaseService master = cluster1.getMaster();
                logging.getLogger().info( ""CREATE NODE"" );
                Transaction tx = master.beginTx();
                Node node = master.createNode();
                node.setProperty( ""cluster"", ""neo4j.ha"" );
                cluster1NodeId = node.getId();
                logging.getLogger().info( ""CREATED NODE"" );
                tx.success();
                tx.finish();
            }

            ManagedCluster cluster2 = clusterManager.getCluster( ""neo4j.ha2"" );
            long cluster2NodeId;
            {
                GraphDatabaseService master = cluster2.getMaster();
                logging.getLogger().info( ""CREATE NODE"" );
                Transaction tx = master.beginTx();
                Node node = master.createNode();
                node.setProperty( ""cluster"", ""neo4j.ha2"" );
                cluster2NodeId = node.getId();
                logging.getLogger().info( ""CREATED NODE"" );
                tx.success();
                tx.finish();
            }

            // Verify properties in all cluster nodes
            for ( HighlyAvailableGraphDatabase highlyAvailableGraphDatabase : cluster1.getAllMembers() )
            {
                highlyAvailableGraphDatabase.getDependencyResolver().resolveDependency( UpdatePullerClient.class ).pullUpdates();

                Transaction transaction = highlyAvailableGraphDatabase.beginTx();
                assertEquals( ""neo4j.ha"", highlyAvailableGraphDatabase.getNodeById( cluster1NodeId ).getProperty(
                        ""cluster"" ) );
                transaction.finish();
            }

            for ( HighlyAvailableGraphDatabase highlyAvailableGraphDatabase : cluster2.getAllMembers() )
            {
                highlyAvailableGraphDatabase.getDependencyResolver().resolveDependency( UpdatePullerClient.class ).pullUpdates();

                Transaction transaction = highlyAvailableGraphDatabase.beginTx();
                assertEquals( ""neo4j.ha2"", highlyAvailableGraphDatabase.getNodeById( cluster2NodeId ).getProperty(
                        ""cluster"" ) );
                transaction.finish();
            }
        }
        finally
        {
            clusterManager.stop();
        }
    }
",non-flaky,5
13918,neo4j_neo4j,QuorumWritesIT.testMasterStopsWritesWhenMajorityIsUnavailable,"    @Test
    public void testMasterStopsWritesWhenMajorityIsUnavailable() throws Throwable
    {
        File root = TargetDirectory.forTest( getClass() ).cleanDirectory(
                ""testMasterStopsWritesWhenMajorityIsUnavailable"" );
        ClusterManager clusterManager = new ClusterManager( clusterOfSize( 3 ), root,
                MapUtil.stringMap( HaSettings.tx_push_factor.name(), ""2"", HaSettings.state_switch_timeout.name(), ""5s""
                ) );
        try
        {
            clusterManager.start();
            ClusterManager.ManagedCluster cluster = clusterManager.getDefaultCluster();
            cluster.await( ClusterManager.masterAvailable(  ) );
            cluster.await( ClusterManager.masterSeesAllSlavesAsAvailable() );

            HighlyAvailableGraphDatabase master = cluster.getMaster();

            doTx( master );

            final CountDownLatch latch1 = new CountDownLatch( 1 );
            waitOnHeartbeatFail( master, latch1 );

            HighlyAvailableGraphDatabase slave1 = cluster.getAnySlave();
            cluster.fail( slave1 );

            latch1.await();
            slave1.shutdown();

            doTx( master );

            final CountDownLatch latch2 = new CountDownLatch( 1 );
            waitOnHeartbeatFail( master, latch2 );

            HighlyAvailableGraphDatabase slave2 = cluster.getAnySlave( slave1 );
            ClusterManager.RepairKit rk2 = cluster.fail( slave2 );

            latch2.await();

            // The master should stop saying that it's master
            assertFalse( master.isMaster() );

            try
            {
                doTx( master );
                fail( ""After both slaves fail txs should not go through"" );
            }
            catch ( TransactionFailureException e )
            {
                assertEquals( ""Timeout waiting for cluster to elect master"", e.getMessage() );
            }

            // This is not a hack, this simulates a period of inactivity in the cluster.
            Thread.sleep( 120000 ); // TODO Define ""inactivity"" and await that condition instead of 120 seconds.

            final CountDownLatch latch3 = new CountDownLatch( 1 );
            final CountDownLatch latch4 = new CountDownLatch( 1 );
            final CountDownLatch latch5 = new CountDownLatch( 1 );
            waitOnHeartbeatAlive( master, latch3 );
//            waitOnRoleIsAvailable( master, latch4, HighAvailabilityModeSwitcher.MASTER );
            waitOnRoleIsAvailable( master, latch5, HighAvailabilityModeSwitcher.SLAVE );

            rk2.repair();

            latch3.await();

            cluster.await( ClusterManager.masterAvailable( slave1, slave2 ) );

//            latch4.await();
            latch5.await();

            cluster.await( ClusterManager.masterAvailable(  ) );

            assertTrue( master.isMaster() );
            assertFalse( slave2.isMaster() );

            Node finalNode = doTx( master );

            try ( Transaction transaction = slave2.beginTx() )
            {
                slave2.getNodeById( finalNode.getId() );
                transaction.success();
            }
        }
        finally
        {
            clusterManager.stop();
        }
    }
",non-flaky,5
13919,neo4j_neo4j,QuorumWritesIT.testInstanceCanBeReplacedToReestablishQuorum,"    @Test
    public void testInstanceCanBeReplacedToReestablishQuorum() throws Throwable
    {
        File root = TargetDirectory.forTest( getClass() ).cleanDirectory(
                ""testInstanceCanBeReplacedToReestablishQuorum""
        );
        ClusterManager clusterManager = new ClusterManager( clusterOfSize( 3 ), root,
                MapUtil.stringMap( HaSettings.tx_push_factor.name(), ""2"", HaSettings.state_switch_timeout.name(), ""5s"" ) );
        clusterManager.start();
        ClusterManager.ManagedCluster cluster = clusterManager.getDefaultCluster();

        HighlyAvailableGraphDatabase master = cluster.getMaster();

        cluster.await( ClusterManager.masterSeesAllSlavesAsAvailable() );

        doTx( master );

        final CountDownLatch latch1 = new CountDownLatch( 1 );
        waitOnHeartbeatFail( master, latch1 );

        HighlyAvailableGraphDatabase slave1 = cluster.getAnySlave();
        cluster.fail( slave1 );

        latch1.await();
        slave1.shutdown();

        doTx( master );

        final CountDownLatch latch2 = new CountDownLatch( 1 );
        waitOnHeartbeatFail( master, latch2 );

        HighlyAvailableGraphDatabase slave2 = cluster.getAnySlave( slave1 );
        cluster.fail( slave2 );

        latch2.await();

        // The master should stop saying that it's master
        assertFalse( master.isMaster() );

        try
        {
            doTx( master );
            fail( ""After both slaves fail txs should not go through"" );
        }
        catch ( TransactionFailureException e )
        {
            assertEquals( ""Timeout waiting for cluster to elect master"", e.getMessage() );
        }

        // This is not a hack, this simulates a period of inactivity in the cluster.
        Thread.sleep( 120000 ); // TODO Define ""inactivity"" and await that condition instead of 120 seconds.

        final CountDownLatch latch3 = new CountDownLatch( 1 );
        final CountDownLatch latch4 = new CountDownLatch( 1 );
        final CountDownLatch latch5 = new CountDownLatch( 1 );
        waitOnHeartbeatAlive( master, latch3 );
        waitOnRoleIsAvailable( master, latch4, HighAvailabilityModeSwitcher.MASTER );
        waitOnRoleIsAvailable( master, latch5, HighAvailabilityModeSwitcher.SLAVE );

        HighlyAvailableGraphDatabase replacement =
                (HighlyAvailableGraphDatabase) new TestHighlyAvailableGraphDatabaseFactory().
                newHighlyAvailableDatabaseBuilder( new File( root, ""replacement"" ).getAbsolutePath() ).
                setConfig( ClusterSettings.cluster_server, "":5010"" ).
                setConfig( HaSettings.ha_server, "":6010"" ).
                setConfig( ClusterSettings.server_id, ""3"" ).
                setConfig( ClusterSettings.initial_hosts, cluster.getInitialHostsConfigString() ).
                setConfig( HaSettings.tx_push_factor, ""0"" ).
                newGraphDatabase();

        latch3.await();
        latch4.await();
        latch5.await();

        assertTrue( master.isMaster() );
        assertFalse( replacement.isMaster() );

        Node finalNode = doTx( master );

        Transaction transaction = replacement.beginTx();
        try
        {
            replacement.getNodeById( finalNode.getId() );
        }
        finally
        {
            transaction.finish();
        }

        clusterManager.stop();
        replacement.shutdown();
    }
",non-flaky,5
13920,neo4j_neo4j,ConstraintsInHAIT.creatingConstraintOnSlaveIsNotAllowed,"    @Test
    public void creatingConstraintOnSlaveIsNotAllowed() throws Exception
    {
        // given
        ClusterManager.ManagedCluster cluster = clusterRule.startCluster();
        HighlyAvailableGraphDatabase slave = cluster.getAnySlave();

        slave.beginTx();
        try
        {
            ConstraintCreator constraintCreator = slave.schema()
                    .constraintFor( DynamicLabel.label( ""LabelName"" ) ).assertPropertyIsUnique( ""PropertyName"" );

            // when
            constraintCreator.create();
            fail( ""should have thrown exception"" );
        }
        catch ( InvalidTransactionTypeException e )
        {
            assertThat(e.getMessage(), equalTo(""Modifying the database schema can only be done on the master server, "" +
                    ""this server is a slave. Please issue schema modification commands directly to the master.""));
        }
    }
",non-flaky,5
13921,neo4j_neo4j,ForeignStoreIdIT.emptyForeignDbShouldJoinAfterHavingItsEmptyDbDeleted,"    @Test
    public void emptyForeignDbShouldJoinAfterHavingItsEmptyDbDeleted() throws Exception
    {
        // GIVEN
        // -- one instance running
        firstInstance = new TestHighlyAvailableGraphDatabaseFactory()
                .newHighlyAvailableDatabaseBuilder( DIR.cleanDirectory( ""1"" ).getAbsolutePath() )
                .setConfig( server_id, ""1"" )
                .setConfig( cluster_server, ""127.0.0.1:5001"" )
                .setConfig( ha_server, ""127.0.0.1:6031"" )
                .setConfig( initial_hosts, ""127.0.0.1:5001"" )
                .newGraphDatabase();
        // -- another instance preparing to join with a store with a different store ID
        String foreignDbStoreDir = createAnotherStore( DIR.cleanDirectory( ""2"" ), 0 );

        // WHEN
        // -- the other joins
        foreignInstance = new TestHighlyAvailableGraphDatabaseFactory()
                .newHighlyAvailableDatabaseBuilder( foreignDbStoreDir )
                .setConfig( server_id, ""2"" )
                .setConfig( initial_hosts, ""127.0.0.1:5001"" )
                .setConfig( cluster_server, ""127.0.0.1:5002"" )
                .setConfig( ha_server, ""127.0.0.1:6032"" )
                .newGraphDatabase();
        // -- and creates a node
        long foreignNode = createNode( foreignInstance, ""foreigner"" );

        // THEN
        // -- that node should arrive at the master
        assertEquals( foreignNode, findNode( firstInstance, ""foreigner"" ) );
    }
",non-flaky,5
13922,neo4j_neo4j,ForeignStoreIdIT.nonEmptyForeignDbShouldNotBeAbleToJoin,"    @Test
    public void nonEmptyForeignDbShouldNotBeAbleToJoin() throws Exception
    {
        // GIVEN
        // -- one instance running
        firstInstance = new TestHighlyAvailableGraphDatabaseFactory()
                .newHighlyAvailableDatabaseBuilder( DIR.cleanDirectory( ""1"" ).getAbsolutePath() )
                .setConfig( server_id, ""1"" )
                .setConfig( initial_hosts, ""127.0.0.1:5001"" )
                .setConfig( cluster_server, ""127.0.0.1:5001"" )
                .setConfig( ha_server, ""127.0.0.1:6041"" )
                .newGraphDatabase();
        createNodes( firstInstance, 3, ""first"" );
        // -- another instance preparing to join with a store with a different store ID
        String foreignDbStoreDir = createAnotherStore( DIR.cleanDirectory( ""2"" ), 1 );

        // WHEN
        // -- the other joins
        foreignInstance = new TestHighlyAvailableGraphDatabaseFactory()
                .newHighlyAvailableDatabaseBuilder( foreignDbStoreDir )
                .setConfig( server_id, ""2"" )
                .setConfig( initial_hosts, ""127.0.0.1:5001"" )
                .setConfig( cluster_server, ""127.0.0.1:5002"" )
                .setConfig( ha_server, ""127.0.0.1:6042"" )
                .setConfig( state_switch_timeout, ""5s"" )
                .newGraphDatabase();

        try
        {
            // THEN
            // -- that node should arrive at the master
            createNode( foreignInstance, ""foreigner"" );
            fail( ""Shouldn't be able to create a node, since it shouldn't have joined"" );
        }
        catch ( Exception e )
        {
            // Good
        }
    }
",non-flaky,5
13923,neo4j_neo4j,TestPullUpdates.makeSureUpdatePullerGetsGoingAfterMasterSwitch,"    @Test
    public void makeSureUpdatePullerGetsGoingAfterMasterSwitch() throws Throwable
    {
        File root = TargetDirectory.forTest( getClass() ).cleanDirectory( testName.getMethodName() );
        ClusterManager clusterManager = new ClusterManager( clusterOfSize( 3 ), root, MapUtil.stringMap(
                HaSettings.pull_interval.name(), PULL_INTERVAL+""ms"",
                ClusterSettings.heartbeat_interval.name(), ""2s"",
                ClusterSettings.heartbeat_timeout.name(), ""30s"") );
        clusterManager.start();
        cluster = clusterManager.getDefaultCluster();
        cluster.await( allSeesAllAsAvailable() );

        cluster.info( ""### Creating initial dataset"" );
        long commonNodeId = createNodeOnMaster();

        HighlyAvailableGraphDatabase master = cluster.getMaster();
        setProperty( master, commonNodeId, 1 );
        cluster.info( ""### Initial dataset created"" );
        awaitPropagation( 1, commonNodeId, cluster );

        cluster.info( ""### Shutting down master"" );
        ClusterManager.RepairKit masterShutdownRK = cluster.shutdown( master );

        cluster.info( ""### Awaiting new master"" );
        cluster.await( masterAvailable( master ) );
        cluster.await( masterSeesSlavesAsAvailable( 1 ) );

        cluster.info( ""### Doing a write to master"" );
        setProperty( cluster.getMaster(), commonNodeId, 2 );
        awaitPropagation( 2, commonNodeId, cluster, master );

        cluster.info( ""### Repairing cluster"" );
        masterShutdownRK.repair();
        cluster.await( masterAvailable() );
        cluster.await( masterSeesSlavesAsAvailable( 2 ) );
        cluster.await( allSeesAllAsAvailable() );

        cluster.info( ""### Awaiting change propagation"" );
        awaitPropagation( 2, commonNodeId, cluster );
    }
",non-flaky,5
13924,neo4j_neo4j,TestPullUpdates.pullUpdatesShellAppPullsUpdates,"    @Test
    public void pullUpdatesShellAppPullsUpdates() throws Throwable
    {
        File root = TargetDirectory.forTest( getClass() ).cleanDirectory( testName.getMethodName() );
        Map<Integer, Map<String, String>> instanceConfig = new HashMap<>();
        for (int i = 1; i <= 2; i++)
        {
            Map<String, String> thisInstance =
                    MapUtil.stringMap( ShellSettings.remote_shell_port.name(), """" + (SHELL_PORT + i) );
            instanceConfig.put( i, thisInstance );
        }
        ClusterManager clusterManager = new ClusterManager( clusterOfSize( 2 ), root, MapUtil.stringMap(
                HaSettings.pull_interval.name(), ""0"",
                HaSettings.tx_push_factor.name(), ""0"" ,
                ShellSettings.remote_shell_enabled.name(), ""true""
                ), instanceConfig );
        clusterManager.start();
        cluster = clusterManager.getDefaultCluster();

        long commonNodeId = createNodeOnMaster();

        setProperty( cluster.getMaster(), commonNodeId, 1 );
        callPullUpdatesViaShell( 2 );
        HighlyAvailableGraphDatabase slave = cluster.getAnySlave();
        try ( Transaction tx = slave.beginTx() )
        {
            assertEquals( 1, slave.getNodeById( commonNodeId ).getProperty( ""i"" ) );
        }
    }
",non-flaky,5
13925,neo4j_neo4j,TestPullUpdates.leftCluster,"    @Test
    public void shouldPullUpdatesOnStartupNoMatterWhat() throws Exception
    {
        GraphDatabaseService slave = null;
        GraphDatabaseService master = null;
        try
        {
            File testRootDir = TargetDirectory.forTest( getClass() ).cleanDirectory( testName.getMethodName() );
            File masterDir = new File( testRootDir, ""master"" );
            master = new TestHighlyAvailableGraphDatabaseFactory().
                    newHighlyAvailableDatabaseBuilder( masterDir.getAbsolutePath() )
                    .setConfig( ClusterSettings.server_id, ""1"" )
                    .setConfig( ClusterSettings.initial_hosts, "":5001"" )
                    .newGraphDatabase();

            // Copy the store, then shutdown, so update pulling later makes sense
            File slaveDir = new File( testRootDir, ""slave"" );
            slave = new TestHighlyAvailableGraphDatabaseFactory().
                    newHighlyAvailableDatabaseBuilder( slaveDir.getAbsolutePath() )
                    .setConfig( ClusterSettings.server_id, ""2"" )
                    .setConfig( ClusterSettings.initial_hosts, "":5001"" )
                    .newGraphDatabase();

            // Required to block until the slave has left for sure
            final CountDownLatch slaveLeftLatch = new CountDownLatch( 1 );

            final ClusterClient masterClusterClient = ( (HighlyAvailableGraphDatabase) master ).getDependencyResolver()
                    .resolveDependency( ClusterClient.class );

            masterClusterClient.addClusterListener( new ClusterListener.Adapter()
            {
                @Override
                public void leftCluster( InstanceId instanceId, URI member )
                {
                    slaveLeftLatch.countDown();
                    masterClusterClient.removeClusterListener( this );
                }
",non-flaky,5
13926,neo4j_neo4j,TestProver.aClusterSnapshotShouldEqualItsOrigin,"    @Test
    public void aClusterSnapshotShouldEqualItsOrigin() throws Exception
    {
        // Given
        Logging logging = new TestLogging();
        ClusterConfiguration config = new ClusterConfiguration( ""default"",
                logging.getMessagesLog( ClusterConfiguration.class ),
                ""cluster://localhost:5001"",
                ""cluster://localhost:5002"",
                ""cluster://localhost:5003"" );

        ClusterState state = new ClusterState(
                asList(
                        newClusterInstance( new InstanceId( 1 ), new URI( ""cluster://localhost:5001"" ),
                                new Monitors(), config, logging ),
                        newClusterInstance( new InstanceId( 2 ), new URI( ""cluster://localhost:5002"" ),
                                new Monitors(), config, logging ),
                        newClusterInstance( new InstanceId( 3 ), new URI( ""cluster://localhost:5003"" ),
                                new Monitors(), config, logging ) ),
                emptySetOf( ClusterAction.class )
        );

        // When
        ClusterState snapshot = state.snapshot();

        // Then
        assertEquals( state, snapshot );
        assertEquals( state.hashCode(), snapshot.hashCode() );
    }
",non-flaky,5
13927,neo4j_neo4j,TestProver.twoStatesWithSameSetupAndPendingMessagesShouldBeEqual,"    @Test
    public void twoStatesWithSameSetupAndPendingMessagesShouldBeEqual() throws Exception
    {
        // Given
        Logging logging = new TestLogging();
        ClusterConfiguration config = new ClusterConfiguration( ""default"",
                logging.getMessagesLog( ClusterConfiguration.class ),
                ""cluster://localhost:5001"",
                ""cluster://localhost:5002"",
                ""cluster://localhost:5003"" );

        ClusterState state = new ClusterState(
                asList(
                        newClusterInstance( new InstanceId( 1 ), new URI( ""cluster://localhost:5001"" ),
                                new Monitors(), config, logging ),
                        newClusterInstance( new InstanceId( 2 ), new URI( ""cluster://localhost:5002"" ),
                                new Monitors(), config, logging ),
                        newClusterInstance( new InstanceId( 3 ), new URI( ""cluster://localhost:5003"" ),
                                new Monitors(), config, logging ) ),
                emptySetOf( ClusterAction.class )
        );

        // When
        ClusterState firstState = state.performAction( new MessageDeliveryAction( Message.to( ClusterMessage.join,
                new URI( ""cluster://localhost:5002"" ), new Object[]{""defaultcluster"",
                        new URI[]{new URI( ""cluster://localhost:5003"" )}} ).setHeader( Message.CONVERSATION_ID,
                ""-1"" ).setHeader( Message.FROM, ""cluster://localhost:5002"" ) ) );
        ClusterState secondState = state.performAction( new MessageDeliveryAction( Message.to( ClusterMessage.join,
                new URI( ""cluster://localhost:5002"" ), new Object[]{""defaultcluster"",
                        new URI[]{new URI( ""cluster://localhost:5003"" )}} ).setHeader( Message.CONVERSATION_ID,
                ""-1"" ).setHeader( Message.FROM, ""cluster://localhost:5002"" ) ) );

        // Then
        assertEquals( firstState, secondState );
        assertEquals( firstState.hashCode(), secondState.hashCode() );
    }
",non-flaky,5
13928,neo4j_neo4j,TestProverTimeouts.equalsShouldBeLogicalAndNotExact,"    @Test
    public void equalsShouldBeLogicalAndNotExact() throws Exception
    {
        // Given
        ProverTimeouts timeouts1 = new ProverTimeouts( new URI(""http://asd"") );
        ProverTimeouts timeouts2 = new ProverTimeouts( new URI(""http://asd"") );

        timeouts1.setTimeout( ""a"", Message.internal( ProposerMessage.join ) );
        timeouts1.setTimeout( ""b"", Message.internal( ProposerMessage.join ) );
        timeouts1.setTimeout( ""c"", Message.internal( ProposerMessage.join ) );

        timeouts2.setTimeout( ""b"", Message.internal( ProposerMessage.join ) );
        timeouts2.setTimeout( ""c"", Message.internal( ProposerMessage.join ) );

        // When
        timeouts1.cancelTimeout( ""a"" );

        // Then
        assertEquals(timeouts1, timeouts2);
    }
",non-flaky,5
13929,neo4j_neo4j,ClusterTransactionTest.call,"    @Test
    public void givenClusterWhenShutdownMasterThenCannotStartTransactionOnSlave() throws Throwable
    {
        // Given
        ClusterManager clusterManager = new ClusterManager(
                fromXml( getClass().getResource( ""/threeinstances.xml"" ).toURI() ),
                forTest( getClass() ).cleanDirectory( ""testCluster"" ),
                stringMap( HaSettings.ha_server.name(), "":6001-6005"", HaSettings.tx_push_factor.name(), ""2"" ) );
        try
        {
            clusterManager.start();

            clusterManager.getDefaultCluster().await( ClusterManager.allSeesAllAsAvailable() );

            GraphDatabaseAPI master = clusterManager.getDefaultCluster().getMaster();
            final GraphDatabaseAPI slave = clusterManager.getDefaultCluster().getAnySlave();

            // When
            final FutureTask<Boolean> result = new FutureTask<>( new Callable<Boolean>()
            {
                @Override
                public Boolean call() throws Exception
                {
                    try ( Transaction tx = slave.beginTx() )
                    {
                        tx.acquireWriteLock( slave.getNodeById( 0 ) );
                        // Fail
                        return false;
                    }
                    catch ( Exception e )
                    {
                        // Ok!
                        return true;
                    }
                }
",non-flaky,5
13930,neo4j_neo4j,TransactionConstraintsIT.startTxAsSlaveAndFinishItAfterHavingSwitchedToMasterShouldNotSucceed,"    @Test
    public void startTxAsSlaveAndFinishItAfterHavingSwitchedToMasterShouldNotSucceed() throws Exception
    {
        // GIVEN
        GraphDatabaseService db = cluster.getAnySlave();
        takeTheLeadInAnEventualMasterSwitch( db );

        // WHEN
        Transaction tx = db.beginTx();
        try
        {
            db.createNode().setProperty( ""name"", ""slave"" );
            tx.success();
        }
        finally
        {
            cluster.shutdown( cluster.getMaster() );
            assertFinishGetsTransactionFailure( tx );
        }

        cluster.await( masterAvailable() );

        // THEN
        assertEquals( db, cluster.getMaster() );
        // to prevent a deadlock scenario which occurs if this test exists (and @After starts)
        // before the db has recovered from its KERNEL_PANIC
        awaitFullyOperational( db );
    }
",non-flaky,5
13931,neo4j_neo4j,TransactionConstraintsIT.startTxAsSlaveAndFinishItAfterAnotherMasterBeingAvailableShouldNotSucceed,"    @Test
    public void startTxAsSlaveAndFinishItAfterAnotherMasterBeingAvailableShouldNotSucceed() throws Exception
    {
        // GIVEN
        HighlyAvailableGraphDatabase db = cluster.getAnySlave();

        // WHEN
        HighlyAvailableGraphDatabase theOtherSlave;
        Transaction tx = db.beginTx();
        try
        {
            db.createNode().setProperty( ""name"", ""slave"" );
            tx.success();
        }
        finally
        {
            theOtherSlave = cluster.getAnySlave( db );
            takeTheLeadInAnEventualMasterSwitch( theOtherSlave );
            cluster.shutdown( cluster.getMaster() );
            assertFinishGetsTransactionFailure( tx );
        }

        cluster.await( ClusterManager.masterAvailable() );

        // THEN
        assertFalse( db.isMaster() );
        assertTrue( theOtherSlave.isMaster() );
        // to prevent a deadlock scenario which occurs if this test exists (and @After starts)
        // before the db has recovered from its KERNEL_PANIC
        awaitFullyOperational( db );
    }
",non-flaky,5
26150,Ericsson_ecchronos,TestRepairManagementRESTImpl.testStatusEmpty,"    @Test
    public void testStatusEmpty()
    {
        when(myRepairScheduler.getCurrentRepairJobs()).thenReturn(new ArrayList<>());

        List<ScheduledRepairJob> response = GSON.fromJson(repairManagementREST.status(), scheduledRepairJobListType);

        assertThat(response).isEmpty();
    }
",non-flaky,5
26151,Ericsson_ecchronos,TestRepairManagementRESTImpl.testStatusEntry,"    @Test
    public void testStatusEntry()
    {
        long repairInterval = TimeUnit.DAYS.toMillis(7);
        long lastRepairedAt = System.currentTimeMillis();

        RepairJobView repairJobView = new TestUtils.ScheduledRepairJobBuilder()
                .withKeyspace(""ks"")
                .withTable(""tb"")
                .withLastRepairedAt(lastRepairedAt)
                .withRepairInterval(repairInterval)
                .build();
        ScheduledRepairJob expectedResponse = new ScheduledRepairJob(repairJobView);

        when(myRepairScheduler.getCurrentRepairJobs()).thenReturn(Collections.singletonList(repairJobView));

        List<ScheduledRepairJob> response = GSON.fromJson(repairManagementREST.status(), scheduledRepairJobListType);

        assertThat(response).containsExactly(expectedResponse);
    }
",non-flaky,5
26152,Ericsson_ecchronos,TestRepairManagementRESTImpl.testStatusMultipleEntries,"    @Test
    public void testStatusMultipleEntries()
    {
        RepairJobView job1 = new TestUtils.ScheduledRepairJobBuilder()
                .withKeyspace(""ks"")
                .withTable(""tb"")
                .withLastRepairedAt(1234L)
                .withRepairInterval(11)
                .build();
        RepairJobView job2 = new TestUtils.ScheduledRepairJobBuilder()
                .withKeyspace(""ks"")
                .withTable(""tb2"")
                .withLastRepairedAt(2345L)
                .withRepairInterval(12)
                .build();
        RepairJobView job3 = new TestUtils.OnDemandRepairJobBuilder()
                .withKeyspace(""ks"")
                .withTable(""tb2"")
                .withCompletedAt(3456L)
                .build();
        List<RepairJobView> repairJobViews = Arrays.asList(
                job1,
                job2,
                job3
        );

        List<ScheduledRepairJob> expectedResponse = repairJobViews.stream()
                .map(ScheduledRepairJob::new)
                .collect(Collectors.toList());

        when(myRepairScheduler.getCurrentRepairJobs()).thenReturn(repairJobViews);

        List<ScheduledRepairJob> response = GSON.fromJson(repairManagementREST.status(), scheduledRepairJobListType);

        assertThat(response).isEqualTo(expectedResponse);
    }
",non-flaky,5
26153,Ericsson_ecchronos,TestRepairManagementRESTImpl.testKeyspaceStatusEmpty,"    @Test
    public void testKeyspaceStatusEmpty()
    {
        when(myRepairScheduler.getCurrentRepairJobs()).thenReturn(new ArrayList<>());

        List<ScheduledRepairJob> response = GSON.fromJson(repairManagementREST.keyspaceStatus(""""), scheduledRepairJobListType);

        assertThat(response).isEmpty();
    }
",non-flaky,5
26154,Ericsson_ecchronos,TestRepairManagementRESTImpl.testKeyspaceStatusNonExisting,"    @Test
    public void testKeyspaceStatusNonExisting()
    {
        long expectedLastRepairedAt = 234;
        long expectedRepairInterval = 123;

        RepairJobView repairJobView = new TestUtils.ScheduledRepairJobBuilder()
                .withKeyspace(""ks"")
                .withTable(""tb"")
                .withLastRepairedAt(expectedLastRepairedAt)
                .withRepairInterval(expectedRepairInterval)
                .build();

        when(myRepairScheduler.getCurrentRepairJobs()).thenReturn(Collections.singletonList(repairJobView));

        List<ScheduledRepairJob> response = GSON.fromJson(repairManagementREST.keyspaceStatus(""nonexistingkeyspace""), scheduledRepairJobListType);

        assertThat(response).isEmpty();
    }
",non-flaky,5
26155,Ericsson_ecchronos,TestRepairManagementRESTImpl.testKeyspaceStatusEntry,"    @Test
    public void testKeyspaceStatusEntry()
    {
        long expectedLastRepairedAt = 234;
        long repairInterval = 123;

        RepairJobView repairJobView = new TestUtils.ScheduledRepairJobBuilder()
            .withKeyspace(""ks"")
            .withTable(""tb"")
            .withLastRepairedAt(expectedLastRepairedAt)
            .withRepairInterval(repairInterval)
            .build();
        ScheduledRepairJob expectedResponse = new ScheduledRepairJob(repairJobView);

        when(myRepairScheduler.getCurrentRepairJobs()).thenReturn(Collections.singletonList(repairJobView));

        List<ScheduledRepairJob> response = GSON.fromJson(repairManagementREST.keyspaceStatus(""ks""), scheduledRepairJobListType);

        assertThat(response).containsExactly(expectedResponse);
    }
",non-flaky,5
26156,Ericsson_ecchronos,TestRepairManagementRESTImpl.testKeyspaceStatusMultipleEntries,"    @Test
    public void testKeyspaceStatusMultipleEntries()
    {
        RepairJobView job1 = new TestUtils.ScheduledRepairJobBuilder()
                .withKeyspace(""ks"")
                .withTable(""tb"")
                .withLastRepairedAt(1234L)
                .withRepairInterval(11)
                .build();
        RepairJobView job2 = new TestUtils.ScheduledRepairJobBuilder()
                .withKeyspace(""ks"")
                .withTable(""tb2"")
                .withLastRepairedAt(2345L)
                .withRepairInterval(45)
                .build();
        RepairJobView job3 = new TestUtils.OnDemandRepairJobBuilder()
                .withKeyspace(""ks"")
                .withTable(""tb2"")
                .withCompletedAt(3456L)
                .build();
        List<RepairJobView> repairJobViews = Arrays.asList(
                job1,
                job2,
                job3
        );
        List<ScheduledRepairJob> expectedResponse = repairJobViews.stream()
                .map(ScheduledRepairJob::new)
                .collect(Collectors.toList());

        when(myRepairScheduler.getCurrentRepairJobs()).thenReturn(repairJobViews);

        List<ScheduledRepairJob> response = GSON.fromJson(repairManagementREST.keyspaceStatus(""ks""), scheduledRepairJobListType);

        assertThat(response).isEqualTo(expectedResponse);
    }
",non-flaky,5
26157,Ericsson_ecchronos,TestRepairManagementRESTImpl.testTableNonExisting,"    @Test
    public void testTableNonExisting()
    {
        when(myRepairScheduler.getCurrentRepairJobs()).thenReturn(new ArrayList<>());

        Map<Object, Object> response = GSON.fromJson(repairManagementREST.tableStatus(""ks"", ""tb""), new TypeToken<Map<Object, Object>>(){}.getType());

        assertThat(response).isEmpty();
    }
",non-flaky,5
26158,Ericsson_ecchronos,TestRepairManagementRESTImpl.testTableEntry,"    @Test
    public void testTableEntry() throws UnknownHostException
    {
        long expectedLastRepairedAt = 234;
        long repairInterval = 123;
        Node replica = mock(Node.class);
        when(replica.getPublicAddress()).thenReturn(InetAddress.getLocalHost());

        VnodeRepairState vnodeRepairState = TestUtils
                .createVnodeRepairState(2, 3, ImmutableSet.of(replica), expectedLastRepairedAt);
        RepairJobView repairJobView = new TestUtils.ScheduledRepairJobBuilder()
                .withKeyspace(""ks"")
                .withTable(""tb"")
                .withLastRepairedAt(expectedLastRepairedAt)
                .withRepairInterval(repairInterval)
                .withVnodeRepairStateSet(ImmutableSet.of(vnodeRepairState))
                .withStatus(Status.IN_QUEUE)
                .build();

        List<ScheduledRepairJob> expectedResponse = Collections.singletonList(new ScheduledRepairJob(repairJobView));

        when(myRepairScheduler.getCurrentRepairJobs()).thenReturn(Collections.singletonList(repairJobView));

        List<ScheduledRepairJob> response = GSON.fromJson(repairManagementREST.tableStatus(""ks"", ""tb""), scheduledRepairJobListType);

        assertThat(response).isEqualTo(expectedResponse);
    }
",non-flaky,5
26159,Ericsson_ecchronos,TestRepairManagementRESTImpl.testTableMultipleEntries,"    @Test
    public void testTableMultipleEntries() throws UnknownHostException
    {
        Host host = mock(Host.class);
        when(host.getBroadcastAddress()).thenReturn(InetAddress.getLocalHost());
        RepairJobView job1 = new TestUtils.ScheduledRepairJobBuilder()
                .withKeyspace(""ks"")
                .withTable(""tb"")
                .withLastRepairedAt(1234L)
                .withRepairInterval(11)
                .build();
        RepairJobView job2 = new TestUtils.ScheduledRepairJobBuilder()
                .withKeyspace(""ks"")
                .withTable(""tb2"")
                .withLastRepairedAt(134L)
                .withRepairInterval(112)
                .build();
        RepairJobView job3 = new TestUtils.OnDemandRepairJobBuilder()
                .withKeyspace(""ks"")
                .withTable(""tb2"")
                .withCompletedAt(3456L)
                .build();
        List<RepairJobView> repairJobViews = Arrays.asList(
                job1,
                job2,
                job3
        );

        List<ScheduledRepairJob> expectedResponse = repairJobViews.stream()
                .filter(job -> ""tb"".equals(job.getTableReference().getTable()))
                .map(ScheduledRepairJob::new)
                .collect(Collectors.toList());

        when(myRepairScheduler.getCurrentRepairJobs()).thenReturn(repairJobViews);

        List<ScheduledRepairJob> response = GSON.fromJson(repairManagementREST.tableStatus(""ks"", ""tb""), scheduledRepairJobListType);

        assertThat(response).isEqualTo(expectedResponse);
    }
",non-flaky,5
26160,Ericsson_ecchronos,TestRepairManagementRESTImpl.testIdEntry,"    @Test
    public void testIdEntry() throws UnknownHostException
    {
        Host host = mock(Host.class);
        when(host.getBroadcastAddress()).thenReturn(InetAddress.getLocalHost());
        UUID expectedId = UUID.randomUUID();
        RepairJobView expectedRepairJob = new TestUtils.ScheduledRepairJobBuilder()
                .withId(expectedId)
                .withKeyspace(""ks"")
                .withTable(""tb"")
                .withLastRepairedAt(1234L)
                .withRepairInterval(11)
                .build();
        RepairJobView job1 = new TestUtils.ScheduledRepairJobBuilder()
                .withKeyspace(""ks"")
                .withTable(""tb2"")
                .withLastRepairedAt(134L)
                .withRepairInterval(112)
                .build();
        RepairJobView job2 = new TestUtils.ScheduledRepairJobBuilder()
                .withKeyspace(""ks"")
                .withTable(""tb"")
                .withLastRepairedAt(132L)
                .withRepairInterval(132)
                .build();
        RepairJobView job3 = new TestUtils.OnDemandRepairJobBuilder()
                .withKeyspace(""ks"")
                .withTable(""tb2"")
                .withCompletedAt(3456L)
                .build();

        CompleteRepairJob expectedResponse = new CompleteRepairJob(expectedRepairJob);

        List<RepairJobView> repairJobViews = Arrays.asList(
                expectedRepairJob,
                job3,
                job1,
                job2
        );

        when(myRepairScheduler.getCurrentRepairJobs()).thenReturn(repairJobViews);

        CompleteRepairJob response = GSON.fromJson(repairManagementREST.jobStatus(expectedId.toString()), CompleteRepairJob.class);

        assertThat(response).isEqualTo(expectedResponse);
    }
",non-flaky,5
26161,Ericsson_ecchronos,TestRepairManagementRESTImpl.testIdEntryNotFound,"    @Test
    public void testIdEntryNotFound() throws UnknownHostException
    {
        Host host = mock(Host.class);
        when(host.getBroadcastAddress()).thenReturn(InetAddress.getLocalHost());
        RepairJobView job1 = new TestUtils.ScheduledRepairJobBuilder()
                .withKeyspace(""ks"")
                .withTable(""tb"")
                .withLastRepairedAt(1234L)
                .withRepairInterval(11)
                .build();
        RepairJobView job2 = new TestUtils.ScheduledRepairJobBuilder()
                .withKeyspace(""ks"")
                .withTable(""tb2"")
                .withLastRepairedAt(134L)
                .withRepairInterval(112)
                .build();
        RepairJobView job3 = new TestUtils.OnDemandRepairJobBuilder()
                .withKeyspace(""ks"")
                .withTable(""tb2"")
                .withCompletedAt(3456L)
                .build();
        List<RepairJobView> repairJobViews = Arrays.asList(
                job1,
                job2,
                job3
        );

        when(myRepairScheduler.getCurrentRepairJobs()).thenReturn(repairJobViews);

        String response = repairManagementREST.jobStatus(UUID.randomUUID().toString());

        assertThat(response).isEqualTo(""{}"");
    }
",non-flaky,5
26162,Ericsson_ecchronos,TestRepairManagementRESTImpl.testIdEntryEmpty,"    @Test
    public void testIdEntryEmpty()
    {
        when(myRepairScheduler.getCurrentRepairJobs()).thenReturn(Collections.emptyList());

        String response = repairManagementREST.jobStatus(UUID.randomUUID().toString());

        assertThat(response).isEqualTo(""{}"");
    }
",non-flaky,5
26163,Ericsson_ecchronos,TestRepairManagementRESTImpl.testIdInvalidUUID,"    @Test
    public void testIdInvalidUUID()
    {
        when(myRepairScheduler.getCurrentRepairJobs()).thenReturn(Collections.emptyList());

        String response = repairManagementREST.jobStatus(""123"");

        assertThat(response).isEqualTo(""{}"");
    }
",non-flaky,5
26164,Ericsson_ecchronos,TestRepairManagementRESTImpl.testConfigEmpty,"    @Test
    public void testConfigEmpty()
    {
        when(myRepairScheduler.getCurrentRepairJobs()).thenReturn(new ArrayList<>());

        List<TableRepairConfig> response = GSON.fromJson(repairManagementREST.config(), tableRepairConfigListType);

        assertThat(response).isEmpty();
    }
",non-flaky,5
26165,Ericsson_ecchronos,TestRepairManagementRESTImpl.testConfigEntry,"    @Test
    public void testConfigEntry()
    {
        // Given
        RepairConfiguration repairConfig = TestUtils.createRepairConfiguration(11, 2.2, 33, 44);
        RepairJobView repairJobView = new ScheduledRepairJobView(UUID.randomUUID(), myTableReferenceFactory.forTable(""ks"", ""tbl""), repairConfig, null, Status.IN_QUEUE, 0);
        TableRepairConfig expectedResponse = new TableRepairConfig(repairJobView);

        when(myRepairScheduler.getCurrentRepairJobs()).thenReturn(Collections.singletonList(repairJobView));

        List<TableRepairConfig> response = GSON.fromJson(repairManagementREST.config(), tableRepairConfigListType);

        assertThat(response).containsExactly(expectedResponse);
    }
",non-flaky,5
26166,Ericsson_ecchronos,TestRepairManagementRESTImpl.testConfigMultipleEntries,"    @Test
    public void testConfigMultipleEntries()
    {
        // Given
        RepairConfiguration repairConfig = TestUtils.createRepairConfiguration(11, 2.2, 33, 44);
        RepairJobView repairJobView = new ScheduledRepairJobView(UUID.randomUUID(), myTableReferenceFactory.forTable(""ks"", ""tbl""), repairConfig, null, Status.IN_QUEUE, 0);

        RepairConfiguration repairConfig2 = TestUtils.createRepairConfiguration(22, 3.3, 44, 55);
        RepairJobView repairJobView2 = new ScheduledRepairJobView(UUID.randomUUID(), myTableReferenceFactory.forTable(""ks2"", ""tbl""), repairConfig2, null, Status.IN_QUEUE, 0);

        RepairJobView repairJobView3 = new OnDemandRepairJobView(UUID.randomUUID(), myTableReferenceFactory.forTable(""ks"", ""tbl""), RepairConfiguration.DEFAULT, Status.IN_QUEUE, 0, System.currentTimeMillis());

        List<TableRepairConfig> expectedResponse = Arrays.asList(
                new TableRepairConfig(repairJobView),
                new TableRepairConfig(repairJobView2),
                new TableRepairConfig(repairJobView3)
        );

        when(myRepairScheduler.getCurrentRepairJobs()).thenReturn(Arrays.asList(repairJobView, repairJobView2, repairJobView3));

        List<TableRepairConfig> response = GSON.fromJson(repairManagementREST.config(), tableRepairConfigListType);

        assertThat(response).isEqualTo(expectedResponse);
    }
",non-flaky,5
26167,Ericsson_ecchronos,TestRepairManagementRESTImpl.testKeyspaceConfigEmpty,"    @Test
    public void testKeyspaceConfigEmpty()
    {
        when(myRepairScheduler.getCurrentRepairJobs()).thenReturn(new ArrayList<>());

        List<TableRepairConfig> response = GSON.fromJson(repairManagementREST.keyspaceConfig(""""), tableRepairConfigListType);

        assertThat(response).isEmpty();
    }
",non-flaky,5
26168,Ericsson_ecchronos,TestRepairManagementRESTImpl.testKeyspaceConfigNonExisting,"    @Test
    public void testKeyspaceConfigNonExisting()
    {
        RepairConfiguration repairConfig = TestUtils.createRepairConfiguration(11, 2.2, 33, 44);
        RepairJobView repairJobView = new ScheduledRepairJobView(UUID.randomUUID(), myTableReferenceFactory.forTable(""ks"", ""tbl""), repairConfig, null, Status.IN_QUEUE, 0);

        when(myRepairScheduler.getCurrentRepairJobs()).thenReturn(Collections.singletonList(repairJobView));

        List<TableRepairConfig> response = GSON.fromJson(repairManagementREST.keyspaceConfig(""nonexistingkeyspace""), tableRepairConfigListType);

        assertThat(response).isEmpty();
    }
",non-flaky,5
26169,Ericsson_ecchronos,TestRepairManagementRESTImpl.testKeyspaceConfigEntry,"    @Test
    public void testKeyspaceConfigEntry()
    {
        RepairConfiguration repairConfig = TestUtils.createRepairConfiguration(11, 2.2, 33, 44);
        RepairJobView repairJobView = new ScheduledRepairJobView(UUID.randomUUID(), myTableReferenceFactory.forTable(""ks"", ""tbl""), repairConfig, null, Status.IN_QUEUE, 0);

        TableRepairConfig expectedResponse = new TableRepairConfig(repairJobView);

        when(myRepairScheduler.getCurrentRepairJobs()).thenReturn(Collections.singletonList(repairJobView));

        List<TableRepairConfig> response = GSON.fromJson(repairManagementREST.keyspaceConfig(""ks""), tableRepairConfigListType);

        assertThat(response).containsExactly(expectedResponse);
    }
",non-flaky,5
26170,Ericsson_ecchronos,TestRepairManagementRESTImpl.testKeyspaceConfigMultipleEntries,"    @Test
    public void testKeyspaceConfigMultipleEntries()
    {
        RepairConfiguration repairConfig = TestUtils.createRepairConfiguration(11, 2.2, 33, 44);
        RepairJobView repairJobView = new ScheduledRepairJobView(UUID.randomUUID(), myTableReferenceFactory.forTable(""ks"", ""tbl""), repairConfig, null, Status.IN_QUEUE, 0);

        RepairConfiguration repairConfig2 = TestUtils.createRepairConfiguration(22, 3.3, 44, 55);
        RepairJobView repairJobView2 = new ScheduledRepairJobView(UUID.randomUUID(), myTableReferenceFactory.forTable(""ks"", ""tbl2""), repairConfig2, null, Status.IN_QUEUE, 0);

        List<TableRepairConfig> expectedResponse = Arrays.asList(
                new TableRepairConfig(repairJobView),
                new TableRepairConfig(repairJobView2)
        );

        when(myRepairScheduler.getCurrentRepairJobs()).thenReturn(Arrays.asList(repairJobView, repairJobView2));

        List<TableRepairConfig> response = GSON.fromJson(repairManagementREST.keyspaceConfig(""ks""), tableRepairConfigListType);

        assertThat(response).isEqualTo(expectedResponse);
    }
",non-flaky,5
26171,Ericsson_ecchronos,TestRepairManagementRESTImpl.testTableConfigEmpty,"    @Test
    public void testTableConfigEmpty()
    {
        when(myRepairScheduler.getCurrentRepairJobs()).thenReturn(new ArrayList<>());

        Map<Object, Object> response = GSON.fromJson(repairManagementREST.tableConfig(""ks"", ""tbl""), new TypeToken<Map<Object, Object>>(){}.getType());

        assertThat(response).isEmpty();
    }
",non-flaky,5
26172,Ericsson_ecchronos,TestRepairManagementRESTImpl.testTableConfigNonExisting,"    @Test
    public void testTableConfigNonExisting()
    {
        RepairConfiguration repairConfig = TestUtils.createRepairConfiguration(11, 2.2, 33, 44);
        RepairJobView repairJobView = new ScheduledRepairJobView(UUID.randomUUID(), myTableReferenceFactory.forTable(""ks"", ""tbl""), repairConfig, null, Status.IN_QUEUE, 0);

        when(myRepairScheduler.getCurrentRepairJobs()).thenReturn(Collections.singletonList(repairJobView));

        Map<Object, Object> response = GSON.fromJson(repairManagementREST.tableConfig(""nonexisting"", ""tbl""), new TypeToken<Map<Object, Object>>(){}.getType());

        assertThat(response).isEmpty();
    }
",non-flaky,5
26173,Ericsson_ecchronos,TestRepairManagementRESTImpl.testTableConfigEntry,"    @Test
    public void testTableConfigEntry()
    {
        RepairConfiguration repairConfig = TestUtils.createRepairConfiguration(11, 2.2, 33, 44);
        RepairJobView repairJobView = new ScheduledRepairJobView(UUID.randomUUID(), myTableReferenceFactory.forTable(""ks"", ""tbl""), repairConfig, null, Status.IN_QUEUE, 0);

        List<TableRepairConfig> expectedResponse = Collections.singletonList(new TableRepairConfig(repairJobView));

        when(myRepairScheduler.getCurrentRepairJobs()).thenReturn(Collections.singletonList(repairJobView));

        List<TableRepairConfig> response = GSON.fromJson(repairManagementREST.tableConfig(""ks"", ""tbl""), tableRepairConfigListType);

        assertThat(response).isEqualTo(expectedResponse);
    }
",non-flaky,5
26174,Ericsson_ecchronos,TestRepairManagementRESTImpl.testConfigIdEntry,"    @Test
    public void testConfigIdEntry() throws UnknownHostException
    {
        Host host = mock(Host.class);
        when(host.getBroadcastAddress()).thenReturn(InetAddress.getLocalHost());
        UUID expectedId = UUID.randomUUID();
        RepairJobView expectedRepairJob = new TestUtils.ScheduledRepairJobBuilder()
                .withId(expectedId)
                .withKeyspace(""ks"")
                .withTable(""tb"")
                .withLastRepairedAt(1234L)
                .withRepairInterval(11)
                .build();
        RepairJobView job1 = new TestUtils.ScheduledRepairJobBuilder()
                .withKeyspace(""ks"")
                .withTable(""tb2"")
                .withLastRepairedAt(134L)
                .withRepairInterval(112)
                .build();
        RepairJobView job2 = new TestUtils.ScheduledRepairJobBuilder()
                .withKeyspace(""ks"")
                .withTable(""tb"")
                .withLastRepairedAt(132L)
                .withRepairInterval(132)
                .build();
        RepairJobView job3 = new TestUtils.OnDemandRepairJobBuilder()
                .withKeyspace(""ks"")
                .withTable(""tb2"")
                .withCompletedAt(3456L)
                .build();

        TableRepairConfig expectedResponse = new TableRepairConfig(expectedRepairJob);

        List<RepairJobView> repairJobViews = Arrays.asList(
                expectedRepairJob,
                job3,
                job1,
                job2
        );

        when(myRepairScheduler.getCurrentRepairJobs()).thenReturn(repairJobViews);

        TableRepairConfig response = GSON.fromJson(repairManagementREST.jobConfig(expectedId.toString()), TableRepairConfig.class);
        assertThat(response).isEqualTo(expectedResponse);
    }
",non-flaky,5
26175,Ericsson_ecchronos,TestRepairManagementRESTImpl.testConfigIdEntryNotFound,"    @Test
    public void testConfigIdEntryNotFound() throws UnknownHostException
    {
        Host host = mock(Host.class);
        when(host.getBroadcastAddress()).thenReturn(InetAddress.getLocalHost());
        RepairJobView job1 = new TestUtils.ScheduledRepairJobBuilder()
                .withKeyspace(""ks"")
                .withTable(""tb"")
                .withLastRepairedAt(1234L)
                .withRepairInterval(11)
                .build();
        RepairJobView job2 = new TestUtils.ScheduledRepairJobBuilder()
                .withKeyspace(""ks"")
                .withTable(""tb2"")
                .withLastRepairedAt(134L)
                .withRepairInterval(112)
                .build();
        RepairJobView job3 = new TestUtils.OnDemandRepairJobBuilder()
                .withKeyspace(""ks"")
                .withTable(""tb2"")
                .withCompletedAt(3456L)
                .build();
        List<RepairJobView> repairJobViews = Arrays.asList(
                job1,
                job2,
                job3
        );

        when(myRepairScheduler.getCurrentRepairJobs()).thenReturn(repairJobViews);

        String response = repairManagementREST.jobConfig(UUID.randomUUID().toString());

        assertThat(response).isEqualTo(""{}"");
    }
",non-flaky,5
26176,Ericsson_ecchronos,TestRepairManagementRESTImpl.testConfigIdEntryEmpty,"    @Test
    public void testConfigIdEntryEmpty()
    {
        when(myRepairScheduler.getCurrentRepairJobs()).thenReturn(Collections.emptyList());

        String response = repairManagementREST.jobConfig(UUID.randomUUID().toString());

        assertThat(response).isEqualTo(""{}"");
    }
",non-flaky,5
26177,Ericsson_ecchronos,TestRepairManagementRESTImpl.testConfigIdInvalidUUID,"    @Test
    public void testConfigIdInvalidUUID()
    {
        when(myRepairScheduler.getCurrentRepairJobs()).thenReturn(Collections.emptyList());

        String response = repairManagementREST.jobConfig(""123"");

        assertThat(response).isEqualTo(""{}"");
    }
",non-flaky,5
26178,Ericsson_ecchronos,TestRepairManagementRESTImpl.testScheduleRepair,"    @Test
    public void testScheduleRepair() throws EcChronosException
    {
        long expectedLastRepairedAt = 234;
        long repairInterval = 123;

        RepairJobView repairJobView = new TestUtils.ScheduledRepairJobBuilder()
                .withKeyspace(""ks"")
                .withTable(""tb"")
                .withLastRepairedAt(expectedLastRepairedAt)
                .withRepairInterval(repairInterval)
                .build();
        ScheduledRepairJob expectedResponse = new ScheduledRepairJob(repairJobView);

        when(myOnDemandRepairScheduler.scheduleJob(myTableReferenceFactory.forTable(""ks"",""tb""))).thenReturn(repairJobView);
        ScheduledRepairJob response = GSON.fromJson(repairManagementREST.scheduleJob(""ks"", ""tb""), ScheduledRepairJob.class);
        assertThat(response).isEqualTo(expectedResponse);
    }
",non-flaky,5
26179,Ericsson_ecchronos,TestLockCollection.testCloseAllLocks,"    @Test
    public void testCloseAllLocks()
    {
        List<DummyLock> locks = new ArrayList<>();
        for (int i = 0; i < 10; i++)
        {
            locks.add(new DummyLock());
        }

        new LockCollection(locks).close();

        for (DummyLock lock : locks)
        {
            assertThat(lock.closed).isTrue();
        }
    }
",non-flaky,5
26180,Ericsson_ecchronos,TestLockCollection.testCloseAllLocksOneThrowing,"    @Test
    public void testCloseAllLocksOneThrowing()
    {
        List<DistributedLock> locks = new ArrayList<>();
        for (int i = 0; i < 4; i++)
        {
            locks.add(new DummyLock());
        }

        locks.add(new ThrowingLock());

        for (int i = 0; i < 5; i++)
        {
            locks.add(new DummyLock());
        }

        new LockCollection(locks).close();

        for (DistributedLock lock : locks)
        {
            if (lock instanceof DummyLock)
            {
                assertThat(((DummyLock) lock).closed).isTrue();
            }
        }
    }
",non-flaky,5
26181,Ericsson_ecchronos,TestScheduledJobQueue.testInsertRemoveOne,"    @Test
    public void testInsertRemoveOne()
    {
        DummyJob job = new DummyJob(Priority.LOW);

        queue.add(job);

        assertThat(queue.iterator()).toIterable().containsExactly(job);
    }
",non-flaky,5
26182,Ericsson_ecchronos,TestScheduledJobQueue.testInsertDifferentPrio,"    @Test
    public void testInsertDifferentPrio()
    {
        DummyJob job = new DummyJob(Priority.LOW);
        DummyJob job2 = new DummyJob(Priority.HIGH);

        queue.add(job);
        queue.add(job2);

        assertThat(queue.iterator()).toIterable().containsExactly(job2, job);
    }
",non-flaky,5
26183,Ericsson_ecchronos,TestScheduledJobQueue.testEmptyQueue,"    @Test
    public void testEmptyQueue()
    {
        assertThat(queue.iterator()).toIterable().isEmpty();
    }
",non-flaky,5
26184,Ericsson_ecchronos,TestScheduledJobQueue.testNonRunnableQueueIsEmpty,"    @Test
    public void testNonRunnableQueueIsEmpty() throws ScheduledJobException
    {
        final int nJobs = 10;

        for (int i = 0; i < nJobs; i++)
        {
            queue.add(new RunnableOnce(Priority.LOW));
        }

        for (ScheduledJob job : queue)
        {
            job.postExecute(true, null);
        }

        assertThat(queue.iterator()).toIterable().isEmpty();
    }
",non-flaky,5
26185,Ericsson_ecchronos,TestScheduledJobQueue.testRemoveJobInQueueIsPossible,"    @Test
    public void testRemoveJobInQueueIsPossible()
    {
        DummyJob job = new DummyJob(Priority.HIGH);
        DummyJob job2 = new DummyJob(Priority.LOW);

        queue.add(job);
        queue.add(job2);

        Iterator<ScheduledJob> iterator = queue.iterator();

        queue.remove(job2);

        assertThat(iterator).toIterable().containsExactly(job, job2);
        assertThat(queue.iterator()).toIterable().containsExactly(job);
    }
",non-flaky,5
26186,Ericsson_ecchronos,TestScheduledJobQueue.testRunOnceJobRemovedOnFinish,"    @Test
    public void testRunOnceJobRemovedOnFinish()
    {
        StateJob job = new StateJob(ScheduledJob.Priority.LOW, ScheduledJob.State.FINISHED);
        StateJob job2 = new StateJob(ScheduledJob.Priority.LOW, ScheduledJob.State.RUNNABLE);

        queue.add(job);
        queue.add(job2);

        for (ScheduledJob next : queue)
        {
            assertThat(next.getState()).isEqualTo(ScheduledJob.State.RUNNABLE);
        }

        assertThat(queue.size()).isEqualTo(1);
        assertThat(queue.iterator()).toIterable().containsExactly(job2);
    }
",non-flaky,5
26187,Ericsson_ecchronos,TestScheduledJobQueue.testRunOnceJobRemovedOnFailure,"    @Test
    public void testRunOnceJobRemovedOnFailure()
    {
        StateJob job = new StateJob(ScheduledJob.Priority.LOW, ScheduledJob.State.FAILED);
        StateJob job2 = new StateJob(ScheduledJob.Priority.LOW, ScheduledJob.State.RUNNABLE);

        queue.add(job);
        queue.add(job2);

        for (ScheduledJob next : queue)
        {
            assertThat(next.getState()).isEqualTo(ScheduledJob.State.RUNNABLE);
        }

        assertThat(queue.size()).isEqualTo(1);
        assertThat(queue.iterator()).toIterable().containsExactly(job2);
    }
",non-flaky,5
26188,Ericsson_ecchronos,TestScheduleManager.testRunningNoJobs,"    @Test
    public void testRunningNoJobs() throws LockException
    {
        myScheduler.run();

        verify(myLockFactory, never()).tryLock(any(), anyString(), anyInt(), anyMap());
    }
",non-flaky,5
26189,Ericsson_ecchronos,TestScheduleManager.testRunningOneJob,"    @Test
    public void testRunningOneJob()
    {
        DummyJob job = new DummyJob(ScheduledJob.Priority.LOW);
        myScheduler.schedule(job);

        myScheduler.run();

        assertThat(job.hasRun()).isTrue();
        assertThat(myScheduler.getQueueSize()).isEqualTo(1);
    }
",non-flaky,5
26190,Ericsson_ecchronos,TestScheduleManager.testRunningJobWithFailingRunPolicy,"    @Test
    public void testRunningJobWithFailingRunPolicy()
    {
        DummyJob job = new DummyJob(ScheduledJob.Priority.LOW);
        myScheduler.schedule(job);

        when(myRunPolicy.validate(any(ScheduledJob.class))).thenReturn(1L);

        myScheduler.run();

        assertThat(job.hasRun()).isFalse();
        assertThat(myScheduler.getQueueSize()).isEqualTo(1);
    }
",non-flaky,5
26191,Ericsson_ecchronos,TestScheduleManager.testRunningTwoTasksStoppedAfterFirstByPolicy,"    @Test
    public void testRunningTwoTasksStoppedAfterFirstByPolicy() throws LockException
    {
        ShortRunningMultipleTasks job = new ShortRunningMultipleTasks(ScheduledJob.Priority.LOW, 2, () -> {
            when(myRunPolicy.validate(any(ScheduledJob.class))).thenReturn(1L);
        });
        myScheduler.schedule(job);

        when(myLockFactory.tryLock(any(), anyString(), anyInt(), anyMap())).thenReturn(new DummyLock());

        myScheduler.run();

        assertThat(job.getNumRuns()).isEqualTo(1);
        assertThat(myScheduler.getQueueSize()).isEqualTo(1);
        verify(myLockFactory).tryLock(any(), anyString(), anyInt(), anyMap());
    }
",non-flaky,5
26192,Ericsson_ecchronos,TestScheduleManager.testRunningJobWithThrowingRunPolicy,"    @Test
    public void testRunningJobWithThrowingRunPolicy()
    {
        DummyJob job = new DummyJob(ScheduledJob.Priority.LOW);
        myScheduler.schedule(job);

        when(myRunPolicy.validate(any(ScheduledJob.class))).thenThrow(new IllegalStateException());

        myScheduler.run();

        assertThat(job.hasRun()).isFalse();
        assertThat(myScheduler.getQueueSize()).isEqualTo(1);
    }
",non-flaky,5
26193,Ericsson_ecchronos,TestScheduleManager.testRunningOneJobWithThrowingLock,"    @Test
    public void testRunningOneJobWithThrowingLock() throws LockException
    {
        DummyJob job = new DummyJob(ScheduledJob.Priority.LOW);
        myScheduler.schedule(job);

        when(myLockFactory.tryLock(any(), anyString(), anyInt(), anyMap())).thenThrow(new LockException(""""));

        myScheduler.run();

        assertThat(job.hasRun()).isFalse();
        assertThat(myScheduler.getQueueSize()).isEqualTo(1);
    }
",non-flaky,5
26194,Ericsson_ecchronos,TestScheduleManager.run,"    @Test (timeout = 2000L)
    public void testRunningTwoJobsInParallelShouldFail() throws InterruptedException
    {
        LongRunningJob job = new LongRunningJob(ScheduledJob.Priority.HIGH);
        LongRunningJob job2 = new LongRunningJob(ScheduledJob.Priority.LOW);
        myScheduler.schedule(job);
        myScheduler.schedule(job2);

        final CountDownLatch cdl = new CountDownLatch(1);

        new Thread()
        {

            @Override
            public void run()
            {
                myScheduler.run();
                cdl.countDown();
            }
",non-flaky,5
26195,Ericsson_ecchronos,TestScheduleManager.testTwoJobsRejected,"    @Test
    public void testTwoJobsRejected()
    {
        DummyJob job = new DummyJob(ScheduledJob.Priority.LOW);
        DummyJob job2 = new DummyJob(ScheduledJob.Priority.LOW);
        myScheduler.schedule(job);
        myScheduler.schedule(job2);

        when(myRunPolicy.validate(any(ScheduledJob.class))).thenReturn(1L);

        myScheduler.run();

        assertThat(job.hasRun()).isFalse();
        assertThat(myScheduler.getQueueSize()).isEqualTo(2);
        verify(myRunPolicy, times(2)).validate(any(ScheduledJob.class));
    }
",non-flaky,5
26196,Ericsson_ecchronos,TestScheduleManager.testTwoJobsThrowingLock,"    @Test
    public void testTwoJobsThrowingLock() throws LockException
    {
        DummyJob job = new DummyJob(ScheduledJob.Priority.LOW);
        DummyJob job2 = new DummyJob(ScheduledJob.Priority.LOW);
        myScheduler.schedule(job);
        myScheduler.schedule(job2);

        when(myLockFactory.tryLock(any(), anyString(), anyInt(), anyMap())).thenThrow(new LockException(""""));

        myScheduler.run();

        assertThat(job.hasRun()).isFalse();
        assertThat(myScheduler.getQueueSize()).isEqualTo(2);
        verify(myLockFactory, times(2)).tryLock(any(), anyString(), anyInt(), anyMap());
    }
",non-flaky,5
26197,Ericsson_ecchronos,TestScheduleManager.testThreeTasksOneThrowing,"    @Test
    public void testThreeTasksOneThrowing() throws LockException
    {
        ShortRunningMultipleTasks job = new ShortRunningMultipleTasks(ScheduledJob.Priority.LOW, 3);
        myScheduler.schedule(job);

        when(myLockFactory.tryLock(any(), anyString(), anyInt(), anyMap()))
                .thenReturn(new DummyLock())
                .thenThrow(new LockException(""""))
                .thenReturn(new DummyLock());

        myScheduler.run();

        assertThat(job.getNumRuns()).isEqualTo(2);
        assertThat(myScheduler.getQueueSize()).isEqualTo(1);
        verify(myLockFactory, times(3)).tryLock(any(), anyString(), anyInt(), anyMap());
    }
",non-flaky,5
26198,Ericsson_ecchronos,TestScheduleManager.run,"    @Test (timeout = 2000L)
    public void testRemoveLongRunningJob() throws InterruptedException
    {
        LongRunningJob job = new LongRunningJob(ScheduledJob.Priority.HIGH);
        myScheduler.schedule(job);

        final CountDownLatch cdl = new CountDownLatch(1);

        new Thread()
        {
            @Override
            public void run()
            {
                myScheduler.run();
                cdl.countDown();
            }
",non-flaky,5
26199,Ericsson_ecchronos,TestLockCache.testGetLock,"    @Test
    public void testGetLock() throws LockException
    {
        DistributedLock expectedLock = doReturnLockOnGetLock();

        assertGetLockRetrievesExpectedLock(expectedLock);
    }
",non-flaky,5
26200,Ericsson_ecchronos,TestLockCache.testGetThrowingLockIsCached,"    @Test
    public void testGetThrowingLockIsCached() throws LockException
    {
        LockException expectedExcetion = doThrowOnGetLock();

        assertGetLockThrowsException(expectedExcetion);

        // Reset return type, locking should still throw
        doReturnLockOnGetLock();

        assertGetLockThrowsException(expectedExcetion);
    }
",non-flaky,5
26201,Ericsson_ecchronos,TestLockCache.testGetMultipleLocks,"    @Test
    public void testGetMultipleLocks() throws LockException
    {
        String otherResource = ""RepairResource-b2e33e60-7af6-11e9-8f9e-2a86e4085a59-1"";

        DistributedLock expectedLock = doReturnLockOnGetLock(RESOURCE);
        DistributedLock expectedOtherLock = doReturnLockOnGetLock(otherResource);

        assertGetLockRetrievesExpectedLock(RESOURCE, expectedLock);
        assertGetLockRetrievesExpectedLock(otherResource, expectedOtherLock);
    }
",non-flaky,5
26202,Ericsson_ecchronos,TestLockCache.testGetOtherLockAfterThrowingOnAnotherResource,"    @Test
    public void testGetOtherLockAfterThrowingOnAnotherResource() throws LockException
    {
        String otherResource = ""RepairResource-b2e33e60-7af6-11e9-8f9e-2a86e4085a59-1"";

        LockException expectedException = doThrowOnGetLock(RESOURCE);
        DistributedLock expectedOtherLock = doReturnLockOnGetLock(otherResource);

        assertGetLockThrowsException(RESOURCE, expectedException);
        assertGetLockRetrievesExpectedLock(otherResource, expectedOtherLock);
    }
",non-flaky,5
26203,Ericsson_ecchronos,TestLockCache.testGetLockAfterCachedExceptionHasExpired,"    @Test
    public void testGetLockAfterCachedExceptionHasExpired() throws LockException, InterruptedException
    {
        myLockCache = new LockCache(mockedLockSupplier, 20, TimeUnit.MILLISECONDS);

        LockException expectedException = doThrowOnGetLock();
        assertGetLockThrowsException(expectedException);

        Thread.sleep(20);

        DistributedLock expectedLock = doReturnLockOnGetLock();
        assertGetLockRetrievesExpectedLock(expectedLock);
    }
",non-flaky,5
26204,Ericsson_ecchronos,TestLockCache.testEqualsContract,"    @Test
    public void testEqualsContract()
    {
        EqualsVerifier.forClass(LockCache.LockKey.class).usingGetClass().verify();
    }
",non-flaky,5
26205,Ericsson_ecchronos,TestHostStatesImpl.testUseNullJmxProxyFactoryShouldThrow,"    @Test
    public void testUseNullJmxProxyFactoryShouldThrow()
    {
        assertThatExceptionOfType(IllegalArgumentException.class)
                .isThrownBy(() -> HostStatesImpl.builder()
                        .withJmxProxyFactory(null)
                        .build());
    }
",non-flaky,5
26206,Ericsson_ecchronos,TestHostStatesImpl.testIsInetAddressUp,"    @Test
    public void testIsInetAddressUp() throws UnknownHostException
    {
        InetAddress expectedAddress = InetAddress.getLocalHost();

        List<String> expectedLiveNodes = Collections.singletonList(expectedAddress.getHostName());
        List<String> expectedUnreachableNodes = Collections.emptyList();

        when(myJmxProxy.getLiveNodes()).thenReturn(expectedLiveNodes);
        when(myJmxProxy.getUnreachableNodes()).thenReturn(expectedUnreachableNodes);

        assertThat(myHostStates.isUp(expectedAddress)).isTrue();
    }
",non-flaky,5
26207,Ericsson_ecchronos,TestHostStatesImpl.testIsHostUp,"    @Test
    public void testIsHostUp() throws UnknownHostException
    {
        InetAddress expectedAddress = InetAddress.getLocalHost();
        Host expectedHost = mock(Host.class);

        List<String> expectedLiveNodes = Collections.singletonList(expectedAddress.getHostName());
        List<String> expectedUnreachableNodes = Collections.emptyList();

        when(myJmxProxy.getLiveNodes()).thenReturn(expectedLiveNodes);
        when(myJmxProxy.getUnreachableNodes()).thenReturn(expectedUnreachableNodes);

        when(expectedHost.getBroadcastAddress()).thenReturn(expectedAddress);

        assertThat(myHostStates.isUp(expectedHost)).isTrue();
    }
",non-flaky,5
26208,Ericsson_ecchronos,TestHostStatesImpl.testIsInetAddressUpFaultyNode,"    @Test
    public void testIsInetAddressUpFaultyNode() throws UnknownHostException
    {
        InetAddress expectedAddress = InetAddress.getLocalHost();

        when(myJmxProxy.getLiveNodes()).thenReturn(Collections.emptyList());
        when(myJmxProxy.getUnreachableNodes()).thenReturn(Collections.emptyList());

        assertThat(myHostStates.isUp(expectedAddress)).isFalse();
    }
",non-flaky,5
26209,Ericsson_ecchronos,TestHostStatesImpl.answer,"    @Test
    public void testNodeIsNotRefreshed() throws UnknownHostException
    {
        final InetAddress expectedAddress = InetAddress.getLocalHost();

        when(myJmxProxy.getLiveNodes()).thenAnswer(new Answer<List<String>>()
        {
            private int counter = 0;

            @Override
            public List<String> answer(InvocationOnMock invocation)
            {
                if (counter++ == 2)
                {
                    return Collections.singletonList(expectedAddress.getHostAddress());
                }

                return Collections.emptyList();
            }
",non-flaky,5
26210,Ericsson_ecchronos,TestHostStatesImpl.answer,"    @Test
    public void testNodeIsRefreshed() throws UnknownHostException
    {
        final InetAddress expectedAddress = InetAddress.getLocalHost();

        HostStatesImpl hostStates = HostStatesImpl.builder()
                .withJmxProxyFactory(myJmxProxyFactory)
                .withRefreshIntervalInMs(1)
                .build();

        when(myJmxProxy.getLiveNodes()).thenAnswer(new Answer<List<String>>()
        {
            private int counter = 0;

            @Override
            public List<String> answer(InvocationOnMock invocation)
            {
                if (counter++ == 1)
                {
                    return Collections.singletonList(expectedAddress.getHostAddress());
                }
                return Collections.emptyList();
            }
",non-flaky,5
26211,Ericsson_ecchronos,TestRepairSchedulerImpl.testConfigureNewTable,"    @Test
    public void testConfigureNewTable()
    {
        RepairSchedulerImpl repairSchedulerImpl = defaultRepairSchedulerImplBuilder().build();

        repairSchedulerImpl.putConfiguration(TABLE_REFERENCE, RepairConfiguration.DEFAULT);

        verify(scheduleManager, timeout(1000)).schedule(any(ScheduledJob.class));
        verify(scheduleManager, never()).deschedule(any(ScheduledJob.class));
        verify(myRepairStateFactory).create(eq(TABLE_REFERENCE), eq(RepairConfiguration.DEFAULT), any());
        verify(myRepairState, atLeastOnce()).update();
        assertOneTableViewExist(repairSchedulerImpl, TABLE_REFERENCE, RepairConfiguration.DEFAULT);

        repairSchedulerImpl.close();
        verify(scheduleManager).deschedule(any(ScheduledJob.class));

        verifyNoMoreInteractions(ignoreStubs(myTableRepairMetrics));
        verifyNoMoreInteractions(myRepairStateFactory);
        verifyNoMoreInteractions(scheduleManager);
    }
",non-flaky,5
26212,Ericsson_ecchronos,TestRepairSchedulerImpl.testConfigureTwoTables,"    @Test
    public void testConfigureTwoTables()
    {
        RepairSchedulerImpl repairSchedulerImpl = defaultRepairSchedulerImplBuilder().build();

        repairSchedulerImpl.putConfiguration(TABLE_REFERENCE, RepairConfiguration.DEFAULT);
        repairSchedulerImpl.putConfiguration(TABLE_REFERENCE2, RepairConfiguration.DEFAULT);

        verify(scheduleManager, timeout(1000).times(2)).schedule(any(ScheduledJob.class));
        verify(scheduleManager, never()).deschedule(any(ScheduledJob.class));
        verify(myRepairStateFactory).create(eq(TABLE_REFERENCE), eq(RepairConfiguration.DEFAULT), any());
        verify(myRepairStateFactory).create(eq(TABLE_REFERENCE2), eq(RepairConfiguration.DEFAULT), any());
        verify(myRepairState, atLeastOnce()).update();

        repairSchedulerImpl.close();
        verify(scheduleManager, times(2)).deschedule(any(ScheduledJob.class));

        verifyNoMoreInteractions(ignoreStubs(myTableRepairMetrics));
        verifyNoMoreInteractions(myRepairStateFactory);
        verifyNoMoreInteractions(scheduleManager);
    }
",non-flaky,5
26213,Ericsson_ecchronos,TestRepairSchedulerImpl.testRemoveTableConfiguration,"    @Test
    public void testRemoveTableConfiguration()
    {
        RepairSchedulerImpl repairSchedulerImpl = defaultRepairSchedulerImplBuilder().build();

        repairSchedulerImpl.putConfiguration(TABLE_REFERENCE, RepairConfiguration.DEFAULT);

        verify(scheduleManager, timeout(1000)).schedule(any(ScheduledJob.class));
        verify(scheduleManager, never()).deschedule(any(ScheduledJob.class));
        verify(myRepairStateFactory).create(eq(TABLE_REFERENCE), eq(RepairConfiguration.DEFAULT), any());
        verify(myRepairState, atLeastOnce()).update();
        assertOneTableViewExist(repairSchedulerImpl, TABLE_REFERENCE, RepairConfiguration.DEFAULT);

        repairSchedulerImpl.removeConfiguration(TABLE_REFERENCE);
        verify(scheduleManager, timeout(1000)).deschedule(any(ScheduledJob.class));
        assertThat(repairSchedulerImpl.getCurrentRepairJobs()).isEmpty();

        repairSchedulerImpl.close();
        verifyNoMoreInteractions(ignoreStubs(myTableRepairMetrics));
        verifyNoMoreInteractions(myRepairStateFactory);
        verifyNoMoreInteractions(scheduleManager);
    }
",non-flaky,5
26214,Ericsson_ecchronos,TestRepairSchedulerImpl.testUpdateTableConfiguration,"    @Test
    public void testUpdateTableConfiguration()
    {
        RepairSchedulerImpl repairSchedulerImpl = defaultRepairSchedulerImplBuilder().build();

        long expectedUpdatedRepairInterval = TimeUnit.DAYS.toMillis(1);

        RepairConfiguration updatedRepairConfiguration = RepairConfiguration.newBuilder()
                .withRepairInterval(expectedUpdatedRepairInterval, TimeUnit.MILLISECONDS)
                .build();

        repairSchedulerImpl.putConfiguration(TABLE_REFERENCE, RepairConfiguration.DEFAULT);

        verify(scheduleManager, timeout(1000)).schedule(any(ScheduledJob.class));
        verify(scheduleManager, never()).deschedule(any(ScheduledJob.class));
        verify(myRepairStateFactory).create(eq(TABLE_REFERENCE), eq(RepairConfiguration.DEFAULT), any());
        verify(myRepairState, atLeastOnce()).update();
        assertOneTableViewExist(repairSchedulerImpl, TABLE_REFERENCE, RepairConfiguration.DEFAULT);

        repairSchedulerImpl.putConfiguration(TABLE_REFERENCE, updatedRepairConfiguration);

        verify(scheduleManager, timeout(1000).times(2)).schedule(any(ScheduledJob.class));
        verify(scheduleManager, timeout(1000)).deschedule(any(ScheduledJob.class));
        verify(myRepairStateFactory).create(eq(TABLE_REFERENCE), eq(updatedRepairConfiguration), any());
        verify(myRepairState, atLeastOnce()).update();
        assertOneTableViewExist(repairSchedulerImpl, TABLE_REFERENCE, updatedRepairConfiguration);

        repairSchedulerImpl.close();
        verify(scheduleManager, times(2)).deschedule(any(ScheduledJob.class));
        assertThat(repairSchedulerImpl.getCurrentRepairJobs()).isEmpty();

        verifyNoMoreInteractions(ignoreStubs(myTableRepairMetrics));
        verifyNoMoreInteractions(myRepairStateFactory);
        verifyNoMoreInteractions(scheduleManager);
    }
",non-flaky,5
26215,Ericsson_ecchronos,TestRepairSchedulerImpl.testUpdateTableConfigurationToSame,"    @Test
    public void testUpdateTableConfigurationToSame()
    {
        RepairSchedulerImpl repairSchedulerImpl = defaultRepairSchedulerImplBuilder().build();

        repairSchedulerImpl.putConfiguration(TABLE_REFERENCE, RepairConfiguration.DEFAULT);

        verify(scheduleManager, timeout(1000)).schedule(any(ScheduledJob.class));
        verify(scheduleManager, never()).deschedule(any(ScheduledJob.class));
        verify(myRepairStateFactory).create(eq(TABLE_REFERENCE), eq(RepairConfiguration.DEFAULT), any());
        verify(myRepairState, atLeastOnce()).update();
        assertOneTableViewExist(repairSchedulerImpl, TABLE_REFERENCE, RepairConfiguration.DEFAULT);

        repairSchedulerImpl.putConfiguration(TABLE_REFERENCE, RepairConfiguration.DEFAULT);

        assertOneTableViewExist(repairSchedulerImpl, TABLE_REFERENCE, RepairConfiguration.DEFAULT);

        repairSchedulerImpl.close();
        verify(scheduleManager).deschedule(any(ScheduledJob.class));
        assertThat(repairSchedulerImpl.getCurrentRepairJobs()).isEmpty();

        verifyNoMoreInteractions(ignoreStubs(myTableRepairMetrics));
        verifyNoMoreInteractions(myRepairStateFactory);
        verifyNoMoreInteractions(scheduleManager);
    }
",non-flaky,5
26216,Ericsson_ecchronos,TestOnDemandRepairJob.testJobCorrectlyReturned,"    @Test
    public void testJobCorrectlyReturned()
    {
        OnDemandRepairJob repairJob = createOnDemandRepairJob();
        RepairJobView expectedView = new OnDemandRepairJobView(repairJob.getId(), myTableReference, RepairConfiguration.DEFAULT, RepairJobView.Status.IN_QUEUE, 0, System.currentTimeMillis());
        assertThat(repairJob.getId()).isEqualTo(repairJob.getId());
        assertThat(repairJob.getLastSuccessfulRun()).isEqualTo(-1);
        assertThat(repairJob.getRepairConfiguration()).isEqualTo(RepairConfiguration.DEFAULT);
        assertThat(repairJob.getTableReference()).isEqualTo(myTableReference);
        assertThat(repairJob.getView().getRepairConfiguration()).isEqualTo(expectedView.getRepairConfiguration());
        assertThat(repairJob.getView().getRepairStateSnapshot()).isNull();
        assertThat(repairJob.getView().getTableReference()).isEqualTo(expectedView.getTableReference());
        assertThat(repairJob.getView().getStatus()).isEqualTo(expectedView.getStatus());
    }
",non-flaky,5
26217,Ericsson_ecchronos,TestOnDemandRepairJob.testFailedJobCorrectlyReturned,"    @Test
    public void testFailedJobCorrectlyReturned()
    {
        OnDemandRepairJob repairJob = createOnDemandRepairJob();
        Iterator<ScheduledTask> it = repairJob.iterator();
        repairJob.postExecute(false, it.next());
        RepairJobView expectedView = new OnDemandRepairJobView(repairJob.getId(), myTableReference, RepairConfiguration.DEFAULT, RepairJobView.Status.ERROR, 0, System.currentTimeMillis());
        assertThat(repairJob.getLastSuccessfulRun()).isEqualTo(-1);
        assertThat(repairJob.getRepairConfiguration()).isEqualTo(RepairConfiguration.DEFAULT);
        assertThat(repairJob.getTableReference()).isEqualTo(myTableReference);
        assertThat(repairJob.getView().getRepairConfiguration()).isEqualTo(expectedView.getRepairConfiguration());
        assertThat(repairJob.getView().getRepairStateSnapshot()).isNull();
        assertThat(repairJob.getView().getTableReference()).isEqualTo(expectedView.getTableReference());
        assertThat(repairJob.getView().getStatus()).isEqualTo(expectedView.getStatus());
    }
",non-flaky,5
26218,Ericsson_ecchronos,TestOnDemandRepairJob.testJobFinishedAfterExecution,"    @Test
    public void testJobFinishedAfterExecution()
    {
        OnDemandRepairJob repairJob = createOnDemandRepairJob();
        Iterator<ScheduledTask> it = repairJob.iterator();
        assertThat(repairJob.getState()).isEqualTo(ScheduledJob.State.RUNNABLE);
        repairJob.postExecute(true, it.next());
        assertThat(repairJob.getState()).isEqualTo(ScheduledJob.State.RUNNABLE);
        repairJob.postExecute(true, it.next());
        assertThat(repairJob.getState()).isEqualTo(ScheduledJob.State.FINISHED);
    }
",non-flaky,5
26219,Ericsson_ecchronos,TestOnDemandRepairJob.testJobFinishedAfterRestart,"    @Test
    public void testJobFinishedAfterRestart()
    {
        OnDemandRepairJob repairJob = createRestartedOnDemandRepairJob();
        Iterator<ScheduledTask> it = repairJob.iterator();
        assertThat(repairJob.getState()).isEqualTo(ScheduledJob.State.RUNNABLE);
        repairJob.postExecute(true, it.next());
        assertThat(repairJob.getState()).isEqualTo(ScheduledJob.State.FINISHED);
    }
",non-flaky,5
26220,Ericsson_ecchronos,TestOnDemandRepairJob.testJobFailedWhenTopologyChange,"    @Test
    public void testJobFailedWhenTopologyChange()
    {
        OnDemandRepairJob repairJob = createOnDemandRepairJob();
        when(myOngoingJob.hasTopologyChanged()).thenReturn(true);
        assertThat(repairJob.getState()).isEqualTo(ScheduledJob.State.FAILED);
    }
",non-flaky,5
26221,Ericsson_ecchronos,TestOnDemandRepairJob.testJobUnsuccessful,"    @Test
    public void testJobUnsuccessful()
    {
        OnDemandRepairJob repairJob = createOnDemandRepairJob();
        Iterator<ScheduledTask> it = repairJob.iterator();
        repairJob.postExecute(true, it.next());
        assertThat(repairJob.getState()).isEqualTo(ScheduledJob.State.RUNNABLE);
        repairJob.postExecute(false, it.next());
        assertThat(repairJob.getState()).isEqualTo(ScheduledJob.State.FAILED);
    }
",non-flaky,5
26222,Ericsson_ecchronos,TestOnDemandRepairJob.testGetProgress,"    @Test
    public void testGetProgress()
    {
        OnDemandRepairJob repairJob = createOnDemandRepairJob();
        assertThat(repairJob.getProgress()).isEqualTo(0);
        Iterator<ScheduledTask> it = repairJob.iterator();
        repairJob.postExecute(true, it.next());
        assertThat(repairJob.getProgress()).isEqualTo(0.5);
        repairJob.postExecute(true, it.next());
        assertThat(repairJob.getProgress()).isEqualTo(1);
    }
",non-flaky,5
26223,Ericsson_ecchronos,TestOnDemandRepairSchedulerImpl.testScheduleRepairOnTable,"    @Test
    public void testScheduleRepairOnTable() throws EcChronosException
    {
        OnDemandRepairSchedulerImpl repairScheduler = defaultOnDemandRepairSchedulerImplBuilder().build();
        when(metadata.getKeyspace(TABLE_REFERENCE.getKeyspace())).thenReturn(myKeyspaceMetadata);
        when(myKeyspaceMetadata.getTable(TABLE_REFERENCE.getTable())).thenReturn(myTableMetadata);

        verify(scheduleManager, never()).schedule(any(ScheduledJob.class));
        RepairJobView repairJobView = repairScheduler.scheduleJob(TABLE_REFERENCE);
        verify(scheduleManager).schedule(any(ScheduledJob.class));

        assertTableViewExist(repairScheduler, repairJobView);

        repairScheduler.close();
        verify(scheduleManager).deschedule(any(ScheduledJob.class));

        verifyNoMoreInteractions(ignoreStubs(myTableRepairMetrics));
        verifyNoMoreInteractions(scheduleManager);
    }
",non-flaky,5
26224,Ericsson_ecchronos,TestOnDemandRepairSchedulerImpl.testScheduleTwoRepairOnTable,"    @Test
    public void testScheduleTwoRepairOnTable() throws EcChronosException
    {
        OnDemandRepairSchedulerImpl repairScheduler = defaultOnDemandRepairSchedulerImplBuilder().build();
        when(metadata.getKeyspace(TABLE_REFERENCE.getKeyspace())).thenReturn(myKeyspaceMetadata);
        when(myKeyspaceMetadata.getTable(TABLE_REFERENCE.getTable())).thenReturn(myTableMetadata);

        verify(scheduleManager, never()).schedule(any(ScheduledJob.class));
        RepairJobView repairJobView = repairScheduler.scheduleJob(TABLE_REFERENCE);
        RepairJobView repairJobView2 = repairScheduler.scheduleJob(TABLE_REFERENCE);
        verify(scheduleManager, times(2)).schedule(any(ScheduledJob.class));

        assertTableViewExist(repairScheduler, repairJobView, repairJobView2);

        repairScheduler.close();
        verify(scheduleManager, times(2)).deschedule(any(ScheduledJob.class));

        verifyNoMoreInteractions(ignoreStubs(myTableRepairMetrics));
        verifyNoMoreInteractions(scheduleManager);
    }
",non-flaky,5
26225,Ericsson_ecchronos,TestOnDemandRepairSchedulerImpl.testRestartRepairOnTable,"    @Test
    public void testRestartRepairOnTable() throws EcChronosException
    {
        Set<OngoingJob> ongoingJobs = new HashSet<>();
        ongoingJobs.add(myOngingJob);
        when(myOnDemandStatus.getOngoingJobs(replicationState)).thenReturn(ongoingJobs);

        OnDemandRepairSchedulerImpl repairScheduler = defaultOnDemandRepairSchedulerImplBuilder().build();

        verify(scheduleManager, timeout(1000)).schedule(any(ScheduledJob.class));

        repairScheduler.close();
        verify(scheduleManager).deschedule(any(ScheduledJob.class));

        verifyNoMoreInteractions(ignoreStubs(myTableRepairMetrics));
        verifyNoMoreInteractions(scheduleManager);
    }
",non-flaky,5
26226,Ericsson_ecchronos,TestOnDemandRepairSchedulerImpl.testRestartRepairOnTableWithException,"    @Test
    public void testRestartRepairOnTableWithException() throws EcChronosException
    {
        Set<OngoingJob> ongoingJobs = new HashSet<>();
        ongoingJobs.add(myOngingJob);
        Map<EndPoint, Throwable> errors = new HashMap<>();
        when(myOnDemandStatus.getOngoingJobs(replicationState)).thenThrow(new NoHostAvailableException(errors )).thenReturn(ongoingJobs);

        OnDemandRepairSchedulerImpl repairScheduler = defaultOnDemandRepairSchedulerImplBuilder().build();

        verify(scheduleManager, timeout(15000)).schedule(any(ScheduledJob.class));

        repairScheduler.close();
        verify(scheduleManager).deschedule(any(ScheduledJob.class));

        verifyNoMoreInteractions(ignoreStubs(myTableRepairMetrics));
        verifyNoMoreInteractions(scheduleManager);
    }
",non-flaky,5
26227,Ericsson_ecchronos,TestOnDemandRepairSchedulerImpl.testScheduleRepairOnNonExistentKeyspaceTable,"    @Test (expected = EcChronosException.class)
    public void testScheduleRepairOnNonExistentKeyspaceTable() throws EcChronosException
    {
        OnDemandRepairSchedulerImpl repairScheduler = defaultOnDemandRepairSchedulerImplBuilder().build();

        verify(scheduleManager, never()).schedule(any(ScheduledJob.class));
        repairScheduler.scheduleJob(TABLE_REFERENCE);
    }
",non-flaky,5
26228,Ericsson_ecchronos,TestOnDemandRepairSchedulerImpl.testScheduleRepairOnNonExistentTable,"    @Test (expected = EcChronosException.class)
    public void testScheduleRepairOnNonExistentTable() throws EcChronosException
    {
        OnDemandRepairSchedulerImpl repairScheduler = defaultOnDemandRepairSchedulerImplBuilder().build();
        when(metadata.getKeyspace(TABLE_REFERENCE.getKeyspace())).thenReturn(myKeyspaceMetadata);
        verify(scheduleManager, never()).schedule(any(ScheduledJob.class));
        repairScheduler.scheduleJob(TABLE_REFERENCE);
    }
",non-flaky,5
26229,Ericsson_ecchronos,TestOnDemandRepairSchedulerImpl.testScheduleRepairOnNull,"    @Test (expected = EcChronosException.class)
    public void testScheduleRepairOnNull() throws EcChronosException
    {
        OnDemandRepairSchedulerImpl repairScheduler = defaultOnDemandRepairSchedulerImplBuilder().build();
        verify(scheduleManager, never()).schedule(any(ScheduledJob.class));
        repairScheduler.scheduleJob(null);
    }
",non-flaky,5
26230,Ericsson_ecchronos,TestRepairResource.testRepairResource,"    @Test
    public void testRepairResource()
    {
        RepairResource repairResource = new RepairResource(""dc1"", ""my-resource"");

        assertThat(repairResource.getDataCenter()).isEqualTo(""dc1"");
        assertThat(repairResource.getResourceName(1)).isEqualTo(""RepairResource-my-resource-1"");
        assertThat(repairResource.getResourceName(2)).isEqualTo(""RepairResource-my-resource-2"");
    }
",non-flaky,5
26231,Ericsson_ecchronos,TestRepairResource.testRepairResourceEquality,"    @Test
    public void testRepairResourceEquality()
    {
        RepairResource repairResource = new RepairResource(""dc1"", ""my-resource"");
        RepairResource equalRepairResource = new RepairResource(""dc1"", ""my-resource"");
        RepairResource repairResourceWithDifferentDc = new RepairResource(""dc2"", ""my-resource"");
        RepairResource repairResourceWithDifferentResource = new RepairResource(""dc1"", ""not-my-resource"");

        assertThat(repairResource).isEqualTo(equalRepairResource);
        assertThat(repairResource).isNotEqualTo(repairResourceWithDifferentDc);
        assertThat(repairResource).isNotEqualTo(repairResourceWithDifferentResource);
    }
",non-flaky,5
26232,Ericsson_ecchronos,TestRepairResource.testEqualsContract,"    @Test
    public void testEqualsContract()
    {
        EqualsVerifier.forClass(RepairResource.class).usingGetClass().verify();
    }
",non-flaky,5
26233,Ericsson_ecchronos,TestTableRepairJob.testPrevalidateNotRepairable,"    @Test
    public void testPrevalidateNotRepairable()
    {
        // mock
        doReturn(false).when(myRepairStateSnapshot).canRepair();

        assertThat(myRepairJob.runnable()).isFalse();

        verify(myRepairState, times(1)).update();
        verify(myRepairStateSnapshot, times(1)).canRepair();
    }
",non-flaky,5
26234,Ericsson_ecchronos,TestTableRepairJob.testPrevalidateNeedRepair,"    @Test
    public void testPrevalidateNeedRepair()
    {
        // mock
        doReturn(true).when(myRepairStateSnapshot).canRepair();

        assertThat(myRepairJob.runnable()).isTrue();

        verify(myRepairState, times(1)).update();
        verify(myRepairStateSnapshot, times(1)).canRepair();
    }
",non-flaky,5
26235,Ericsson_ecchronos,TestTableRepairJob.testPrevalidateNotRepairableThenRepairable,"    @Test
    public void testPrevalidateNotRepairableThenRepairable()
    {
        // mock
        doReturn(false).doReturn(true).when(myRepairStateSnapshot).canRepair();

        assertThat(myRepairJob.runnable()).isFalse();
        assertThat(myRepairJob.runnable()).isTrue();

        verify(myRepairState, times(2)).update();
        verify(myRepairStateSnapshot, times(2)).canRepair();
    }
",non-flaky,5
26236,Ericsson_ecchronos,TestTableRepairJob.testPrevalidateUpdateThrowsOverloadException,"    @Test
    public void testPrevalidateUpdateThrowsOverloadException()
    {
        // mock
        doReturn(false).when(myRepairStateSnapshot).canRepair();
        doThrow(new OverloadedException(null, ""Expected exception"")).when(myRepairState).update();

        assertThat(myRepairJob.runnable()).isFalse();

        verify(myRepairStateSnapshot, times(1)).canRepair();
    }
",non-flaky,5
26237,Ericsson_ecchronos,TestTableRepairJob.testPrevalidateUpdateThrowsException,"    @Test
    public void testPrevalidateUpdateThrowsException()
    {
        // mock
        doReturn(false).when(myRepairStateSnapshot).canRepair();
        doThrow(new RuntimeException(""Expected exception"")).when(myRepairState).update();

        assertThat(myRepairJob.runnable()).isFalse();

        verify(myRepairStateSnapshot, times(1)).canRepair();
    }
",non-flaky,5
26238,Ericsson_ecchronos,TestTableRepairJob.testPostExecuteRepaired,"    @Test
    public void testPostExecuteRepaired()
    {
        // mock
        long repairedAt = System.currentTimeMillis();
        doReturn(repairedAt).when(myRepairStateSnapshot).lastCompletedAt();
        doReturn(false).when(myRepairStateSnapshot).canRepair();

        myRepairJob.postExecute(true, null);

        assertThat(myRepairJob.getLastSuccessfulRun()).isEqualTo(repairedAt);
        verify(myRepairState, times(1)).update();
    }
",non-flaky,5
26239,Ericsson_ecchronos,TestTableRepairJob.testPostExecuteRepairedWithFailure,"    @Test
    public void testPostExecuteRepairedWithFailure()
    {
        // mock
        long repairedAt = System.currentTimeMillis();
        doReturn(repairedAt).when(myRepairStateSnapshot).lastCompletedAt();
        doReturn(false).when(myRepairStateSnapshot).canRepair();

        myRepairJob.postExecute(false, null);

        assertThat(myRepairJob.getLastSuccessfulRun()).isEqualTo(repairedAt);
        verify(myRepairState, times(1)).update();
    }
",non-flaky,5
26240,Ericsson_ecchronos,TestTableRepairJob.testPostExecuteNotRepaired,"    @Test
    public void testPostExecuteNotRepaired()
    {
        // mock
        doReturn(true).when(myRepairStateSnapshot).canRepair();

        long lastRun = myRepairJob.getLastSuccessfulRun();

        myRepairJob.postExecute(true, null);

        assertThat(myRepairJob.getLastSuccessfulRun()).isEqualTo(lastRun);
        verify(myRepairState, times(1)).update();
    }
",non-flaky,5
26241,Ericsson_ecchronos,TestTableRepairJob.testPostExecuteNotRepairedWithFailure,"    @Test
    public void testPostExecuteNotRepairedWithFailure()
    {
        // mock
        doReturn(true).when(myRepairStateSnapshot).canRepair();

        long lastRun = myRepairJob.getLastSuccessfulRun();

        myRepairJob.postExecute(false, null);

        assertThat(myRepairJob.getLastSuccessfulRun()).isEqualTo(lastRun);
        verify(myRepairState, times(1)).update();
    }
",non-flaky,5
26242,Ericsson_ecchronos,TestTableRepairJob.testPostExecuteUpdateThrowsException,"    @Test
    public void testPostExecuteUpdateThrowsException()
    {
        // mock
        doThrow(new RuntimeException(""Expected exception"")).when(myRepairState).update();

        long lastRun = myRepairJob.getLastSuccessfulRun();

        myRepairJob.postExecute(true, null);

        assertThat(myRepairJob.getLastSuccessfulRun()).isEqualTo(lastRun);
    }
",non-flaky,5
26243,Ericsson_ecchronos,TestTableRepairJob.testGetView,"    @Test
    public void testGetView()
    {
        VnodeRepairState vnodeRepairState = TestUtils.createVnodeRepairState(1, 2, ImmutableSet.of(), System.currentTimeMillis());
        VnodeRepairStatesImpl vnodeRepairStates = VnodeRepairStatesImpl.newBuilder(Arrays.asList(vnodeRepairState)).build();
        when(myRepairStateSnapshot.getVnodeRepairStates()).thenReturn(vnodeRepairStates);
        RepairJobView repairJobView = myRepairJob.getView();

        assertThat(repairJobView.getId()).isEqualTo(myTableReference.getId());
        assertThat(repairJobView.getTableReference()).isEqualTo(myTableReference);
        assertThat(repairJobView.getRepairConfiguration()).isEqualTo(myRepairConfiguration);
        assertThat(repairJobView.getRepairStateSnapshot()).isEqualTo(myRepairStateSnapshot);
        assertThat(repairJobView.getStatus()).isEqualTo(RepairJobView.Status.ERROR);
    }
",non-flaky,5
26244,Ericsson_ecchronos,TestTableRepairJob.testIterator,"    @Test
    public void testIterator()
    {
        LongTokenRange tokenRange = new LongTokenRange(0, 10);
        ImmutableSet<Node> replicas = ImmutableSet.of(mock(Node.class), mock(Node.class));
        ImmutableList<LongTokenRange> vnodes = ImmutableList.of(tokenRange);

        VnodeRepairStates vnodeRepairStates = VnodeRepairStatesImpl
                .newBuilder(ImmutableList.of(new VnodeRepairState(tokenRange, replicas, 1234L)))
                .build();
        ReplicaRepairGroup replicaRepairGroup = new ReplicaRepairGroup(replicas, vnodes);

        RepairStateSnapshot repairStateSnapshot = RepairStateSnapshot.newBuilder()
                .withReplicaRepairGroups(Collections.singletonList(replicaRepairGroup))
                .withLastCompletedAt(1234L)
                .withVnodeRepairStates(vnodeRepairStates)
                .build();
        when(myRepairState.getSnapshot()).thenReturn(repairStateSnapshot);

        Iterator<ScheduledTask> iterator = myRepairJob.iterator();

        ScheduledTask task = iterator.next();
        assertThat(task).isInstanceOf(RepairGroup.class);
        Collection<RepairTask> repairTasks = ((RepairGroup)task).getRepairTasks();

        assertThat(repairTasks).hasSize(1);
        RepairTask repairTask = repairTasks.iterator().next();
        assertThat(repairTask.getReplicas()).containsExactlyInAnyOrderElementsOf(replicas);
        assertThat(repairTask.getTokenRanges()).containsExactly(tokenRange);
        assertThat(repairTask.getRepairConfiguration()).isEqualTo(myRepairConfiguration);
        assertThat(repairTask.getTableReference()).isEqualTo(myTableReference);
    }
",non-flaky,5
26245,Ericsson_ecchronos,TestTableRepairJob.testIteratorWithTargetSize,"    @Test
    public void testIteratorWithTargetSize()
    {
        List<LongTokenRange> expectedTokenRanges = Arrays.asList(
                new LongTokenRange(0, 1),
                new LongTokenRange(1, 2),
                new LongTokenRange(2, 3),
                new LongTokenRange(3, 4),
                new LongTokenRange(4, 5),
                new LongTokenRange(5, 6),
                new LongTokenRange(6, 7),
                new LongTokenRange(7, 8),
                new LongTokenRange(8, 9),
                new LongTokenRange(9, 10)
        );

        LongTokenRange tokenRange = new LongTokenRange(0, 10);
        ImmutableSet<Node> replicas = ImmutableSet.of(mock(Node.class), mock(Node.class));
        ImmutableList<LongTokenRange> vnodes = ImmutableList.of(tokenRange);

        VnodeRepairStates vnodeRepairStates = VnodeRepairStatesImpl.newBuilder(ImmutableList.of(new VnodeRepairState(tokenRange, replicas, 1234L))).build();
        ReplicaRepairGroup replicaRepairGroup = new ReplicaRepairGroup(replicas, vnodes);

        RepairStateSnapshot repairStateSnapshot = RepairStateSnapshot.newBuilder()
                .withReplicaRepairGroups(Collections.singletonList(replicaRepairGroup))
                .withLastCompletedAt(1234L)
                .withVnodeRepairStates(vnodeRepairStates)
                .build();
        when(myRepairState.getSnapshot()).thenReturn(repairStateSnapshot);
        // 100 MB target size, 1000MB in table
        when(myTableStorageStates.getDataSize(eq(myTableReference))).thenReturn(THOUSAND_MB_IN_BYTES);

        Iterator<ScheduledTask> iterator = myRepairJob.iterator();

        ScheduledTask task = iterator.next();
        assertThat(task).isInstanceOf(RepairGroup.class);
        Collection<RepairTask> repairTasks = ((RepairGroup)task).getRepairTasks();

        assertThat(repairTasks).hasSize(expectedTokenRanges.size());

        Iterator<RepairTask> repairTaskIterator = repairTasks.iterator();
        for (LongTokenRange expectedRange : expectedTokenRanges)
        {
            assertThat(repairTaskIterator.hasNext()).isTrue();
            RepairTask repairTask = repairTaskIterator.next();
            assertThat(repairTask.getReplicas()).containsExactlyInAnyOrderElementsOf(replicas);
            assertThat(repairTask.getRepairConfiguration()).isEqualTo(myRepairConfiguration);
            assertThat(repairTask.getTableReference()).isEqualTo(myTableReference);

            assertThat(repairTask.getTokenRanges()).containsExactly(expectedRange);
        }
    }
",non-flaky,5
26246,Ericsson_ecchronos,TestTableRepairJob.testStatusCompleted,"    @Test
    public void testStatusCompleted()
    {
        long repairedAt = System.currentTimeMillis();
        doReturn(repairedAt).when(myRepairStateSnapshot).lastCompletedAt();
        VnodeRepairState vnodeRepairState = TestUtils.createVnodeRepairState(1, 2, ImmutableSet.of(), repairedAt);
        VnodeRepairStatesImpl vnodeRepairStates = VnodeRepairStatesImpl.newBuilder(Arrays.asList(vnodeRepairState)).build();
        when(myRepairStateSnapshot.getVnodeRepairStates()).thenReturn(vnodeRepairStates);

        assertThat(myRepairJob.getView().getStatus()).isEqualTo(RepairJobView.Status.COMPLETED);
    }
",non-flaky,5
26247,Ericsson_ecchronos,TestTableRepairJob.testStatusError,"    @Test
    public void testStatusError()
    {
        long repairedAt = System.currentTimeMillis() - TimeUnit.DAYS.toMillis(10);
        VnodeRepairState vnodeRepairState = TestUtils.createVnodeRepairState(1, 2, ImmutableSet.of(), repairedAt);
        VnodeRepairStatesImpl vnodeRepairStates = VnodeRepairStatesImpl.newBuilder(Arrays.asList(vnodeRepairState)).build();
        when(myRepairStateSnapshot.getVnodeRepairStates()).thenReturn(vnodeRepairStates);
        doReturn(repairedAt).when(myRepairStateSnapshot).lastCompletedAt();

        assertThat(myRepairJob.getView().getStatus()).isEqualTo(RepairJobView.Status.ERROR);
    }
",non-flaky,5
26248,Ericsson_ecchronos,TestTableRepairJob.testStatusInQueue,"    @Test
    public void testStatusInQueue()
    {
        long repairedAt = System.currentTimeMillis() - TimeUnit.DAYS.toMillis(1);
        VnodeRepairState vnodeRepairState = TestUtils.createVnodeRepairState(1, 2, ImmutableSet.of(), repairedAt);
        VnodeRepairStatesImpl vnodeRepairStates = VnodeRepairStatesImpl.newBuilder(Arrays.asList(vnodeRepairState)).build();
        when(myRepairStateSnapshot.getVnodeRepairStates()).thenReturn(vnodeRepairStates);
        doReturn(repairedAt).when(myRepairStateSnapshot).lastCompletedAt();

        assertThat(myRepairJob.getView().getStatus()).isEqualTo(RepairJobView.Status.IN_QUEUE);
    }
",non-flaky,5
26249,Ericsson_ecchronos,TestTableRepairJob.testStatusWarning,"    @Test
    public void testStatusWarning()
    {
        long repairedAt = System.currentTimeMillis() - TimeUnit.DAYS.toMillis(7);
        VnodeRepairState vnodeRepairState = TestUtils.createVnodeRepairState(1, 2, ImmutableSet.of(), repairedAt);
        VnodeRepairStatesImpl vnodeRepairStates = VnodeRepairStatesImpl.newBuilder(Arrays.asList(vnodeRepairState)).build();
        when(myRepairStateSnapshot.getVnodeRepairStates()).thenReturn(vnodeRepairStates);
        doReturn(repairedAt).when(myRepairStateSnapshot).lastCompletedAt();

        assertThat(myRepairJob.getView().getStatus()).isEqualTo(RepairJobView.Status.WARNING);
    }
",non-flaky,5
35660,cdapio_cdap,Spark2Test.testSpark2Service,"  @Test
  public void testSpark2Service() throws Exception {
    ApplicationManager applicationManager = deploy(NamespaceId.DEFAULT, Spark2TestApp.class);
    SparkManager manager = applicationManager.getSparkManager(ScalaSparkServiceProgram.class.getSimpleName()).start();

    URL url = manager.getServiceURL(5, TimeUnit.MINUTES);
    Assert.assertNotNull(url);

    // GET request to sum n numbers.
    URL sumURL = url.toURI().resolve(""sum?n="" + Joiner.on(""&n="").join(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)).toURL();
    HttpURLConnection urlConn = (HttpURLConnection) sumURL.openConnection();
    Assert.assertEquals(HttpURLConnection.HTTP_OK, urlConn.getResponseCode());
    try (InputStream is = urlConn.getInputStream()) {
      Assert.assertEquals(55, Integer.parseInt(new String(ByteStreams.toByteArray(is), StandardCharsets.UTF_8)));
    }
  }
",non-flaky,5
35661,cdapio_cdap,Spark2Test.call,"  @Test
  public void testSparkWithObjectStore() throws Exception {
    ApplicationManager applicationManager = deploy(NamespaceId.DEFAULT, SparkAppUsingObjectStore.class);

    DataSetManager<ObjectStore<String>> keysManager = getDataset(""keys"");
    prepareInputData(keysManager);

    SparkManager sparkManager = applicationManager.getSparkManager(CharCountProgram.class.getSimpleName()).start();
    sparkManager.waitForRun(ProgramRunStatus.RUNNING, 10, TimeUnit.SECONDS);
    sparkManager.waitForStopped(60, TimeUnit.SECONDS);

    DataSetManager<KeyValueTable> countManager = getDataset(""count"");
    checkOutputData(countManager);

    // validate that the table emitted metrics
    // one read + one write in beforeSubmit(), increment (= read + write) in main -> 4
    Tasks.waitFor(4L, new Callable<Long>() {
      @Override
      public Long call() throws Exception {
        Collection<MetricTimeSeries> metrics =
          getMetricsManager().query(new MetricDataQuery(
            0,
            System.currentTimeMillis() / 1000L,
            Integer.MAX_VALUE,
            ""system."" + Constants.Metrics.Name.Dataset.OP_COUNT,
            AggregationFunction.SUM,
            ImmutableMap.of(Constants.Metrics.Tag.NAMESPACE, DefaultId.NAMESPACE.getNamespace(),
                            Constants.Metrics.Tag.APP, SparkAppUsingObjectStore.class.getSimpleName(),
                            Constants.Metrics.Tag.SPARK, CharCountProgram.class.getSimpleName(),
                            Constants.Metrics.Tag.DATASET, ""totals""),
            Collections.<String>emptyList()));
        if (metrics.isEmpty()) {
          return 0L;
        }
        Assert.assertEquals(1, metrics.size());
        MetricTimeSeries ts = metrics.iterator().next();
        Assert.assertEquals(1, ts.getTimeValues().size());
        return ts.getTimeValues().get(0).getValue();
      }
",non-flaky,5
35662,cdapio_cdap,Spark2Test.testScalaSparkWithObjectStore,"  @Test
  public void testScalaSparkWithObjectStore() throws Exception {
    ApplicationManager applicationManager = deploy(NamespaceId.DEFAULT, SparkAppUsingObjectStore.class);

    DataSetManager<ObjectStore<String>> keysManager = getDataset(""keys"");
    prepareInputData(keysManager);

    SparkManager sparkManager = applicationManager.getSparkManager(ScalaCharCountProgram.class.getSimpleName()).start();
    sparkManager.waitForRun(ProgramRunStatus.RUNNING, 10, TimeUnit.SECONDS);
    sparkManager.waitForStopped(60, TimeUnit.SECONDS);

    DataSetManager<KeyValueTable> countManager = getDataset(""count"");
    checkOutputData(countManager);
  }
",non-flaky,5
35663,cdapio_cdap,Spark2Test.testScalaSparkCrossNSStream,"  @Test
  public void testScalaSparkCrossNSStream() throws Exception {
    // create a namespace for input and create a file set instance
    NamespaceMeta inputNSMeta = new NamespaceMeta.Builder().setName(""inputSpaceForSpark"").build();
    getNamespaceAdmin().create(inputNSMeta);
    DatasetId inputDatasetId = inputNSMeta.getNamespaceId().dataset(""input"");
    addDatasetInstance(FileSet.class.getName(), inputDatasetId,
                       FileSetProperties.builder().setInputFormat(TextInputFormat.class).build());

    // create a namespace for dataset and add the dataset instance in it
    NamespaceMeta outputNSMeta = new NamespaceMeta.Builder().setName(""crossNSDataset"").build();
    getNamespaceAdmin().create(outputNSMeta);
    addDatasetInstance(outputNSMeta.getNamespaceId().dataset(""count""), ""keyValueTable"");

    // write something to the input dataset
    Location inputFile = this.<FileSet>getDataset(inputDatasetId).get().getLocation(""inputFile"");
    try (PrintStream printer = new PrintStream(inputFile.getOutputStream(), true, ""UTF-8"")) {
      for (int i = 0; i < 50; i++) {
        printer.println(String.valueOf(i));
      }
    }

    // deploy the spark app in another namespace (default)
    ApplicationManager applicationManager = deploy(NamespaceId.DEFAULT, SparkAppUsingObjectStore.class);

    Map<String, String> args = new HashMap<>();
    args.put(ScalaCrossNSProgram.INPUT_NAMESPACE(), inputNSMeta.getNamespaceId().getNamespace());
    args.put(ScalaCrossNSProgram.OUTPUT_NAMESPACE(), outputNSMeta.getNamespaceId().getNamespace());
    args.put(ScalaCrossNSProgram.OUTPUT_NAME(), ""count"");

    FileSetArguments.setInputPath(args, ""inputFile"");

    SparkManager sparkManager =
      applicationManager.getSparkManager(ScalaCrossNSProgram.class.getSimpleName()).start(args);
    sparkManager.waitForRun(ProgramRunStatus.RUNNING, 10, TimeUnit.SECONDS);
    sparkManager.waitForStopped(60, TimeUnit.SECONDS);

    // get the dataset from the other namespace where we expect it to exist and compare the data
    DataSetManager<KeyValueTable> countManager = getDataset(outputNSMeta.getNamespaceId().dataset(""count""));
    KeyValueTable results = countManager.get();
    for (int i = 0; i < 50; i++) {
      byte[] key = String.valueOf(i).getBytes(Charsets.UTF_8);
      Assert.assertArrayEquals(key, results.read(key));
    }
  }
",non-flaky,5
35664,cdapio_cdap,Spark2Test.testScalaSparkCrossNSDataset,"  @Test
  public void testScalaSparkCrossNSDataset() throws Exception {
    // Deploy and create a dataset in namespace datasetSpaceForSpark
    NamespaceMeta inputDSNSMeta = new NamespaceMeta.Builder().setName(""datasetSpaceForSpark"").build();
    getNamespaceAdmin().create(inputDSNSMeta);
    deploy(inputDSNSMeta.getNamespaceId(), SparkAppUsingObjectStore.class);
    DataSetManager<ObjectStore<String>> keysManager = getDataset(inputDSNSMeta.getNamespaceId().dataset(""keys""));
    prepareInputData(keysManager);

    Map<String, String> args = ImmutableMap.of(ScalaCharCountProgram.INPUT_DATASET_NAMESPACE(),
                                               inputDSNSMeta.getNamespaceId().getNamespace(),
                                               ScalaCharCountProgram.INPUT_DATASET_NAME(), ""keys"");

    ApplicationManager applicationManager = deploy(NamespaceId.DEFAULT, SparkAppUsingObjectStore.class);
    SparkManager sparkManager =
      applicationManager.getSparkManager(ScalaCharCountProgram.class.getSimpleName()).start(args);
    sparkManager.waitForRun(ProgramRunStatus.RUNNING, 10, TimeUnit.SECONDS);
    sparkManager.waitForStopped(60, TimeUnit.SECONDS);

    DataSetManager<KeyValueTable> countManager = getDataset(""count"");
    checkOutputData(countManager);
  }
",non-flaky,5
35665,cdapio_cdap,Spark2Test.testSparkWithLocalFiles,"  @Test
  public void testSparkWithLocalFiles() throws Exception {
    testSparkWithLocalFiles(SparkAppUsingLocalFiles.class,
                            SparkAppUsingLocalFiles.JavaSparkUsingLocalFiles.class.getSimpleName(), ""java"");
    testSparkWithLocalFiles(SparkAppUsingLocalFiles.class,
                            SparkAppUsingLocalFiles.ScalaSparkUsingLocalFiles.class.getSimpleName(), ""scala"");
  }
",non-flaky,5
35666,cdapio_cdap,Spark2Test.testPySpark,"  @Test
  public void testPySpark() throws Exception {
    if (TestBase.getCurrentSparkCompat() == SparkCompat.SPARK3_2_12) {
      //For spark 3 we need python 3, otherwise skip test
      try {
        Process python = new ProcessBuilder(""python3"", ""-V"").start();
        int resultCode = python.waitFor();
        assumeTrue(""Python3 returned error, result code: "" + resultCode,
                   resultCode == 0);
      } catch (IOException e) {
        assumeNoException(""Python3 can't be started"", e);
      }
    }
    ApplicationManager appManager = deploy(NamespaceId.DEFAULT, Spark2TestApp.class);

    // Write some data to a local file
    File inputFile = TEMP_FOLDER.newFile();
    try (BufferedWriter writer = Files.newBufferedWriter(inputFile.toPath(), StandardCharsets.UTF_8)) {
      for (int i = 0; i < 100; i++) {
        writer.write(""Event "" + i);
        writer.newLine();
      }
    }

    File outputDir = new File(TMP_FOLDER.newFolder(), ""output"");
    appManager.getSparkManager(PythonSpark2.class.getSimpleName()).startAndWaitForGoodRun(
      ImmutableMap.of(""input.file"", inputFile.getAbsolutePath(),
                      ""output.path"", outputDir.getAbsolutePath()),
      ProgramRunStatus.COMPLETED, 2, TimeUnit.MINUTES);

    // Verify the result
    File resultFile = DirUtils.listFiles(outputDir).stream()
      .filter(f -> !f.getName().endsWith("".crc""))
      .filter(f -> !f.getName().startsWith(""_SUCCESS""))
      .findFirst()
      .orElse(null);
    Assert.assertNotNull(resultFile);

    List<String> lines = Files.readAllLines(resultFile.toPath(), StandardCharsets.UTF_8);
    Assert.assertFalse(lines.isEmpty());

    // Expected only even number
    int count = 0;
    for (String line : lines) {
      line = line.trim();
      if (!line.isEmpty()) {
        Assert.assertEquals(""Event "" + count, line);
        count += 2;
      }
    }

    Assert.assertEquals(100, count);

    final Map<String, String> tags = ImmutableMap.of(
      Constants.Metrics.Tag.NAMESPACE, NamespaceId.DEFAULT.getNamespace(),
      Constants.Metrics.Tag.APP, Spark2TestApp.class.getSimpleName(),
      Constants.Metrics.Tag.SPARK, PythonSpark2.class.getSimpleName());

    Tasks.waitFor(100L, () ->  getMetricsManager().getTotalMetric(tags, ""user.body""),
                  5, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);
  }
",non-flaky,5
35667,cdapio_cdap,IntegrationTestBaseTest.testDeployApplicationInNamespace,"  @Test
  public void testDeployApplicationInNamespace() throws Exception {
    NamespaceId namespace = new NamespaceId(""Test1"");
    NamespaceMeta namespaceMeta = new NamespaceMeta.Builder().setName(namespace).build();
    getNamespaceClient().create(namespaceMeta);
    ClientConfig clientConfig = new ClientConfig.Builder(getClientConfig()).build();
    deployApplication(namespace, AllProgramsApp.class);

    // Check the default namespaces applications to see whether the application wasn't made in the default namespace
    ClientConfig defaultClientConfig = new ClientConfig.Builder(getClientConfig()).build();
    Assert.assertTrue(new ApplicationClient(defaultClientConfig).list(NamespaceId.DEFAULT).isEmpty());

    ApplicationClient applicationClient = new ApplicationClient(clientConfig);
    Assert.assertEquals(AllProgramsApp.NAME, applicationClient.list(namespace).get(0).getName());
    applicationClient.delete(namespace.app(AllProgramsApp.NAME));
    Assert.assertTrue(new ApplicationClient(clientConfig).list(namespace).isEmpty());

  }
",non-flaky,5
35668,cdapio_cdap,IntegrationTestBaseTest.testSQLQuery,"  @Test
  public void testSQLQuery() throws Exception {
    getTestManager().deployDatasetModule(NamespaceId.DEFAULT.datasetModule(""my-kv""), AppUsingCustomModule.Module.class);

    DatasetAdmin dsAdmin = getTestManager().addDatasetInstance(""myKeyValueTable"",
                                                               NamespaceId.DEFAULT.dataset(""myTable""));
    Assert.assertTrue(dsAdmin.exists());

    ApplicationManager appManager = deployApplication(NamespaceId.DEFAULT, AppUsingCustomModule.class);
    ServiceManager serviceManager = appManager.getServiceManager(""MyService"").start();
    serviceManager.waitForRun(ProgramRunStatus.RUNNING, 10, TimeUnit.SECONDS);

    put(serviceManager, ""a"", ""1"");
    put(serviceManager, ""b"", ""2"");
    put(serviceManager, ""c"", ""1"");

    try (
      Connection connection = getTestManager().getQueryClient(NamespaceId.DEFAULT);
      // the value (character) ""1"" corresponds to the decimal 49. In hex, that is 31.
      ResultSet results = connection.prepareStatement(""select key from dataset_mytable where hex(value) = '31'"")
        .executeQuery()
    ) {
      // run a query over the dataset
      Assert.assertTrue(results.next());
      Assert.assertEquals(""a"", results.getString(1));
      Assert.assertTrue(results.next());
      Assert.assertEquals(""c"", results.getString(1));
      Assert.assertFalse(results.next());
    }

    dsAdmin.drop();
    Assert.assertFalse(dsAdmin.exists());
  }
",non-flaky,5
35669,cdapio_cdap,LogBufferWriterTest.testLogBufferWriter,"  @Test
  public void testLogBufferWriter() throws Exception {
    String absolutePath = TMP_FOLDER.newFolder().getAbsolutePath();

    LogBufferWriter writer = new LogBufferWriter(absolutePath, 100000, () -> { });
    ImmutableList<byte[]> events = getLoggingEvents();
    Iterator<LogBufferEvent> writtenEvents = writer.write(events.iterator()).iterator();
    writer.close();

    int i = 0;
    int startPos = 0;
    // verify if correct offsets were set without file rotation
    while (writtenEvents.hasNext()) {
      LogBufferEvent bufferEvent = writtenEvents.next();
      Assert.assertEquals(String.valueOf(i++), bufferEvent.getLogEvent().getMessage());
      // There will not be any rotation.
      Assert.assertEquals(bufferEvent.getOffset().getFilePos(), startPos);
      startPos = startPos + Bytes.SIZEOF_INT + serializer.toBytes(bufferEvent.getLogEvent()).length;
    }

    // verify if the events were serialized and written correctly
    try (DataInputStream dis = new DataInputStream(new FileInputStream(absolutePath + ""/0.buf""))) {
      for (byte[] eventBytes : events) {
        ILoggingEvent event = serializer.fromBytes(ByteBuffer.wrap(eventBytes));
        Assert.assertEquals(event.getMessage(), getEvent(dis, serializer.toBytes(event).length).getMessage());
      }
    }
  }
",non-flaky,5
35670,cdapio_cdap,LogBufferWriterTest.testFileRotation,"  @Test
  public void testFileRotation() throws Exception {
    // Make sure rotation happens after every event is written
    LogBufferWriter writer = new LogBufferWriter(TMP_FOLDER.newFolder().getAbsolutePath(), 10, () -> { });
    ImmutableList<byte[]> events = getLoggingEvents();
    Iterator<LogBufferEvent> writtenEvents = writer.write(events.iterator()).iterator();
    writer.close();

    int i = 0;
    // verify if correct offsets and file id were set with file rotation
    while (writtenEvents.hasNext()) {
      LogBufferEvent bufferEvent = writtenEvents.next();
      Assert.assertEquals(bufferEvent.getLogEvent().getMessage(), String.valueOf(i));
      // There will be 2 events in one file.
      Assert.assertEquals(i++ + "".buf"", bufferEvent.getOffset().getFileId() + "".buf"");
      Assert.assertEquals(0, bufferEvent.getOffset().getFilePos());
    }
  }
",non-flaky,5
35671,cdapio_cdap,LogBufferWriterTest.testWritesOnClosedWriter,"  @Test (expected = IOException.class)
  public void testWritesOnClosedWriter() throws IOException {
    LogBufferWriter writer = new LogBufferWriter(TMP_FOLDER.newFolder().getAbsolutePath(), 100000, () -> { });
    writer.close();
    // should throw IOException
    writer.write(ImmutableList.of(
      serializer.toBytes(createLoggingEvent(""test.logger"", Level.INFO, ""0"", 1,
                                            new WorkerLoggingContext(""default"", ""app1"", ""worker1"", ""run1"",
                                                                     ""instance1"")))).iterator());
  }
",non-flaky,5
35672,cdapio_cdap,LogBufferRecoveryServiceTest.testLogBufferRecoveryService,"  @Test
  public void testLogBufferRecoveryService() throws Exception {
    String absolutePath = TMP_FOLDER.newFolder().getAbsolutePath();

    // create and start pipeline
    LoggerContext loggerContext = LogPipelineTestUtil.createLoggerContext(""WARN"",
                                                                          ImmutableMap.of(""test.logger"", ""INFO""),
                                                                          MockAppender.class.getName());
    final MockAppender appender = LogPipelineTestUtil.getAppender(loggerContext.getLogger(Logger.ROOT_LOGGER_NAME),
                                                                  ""Test"", MockAppender.class);
    MockCheckpointManager checkpointManager = new MockCheckpointManager();
    LogBufferPipelineConfig config = new LogBufferPipelineConfig(1024L, 300L, 500L, 4);
    loggerContext.start();
    LogBufferProcessorPipeline pipeline = new LogBufferProcessorPipeline(
      new LogProcessorPipelineContext(CConfiguration.create(), ""test"", loggerContext, NO_OP_METRICS_CONTEXT, 0),
      config, checkpointManager, 0);

    // start the pipeline
    pipeline.startAndWait();

    // write directly to log buffer
    LogBufferWriter writer = new LogBufferWriter(absolutePath, 250, () -> { });
    ImmutableList<byte[]> events = getLoggingEvents();
    writer.write(events.iterator()).iterator();
    writer.close();

    // start log buffer reader to read log events from files. keep the batch size as 2 so that there are more than 1
    // iterations
    LogBufferRecoveryService service = new LogBufferRecoveryService(ImmutableList.of(pipeline),
                                                                    ImmutableList.of(checkpointManager),
                                                                    absolutePath, 2, new AtomicBoolean(true));
    service.startAndWait();

    Tasks.waitFor(5, () -> appender.getEvents().size(), 120, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);

    service.stopAndWait();
    pipeline.stopAndWait();
    loggerContext.stop();
  }
",non-flaky,5
35673,cdapio_cdap,ConcurrentLogBufferWriterTest.testWrites,"  @Test
  public void testWrites() throws Exception {
    CConfiguration cConf = CConfiguration.create();
    String absolutePath = TMP_FOLDER.newFolder().getAbsolutePath();
    cConf.set(Constants.LogBuffer.LOG_BUFFER_BASE_DIR, absolutePath);
    cConf.setLong(Constants.LogBuffer.LOG_BUFFER_MAX_FILE_SIZE_BYTES, 100000);

    LoggerContext loggerContext = LogPipelineTestUtil
      .createLoggerContext(""WARN"", ImmutableMap.of(""test.logger"", ""INFO""), MockAppender.class.getName());
    final MockAppender appender =
      LogPipelineTestUtil.getAppender(loggerContext.getLogger(ch.qos.logback.classic.Logger.ROOT_LOGGER_NAME),
                                      ""Test"", MockAppender.class);
    MockCheckpointManager checkpointManager = new MockCheckpointManager();
    LogBufferPipelineConfig config = new LogBufferPipelineConfig(1024L, 300L, 500L, 4);
    loggerContext.start();
    LogBufferProcessorPipeline pipeline = new LogBufferProcessorPipeline(
      new LogProcessorPipelineContext(CConfiguration.create(), ""test"", loggerContext, NO_OP_METRICS_CONTEXT, 0),
      config, checkpointManager, 0);
    // start the pipeline
    pipeline.startAndWait();

    ConcurrentLogBufferWriter writer = new ConcurrentLogBufferWriter(cConf, ImmutableList.of(pipeline), () -> { });
    ImmutableList<byte[]> events = getLoggingEvents();
    writer.process(new LogBufferRequest(0, events));

    // verify if the events were written to log buffer
    try (DataInputStream dis = new DataInputStream(new FileInputStream(absolutePath + ""/0.buf""))) {
      for (byte[] eventBytes : events) {
        ILoggingEvent event = serializer.fromBytes(ByteBuffer.wrap(eventBytes));
        Assert.assertEquals(event.getMessage(), getEvent(dis, serializer.toBytes(event).length).getMessage());
      }
    }

    // verify if the pipeline has processed the messages.
    Tasks.waitFor(5, () -> appender.getEvents().size(), 60, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);
    pipeline.stopAndWait();
    loggerContext.stop();
  }
",non-flaky,5
35674,cdapio_cdap,ConcurrentLogBufferWriterTest.testConcurrentWrites,"  @Test
  public void testConcurrentWrites() throws Exception {
    int threadCount = 20;

    CConfiguration cConf = CConfiguration.create();
    String absolutePath = TMP_FOLDER.newFolder().getAbsolutePath();
    cConf.set(Constants.LogBuffer.LOG_BUFFER_BASE_DIR, absolutePath);
    cConf.setLong(Constants.LogBuffer.LOG_BUFFER_MAX_FILE_SIZE_BYTES, 100000);

    LoggerContext loggerContext = LogPipelineTestUtil
      .createLoggerContext(""WARN"", ImmutableMap.of(""test.logger"", ""INFO""), MockAppender.class.getName());
    final MockAppender appender =
      LogPipelineTestUtil.getAppender(loggerContext.getLogger(ch.qos.logback.classic.Logger.ROOT_LOGGER_NAME),
                                      ""Test"", MockAppender.class);
    MockCheckpointManager checkpointManager = new MockCheckpointManager();
    LogBufferPipelineConfig config = new LogBufferPipelineConfig(1024L, 300L, 500L, 4);
    loggerContext.start();
    LogBufferProcessorPipeline pipeline = new LogBufferProcessorPipeline(
      new LogProcessorPipelineContext(CConfiguration.create(), ""test"", loggerContext, NO_OP_METRICS_CONTEXT, 0),
      config, checkpointManager, 0);
    // start the pipeline
    pipeline.startAndWait();

    ConcurrentLogBufferWriter writer = new ConcurrentLogBufferWriter(cConf, ImmutableList.of(pipeline), () -> { });
    ImmutableList<byte[]> events = getLoggingEvents();

    ExecutorService executor = Executors.newFixedThreadPool(threadCount);
    final CyclicBarrier barrier = new CyclicBarrier(threadCount + 1);
    for (int i = 0; i < threadCount; i++) {
      executor.submit(() -> {
        try {
          barrier.await();
          writer.process(new LogBufferRequest(0, events));
        } catch (Exception e) {
          LOG.error(""Exception raised when processing log events."", e);
        }
      });
    }

    barrier.await();
    executor.shutdown();
    Assert.assertTrue(executor.awaitTermination(1, TimeUnit.MINUTES));

    // verify if the events were written to log buffer
    try (DataInputStream dis = new DataInputStream(new FileInputStream(absolutePath + ""/0.buf""))) {
      for (int i = 0; i < threadCount; i++) {
        for (byte[] eventBytes : events) {
          ILoggingEvent event = serializer.fromBytes(ByteBuffer.wrap(eventBytes));
          Assert.assertEquals(event.getMessage(), getEvent(dis, serializer.toBytes(event).length).getMessage());
        }
      }
    }

    // verify if the pipeline has processed the messages.
    Tasks.waitFor(100, () -> appender.getEvents().size(), 60, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);
    pipeline.stopAndWait();
    loggerContext.stop();
  }
",non-flaky,5
35675,cdapio_cdap,LogBufferHandlerTest.testHandler,"  @Test
  public void testHandler() throws Exception {
    CConfiguration cConf = CConfiguration.create();
    String absolutePath = TMP_FOLDER.newFolder().getAbsolutePath();
    cConf.set(Constants.LogBuffer.LOG_BUFFER_BASE_DIR, absolutePath);
    cConf.setLong(Constants.LogBuffer.LOG_BUFFER_MAX_FILE_SIZE_BYTES, 100000);

    LoggerContext loggerContext = LogPipelineTestUtil
      .createLoggerContext(""WARN"", ImmutableMap.of(""test.logger"", ""INFO""), MockAppender.class.getName());
    final MockAppender appender =
      LogPipelineTestUtil.getAppender(loggerContext.getLogger(ch.qos.logback.classic.Logger.ROOT_LOGGER_NAME),
                                      ""Test"", MockAppender.class);

    LogBufferProcessorPipeline pipeline = getLogPipeline(loggerContext);
    pipeline.startAndWait();

    ConcurrentLogBufferWriter writer = new ConcurrentLogBufferWriter(cConf, ImmutableList.of(pipeline), () -> { });

    NettyHttpService httpService = NettyHttpService.builder(""RemoteAppenderTest"")
      .setHttpHandlers(new LogBufferHandler(writer))
      .setExceptionHandler(new HttpExceptionHandler())
      .build();

    httpService.start();

    RemoteLogAppender remoteLogAppender = getRemoteAppender(cConf, httpService);
    remoteLogAppender.start();

    List<ILoggingEvent> events = getLoggingEvents();
    WorkerLoggingContext loggingContext =
      new WorkerLoggingContext(""default"", ""app1"", ""worker1"", ""run1"", ""instance1"");
    for (int i = 0; i < 1000; i++) {
      remoteLogAppender.append(new LogMessage(events.get(i % events.size()), loggingContext));
    }

    Tasks.waitFor(1000, () -> appender.getEvents().size(), 120, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);

    remoteLogAppender.stop();
    httpService.stop();
    pipeline.stopAndWait();
    loggerContext.stop();
  }
",non-flaky,5
35676,cdapio_cdap,LogBufferCleanerTest.testLogBufferCleanerService,"  @Test
  public void testLogBufferCleanerService() throws Exception {
    String absolutePath = TMP_FOLDER.newFolder().getAbsolutePath();

    MockCheckpointManager checkpointManager = new MockCheckpointManager();
    // update checkpoints
    checkpointManager.saveCheckpoints(ImmutableMap.of(0, new TestCheckpoint(2L, 0L, 1L)));

    // write directly to log buffer, keep file size 10 bytes so that more files are created
    LogBufferWriter writer = new LogBufferWriter(absolutePath, 10,
                                                 new LogBufferCleaner(ImmutableList.of(checkpointManager),
                                                                      absolutePath, new AtomicBoolean(true)));
    ImmutableList<byte[]> events = getLoggingEvents();
    List<byte[]> subset = new ArrayList<>();
    subset.add(events.get(0));
    subset.add(events.get(1));
    subset.add(events.get(2));
    writer.write(subset.iterator()).iterator();

    // should delete file 0 an1
    File file0 = new File(absolutePath, ""0.buff"");
    File file1 = new File(absolutePath, ""1.buff"");
    Tasks.waitFor(true, () -> !file0.exists() && !file1.exists(), 120, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);

    // update checkpoints
    checkpointManager.saveCheckpoints(ImmutableMap.of(0, new TestCheckpoint(5L, 0L, 1L)));

    subset.add(events.get(3));
    subset.add(events.get(4));
    subset.add(events.get(5));
    writer.write(subset.iterator()).iterator();
    writer.close();

    // should delete file 2, 3 and 4
    File file2 = new File(absolutePath, ""2.buff"");
    File file3 = new File(absolutePath, ""3.buff"");
    File file4 = new File(absolutePath, ""4.buff"");
    Tasks.waitFor(true, () -> !file2.exists() && !file3.exists() && !file4.exists(), 120, TimeUnit.SECONDS,
                  100, TimeUnit.MILLISECONDS);

  }
",non-flaky,5
35677,cdapio_cdap,LogBufferReaderTest.testLogReader,"  @Test
  public void testLogReader() throws Exception {
    String absolutePath = TMP_FOLDER.newFolder().getAbsolutePath();

    LogBufferWriter writer = new LogBufferWriter(absolutePath, 250, () -> { });
    ImmutableList<byte[]> events = getLoggingEvents();
    Iterable<LogBufferEvent> writtenEvents = writer.write(events.iterator());
    writer.close();

    List<LogBufferEvent> logBufferEvents = new LinkedList<>();
    // read from start positions, tests case where no checkpoints are persisted
    LogBufferReader reader = new LogBufferReader(absolutePath, 2, 3, -1, -1);
    Iterator<LogBufferEvent> iterator = writtenEvents.iterator();
    verifyEvents(logBufferEvents, reader, iterator);
    reader.close();

    // this should skip first and second event, this is because log buffer offsets are offset for event that is
    // already stored. so in this case, skip first event and skip second event as second event is the last stored event
    reader = new LogBufferReader(absolutePath, 2, 3, 0, 145);
    iterator = writtenEvents.iterator();
    iterator.next();
    iterator.next();
    verifyEvents(logBufferEvents, reader, iterator);
    reader.close();
  }
",non-flaky,5
35678,cdapio_cdap,LogBufferProcessorPipelineTest.testSingleAppender,"  @Test
  public void testSingleAppender() throws Exception {
    LoggerContext loggerContext = LogPipelineTestUtil.createLoggerContext(""WARN"",
                                                                          ImmutableMap.of(""test.logger"", ""INFO""),
                                                                          MockAppender.class.getName());
    final MockAppender appender = LogPipelineTestUtil.getAppender(loggerContext.getLogger(Logger.ROOT_LOGGER_NAME),
                                                                  ""Test"", MockAppender.class);
    MockCheckpointManager checkpointManager = new MockCheckpointManager();
    LogBufferPipelineConfig config = new LogBufferPipelineConfig(1024L, 300L, 500L, 4);
    loggerContext.start();
    LogBufferProcessorPipeline pipeline = new LogBufferProcessorPipeline(
      new LogProcessorPipelineContext(CConfiguration.create(), ""test"", loggerContext, NO_OP_METRICS_CONTEXT, 0),
      config, checkpointManager, 0);
    // start the pipeline
    pipeline.startAndWait();

    // start thread to write to incomingEventQueue
    List<ILoggingEvent> events = getLoggingEvents();
    AtomicInteger i = new AtomicInteger(0);
    List<LogBufferEvent> bufferEvents = events.stream().map(event -> {
      LogBufferEvent lbe = new LogBufferEvent(event, serializer.toBytes(event).length,
                                              new LogBufferFileOffset(0, i.get()));
      i.incrementAndGet();
      return lbe;
    }).collect(Collectors.toList());

    // start a thread to send log buffer events to pipeline
    ExecutorService executorService = Executors.newSingleThreadExecutor();
    executorService.execute(() -> {
      for (int count = 0; count < 40; count++) {
        pipeline.processLogEvents(bufferEvents.iterator());
        try {
          Thread.sleep(100);
        } catch (InterruptedException e) {
          // should not happen
        }
      }
    });

    // wait for pipeline to append all the logs to appender. The DEBUG message should get filtered out.
    Tasks.waitFor(200, () -> appender.getEvents().size(), 60, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);
    executorService.shutdown();
    pipeline.stopAndWait();
    loggerContext.stop();
  }
",non-flaky,5
35679,cdapio_cdap,KafkaLogProcessorPipelineTest.testBasicSort,"  @Test
  public void testBasicSort() throws Exception {
    String topic = ""testPipeline"";
    LoggerContext loggerContext = LogPipelineTestUtil.createLoggerContext(""WARN"",
                                                                          ImmutableMap.of(""test.logger"", ""INFO""),
                                                                          MockAppender.class.getName());
    final MockAppender appender = LogPipelineTestUtil.getAppender(loggerContext.getLogger(Logger.ROOT_LOGGER_NAME),
                                                                  ""Test"", MockAppender.class);
    TestCheckpointManager checkpointManager = new TestCheckpointManager();
    KafkaPipelineConfig config = new KafkaPipelineConfig(topic, Collections.singleton(0), 1024L, 300L, 1048576, 500L);
    KAFKA_TESTER.createTopic(topic, 1);

    loggerContext.start();
    KafkaLogProcessorPipeline pipeline = new KafkaLogProcessorPipeline(
      new LogProcessorPipelineContext(CConfiguration.create(), ""test"", loggerContext, NO_OP_METRICS_CONTEXT, 0),
      checkpointManager,
      KAFKA_TESTER.getBrokerService(), config);

    pipeline.startAndWait();

    // Publish some log messages to Kafka
    long now = System.currentTimeMillis();
    publishLog(topic, ImmutableList.of(
      LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""0"", now - 1000),
      LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""2"", now - 700),
      LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""3"", now - 500),
      LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""1"", now - 900),
      LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.DEBUG, ""hidden"", now - 600),
      LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""4"", now - 100))
    );

    // Since the messages are published in one batch, the processor should be able to fetch all of them,
    // hence the sorting order should be deterministic.
    // The DEBUG message should get filtered out
    Tasks.waitFor(5, () -> appender.getEvents().size(), 5, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);

    for (int i = 0; i < 5; i++) {
      Assert.assertEquals(Integer.toString(i), appender.getEvents().poll().getMessage());
    }

    // Now publish large messages that exceed the maximum queue size (1024). It should trigger writing regardless of
    // the event timestamp
    List<ILoggingEvent> events = new ArrayList<>(500);
    now = System.currentTimeMillis();
    for (int i = 0; i < 500; i++) {
      // The event timestamp is 10 seconds in future.
      events.add(LogPipelineTestUtil.createLoggingEvent(""test.large.logger"",
                                                        Level.WARN, ""Large logger "" + i, now + 10000));
    }
    publishLog(topic, events);

    Tasks.waitFor(true, () -> !appender.getEvents().isEmpty(), 5, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);

    events.clear();
    events.addAll(appender.getEvents());

    for (int i = 0; i < events.size(); i++) {
      Assert.assertEquals(""Large logger "" + i, events.get(i).getMessage());
    }

    pipeline.stopAndWait();
    loggerContext.stop();

    Assert.assertNull(appender.getEvents());
  }
",non-flaky,5
35680,cdapio_cdap,KafkaLogProcessorPipelineTest.testRegularFlush,"  @Test
  public void testRegularFlush() throws Exception {
    String topic = ""testFlush"";
    LoggerContext loggerContext = LogPipelineTestUtil.createLoggerContext(""WARN"",
                                                                          ImmutableMap.of(""test.logger"", ""INFO""),
                                                                          MockAppender.class.getName());
    final MockAppender appender = LogPipelineTestUtil.getAppender(loggerContext.getLogger(Logger.ROOT_LOGGER_NAME),
                                                                  ""Test"", MockAppender.class);
    TestCheckpointManager checkpointManager = new TestCheckpointManager();

    // Use a longer checkpoint time and a short event delay. Should expect flush called at least once
    // per event delay.
    KafkaPipelineConfig config = new KafkaPipelineConfig(topic, Collections.singleton(0), 1024, 100, 1048576, 2000);
    KAFKA_TESTER.createTopic(topic, 1);

    loggerContext.start();
    KafkaLogProcessorPipeline pipeline = new KafkaLogProcessorPipeline(
      new LogProcessorPipelineContext(CConfiguration.create(), ""test"", loggerContext, NO_OP_METRICS_CONTEXT, 0),
      checkpointManager,
      KAFKA_TESTER.getBrokerService(), config);

    pipeline.startAndWait();

    // Even when there is no event, the flush should still get called.
    Tasks.waitFor(5, appender::getFlushCount, 3, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);

    // Publish some logs
    long now = System.currentTimeMillis();
    publishLog(topic, ImmutableList.of(
      LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""0"", now - 500),
      LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""1"", now - 300),
      LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""2"", now + 100)
    ));

    // Wait until getting all logs.
    Tasks.waitFor(3, () -> appender.getEvents().size(), 3, TimeUnit.SECONDS, 200, TimeUnit.MILLISECONDS);

    pipeline.stopAndWait();

    // Should get at least 20 flush calls, since the checkpoint is every 2 seconds
    Assert.assertTrue(appender.getFlushCount() >= 20);
  }
",non-flaky,5
35681,cdapio_cdap,KafkaLogProcessorPipelineTest.testMetricsAppender,"  @Test
  public void testMetricsAppender() throws Exception {
    Injector injector = KAFKA_TESTER.getInjector();
    MetricsCollectionService collectionService = injector.getInstance(MetricsCollectionService.class);
    collectionService.startAndWait();
    LoggerContext loggerContext = new LocalAppenderContext(injector.getInstance(TransactionRunner.class),
                                                           injector.getInstance(LocationFactory.class),
                                                           injector.getInstance(MetricsCollectionService.class));
    final File logDir = TEMP_FOLDER.newFolder();
    loggerContext.putProperty(""logDirectory"", logDir.getAbsolutePath());

    LogPipelineConfigurator configurator = new LogPipelineConfigurator(CConfiguration.create());
    configurator.setContext(loggerContext);
    URL configURL = getClass().getClassLoader().getResource(""pipeline-metric-appender.xml"");
    Assert.assertNotNull(configURL);
    configurator.doConfigure(configURL);

    String topic = ""metricsPipeline"";
    TestCheckpointManager checkpointManager = new TestCheckpointManager();
    KafkaPipelineConfig config = new KafkaPipelineConfig(topic, Collections.singleton(0), 1024L, 100L, 1048576, 200L);
    KAFKA_TESTER.createTopic(topic, 1);

    loggerContext.start();
    KafkaLogProcessorPipeline pipeline = new KafkaLogProcessorPipeline(
      new LogProcessorPipelineContext(CConfiguration.create(), ""testMetricAppender"",
                                      loggerContext, NO_OP_METRICS_CONTEXT, 0),
      checkpointManager,
      KAFKA_TESTER.getBrokerService(), config);

    pipeline.startAndWait();

    // Publish some log messages to Kafka
    long now = System.currentTimeMillis();
    WorkerLoggingContext loggingContext =
      new WorkerLoggingContext(""default"", ""app1"", ""worker1"", ""run1"", ""instance1"");
    publishLog(topic,
               ImmutableList.of(
                 LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""0"", now - 1000),
                 LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""2"", now - 700),
                 LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""3"", now - 500),
                 LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""1"", now - 900),
                 LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.DEBUG, ""hidden"", now - 600),
                 LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""4"", now - 100)),
               loggingContext
    );

    WorkflowProgramLoggingContext workflowProgramLoggingContext =
      new WorkflowProgramLoggingContext(""default"", ""app1"", ""wflow1"", ""run1"", ProgramType.MAPREDUCE, ""mr1"", ""mrun1"");

    publishLog(topic,
               ImmutableList.of(
                 LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.WARN, ""0"", now - 1000),
                 LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.WARN, ""2"", now - 700),
                 LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.TRACE, ""3"", now - 500)),
               workflowProgramLoggingContext
    );

    ServiceLoggingContext serviceLoggingContext =
      new ServiceLoggingContext(NamespaceId.SYSTEM.getNamespace(), Constants.Logging.COMPONENT_NAME,
                                Constants.Service.TRANSACTION);
    publishLog(topic,
               ImmutableList.of(
                 LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.ERROR, ""0"", now - 1000),
                 LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.ERROR, ""2"", now - 700),
                 LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.ERROR, ""3"", now - 500),
                 LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""1"", now - 900)),
               serviceLoggingContext
    );

    final MetricStore metricStore = injector.getInstance(MetricStore.class);
    try {
      verifyMetricsWithRetry(metricStore,
                             new MetricDataQuery(0, Integer.MAX_VALUE, Integer.MAX_VALUE,
                                                 ""system.app.log.info"",
                                                 AggregationFunction.SUM,
                                                 LoggingContextHelper.getMetricsTags(loggingContext),
                                                 new ArrayList<>()), 5L);

      verifyMetricsWithRetry(metricStore,
                             new MetricDataQuery(0, Integer.MAX_VALUE, Integer.MAX_VALUE,
                                                 ""system.app.log.debug"",
                                                 AggregationFunction.SUM,
                                                 LoggingContextHelper.getMetricsTags(loggingContext),
                                                 new ArrayList<>()), 1L);

      verifyMetricsWithRetry(metricStore,
                             new MetricDataQuery(0, Integer.MAX_VALUE, Integer.MAX_VALUE,
                                                 ""system.app.log.warn"",
                                                 AggregationFunction.SUM,
                                                 // mapreduce metrics context
                                                 ImmutableMap.of(Constants.Metrics.Tag.NAMESPACE, ""default"",
                                                                 Constants.Metrics.Tag.APP, ""app1"",
                                                                 Constants.Metrics.Tag.MAPREDUCE, ""mr1"",
                                                                 Constants.Metrics.Tag.RUN_ID, ""mrun1""),
                                                 new ArrayList<>()), 2L);
      verifyMetricsWithRetry(metricStore,
                             new MetricDataQuery(0, Integer.MAX_VALUE, Integer.MAX_VALUE,
                                                 ""system.app.log.trace"",
                                                 AggregationFunction.SUM,
                                                 // workflow metrics context
                                                 ImmutableMap.of(Constants.Metrics.Tag.NAMESPACE, ""default"",
                                                                 Constants.Metrics.Tag.APP, ""app1"",
                                                                 Constants.Metrics.Tag.WORKFLOW, ""wflow1"",
                                                                 Constants.Metrics.Tag.RUN_ID, ""run1""),
                                                 new ArrayList<>()), 1L);
      verifyMetricsWithRetry(metricStore,
                             new MetricDataQuery(0, Integer.MAX_VALUE, Integer.MAX_VALUE,
                                                 ""system.services.log.error"",
                                                 AggregationFunction.SUM,
                                                 LoggingContextHelper.getMetricsTags(serviceLoggingContext),
                                                 new ArrayList<>()), 3L);
    } finally {
      pipeline.stopAndWait();
      loggerContext.stop();
      collectionService.stopAndWait();
    }
  }
",non-flaky,5
35682,cdapio_cdap,KafkaLogProcessorPipelineTest.testMultiAppenders,"  @Test
  public void testMultiAppenders() throws Exception {
    final File logDir = TEMP_FOLDER.newFolder();
    LoggerContext loggerContext = new LoggerContext();
    loggerContext.putProperty(""logDirectory"", logDir.getAbsolutePath());

    LogPipelineConfigurator configurator = new LogPipelineConfigurator(CConfiguration.create());
    configurator.setContext(loggerContext);
    URL configURL = getClass().getClassLoader().getResource(""pipeline-multi-appenders.xml"");
    Assert.assertNotNull(configURL);
    configurator.doConfigure(configURL);

    String topic = ""testMultiAppenders"";
    TestCheckpointManager checkpointManager = new TestCheckpointManager();
    KafkaPipelineConfig config = new KafkaPipelineConfig(topic, Collections.singleton(0), 1024L, 100L, 1048576, 200L);
    KAFKA_TESTER.createTopic(topic, 1);

    loggerContext.start();
    KafkaLogProcessorPipeline pipeline = new KafkaLogProcessorPipeline(
      new LogProcessorPipelineContext(CConfiguration.create(), ""testMultiAppenders"",
                                      loggerContext, NO_OP_METRICS_CONTEXT, 0),
      checkpointManager,
      KAFKA_TESTER.getBrokerService(), config);

    pipeline.startAndWait();

    // Publish some log messages to Kafka using a non-specific logger
    long now = System.currentTimeMillis();
    publishLog(topic, ImmutableList.of(
      LogPipelineTestUtil.createLoggingEvent(""logger.trace"", Level.TRACE, ""TRACE"", now - 1000),
      LogPipelineTestUtil.createLoggingEvent(""logger.debug"", Level.DEBUG, ""DEBUG"", now - 900),
      LogPipelineTestUtil.createLoggingEvent(""logger.info"", Level.INFO, ""INFO"", now - 800),
      LogPipelineTestUtil.createLoggingEvent(""logger.warn"", Level.WARN, ""WARN"", now - 700),
      LogPipelineTestUtil.createLoggingEvent(""logger.error"", Level.ERROR, ""ERROR"", now - 600)
    ));

    // All logs should get logged to the default.log file
    Tasks.waitFor(true, () -> {
      File logFile = new File(logDir, ""default.log"");
      List<String> lines = Files.readAllLines(logFile.toPath(), StandardCharsets.UTF_8);
      return Arrays.asList(""TRACE"", ""DEBUG"", ""INFO"", ""WARN"", ""ERROR"").equals(lines);
    }, 5, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);

    // Publish some more log messages via the non-additive ""test.info"" logger.
    now = System.currentTimeMillis();
    publishLog(topic, ImmutableList.of(
      LogPipelineTestUtil.createLoggingEvent(""test.info.trace"", Level.TRACE, ""TRACE"", now - 1000),
      LogPipelineTestUtil.createLoggingEvent(""test.info.debug"", Level.DEBUG, ""DEBUG"", now - 900),
      LogPipelineTestUtil.createLoggingEvent(""test.info"", Level.INFO, ""INFO"", now - 800),
      LogPipelineTestUtil.createLoggingEvent(""test.info.warn"", Level.WARN, ""WARN"", now - 700),
      LogPipelineTestUtil.createLoggingEvent(""test.info.error"", Level.ERROR, ""ERROR"", now - 600)
    ));

    // Only logs with INFO or above level should get written to the info.log file
    Tasks.waitFor(true, () -> {
      File logFile = new File(logDir, ""info.log"");
      List<String> lines = Files.readAllLines(logFile.toPath(), StandardCharsets.UTF_8);
      return Arrays.asList(""INFO"", ""WARN"", ""ERROR"").equals(lines);
    }, 5, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);

    // The default.log file shouldn't be changed, because the test.info logger is non additive
    File defaultLogFile = new File(logDir, ""default.log"");
    List<String> lines = Files.readAllLines(defaultLogFile.toPath(), StandardCharsets.UTF_8);
    Assert.assertEquals(Arrays.asList(""TRACE"", ""DEBUG"", ""INFO"", ""WARN"", ""ERROR""), lines);

    // Publish a log messages via the additive ""test.error"" logger.
    now = System.currentTimeMillis();
    publishLog(topic, ImmutableList.of(
      LogPipelineTestUtil.createLoggingEvent(""test.error.1.2"", Level.ERROR, ""ERROR"", now - 1000)
    ));

    // Expect the log get appended to both the error.log file as well as the default.log file
    Tasks.waitFor(true, () -> {
      File logFile = new File(logDir, ""error.log"");
      List<String> lines1 = Files.readAllLines(logFile.toPath(), StandardCharsets.UTF_8);
      if (!Collections.singletonList(""ERROR"").equals(lines1)) {
        return false;
      }

      logFile = new File(logDir, ""default.log"");
      lines1 = Files.readAllLines(logFile.toPath(), StandardCharsets.UTF_8);
      return Arrays.asList(""TRACE"", ""DEBUG"", ""INFO"", ""WARN"", ""ERROR"", ""ERROR"").equals(lines1);
    }, 5, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);

    pipeline.stopAndWait();
    loggerContext.stop();
  }
",non-flaky,5
35683,cdapio_cdap,KafkaOffsetResolverTest.testOutOfOrderEvents,"  @Test
  public void testOutOfOrderEvents() throws Exception {
    String topic = ""testOutOfOrderEvents"";
    KafkaPipelineConfig config = new KafkaPipelineConfig(topic, Collections.singleton(0), 1024L, EVENT_DELAY_MILLIS,
                                                         1048576, 200L);
    KAFKA_TESTER.createTopic(topic, 1);

    // Publish log messages to Kafka and wait for all messages to be published
    long baseTime = System.currentTimeMillis() - 2 * EVENT_DELAY_MILLIS;
    List<ILoggingEvent> outOfOrderEvents = ImmutableList.of(
      createLoggingEvent(""test.logger"", Level.INFO, ""0"", baseTime - 20 * 1000 - EVENT_DELAY_MILLIS),
      createLoggingEvent(""test.logger"", Level.INFO, ""0"", baseTime - 20 * 1000 - EVENT_DELAY_MILLIS),
      createLoggingEvent(""test.logger"", Level.INFO, ""1"", baseTime - 7 * 1000 - EVENT_DELAY_MILLIS),
      createLoggingEvent(""test.logger"", Level.INFO, ""2"", baseTime - 9 * 100),
      createLoggingEvent(""test.logger"", Level.INFO, ""3"", baseTime - 500),
      createLoggingEvent(""test.logger"", Level.INFO, ""1"", baseTime - 9 * 1000),
      createLoggingEvent(""test.logger"", Level.INFO, ""1"", baseTime - 9 * 1000 + EVENT_DELAY_MILLIS / 2),
      createLoggingEvent(""test.logger"", Level.INFO, ""1"", baseTime - 9 * 1000),
      createLoggingEvent(""test.logger"", Level.INFO, ""1"", baseTime - 9 * 1000 - EVENT_DELAY_MILLIS / 2),
      createLoggingEvent(""test.logger"", Level.INFO, ""1"", baseTime - 10 * 1000),
      createLoggingEvent(""test.logger"", Level.INFO, ""1"", baseTime - 600),
      createLoggingEvent(""test.logger"", Level.INFO, ""5"", baseTime - 20 * 1000),
      createLoggingEvent(""test.logger"", Level.INFO, ""5"", baseTime - 20 * 1000 + EVENT_DELAY_MILLIS / 2),
      createLoggingEvent(""test.logger"", Level.INFO, ""6"", baseTime - 600),
      createLoggingEvent(""test.logger"", Level.INFO, ""6"", baseTime - 10 * 1000),
      createLoggingEvent(""test.logger"", Level.INFO, ""7"", baseTime - 2 * 1000 + EVENT_DELAY_MILLIS),
      createLoggingEvent(""test.logger"", Level.INFO, ""8"", baseTime - 7 * 1000 + EVENT_DELAY_MILLIS),
      createLoggingEvent(""test.logger"", Level.INFO, ""4"", baseTime - 100 + EVENT_DELAY_MILLIS));
    publishLog(topic, outOfOrderEvents);
    waitForAllLogsPublished(topic, outOfOrderEvents.size());

    KafkaOffsetResolver offsetResolver = new KafkaOffsetResolver(KAFKA_TESTER.getBrokerService(), config);
    // Use every event's timestamp as target time and assert that found offset with target timestamp
    // matches the expected offset
    for (ILoggingEvent event : outOfOrderEvents) {
      assertOffsetResolverResult(offsetResolver, outOfOrderEvents, event.getTimeStamp(), baseTime);
    }

    // Try to find the offset with an event time that has timestamp earlier than all event timestamps in Kafka
    assertOffsetResolverResult(offsetResolver, outOfOrderEvents,
                               baseTime - 10 * EVENT_DELAY_MILLIS, baseTime);

    // Try to find the offset with an event time that has timestamp larger than all event timestamps in Kafka
    assertOffsetResolverResult(offsetResolver, outOfOrderEvents,
                               baseTime + 10 * EVENT_DELAY_MILLIS, baseTime);

    // Use a random number between (timestamp - EVENT_DELAY_MILLIS, timestamp + EVENT_DELAY_MILLIS) as target time
    // and assert that found offset with target timestamp matches the expected offset
    for (int i = 0; i < 10; i++) {
      for (ILoggingEvent event : outOfOrderEvents) {
        assertOffsetResolverResult(offsetResolver, outOfOrderEvents,
                                   event.getTimeStamp() + RANDOM.nextInt() % EVENT_DELAY_MILLIS, baseTime);
      }
    }
  }
",non-flaky,5
35684,cdapio_cdap,KafkaOffsetResolverTest.testInOrderEvents,"  @Test
  public void testInOrderEvents() throws InterruptedException, IOException {
    String topic = ""testInOrderEvents"";
    KafkaPipelineConfig config = new KafkaPipelineConfig(topic, Collections.singleton(0), 1024L, EVENT_DELAY_MILLIS,
                                                         1048576, 200L);
    KAFKA_TESTER.createTopic(topic, 1);

    // Publish log messages to Kafka and wait for all messages to be published
    long baseTime = System.currentTimeMillis() - EVENT_DELAY_MILLIS;
    List<ILoggingEvent> inOrderEvents = new ArrayList<>();
    for (int i = 0; i < 20; i++) {
      inOrderEvents.add(createLoggingEvent(""test.logger"", Level.INFO, Integer.toString(i), baseTime + i));
    }
    publishLog(topic, inOrderEvents);
    waitForAllLogsPublished(topic, inOrderEvents.size());

    KafkaOffsetResolver offsetResolver = new KafkaOffsetResolver(KAFKA_TESTER.getBrokerService(), config);
    // Use every event's timestamp as target time and assert that found offset is the next offset of the current offset
    for (int i = 0; i < inOrderEvents.size(); i++) {
      long targetTime = inOrderEvents.get(i).getTimeStamp();
      long offset = offsetResolver.getStartOffset(Long.MAX_VALUE, targetTime, 0);
      Assert.assertEquals(""Failed to find the expected event with the target time: "" + targetTime,
                          i + 1, offset);
    }
  }
",non-flaky,5
35685,cdapio_cdap,TimeEventQueueTest.compare,"  @Test
  public void testOrdering() {
    TimeEventQueue<TimestampedEvent, Integer> eventQueue = new TimeEventQueue<>(ImmutableSet.of(1, 3));
    List<TimestampedEvent> expected = new ArrayList<>();

    // Put 10 events to partition 1, with both increasing timestamps and offsets
    for (int i = 0; i < 10; i++) {
      TimestampedEvent event = new TimestampedEvent(i, ""m1"" + i);
      eventQueue.add(event, i, 10, 1, i);
      expected.add(event);
    }

    // Put 10 events to partition 3, with increasing offsets, but non-sorted, some duplicated timestamps
    for (int i = 0; i < 10; i++) {
      long timestamp = i % 3;
      TimestampedEvent event = new TimestampedEvent(timestamp, ""m3"" + i);
      eventQueue.add(event, timestamp, 10, 3, i);
      expected.add(event);
    }

    // Sort the expected result based on timestamp. Since Collections.sort is stable sort,
    // partition 1 events will always be before partition 3 if the timestamps are the same
    Collections.sort(expected, new Comparator<TimestampedEvent>() {
      @Override
      public int compare(TimestampedEvent o1, TimestampedEvent o2) {
        return Long.compare(o1.getTimestamp(), o2.getTimestamp());
      }
",non-flaky,5
35686,cdapio_cdap,TimeEventQueueTest.testKafkaOffset,"  @Test
  public void testKafkaOffset() {
    TimeEventQueue<String, Integer> eventQueue = new TimeEventQueue<>(ImmutableSet.of(1, 2));

    // Insert 6 events, with timestamps going back and forth
    eventQueue.add(""m7"", 7L, 10, 1, 0);
    eventQueue.add(""m5"", 5L, 10, 2, 1);
    eventQueue.add(""m10"", 10L, 10, 1, 2);
    eventQueue.add(""m11"", 11L, 10, 2, 3);
    eventQueue.add(""m2"", 2L, 10, 1, 4);
    eventQueue.add(""m8"", 8L, 10, 2, 5);

    // Have 6 events of size 10, so total size should be 60
    Assert.assertEquals(60, eventQueue.getEventSize());

    // Events should be time ordered when getting from iterator
    TimeEventQueue.EventIterator<String, Integer> iterator = eventQueue.iterator();

    Assert.assertEquals(""m2"", iterator.next());
    Assert.assertEquals(1, iterator.getPartition());
    Assert.assertEquals(4, iterator.getOffset().intValue());
    iterator.remove();
    Assert.assertEquals(50, eventQueue.getEventSize());
    Assert.assertEquals(0, eventQueue.getSmallestOffset(1).intValue());
    Assert.assertEquals(1, eventQueue.getSmallestOffset(2).intValue());

    Assert.assertEquals(""m5"", iterator.next());
    Assert.assertEquals(2, iterator.getPartition());
    Assert.assertEquals(1, iterator.getOffset().intValue());
    iterator.remove();
    Assert.assertEquals(40, eventQueue.getEventSize());
    Assert.assertEquals(0, eventQueue.getSmallestOffset(1).intValue());
    Assert.assertEquals(3, eventQueue.getSmallestOffset(2).intValue());

    Assert.assertEquals(""m7"", iterator.next());
    Assert.assertEquals(1, iterator.getPartition());
    Assert.assertEquals(0, iterator.getOffset().intValue());
    iterator.remove();
    Assert.assertEquals(30, eventQueue.getEventSize());
    Assert.assertEquals(2, eventQueue.getSmallestOffset(1).intValue());
    Assert.assertEquals(3, eventQueue.getSmallestOffset(2).intValue());

    Assert.assertEquals(""m8"", iterator.next());
    Assert.assertEquals(2, iterator.getPartition());
    Assert.assertEquals(5, iterator.getOffset().intValue());
    iterator.remove();
    Assert.assertEquals(20, eventQueue.getEventSize());
    Assert.assertEquals(2, eventQueue.getSmallestOffset(1).intValue());
    Assert.assertEquals(3, eventQueue.getSmallestOffset(2).intValue());

    Assert.assertEquals(""m10"", iterator.next());
    Assert.assertEquals(1, iterator.getPartition());
    Assert.assertEquals(2, iterator.getOffset().intValue());
    iterator.remove();
    Assert.assertEquals(10, eventQueue.getEventSize());
    Assert.assertTrue(eventQueue.isEmpty(1));

    Assert.assertEquals(""m11"", iterator.next());
    Assert.assertEquals(2, iterator.getPartition());
    Assert.assertEquals(3, iterator.getOffset().intValue());
    iterator.remove();
    Assert.assertEquals(0, eventQueue.getEventSize());
    Assert.assertTrue(eventQueue.isEmpty(2));

    Assert.assertTrue(eventQueue.isEmpty());
  }
",non-flaky,5
35687,cdapio_cdap,TimeEventQueueTest.testInvalidPartition,"  @Test (expected = IllegalArgumentException.class)
  public void testInvalidPartition() {
    TimeEventQueue<String, Integer> eventQueue = new TimeEventQueue<>(Collections.singleton(1));
    eventQueue.add(""test"", 1L, 10, 2, 0);
  }
",non-flaky,5
35688,cdapio_cdap,TimeEventQueueTest.testIllegalRemove,"  @Test (expected = IllegalStateException.class)
  public void testIllegalRemove() {
    TimeEventQueue<String, Integer> eventQueue = new TimeEventQueue<>(Collections.singleton(1));
    eventQueue.add(""test"", 1L, 10, 1, 0);
    Iterator<String> iterator = eventQueue.iterator();
    iterator.next();
    iterator.remove();
    iterator.remove();
  }
",non-flaky,5
35689,cdapio_cdap,TimeEventQueueProcessorTest.test,"  @Test
  public void test() throws Exception {
    LoggerContext loggerContext = LogPipelineTestUtil.createLoggerContext(""WARN"",
                                                                          ImmutableMap.of(""test.logger"", ""INFO""),
                                                                          MockAppender.class.getName());
    LogProcessorPipelineContext context = new LogProcessorPipelineContext(CConfiguration.create(),
                                                                          ""test"", loggerContext, NO_OP_METRICS_CONTEXT,
                                                                          0);
    context.start();
    TimeEventQueueProcessor<TestOffset> processor = new TimeEventQueueProcessor<>(context, 50, 1,
                                                                                  ImmutableList.of(0));
    long now = System.currentTimeMillis();
    List<ILoggingEvent> events = ImmutableList.of(
      LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""1"", now - 1000),
      LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""3"", now - 700),
      LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""5"", now - 500),
      LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""2"", now - 900),
      LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.ERROR, ""4"", now - 600),
      LogPipelineTestUtil.createLoggingEvent(""test.logger"", Level.INFO, ""6"", now - 100));

    ProcessedEventMetadata<TestOffset> metadata = processor.process(0, new TransformingIterator(events.iterator()));
    // all 6 events should be processed. This is because when the buffer is full after 5 events, time event queue
    // processor should append existing buffered events and enqueue 6th event
    Assert.assertEquals(6, metadata.getTotalEventsProcessed());
    for (Map.Entry<Integer, Checkpoint<TestOffset>> entry : metadata.getCheckpoints().entrySet()) {
      Checkpoint<TestOffset> value = entry.getValue();
      // offset should be max offset processed so far
      Assert.assertEquals(6, value.getOffset().getOffset());
    }
  }
",non-flaky,5
35690,cdapio_cdap,LoggersTest.testEffectiveLevel,"  @Test
  public void testEffectiveLevel() throws Exception {
    LoggerContext context = new LoggerContext();
    JoranConfigurator configurator = new JoranConfigurator();
    configurator.setContext(context);
    configurator.doConfigure(new InputSource(new StringReader(generateLogback(""WARN"", ImmutableMap.of(
      ""test"", ""INFO"",
      ""test.a"", ""ERROR"",
      ""test.a.X"", ""DEBUG"",
      ""test.a.X$1"", ""OFF""
    )))));

    Assert.assertSame(context.getLogger(""test""), Loggers.getEffectiveLogger(context, ""test""));
    Assert.assertSame(context.getLogger(""test.a""), Loggers.getEffectiveLogger(context, ""test.a""));
    Assert.assertSame(context.getLogger(""test.a.X""), Loggers.getEffectiveLogger(context, ""test.a.X""));
    Assert.assertSame(context.getLogger(""test.a.X$1""), Loggers.getEffectiveLogger(context, ""test.a.X$1""));

    Assert.assertSame(context.getLogger(Logger.ROOT_LOGGER_NAME),
                      Loggers.getEffectiveLogger(context, ""defaultToRoot""));
    Assert.assertSame(context.getLogger(""test""),
                      Loggers.getEffectiveLogger(context, ""test.defaultToTest""));
    Assert.assertSame(context.getLogger(""test.a""),
                      Loggers.getEffectiveLogger(context, ""test.a.defaultToTestDotA""));
    Assert.assertSame(context.getLogger(""test.a.X""),
                      Loggers.getEffectiveLogger(context, ""test.a.X.defaultToTestDotADotX""));
  }
",non-flaky,5
35691,cdapio_cdap,FileMetadataTest.testFileMetadataReadWrite,"  @Test
  public void testFileMetadataReadWrite() throws Exception {
    TransactionRunner transactionRunner = injector.getInstance(TransactionRunner.class);
    FileMetaDataWriter fileMetaDataWriter = new FileMetaDataWriter(transactionRunner);
    LogPathIdentifier logPathIdentifier =
      new LogPathIdentifier(NamespaceId.DEFAULT.getNamespace(), ""testApp"", ""testFlow"");
    LocationFactory locationFactory = injector.getInstance(LocationFactory.class);
    Location location = locationFactory.create(TMP_FOLDER.newFolder().getPath()).append(""/logs"");
    long currentTime = System.currentTimeMillis();
    for (int i = 10; i <= 100; i += 10) {
      // i is the event time
      fileMetaDataWriter.writeMetaData(logPathIdentifier, i, currentTime,
                                       location.append(Integer.toString(i)));
    }

    // for the timestamp 80, add new new log path id with different current time.

    fileMetaDataWriter.writeMetaData(logPathIdentifier, 80, currentTime + 1,
                                     location.append(""81""));

    fileMetaDataWriter.writeMetaData(logPathIdentifier, 80, currentTime + 2,
                                     location.append(""82""));

    // reader test
    FileMetaDataReader fileMetadataReader = injector.getInstance(FileMetaDataReader.class);

    Assert.assertEquals(12, fileMetadataReader.listFiles(logPathIdentifier, 0, 100).size());
    Assert.assertEquals(5, fileMetadataReader.listFiles(logPathIdentifier, 20, 50).size());
    Assert.assertEquals(2, fileMetadataReader.listFiles(logPathIdentifier, 100, 150).size());

    // should include the latest file with event start time 80.
    List<LogLocation> locationList = fileMetadataReader.listFiles(logPathIdentifier, 81, 85);
    Assert.assertEquals(1, locationList.size());
    Assert.assertEquals(80, locationList.get(0).getEventTimeMs());
    Assert.assertEquals(location.append(""82""), locationList.get(0).getLocation());

    Assert.assertEquals(1, fileMetadataReader.listFiles(logPathIdentifier, 150, 1000).size());
  }
",non-flaky,5
35692,cdapio_cdap,FileMetadataTest.testFileMetadataReadWriteAcrossFormats,"  @Test
  public void testFileMetadataReadWriteAcrossFormats() throws Exception {
    TransactionRunner transactionRunner = injector.getInstance(TransactionRunner.class);
    FileMetaDataWriter fileMetaDataWriter = new FileMetaDataWriter(transactionRunner);
    LogPathIdentifier logPathIdentifier =
      new LogPathIdentifier(NamespaceId.DEFAULT.getNamespace(), ""testApp"", ""testFlow"");
    LocationFactory locationFactory = injector.getInstance(LocationFactory.class);
    Location location = locationFactory.create(TMP_FOLDER.newFolder().getPath()).append(""/logs"");
    long currentTime = System.currentTimeMillis();

    long eventTime = currentTime + 20;
    long newCurrentTime = currentTime + 100;
    // 10 files in new format
    for (int i = 1; i <= 10; i++) {
      fileMetaDataWriter.writeMetaData(logPathIdentifier, eventTime + i, newCurrentTime + i,
                                       location.append(""testFileNew"" + Integer.toString(i)));
    }
    // reader test
    FileMetaDataReader fileMetadataReader = injector.getInstance(FileMetaDataReader.class);

    // scan only in new files time range
    List<LogLocation> locations = fileMetadataReader.listFiles(logPathIdentifier, eventTime + 2, eventTime + 6);
    // should include files from currentTime (1..6)
    Assert.assertEquals(6, locations.size());
    for (LogLocation logLocation : locations) {
      Assert.assertEquals(LogLocation.VERSION_1, logLocation.getFrameworkVersion());
    }

    // scan time range across formats
    locations = fileMetadataReader.listFiles(logPathIdentifier, currentTime + 2, eventTime + 6);
    // should include files from new range (1..6)
    Assert.assertEquals(6, locations.size());

    for (int i = 0; i < locations.size(); i++) {
      Assert.assertEquals(LogLocation.VERSION_1, locations.get(i).getFrameworkVersion());
      Assert.assertEquals(location.append(""testFileNew"" + Integer.toString(i + 1)), locations.get(i).getLocation());
    }
  }
",non-flaky,5
35693,cdapio_cdap,DistributedLogFrameworkTest.testFramework,"  @Test
  public void testFramework() throws Exception {
    DistributedLogFramework framework = injector.getInstance(DistributedLogFramework.class);
    CConfiguration cConf = injector.getInstance(CConfiguration.class);

    framework.startAndWait();

    // Send some logs to Kafka.
    LoggingContext context = new ServiceLoggingContext(NamespaceId.SYSTEM.getNamespace(),
                                                       Constants.Logging.COMPONENT_NAME,
                                                       ""test"");

    // Make sure all events get flushed in the same batch
    long eventTimeBase = System.currentTimeMillis() + cConf.getInt(Constants.Logging.PIPELINE_EVENT_DELAY_MS);
    final int msgCount = 50;
    for (int i = 0; i < msgCount; i++) {
      // Publish logs in descending timestamp order
      publishLog(
        cConf.get(Constants.Logging.KAFKA_TOPIC), context,
        ImmutableList.of(
          createLoggingEvent(""io.cdap.test."" + i, Level.INFO, ""Testing "" + i, eventTimeBase - i)
        )
      );
    }

    // Read the logs back. They should be sorted by timestamp order.
    final FileMetaDataReader metaDataReader = injector.getInstance(FileMetaDataReader.class);
    Tasks.waitFor(true, () -> {
      List<LogLocation> locations = metaDataReader.listFiles(new LogPathIdentifier(NamespaceId.SYSTEM.getNamespace(),
                                                                                    Constants.Logging.COMPONENT_NAME,
                                                                                   ""test""), 0, Long.MAX_VALUE);
      if (locations.size() != 1) {
        return false;
      }
      LogLocation location = locations.get(0);
      int i = 0;
      try {
        try (CloseableIterator<LogEvent> iter = location.readLog(Filter.EMPTY_FILTER, 0, Long.MAX_VALUE, msgCount)) {
          while (iter.hasNext()) {
            String expectedMsg = ""Testing "" + (msgCount - i - 1);
            LogEvent event = iter.next();
            if (!expectedMsg.equals(event.getLoggingEvent().getMessage())) {
              return false;
            }
            i++;
          }
          return i == msgCount;
        }
      } catch (Exception e) {
        // It's possible the file is an invalid Avro file due to a race between creation of the meta data
        // and the time when actual content are flushed to the file
        return false;
      }
    }, 10, TimeUnit.SECONDS, msgCount, TimeUnit.MILLISECONDS);

    framework.stopAndWait();

    String kafkaTopic = cConf.get(Constants.Logging.KAFKA_TOPIC);
    // Check the checkpoint is persisted correctly. Since all messages are processed,
    // the checkpoint should be the same as the message count.
    CheckpointManager<KafkaOffset> checkpointManager = getCheckpointManager(kafkaTopic);
    Checkpoint<KafkaOffset> checkpoint = checkpointManager.getCheckpoint(0);
    Assert.assertEquals(msgCount, checkpoint.getOffset().getNextOffset());
  }
",non-flaky,5
35694,cdapio_cdap,LoggingEventSerializerTest.testEmptySerialization,"  @Test
  public void testEmptySerialization() throws Exception {
    Logger logger = LoggerFactory.getLogger(LoggingEventSerializerTest.class);
    LoggingEventSerializer serializer = new LoggingEventSerializer();
    ch.qos.logback.classic.spi.LoggingEvent iLoggingEvent = new ch.qos.logback.classic.spi.LoggingEvent(
      getClass().getName(), (ch.qos.logback.classic.Logger) logger, Level.ERROR, ""message"", null, null);
    iLoggingEvent.setThreadName(""thread-1"");
    iLoggingEvent.setTimeStamp(10000000L);

    // Serialize
    ILoggingEvent event = new LogMessage(iLoggingEvent, LoggingContextAccessor.getLoggingContext());
    byte [] serializedBytes = serializer.toBytes(event);

    // De-serialize
    ILoggingEvent actualEvent = serializer.fromBytes(ByteBuffer.wrap(serializedBytes));
    assertLoggingEventEquals(iLoggingEvent, actualEvent);
  }
",non-flaky,5
35695,cdapio_cdap,LoggingEventSerializerTest.testSerialization,"  @Test
  public void testSerialization() throws Exception {
    Map<String, String> mdcMap = Maps.newHashMap();
    mdcMap.put(""mdc1"", ""mdc-val1"");
    mdcMap.put(""mdc2"", null);
    mdcMap.put(null, null);

    Map<String, String> contextMap = Maps.newHashMap();
    contextMap.put(""p1"", ""ctx-val1"");
    contextMap.put(""p2"", null);
    contextMap.put(null, null);

    LoggingEventSerializer serializer = new LoggingEventSerializer();
    ch.qos.logback.classic.spi.LoggingEvent iLoggingEvent = new ch.qos.logback.classic.spi.LoggingEvent();
    iLoggingEvent.setThreadName(""threadName1"");
    iLoggingEvent.setLevel(Level.INFO);
    iLoggingEvent.setMessage(""Log message1"");
    iLoggingEvent.setArgumentArray(new Object[]{null, ""arg2"", ""100"", null});
    iLoggingEvent.setLoggerName(""loggerName1"");

    iLoggingEvent.setLoggerContextRemoteView(new LoggerContextVO(""logger_context1"", contextMap, 12345634234L));

    Exception e1 = new Exception(null, null);
    Exception e2 = new Exception(""Test Exception2"", e1);
    iLoggingEvent.setThrowableProxy(new ThrowableProxy(e2));
    iLoggingEvent.prepareForDeferredProcessing();
    ((ThrowableProxy) iLoggingEvent.getThrowableProxy()).calculatePackagingData();

    iLoggingEvent.setCallerData(new StackTraceElement[]{
      new StackTraceElement(""com.Class1"", ""methodName1"", ""fileName1"", 10),
      null,
      new StackTraceElement(""com.Class2"", ""methodName2"", ""fileName2"", 20),
      new StackTraceElement(""com.Class3"",  ""methodName3"", null, 30),
      null
    });

    iLoggingEvent.setMarker(null);
    iLoggingEvent.getMDCPropertyMap().putAll(mdcMap);
    iLoggingEvent.setTimeStamp(1234567890L);

    // Serialize
    ILoggingEvent event = new LogMessage(iLoggingEvent, LoggingContextAccessor.getLoggingContext());
    byte [] serializedBytes = serializer.toBytes(event);

    // De-serialize
    ILoggingEvent actualEvent = serializer.fromBytes(ByteBuffer.wrap(serializedBytes));
    System.out.println(actualEvent);
    assertLoggingEventEquals(iLoggingEvent, actualEvent);
  }
",non-flaky,5
35696,cdapio_cdap,LoggingEventSerializerTest.testOldSystemLoggingContext,"  @Test
  public void testOldSystemLoggingContext() throws Exception {
    // see: CDAP-7482
    Map<String, String> mdcMap = new HashMap<>();
    mdcMap.put(ServiceLoggingContext.TAG_SYSTEM_ID, ""ns1"");
    mdcMap.put(ComponentLoggingContext.TAG_COMPONENT_ID, ""comp1"");
    mdcMap.put(ServiceLoggingContext.TAG_SERVICE_ID, ""ser1"");

    ch.qos.logback.classic.spi.LoggingEvent iLoggingEvent = new ch.qos.logback.classic.spi.LoggingEvent();
    iLoggingEvent.setCallerData(new StackTraceElement[] { null });
    iLoggingEvent.setMDCPropertyMap(mdcMap);

    Assert.assertTrue(LoggingContextHelper.getLoggingContext(iLoggingEvent.getMDCPropertyMap())
                        instanceof ServiceLoggingContext);
  }
",non-flaky,5
35697,cdapio_cdap,LoggingEventSerializerTest.testNullSerialization,"  @Test
  public void testNullSerialization() throws Exception {
    Logger logger = LoggerFactory.getLogger(LoggingEventSerializerTest.class);
    LoggingEventSerializer serializer = new LoggingEventSerializer();
    ch.qos.logback.classic.spi.LoggingEvent iLoggingEvent = new ch.qos.logback.classic.spi.LoggingEvent(
      null, (ch.qos.logback.classic.Logger) logger, null, null, null, null);
    iLoggingEvent.setThreadName(null);
    iLoggingEvent.setLevel(null);
    iLoggingEvent.setMessage(null);
    iLoggingEvent.setArgumentArray(null);
    iLoggingEvent.setLoggerName(null);
    iLoggingEvent.setLoggerContextRemoteView(null);
    iLoggingEvent.setThrowableProxy(null);
    iLoggingEvent.setCallerData(null);
    iLoggingEvent.setMarker(null);
    iLoggingEvent.setMDCPropertyMap(null);
    iLoggingEvent.setTimeStamp(10000000L);

    // Serialize
    ILoggingEvent event = new LogMessage(iLoggingEvent, LoggingContextAccessor.getLoggingContext());
    byte [] serializedBytes = serializer.toBytes(event);

    // De-serialize
    ILoggingEvent actualEvent = serializer.fromBytes(ByteBuffer.wrap(serializedBytes));

    iLoggingEvent.setLevel(Level.ERROR);
    assertLoggingEventEquals(iLoggingEvent, actualEvent);
  }
",non-flaky,5
35698,cdapio_cdap,LoggingEventSerializerTest.testDecodeTimestamp,"  @Test
  public void testDecodeTimestamp() throws IOException {
    long timestamp = System.currentTimeMillis();

    ch.qos.logback.classic.spi.LoggingEvent event = new ch.qos.logback.classic.spi.LoggingEvent();
    event.setLevel(Level.INFO);
    event.setLoggerName(""test.logger"");
    event.setMessage(""Some test"");
    event.setTimeStamp(timestamp);

    // Serialize it
    LoggingEventSerializer serializer = new LoggingEventSerializer();
    byte[] bytes = serializer.toBytes(event);

    // Decode timestamp
    Assert.assertEquals(timestamp, serializer.decodeEventTimestamp(ByteBuffer.wrap(bytes)));
  }
",non-flaky,5
35699,cdapio_cdap,LoggingEventTest.testSerialize,"  @Test
  public void testSerialize() throws Exception {
    Logger logger = LoggerFactory.getLogger(LoggingEventTest.class);
    ch.qos.logback.classic.spi.LoggingEvent iLoggingEvent = new ch.qos.logback.classic.spi.LoggingEvent(
      getClass().getName(), (ch.qos.logback.classic.Logger) logger, Level.ERROR, ""Log message1"", null,
      new Object[] {""arg1"", ""arg2"", ""100""});
    iLoggingEvent.setLoggerName(""loggerName1"");
    iLoggingEvent.setThreadName(""threadName1"");
    iLoggingEvent.setTimeStamp(1234567890L);
    iLoggingEvent.setLoggerContextRemoteView(new LoggerContextVO(""loggerContextRemoteView"",
                                                                 ImmutableMap.of(""key1"", ""value1"", ""key2"", ""value2""),
                                                                 100000L));
    Map<String, String> mdcMap = Maps.newHashMap(ImmutableMap.of(""mdck1"", ""mdcv1"", ""mdck2"", ""mdck2""));
    iLoggingEvent.setMDCPropertyMap(mdcMap);

    LoggingEventSerializer serializer = new LoggingEventSerializer();
    byte[] encoded = serializer.toBytes(new LogMessage(iLoggingEvent, LoggingContextAccessor.getLoggingContext()));

    ILoggingEvent decodedEvent = serializer.fromBytes(ByteBuffer.wrap(encoded));
    LoggingEventSerializerTest.assertLoggingEventEquals(iLoggingEvent, decodedEvent);
  }
",non-flaky,5
35700,cdapio_cdap,LoggingEventTest.testEmptySerialize,"  @Test
  public void testEmptySerialize() throws Exception {
    Logger logger = LoggerFactory.getLogger(LoggingEventTest.class);
    ch.qos.logback.classic.spi.LoggingEvent iLoggingEvent = new ch.qos.logback.classic.spi.LoggingEvent(
      getClass().getName(), (ch.qos.logback.classic.Logger) logger, Level.ERROR, null, null, null);

    LoggingEventSerializer serializer = new LoggingEventSerializer();
    byte[] encoded = serializer.toBytes(new LogMessage(iLoggingEvent, LoggingContextAccessor.getLoggingContext()));

    ILoggingEvent decodedEvent = serializer.fromBytes(ByteBuffer.wrap(encoded));
    LoggingEventSerializerTest.assertLoggingEventEquals(iLoggingEvent, decodedEvent);
  }
",non-flaky,5
35701,cdapio_cdap,FileMetadataCleanerTest.testScanAndDeleteNewMetadata,"  @Test
  public void testScanAndDeleteNewMetadata() throws Exception {
    TransactionRunner transactionRunner = injector.getInstance(TransactionRunner.class);

    FileMetaDataWriter fileMetaDataWriter = new FileMetaDataWriter(transactionRunner);
    FileMetadataCleaner fileMetadataCleaner = new FileMetadataCleaner(transactionRunner);
    try {
      long currentTime = System.currentTimeMillis();
      long eventTimestamp = currentTime - 100;
      LogPathIdentifier logPathIdentifier = new LogPathIdentifier(""testNs2"", ""testApp"", ""testFlow"");
      LocationFactory locationFactory = injector.getInstance(LocationFactory.class);
      List<String> expected = new ArrayList<>();
      for (int i = 0; i < 100; i++) {
        Location location = locationFactory.create(""testFlowFile"" + i);
        // values : event time is 100ms behind current timestamp
        fileMetaDataWriter.writeMetaData(logPathIdentifier, eventTimestamp + i, currentTime + i, location);
        expected.add(location.toURI().getPath());
      }

      long tillTime = currentTime + 50;
      List<FileMetadataCleaner.DeletedEntry> deletedEntries =
        fileMetadataCleaner.scanAndGetFilesToDelete(tillTime, 100);
      // we should have deleted 51 rows, till time is inclusive
      Assert.assertEquals(51, deletedEntries.size());
      int count = 0;
      for (FileMetadataCleaner.DeletedEntry deletedEntry : deletedEntries) {
        Assert.assertEquals(expected.get(count), deletedEntry.getPath());
        count += 1;
      }
      // now add 10 entries for spark
      logPathIdentifier = new LogPathIdentifier(""testNs2"", ""testApp"", ""testSpark"");
      expected = new ArrayList<>();

      for (int i = 0; i < 10; i++) {
        Location location = locationFactory.create(""testSparkFile"" + i);
        // values : event time is 100ms behind current timestamp
        fileMetaDataWriter.writeMetaData(logPathIdentifier, eventTimestamp + i, currentTime + i, location);
        expected.add(location.toURI().getPath());
      }

      // lets keep the same till time - this should only delete the spark entries now
      deletedEntries = fileMetadataCleaner.scanAndGetFilesToDelete(tillTime, 100);
      // we should have deleted 51 rows, till time is inclusive
      Assert.assertEquals(10, deletedEntries.size());
      count = 0;
      for (FileMetadataCleaner.DeletedEntry deletedEntry : deletedEntries) {
        Assert.assertEquals(expected.get(count), deletedEntry.getPath());
        count += 1;
      }

      // now add 10 entries in mr context in time range 60-70
      logPathIdentifier = new LogPathIdentifier(""testNs2"", ""testApp"", ""testMr"");
      expected = new ArrayList<>();

      // flow should come up at the beginning in the expected list
      for (int i = 51; i <= 70; i++) {
        expected.add(locationFactory.create(""testFlowFile"" + i).toURI().getPath());
      }

      for (int i = 0; i < 10; i++) {
        Location location = locationFactory.create(""testMrFile"" + i);
        // values : event time is 100ms behind current timestamp
        fileMetaDataWriter.writeMetaData(logPathIdentifier, eventTimestamp + i, currentTime + i, location);
        expected.add(location.toURI().getPath());
      }

      List<String> nextExpected = new ArrayList<>();
      logPathIdentifier = new LogPathIdentifier(""testNs2"", ""testApp"", ""testCustomAction"");
      for (int i = 90; i < 100; i++) {
        Location location = locationFactory.create(""testActionFile"" + i);
        // values : event time is 100ms behind current timestamp
        fileMetaDataWriter.writeMetaData(logPathIdentifier, eventTimestamp + i, currentTime + i, location);
        nextExpected.add(location.toURI().getPath());
      }

      tillTime = currentTime + 70;
      // lets delete till 70.
      deletedEntries = fileMetadataCleaner.scanAndGetFilesToDelete(tillTime, 100);
      // we should have deleted 51-70 files of flow and 0-9 files of spark files in that order and 0 files of action.
      Assert.assertEquals(30, deletedEntries.size());
      count = 0;
      for (FileMetadataCleaner.DeletedEntry deletedEntry : deletedEntries) {
        Assert.assertEquals(expected.get(count), deletedEntry.getPath());
        count += 1;
      }

      // now delete till currentTime + 100, this should delete all remaining entries.
      // custom action should come first and then flow entries

      tillTime = currentTime + 100;
      // lets delete till 100.
      deletedEntries = fileMetadataCleaner.scanAndGetFilesToDelete(tillTime, 100);
      // we should have deleted 90-99 of custom action(10) 71-99 (29) files of flow.
      for (int i = 71; i < 100; i++) {
        nextExpected.add(locationFactory.create(""testFlowFile"" + i).toURI().getPath());
      }
      Assert.assertEquals(39, deletedEntries.size());
      count = 0;
      for (FileMetadataCleaner.DeletedEntry deletedEntry : deletedEntries) {
        Assert.assertEquals(nextExpected.get(count), deletedEntry.getPath());
        count += 1;
      }

      // now lets do a delete with till time  = currentTime + 1000, this should return empty result
      tillTime = currentTime + 1000;
      deletedEntries = fileMetadataCleaner.scanAndGetFilesToDelete(tillTime, 100);
      Assert.assertEquals(0, deletedEntries.size());
    } finally {
      // cleanup meta
      deleteAllMetaEntries(transactionRunner);
    }
  }
",non-flaky,5
35702,cdapio_cdap,FileMetadataCleanerTest.testFileMetadataWithCommonContextPrefix,"  @Test
  public void testFileMetadataWithCommonContextPrefix() throws Exception {
    TransactionRunner transactionRunner = injector.getInstance(TransactionRunner.class);

    FileMetaDataWriter fileMetaDataWriter = new FileMetaDataWriter(transactionRunner);
    FileMetaDataReader fileMetadataReader = injector.getInstance(FileMetaDataReader.class);
    FileMetadataCleaner fileMetadataCleaner = new FileMetadataCleaner(transactionRunner);
    try {
      List<LogPathIdentifier> logPathIdentifiers = new ArrayList<>();
      // we write entries where program id is of format testFlow{1..20},
      // this should be able to scan and delete common prefix programs like testFlow1, testFlow10 during clenaup.
      for (int i = 1; i <= 20; i++) {
        logPathIdentifiers.add(new LogPathIdentifier(NamespaceId.DEFAULT.getNamespace(),
                                                     ""testApp"", String.format(""testFlow%s"", i)));
      }

      LocationFactory locationFactory = injector.getInstance(LocationFactory.class);
      Location location = locationFactory.create(TMP_FOLDER.newFolder().getPath()).append(""/logs"");
      long currentTime = System.currentTimeMillis();
      long newCurrentTime = currentTime + 100;

      for (int i = 1; i <= 20; i++) {
        LogPathIdentifier identifier = logPathIdentifiers.get(i - 1);
        for (int j = 0; j < 10; j++) {
          fileMetaDataWriter.writeMetaData(identifier, newCurrentTime + j, newCurrentTime + j,
                                           location.append(""testFileNew"" + Integer.toString(j)));
        }
      }

      List<LogLocation> locations;
      for (int i = 1; i <= 20; i++) {
        locations = fileMetadataReader.listFiles(logPathIdentifiers.get(i - 1),
                                                 newCurrentTime, newCurrentTime + 10);
        // should include files from currentTime (0..9)
        Assert.assertEquals(10, locations.size());
      }

      long tillTime = newCurrentTime + 4;
      List<FileMetadataCleaner.DeletedEntry> deleteEntries =
        fileMetadataCleaner.scanAndGetFilesToDelete(tillTime, 100);
      // 20 context, 5 entries each
      Assert.assertEquals(100, deleteEntries.size());
      for (int i = 1; i <= 20; i++) {
        locations = fileMetadataReader.listFiles(logPathIdentifiers.get(i - 1),
                                                 newCurrentTime, newCurrentTime + 10);
        // should include files from time (5..9)
        Assert.assertEquals(5, locations.size());
        int startIndex = 5;
        for (LogLocation logLocation : locations) {
          Assert.assertEquals(String.format(""testFileNew%s"", startIndex), logLocation.getLocation().getName());
          startIndex++;
        }
      }
    } finally {
      // cleanup meta
      deleteAllMetaEntries(transactionRunner);
    }
  }
",non-flaky,5
35703,cdapio_cdap,FileMetadataCleanerTest.testWithBatchSizeLargerThanNumOfFiles,"  @Test
  public void testWithBatchSizeLargerThanNumOfFiles() throws Exception {
    TransactionRunner transactionRunner = injector.getInstance(TransactionRunner.class);

    FileMetaDataWriter fileMetaDataWriter = new FileMetaDataWriter(transactionRunner);
    FileMetaDataReader fileMetadataReader = injector.getInstance(FileMetaDataReader.class);
    FileMetadataCleaner fileMetadataCleaner = new FileMetadataCleaner(transactionRunner);
    try {
      LogPathIdentifier identifier = new LogPathIdentifier(NamespaceId.DEFAULT.getNamespace(),
                                                           ""testApp"", String.format(""testFlow%s"", 0));

      LocationFactory locationFactory = injector.getInstance(LocationFactory.class);
      Location location = locationFactory.create(TMP_FOLDER.newFolder().getPath()).append(""/logs"");
      long currentTime = System.currentTimeMillis();
      long newCurrentTime = currentTime + 100;

      for (int j = 0; j < 10; j++) {
        fileMetaDataWriter.writeMetaData(identifier, newCurrentTime + j, newCurrentTime + j,
                                         location.append(""testFileNew"" + Integer.toString(j)));
      }

      List<LogLocation> locations;
      locations = fileMetadataReader.listFiles(identifier, newCurrentTime, newCurrentTime + 10);
      // should include files from currentTime (0..9)
      Assert.assertEquals(10, locations.size());

      long tillTime = newCurrentTime + 4;
      List<FileMetadataCleaner.DeletedEntry> deleteEntries =
        fileMetadataCleaner.scanAndGetFilesToDelete(tillTime, 1000);
      Assert.assertEquals(5, deleteEntries.size());
    } finally {
      // cleanup meta
      deleteAllMetaEntries(transactionRunner);
    }
  }
",non-flaky,5
35704,cdapio_cdap,LogCleanerTest.testLogCleanup,"  @Test
  public void testLogCleanup() throws Exception {
    TransactionRunner transactionRunner = injector.getInstance(TransactionRunner.class);
    FileMetadataCleaner fileMetadataCleaner = new FileMetadataCleaner(transactionRunner);
    LocationFactory locationFactory = injector.getInstance(LocationFactory.class);
    long currentTime = System.currentTimeMillis();
    LogPathIdentifier logPathIdentifier = new LogPathIdentifier(""testNs"", ""testApp"", ""testEntity"");
    FileMetaDataWriter fileMetaDataWriter = new FileMetaDataWriter(transactionRunner);
    long startTime = currentTime - 5000;
    Location dirLocation = locationFactory.create(""logs"");
    dirLocation.mkdirs();
    // create 20 files, add them in past time range
    for (int i = 0; i < 20; i++) {
      Location location = dirLocation.append(""test"" + i);
      location.createNew();
      fileMetaDataWriter.writeMetaData(logPathIdentifier, startTime + i, startTime + i, location);
    }

    Assert.assertEquals(20, dirLocation.list().size());
    LogCleaner logCleaner = new LogCleaner(fileMetadataCleaner, locationFactory, 100, 60);
    logCleaner.run();
    FileMetaDataReader fileMetaDataReader = injector.getInstance(FileMetaDataReader.class);
    // all meta data should be deleted
    Assert.assertEquals(0, fileMetaDataReader.listFiles(logPathIdentifier, 0, System.currentTimeMillis()).size());
    // we are not asserting file existence as the delete could fail and we don't guarantee file deletion.
  }
",non-flaky,5
35705,cdapio_cdap,TestFileLogging.testGetLogNext,"  @Test
  public void testGetLogNext() throws Exception {
    LoggingContext loggingContext = new WorkerLoggingContext(""TFL_NS_1"", ""APP_1"", ""WORKER_1"", ""RUN1"", ""INSTANCE1"");
    FileLogReader logReader = injector.getInstance(FileLogReader.class);
    LoggingTester tester = new LoggingTester();
    tester.testGetNext(logReader, loggingContext);
  }
",non-flaky,5
35706,cdapio_cdap,TestFileLogging.testGetLogPrev,"  @Test
  public void testGetLogPrev() throws Exception {
    LoggingContext loggingContext = new WorkerLoggingContext(""TFL_NS_1"", ""APP_1"", ""WORKER_1"", ""RUN1"", ""INSTANCE1"");
    FileLogReader logReader = injector.getInstance(FileLogReader.class);
    LoggingTester tester = new LoggingTester();
    tester.testGetPrev(logReader, loggingContext);
  }
",non-flaky,5
35707,cdapio_cdap,TestFileLogging.testGetLog,"  @Test
  public void testGetLog() throws Exception {
    // LogReader.getLog is tested in LogSaverTest for distributed mode
    LoggingContext loggingContext = new WorkerLoggingContext(""TFL_NS_1"", ""APP_1"", ""WORKER_1"", ""RUN1"", ""INSTANCE1"");
    FileLogReader logTail = injector.getInstance(FileLogReader.class);
    LoggingTester.LogCallback logCallback1 = new LoggingTester.LogCallback();
    logTail.getLogPrev(loggingContext, ReadRange.LATEST, 60, Filter.EMPTY_FILTER,
                       logCallback1);
    List<LogEvent> allEvents = logCallback1.getEvents();
    Assert.assertEquals(60, allEvents.size());

    List<LogEvent> events =
      Lists.newArrayList(logTail.getLog(loggingContext, allEvents.get(10).getLoggingEvent().getTimeStamp(),
                                        allEvents.get(15).getLoggingEvent().getTimeStamp(), Filter.EMPTY_FILTER));

    Assert.assertEquals(5, events.size());
    Assert.assertEquals(allEvents.get(10).getLoggingEvent().getFormattedMessage(),
                        events.get(0).getLoggingEvent().getFormattedMessage());
    Assert.assertEquals(allEvents.get(14).getLoggingEvent().getFormattedMessage(),
                        events.get(4).getLoggingEvent().getFormattedMessage());


    events =
      Lists.newArrayList(logTail.getLog(loggingContext, allEvents.get(0).getLoggingEvent().getTimeStamp(),
                                        allEvents.get(59).getLoggingEvent().getTimeStamp(), Filter.EMPTY_FILTER));
    Assert.assertEquals(59, events.size());
    Assert.assertEquals(allEvents.get(0).getLoggingEvent().getFormattedMessage(),
                        events.get(0).getLoggingEvent().getFormattedMessage());
    Assert.assertEquals(allEvents.get(58).getLoggingEvent().getFormattedMessage(),
                        events.get(58).getLoggingEvent().getFormattedMessage());

    events =
      Lists.newArrayList(logTail.getLog(loggingContext, allEvents.get(12).getLoggingEvent().getTimeStamp(),
                                        allEvents.get(41).getLoggingEvent().getTimeStamp(), Filter.EMPTY_FILTER));
    Assert.assertEquals(29, events.size());
    Assert.assertEquals(allEvents.get(12).getLoggingEvent().getFormattedMessage(),
                        events.get(0).getLoggingEvent().getFormattedMessage());
    Assert.assertEquals(allEvents.get(40).getLoggingEvent().getFormattedMessage(),
                        events.get(28).getLoggingEvent().getFormattedMessage());

    events =
      Lists.newArrayList(logTail.getLog(loggingContext, allEvents.get(22).getLoggingEvent().getTimeStamp(),
                                        allEvents.get(38).getLoggingEvent().getTimeStamp(), Filter.EMPTY_FILTER));
    Assert.assertEquals(16, events.size());
    Assert.assertEquals(allEvents.get(22).getLoggingEvent().getFormattedMessage(),
                        events.get(0).getLoggingEvent().getFormattedMessage());
    Assert.assertEquals(allEvents.get(37).getLoggingEvent().getFormattedMessage(),
                        events.get(15).getLoggingEvent().getFormattedMessage());

    events =
      Lists.newArrayList(logTail.getLog(loggingContext, allEvents.get(41).getLoggingEvent().getTimeStamp(),
                                        allEvents.get(59).getLoggingEvent().getTimeStamp(), Filter.EMPTY_FILTER));
    Assert.assertEquals(18, events.size());
    Assert.assertEquals(allEvents.get(41).getLoggingEvent().getFormattedMessage(),
                        events.get(0).getLoggingEvent().getFormattedMessage());
    Assert.assertEquals(allEvents.get(58).getLoggingEvent().getFormattedMessage(),
                        events.get(17).getLoggingEvent().getFormattedMessage());

    // Try with null run id, should get all logs for WORKER_1
    LoggingContext loggingContext1 = new WorkerLoggingContext(""TFL_NS_1"", ""APP_1"", ""WORKER_1"", null, ""INSTANCE1"");
    events =
      Lists.newArrayList(logTail.getLog(loggingContext1, 0, Long.MAX_VALUE, Filter.EMPTY_FILTER));
    Assert.assertEquals(120, events.size());
  }
",non-flaky,5
35708,cdapio_cdap,CDAPLogAppenderTest.testCDAPLogAppender,"  @Test
  public void testCDAPLogAppender() {
    int syncInterval = 1024 * 1024;
    CDAPLogAppender cdapLogAppender = new CDAPLogAppender();

    cdapLogAppender.setSyncIntervalBytes(syncInterval);
    cdapLogAppender.setMaxFileLifetimeMs(TimeUnit.DAYS.toMillis(1));
    cdapLogAppender.setMaxFileSizeInBytes(104857600);
    cdapLogAppender.setDirPermissions(""700"");
    cdapLogAppender.setFilePermissions(""600"");
    cdapLogAppender.setFileRetentionDurationDays(1);
    cdapLogAppender.setLogCleanupIntervalMins(10);
    cdapLogAppender.setFileCleanupBatchSize(100);
    AppenderContext context = new LocalAppenderContext(injector.getInstance(TransactionRunner.class),
                                                       injector.getInstance(LocationFactory.class),
                                                       new NoOpMetricsCollectionService());
    context.start();
    cdapLogAppender.setContext(context);
    cdapLogAppender.start();

    FileMetaDataReader fileMetaDataReader = injector.getInstance(FileMetaDataReader.class);
    LoggingEvent event =
      new LoggingEvent(""io.cdap.Test"",
                       (ch.qos.logback.classic.Logger) LoggerFactory.getLogger(Logger.ROOT_LOGGER_NAME),
                       Level.ERROR , ""test message"", null, null);
    Map<String, String> properties = new HashMap<>();
    properties.put(NamespaceLoggingContext.TAG_NAMESPACE_ID, ""default"");
    properties.put(ApplicationLoggingContext.TAG_APPLICATION_ID, ""testApp"");
    properties.put(UserServiceLoggingContext.TAG_USER_SERVICE_ID, ""testService"");

    event.setMDCPropertyMap(properties);

    cdapLogAppender.doAppend(event);
    cdapLogAppender.stop();
    context.stop();

    try {
      List<LogLocation> files = fileMetaDataReader.listFiles(cdapLogAppender.getLoggingPath(properties),
                                                             0, Long.MAX_VALUE);
      Assert.assertEquals(1, files.size());
      LogLocation logLocation = files.get(0);
      Assert.assertEquals(LogLocation.VERSION_1, logLocation.getFrameworkVersion());
      Assert.assertTrue(logLocation.getLocation().exists());
      CloseableIterator<LogEvent> logEventCloseableIterator =
        logLocation.readLog(Filter.EMPTY_FILTER, 0, Long.MAX_VALUE, Integer.MAX_VALUE);
      int logCount = 0;
      while (logEventCloseableIterator.hasNext()) {
        logCount++;
        LogEvent logEvent = logEventCloseableIterator.next();
        Assert.assertEquals(event.getMessage(), logEvent.getLoggingEvent().getMessage());
      }
      logEventCloseableIterator.close();
      Assert.assertEquals(1, logCount);
      // checking permission
      String expectedPermissions = ""rw-------"";
      for (LogLocation file : files) {
        Location location = file.getLocation();
        Assert.assertEquals(expectedPermissions, location.getPermissions());
      }
    } catch (Exception e) {
      Assert.fail();
    }
  }
",non-flaky,5
35709,cdapio_cdap,CDAPLogAppenderTest.testCDAPLogAppenderRotation,"  @Test
  public void testCDAPLogAppenderRotation() throws Exception {
    int syncInterval = 1024 * 1024;
    FileMetaDataReader fileMetaDataReader = injector.getInstance(FileMetaDataReader.class);
    CDAPLogAppender cdapLogAppender = new CDAPLogAppender();
    AppenderContext context = new LocalAppenderContext(injector.getInstance(TransactionRunner.class),
                                                       injector.getInstance(LocationFactory.class),
                                                       new NoOpMetricsCollectionService());
    context.start();

    cdapLogAppender.setSyncIntervalBytes(syncInterval);
    cdapLogAppender.setMaxFileLifetimeMs(500);
    cdapLogAppender.setMaxFileSizeInBytes(104857600);
    cdapLogAppender.setDirPermissions(""750"");
    cdapLogAppender.setFilePermissions(""640"");
    cdapLogAppender.setFileRetentionDurationDays(1);
    cdapLogAppender.setLogCleanupIntervalMins(10);
    cdapLogAppender.setFileCleanupBatchSize(100);
    cdapLogAppender.setContext(context);
    cdapLogAppender.start();

    Map<String, String> properties = new HashMap<>();
    properties.put(NamespaceLoggingContext.TAG_NAMESPACE_ID, ""testTimeRotation"");
    properties.put(ApplicationLoggingContext.TAG_APPLICATION_ID, ""testApp"");
    properties.put(UserServiceLoggingContext.TAG_USER_SERVICE_ID, ""testService"");

    long currentTimeMillisEvent1 = System.currentTimeMillis();

    LoggingEvent event1 =
      getLoggingEvent(""io.cdap.Test1"",
                      (ch.qos.logback.classic.Logger) LoggerFactory.getLogger(Logger.ROOT_LOGGER_NAME),
                      Level.ERROR , ""test message 1"", properties);

    event1.setTimeStamp(currentTimeMillisEvent1);
    cdapLogAppender.doAppend(event1);

    // Pause pass the max file lifetime ms
    TimeUnit.MILLISECONDS.sleep(500);

    long currentTimeMillisEvent2 = System.currentTimeMillis();

    LoggingEvent event2 = getLoggingEvent(""io.cdap.Test2"",
                                          (ch.qos.logback.classic.Logger) LoggerFactory.getLogger(
                                            Logger.ROOT_LOGGER_NAME), Level.ERROR , ""test message 2"", properties);
    event2.setTimeStamp(currentTimeMillisEvent1 + 1000);
    cdapLogAppender.doAppend(event2);
    cdapLogAppender.stop();
    context.stop();

    try {
      List<LogLocation> files = fileMetaDataReader.listFiles(cdapLogAppender.getLoggingPath(properties),
                                                             0, Long.MAX_VALUE);
      Assert.assertEquals(2, files.size());
      assertLogEventDetails(event1, files.get(0));
      assertLogEventDetails(event2, files.get(1));
      Assert.assertEquals(currentTimeMillisEvent1, files.get(0).getEventTimeMs());
      Assert.assertEquals(currentTimeMillisEvent1 + 1000, files.get(1).getEventTimeMs());
      Assert.assertTrue(files.get(0).getFileCreationTimeMs() >= currentTimeMillisEvent1);
      Assert.assertTrue(files.get(1).getFileCreationTimeMs() >= currentTimeMillisEvent2);

      // checking permission
      String expectedPermissions = ""rw-r-----"";
      for (LogLocation file : files) {
        Location location = file.getLocation();
        Assert.assertEquals(expectedPermissions, location.getPermissions());
      }
    } catch (Exception e) {
      Assert.fail();
    }
  }
",non-flaky,5
35710,cdapio_cdap,CDAPLogAppenderTest.testCDAPLogAppenderSizeBasedRotation,"  @Test
  public void testCDAPLogAppenderSizeBasedRotation() throws Exception {
    int syncInterval = 1024 * 1024;
    FileMetaDataReader fileMetaDataReader = injector.getInstance(FileMetaDataReader.class);
    CDAPLogAppender cdapLogAppender = new CDAPLogAppender();
    AppenderContext context = new LocalAppenderContext(injector.getInstance(TransactionRunner.class),
                                                       injector.getInstance(LocationFactory.class),
                                                       new NoOpMetricsCollectionService());
    context.start();

    cdapLogAppender.setSyncIntervalBytes(syncInterval);
    cdapLogAppender.setMaxFileLifetimeMs(TimeUnit.DAYS.toMillis(1));
    cdapLogAppender.setMaxFileSizeInBytes(500);
    cdapLogAppender.setDirPermissions(""750"");
    cdapLogAppender.setFilePermissions(""640"");
    cdapLogAppender.setFileRetentionDurationDays(1);
    cdapLogAppender.setLogCleanupIntervalMins(10);
    cdapLogAppender.setFileCleanupBatchSize(100);
    cdapLogAppender.setContext(context);
    cdapLogAppender.start();

    Map<String, String> properties = new HashMap<>();
    properties.put(NamespaceLoggingContext.TAG_NAMESPACE_ID, ""testSizeRotation"");
    properties.put(ApplicationLoggingContext.TAG_APPLICATION_ID, ""testApp"");
    properties.put(UserServiceLoggingContext.TAG_USER_SERVICE_ID, ""testService"");

    long currentTimeMillisEvent1 = System.currentTimeMillis();

    LoggingEvent event1 =
      getLoggingEvent(""io.cdap.Test1"",
                      (ch.qos.logback.classic.Logger) LoggerFactory.getLogger(Logger.ROOT_LOGGER_NAME),
                      Level.ERROR , ""test message 1"", properties);

    event1.setTimeStamp(currentTimeMillisEvent1);
    cdapLogAppender.doAppend(event1);
    // sync updates the file size
    cdapLogAppender.sync();

    long currentTimeMillisEvent2 = System.currentTimeMillis();
    LoggingEvent event2 = getLoggingEvent(""io.cdap.Test2"",
                                          (ch.qos.logback.classic.Logger) LoggerFactory.getLogger(
                                            Logger.ROOT_LOGGER_NAME), Level.ERROR , ""test message 2"", properties);
    event2.setTimeStamp(currentTimeMillisEvent2);
    // one new append, we will rotate to new file as the file size limit is very low and last append exceeded that.
    cdapLogAppender.doAppend(event2);
    cdapLogAppender.stop();
    context.stop();

    try {
      List<LogLocation> files = fileMetaDataReader.listFiles(cdapLogAppender.getLoggingPath(properties),
                                                             0, Long.MAX_VALUE);
      Assert.assertEquals(2, files.size());
      assertLogEventDetails(event1, files.get(0));
      assertLogEventDetails(event2, files.get(1));
      Assert.assertEquals(currentTimeMillisEvent1, files.get(0).getEventTimeMs());
      Assert.assertEquals(currentTimeMillisEvent2, files.get(1).getEventTimeMs());
      Assert.assertTrue(files.get(0).getFileCreationTimeMs() >= currentTimeMillisEvent1);
      Assert.assertTrue(files.get(1).getFileCreationTimeMs() >= currentTimeMillisEvent2);
    } catch (Exception e) {
      Assert.fail();
    }
  }
",non-flaky,5
35711,cdapio_cdap,LogFileManagerTest.testLogFileManager,"  @Test
  public void testLogFileManager() throws Exception {
    int syncInterval = 1024 * 1024;
    long maxLifeTimeMs = 50;
    long maxFileSizeInBytes = 104857600;
    FileMetaDataWriter fileMetaDataWriter = new FileMetaDataWriter(injector.getInstance(TransactionRunner.class));
    LogFileManager logFileManager = new LogFileManager(""700"", ""600"", maxLifeTimeMs, maxFileSizeInBytes, syncInterval,
                                                       fileMetaDataWriter,
                                                       injector.getInstance(LocationFactory.class));
    LogPathIdentifier logPathIdentifier = new LogPathIdentifier(""test"", ""testApp"", ""testFlow"");
    long timestamp = System.currentTimeMillis();
    LogFileOutputStream outputStream = logFileManager.getLogFileOutputStream(logPathIdentifier, timestamp);
    LoggingEvent event1 =
      getLoggingEvent(""io.cdap.Test1"",
                      (ch.qos.logback.classic.Logger) LoggerFactory.getLogger(Logger.ROOT_LOGGER_NAME),
                      Level.ERROR , ""test message 1"");
    outputStream.append(event1);
    // we are doing this, instead of calling getLogFileOutputStream to avoid race, if test machine can be slow.
    Assert.assertNotNull((logFileManager.getActiveOutputStream(logPathIdentifier)));
    TimeUnit.MILLISECONDS.sleep(60);
    logFileManager.flush();
    // should be closed on flush, should return null
    Assert.assertNull((logFileManager.getActiveOutputStream(logPathIdentifier)));
    LogFileOutputStream newLogOutStream = logFileManager.getLogFileOutputStream(logPathIdentifier, timestamp);
    // make sure the new location we got is different
    Assert.assertNotEquals(outputStream.getLocation(), newLogOutStream.getLocation());
  }
",non-flaky,5
35712,cdapio_cdap,TestDistributedLogReader.testDistributedLogPrevBoth,"  @Test
  public void testDistributedLogPrevBoth() throws Exception {
    ReadRange readRange = new ReadRange(0, Long.MAX_VALUE, LogOffset.INVALID_KAFKA_OFFSET);
    testDistributedLogPrev(readRange, LOGGING_CONTEXT_BOTH, 16, 4, ""TestDistributedLogReader Log message1 "", 60);

    readRange = new ReadRange(System.currentTimeMillis() - TimeUnit.DAYS.toMillis(1),
                                        System.currentTimeMillis(), LogOffset.INVALID_KAFKA_OFFSET);
    testDistributedLogPrev(readRange, LOGGING_CONTEXT_BOTH, 16, 4, ""TestDistributedLogReader Log message1 "", 60);

    testDistributedLogPrev(ReadRange.LATEST, LOGGING_CONTEXT_BOTH, 9, 8, ""TestDistributedLogReader Log message1 "", 60);
  }
",non-flaky,5
35713,cdapio_cdap,TestDistributedLogReader.testDistributedLogNextBoth,"  @Test
  public void testDistributedLogNextBoth() throws Exception {
    ReadRange readRange = new ReadRange(0, Long.MAX_VALUE, LogOffset.INVALID_KAFKA_OFFSET);
    testDistributedLogNext(readRange, LOGGING_CONTEXT_BOTH, 20, 3, ""TestDistributedLogReader Log message1 "", 60, 0);

    readRange = new ReadRange(System.currentTimeMillis() - TimeUnit.DAYS.toMillis(1),
                              System.currentTimeMillis(), LogOffset.INVALID_KAFKA_OFFSET);
    testDistributedLogNext(readRange, LOGGING_CONTEXT_BOTH, 20, 3, ""TestDistributedLogReader Log message1 "", 60, 0);

    testDistributedLogNext(ReadRange.LATEST, LOGGING_CONTEXT_BOTH, 1, 3,
                           ""TestDistributedLogReader Log message1 "", 3, 57);
  }
",non-flaky,5
35714,cdapio_cdap,TestDistributedLogReader.testDistributedLogPrevFile,"  @Test
  public void testDistributedLogPrevFile() throws Exception {
    ReadRange readRange = new ReadRange(0, Long.MAX_VALUE, LogOffset.INVALID_KAFKA_OFFSET);
    testDistributedLogPrev(readRange, LOGGING_CONTEXT_FILE, 7, 6, ""TestDistributedLogReader Log message2 "", 40);

    readRange = new ReadRange(System.currentTimeMillis() - TimeUnit.DAYS.toMillis(1),
                                        System.currentTimeMillis(), LogOffset.INVALID_KAFKA_OFFSET);

    testDistributedLogPrev(readRange, LOGGING_CONTEXT_FILE, 7, 6, ""TestDistributedLogReader Log message2 "", 40);

    testDistributedLogPrev(ReadRange.LATEST, LOGGING_CONTEXT_FILE, 7, 6, ""TestDistributedLogReader Log message2 "", 40);
  }
",non-flaky,5
35715,cdapio_cdap,TestDistributedLogReader.testDistributedLogNextFile,"  @Test
  public void testDistributedLogNextFile() throws Exception {
    ReadRange readRange = new ReadRange(0, Long.MAX_VALUE, LogOffset.INVALID_KAFKA_OFFSET);

    testDistributedLogNext(readRange, LOGGING_CONTEXT_FILE, 14, 3, ""TestDistributedLogReader Log message2 "", 40, 0);

    readRange = new ReadRange(System.currentTimeMillis() - TimeUnit.DAYS.toMillis(1),
                              System.currentTimeMillis(), LogOffset.INVALID_KAFKA_OFFSET);
    testDistributedLogNext(readRange, LOGGING_CONTEXT_FILE, 14, 3, ""TestDistributedLogReader Log message2 "", 40, 0);

    testDistributedLogNext(ReadRange.LATEST, LOGGING_CONTEXT_FILE, 1, 5,
                           ""TestDistributedLogReader Log message2 "", 5, 35);
  }
",non-flaky,5
35716,cdapio_cdap,TestDistributedLogReader.testDistributedLogPrevKafka,"  @Test
  public void testDistributedLogPrevKafka() throws Exception {
    ReadRange readRange = new ReadRange(0, Long.MAX_VALUE, LogOffset.INVALID_KAFKA_OFFSET);
    testDistributedLogPrev(readRange, LOGGING_CONTEXT_KAFKA, 5, 6, ""TestDistributedLogReader Log message3 "", 30);

    readRange = new ReadRange(System.currentTimeMillis() - TimeUnit.DAYS.toMillis(1),
                                        System.currentTimeMillis(), LogOffset.INVALID_KAFKA_OFFSET);

    testDistributedLogPrev(readRange, LOGGING_CONTEXT_KAFKA, 5, 6, ""TestDistributedLogReader Log message3 "", 30);

    testDistributedLogPrev(ReadRange.LATEST, LOGGING_CONTEXT_KAFKA, 5, 6, ""TestDistributedLogReader Log message3 "", 30);
  }
",non-flaky,5
35717,cdapio_cdap,TestDistributedLogReader.testDistributedLogNextKafka,"  @Test
  public void testDistributedLogNextKafka() throws Exception {
    ReadRange readRange = new ReadRange(0, Long.MAX_VALUE, LogOffset.INVALID_KAFKA_OFFSET);
    testDistributedLogNext(readRange, LOGGING_CONTEXT_KAFKA, 10, 3, ""TestDistributedLogReader Log message3 "", 30, 0);

    readRange = new ReadRange(System.currentTimeMillis() - TimeUnit.DAYS.toMillis(1),
                              System.currentTimeMillis(), LogOffset.INVALID_KAFKA_OFFSET);
    testDistributedLogNext(readRange, LOGGING_CONTEXT_KAFKA, 10, 3, ""TestDistributedLogReader Log message3 "", 30, 0);

    testDistributedLogNext(ReadRange.LATEST, LOGGING_CONTEXT_KAFKA, 1, 8,
                           ""TestDistributedLogReader Log message3 "", 8, 22);
  }
",non-flaky,5
35718,cdapio_cdap,TestKafkaLogging.testGetNext,"  @Test
  public void testGetNext() throws Exception {
    // Check with null runId and null instanceId
    LoggingContext loggingContext = new WorkerLoggingContext(""TKL_NS_1"", ""APP_1"", ""FLOW_1"", ""RUN1"", ""INSTANCE1"");
    KafkaLogReader logReader = KAFKA_TESTER.getInjector().getInstance(KafkaLogReader.class);
    LoggingTester tester = new LoggingTester();
    tester.testGetNext(logReader, loggingContext);
  }
",non-flaky,5
35719,cdapio_cdap,TestKafkaLogging.testGetPrev,"  @Test
  public void testGetPrev() throws Exception {
    LoggingContext loggingContext = new WorkerLoggingContext(""TKL_NS_1"", ""APP_1"", ""FLOW_1"", ""RUN1"", ""INSTANCE1"");
    KafkaLogReader logReader = KAFKA_TESTER.getInjector().getInstance(KafkaLogReader.class);
    LoggingTester tester = new LoggingTester();
    tester.testGetPrev(logReader, loggingContext);
  }
",non-flaky,5
35720,cdapio_cdap,TestKafkaLogging.apply,"  @Test
  public void testPartitionKey() throws Exception {
    CConfiguration cConf = KAFKA_TESTER.getCConf();
    // set kafka partition key to application
    cConf.set(Constants.Logging.LOG_PUBLISH_PARTITION_KEY, ""application"");

    Logger logger = LoggerFactory.getLogger(""TestKafkaLogging"");
    LoggingContext loggingContext = new WorkerLoggingContext(""TKL_NS_2"", ""APP_2"", ""FLOW_2"", ""RUN2"", ""INSTANCE2"");
    LoggingContextAccessor.setLoggingContext(loggingContext);
    for (int i = 0; i < 40; ++i) {
      logger.warn(""TKL_NS_2 Test log message {} {} {}"", i, ""arg1"", ""arg2"", new Exception(""test exception""));
    }

    loggingContext = new WorkerLoggingContext(""TKL_NS_2"", ""APP_2"", ""FLOW_3"", ""RUN3"", ""INSTANCE3"");
    LoggingContextAccessor.setLoggingContext(loggingContext);
    for (int i = 0; i < 40; ++i) {
      logger.warn(""TKL_NS_2 Test log message {} {} {}"", i, ""arg1"", ""arg2"", new Exception(""test exception""));
    }

    final Multimap<Integer, String> actual = ArrayListMultimap.create();

    KAFKA_TESTER.getPublishedMessages(KAFKA_TESTER.getCConf().get(Constants.Logging.KAFKA_TOPIC),
                                      ImmutableSet.of(0, 1), 40, new Function<FetchedMessage, String>() {
        @Override
        public String apply(final FetchedMessage input) {
          try {
            Map.Entry<Integer, String> entry = convertFetchedMessage(input);
            actual.put(entry.getKey(), entry.getValue());
          } catch (IOException e) {
            // should never happen
          }
          return """";
        }
",non-flaky,5
35721,cdapio_cdap,LocalLogAppenderResilientTest.addStatusEvent,"  @Test
  public void testResilientLogging() throws Exception {
    Configuration hConf = new Configuration();
    CConfiguration cConf = CConfiguration.create();

    File datasetDir = new File(tmpFolder.newFolder(), ""datasetUser"");
    //noinspection ResultOfMethodCallIgnored
    datasetDir.mkdirs();

    cConf.set(Constants.Dataset.Manager.OUTPUT_DIR, datasetDir.getAbsolutePath());
    cConf.set(Constants.Service.MASTER_SERVICES_BIND_ADDRESS, ""localhost"");

    cConf.set(Constants.Dataset.Executor.ADDRESS, ""localhost"");
    cConf.setInt(Constants.Dataset.Executor.PORT, Networks.getRandomPort());

    cConf.set(Constants.CFG_LOCAL_DATA_DIR, tmpFolder.newFolder().getAbsolutePath());

    Injector injector = Guice.createInjector(
      new ConfigModule(cConf, hConf),
      new IOModule(),
      new ZKClientModule(),
      new KafkaClientModule(),
      new InMemoryDiscoveryModule(),
      new NonCustomLocationUnitTestModule(),
      new DataFabricModules().getInMemoryModules(),
      new DataSetsModules().getStandaloneModules(),
      new DataSetServiceModules().getInMemoryModules(),
      new TransactionMetricsModule(),
      new ExploreClientModule(),
      new LocalLogAppenderModule(),
      new NamespaceAdminTestModule(),
      new AuthorizationTestModule(),
      new AuthorizationEnforcementModule().getInMemoryModules(),
      new AuthenticationContextModules().getMasterModule(),
      new AbstractModule() {
        @Override
        protected void configure() {
          bind(UGIProvider.class).to(UnsupportedUGIProvider.class);
          bind(OwnerAdmin.class).to(NoOpOwnerAdmin.class);
          bind(MetadataServiceClient.class).to(NoOpMetadataServiceClient.class);
        }
      });

    TransactionManager txManager = injector.getInstance(TransactionManager.class);
    txManager.startAndWait();
    StructuredTableRegistry structuredTableRegistry = injector.getInstance(StructuredTableRegistry.class);
    structuredTableRegistry.initialize();
    StoreDefinition.createAllTables(injector.getInstance(StructuredTableAdmin.class), structuredTableRegistry);

    DatasetOpExecutorService opExecutorService = injector.getInstance(DatasetOpExecutorService.class);
    opExecutorService.startAndWait();

    // Start the logging before starting the service.
    LoggingContextAccessor.setLoggingContext(new WorkerLoggingContext(""TRL_ACCT_1"", ""APP_1"", ""WORKER_1"",
                                                                      ""RUN"", ""INSTANCE""));
    String logBaseDir = ""trl-log/log_files_"" + new Random(System.currentTimeMillis()).nextLong();

    cConf.set(LoggingConfiguration.LOG_BASE_DIR, logBaseDir);
    cConf.setInt(LoggingConfiguration.LOG_MAX_FILE_SIZE_BYTES, 20 * 1024);
    final LogAppender appender = injector.getInstance(LocalLogAppender.class);
    new LogAppenderInitializer(appender).initialize(""TestResilientLogging"");

    int failureMsgCount = 3;
    final CountDownLatch failureLatch = new CountDownLatch(failureMsgCount);
    LoggerContext loggerContext = (LoggerContext) LoggerFactory.getILoggerFactory();
    loggerContext.getStatusManager().add(new StatusListener() {
      @Override
      public void addStatusEvent(Status status) {
        if (status.getLevel() != Status.ERROR || status.getOrigin() != appender) {
          return;
        }
        Throwable cause = status.getThrowable();
        if (cause != null) {
          Throwable rootCause = Throwables.getRootCause(cause);
          if (rootCause instanceof ServiceUnavailableException) {
            String serviceName = ((ServiceUnavailableException) rootCause).getServiceName();
            if (Constants.Service.DATASET_MANAGER.equals(serviceName)) {
              failureLatch.countDown();
            }
          }
        }
      }
",non-flaky,5
35722,cdapio_cdap,LoggingContextMDCTest.testMDC,"  @Test
  public void testMDC() {
    LoggingContext context = new TestLoggingContext(""namespace"", ""app"", ""run"", ""instance"");

    // Put an entry in the user mdc. It shouldn't override what's in the system tags.
    Map<String, String> userMDC = new HashMap<>();
    userMDC.put(Constants.Logging.TAG_APPLICATION_ID, ""userApp"");

    Map<String, String> mdc = new LoggingContextMDC(context.getSystemTagsAsString(), userMDC);

    Assert.assertEquals(4, mdc.size());

    Map<String, String> copiedMDC = new HashMap<>();
    for (Map.Entry<String, String> entry : mdc.entrySet()) {
      copiedMDC.put(entry.getKey(), entry.getValue());
    }

    Assert.assertEquals(4, copiedMDC.size());
    Assert.assertEquals(""namespace"", copiedMDC.get(Constants.Logging.TAG_NAMESPACE_ID));
    Assert.assertEquals(""app"", copiedMDC.get(Constants.Logging.TAG_APPLICATION_ID));
    Assert.assertEquals(""run"", copiedMDC.get(Constants.Logging.TAG_RUN_ID));
    Assert.assertEquals(""instance"", copiedMDC.get(Constants.Logging.TAG_INSTANCE_ID));

    // Should be able to set user property
    mdc.put(""user"", ""test"");
    Assert.assertEquals(5, mdc.size());
    Assert.assertEquals(5, mdc.entrySet().size());

    // This should fail with exception
    try {
      mdc.put(Constants.Logging.TAG_APPLICATION_ID, ""newApp"");
      Assert.fail();
    } catch (IllegalArgumentException e) {
      // expected
    }
  }
",non-flaky,5
35723,cdapio_cdap,LoggingContextAccessorTest.run,"  @Test
  public void testReset() {
    Cancellable cancellable = LoggingContextAccessor.setLoggingContext(
      new GenericLoggingContext(OLD_NS, OLD_APP, OLD_ENTITY));
    Assert.assertEquals(MDC.get(NamespaceLoggingContext.TAG_NAMESPACE_ID), OLD_NS);
    Assert.assertEquals(MDC.get(ApplicationLoggingContext.TAG_APPLICATION_ID), OLD_APP);
    Assert.assertEquals(MDC.get(GenericLoggingContext.TAG_ENTITY_ID), OLD_ENTITY);

    final Cancellable cancellable2 =
      LoggingContextAccessor.setLoggingContext(new GenericLoggingContext(NS, APP, ENTITY));

    Assert.assertEquals(MDC.get(NamespaceLoggingContext.TAG_NAMESPACE_ID), NS);
    Assert.assertEquals(MDC.get(ApplicationLoggingContext.TAG_APPLICATION_ID), APP);
    Assert.assertEquals(MDC.get(GenericLoggingContext.TAG_ENTITY_ID), ENTITY);

    // Verify a different thread cannot change context
    Thread thread = new Thread(new Runnable() {
      @Override
      public void run() {
        cancellable2.cancel();
      }
",non-flaky,5
35724,cdapio_cdap,LogAppenderInitializerTest.testConfigUpdate,"  @Test (timeout = 10000L)
  public void testConfigUpdate() throws Exception {
    File logbackFile = createLogbackFile(new File(TEMP_FOLDER.newFolder(), ""logback.xml""), """");

    // Create a logger context from the generated logback.xml file
    LoggerContext loggerContext = (LoggerContext) LoggerFactory.getILoggerFactory();
    loggerContext.reset();

    JoranConfigurator configurator = new JoranConfigurator();
    configurator.setContext(loggerContext);
    configurator.doConfigure(logbackFile);

    TestLogAppender testAppender = new TestLogAppender();
    testAppender.setName(""TestAppender"");
    LogAppenderInitializer initializer = new LogAppenderInitializer(testAppender);
    initializer.initialize();

    LoggingContextAccessor.setLoggingContext(new TestLoggingContext(""ns"", ""app"", ""run"", ""instance""));

    Logger logger = loggerContext.getLogger(LogAppenderInitializerTest.class);
    logger.info(""Testing"");

    Assert.assertEquals(""Testing"", testAppender.getLastMessage());

    // Update the logback file
    createLogbackFile(logbackFile, ""<logger name=\""io.cdap.cdap\"" level=\""INFO\"" />"");

    // Wait till the test appender stop() is called. This will happen when logback detected the changes.
    while (!testAppender.isStoppedOnce()) {
      // We need to keep logging because there is some internal thresold in logback implementation to trigger the
      // config reload thread.
      logger.info(""Waiting stop"");
      TimeUnit.MILLISECONDS.sleep(150);
      // Update the last modified time of the logback file to make sure logback can pick it with.
      logbackFile.setLastModified(System.currentTimeMillis());
    }

    // The appender should get automatically started again
    Tasks.waitFor(true, testAppender::isStarted, 5, TimeUnit.SECONDS);
    logger.info(""Reattached"");

    Assert.assertEquals(""Reattached"", testAppender.getLastMessage());
    loggerContext.stop();
  }
",non-flaky,5
35725,cdapio_cdap,TestTMSLogging.testTmsLogAppender,"  @Test
  public void testTmsLogAppender() throws Exception {
    // setup TMSLogAppender and log messages to it
    LogAppenderInitializer logAppenderInitializer = new LogAppenderInitializer(tmsLogAppender);
    logAppenderInitializer.initialize(""TestTMSLogging"");

    Logger logger = LoggerFactory.getLogger(""TestTMSLogging"");
    LoggingTester loggingTester = new LoggingTester();

    LoggingContext loggingContext = new MapReduceLoggingContext(""TKL_NS_1"", ""APP_1"", ""MR_1"", ""RUN1"");
    loggingTester.generateLogs(logger, loggingContext);

    logAppenderInitializer.close();

    // fetch and deserialize all the logs from TMS
    LoggingEventSerializer loggingEventSerializer = new LoggingEventSerializer();

    Map<Integer, List<ILoggingEvent>> partitionedFetchedLogs = new HashMap<>();
    int totalFetchedLogs = 0;

    for (Map.Entry<Integer, TopicId> topicId : topicIds.entrySet()) {
      List<ILoggingEvent> fetchedLogs = new ArrayList<>();
      MessageFetcher messageFetcher = client.prepareFetch(topicId.getValue());
      try (CloseableIterator<RawMessage> messages = messageFetcher.fetch()) {
        while (messages.hasNext()) {
          RawMessage message = messages.next();
          ILoggingEvent iLoggingEvent = loggingEventSerializer.fromBytes(ByteBuffer.wrap(message.getPayload()));
          fetchedLogs.add(iLoggingEvent);
        }
      }

      totalFetchedLogs += fetchedLogs.size();
      partitionedFetchedLogs.put(topicId.getKey(), fetchedLogs);
    }

    // LoggingTester emits 240 logs in total
    Assert.assertEquals(240, totalFetchedLogs);

    // Read the partition that our LoggingContext maps to and filter the logs in there to the logs that correspond
    // to our LoggingContext.
    LogPartitionType logPartitionType =
            LogPartitionType.valueOf(cConf.get(Constants.Logging.LOG_PUBLISH_PARTITION_KEY).toUpperCase());
    String partitionKey = logPartitionType.getPartitionKey(loggingContext);
    int partition = TMSLogAppender.partition(partitionKey, cConf.getInt(Constants.Logging.NUM_PARTITIONS));
    Filter logFilter = LoggingContextHelper.createFilter(loggingContext);

    List<ILoggingEvent> filteredLogs =
            partitionedFetchedLogs.get(partition).stream().filter(logFilter::match).collect(Collectors.toList());

    // LoggingTester emits 60 logs with the given LoggingContext
    Assert.assertEquals(60, filteredLogs.size());

    for (int i = 0; i < filteredLogs.size(); i++) {
      ILoggingEvent loggingEvent = filteredLogs.get(i);
      Assert.assertEquals(String.format(""Test log message %s arg1 arg2"", i), loggingEvent.getFormattedMessage());
    }
  }
",non-flaky,5
35726,cdapio_cdap,RollingLocationLogAppenderTest.testRollingLocationLogAppender,"  @Test
  public void testRollingLocationLogAppender() throws Exception {
    // assume SLF4J is bound to logback in the current environment
    AppenderContext appenderContext = new LocalAppenderContext(injector.getInstance(TransactionRunner.class),
                                                               injector.getInstance(LocationFactory.class),
                                                               new NoOpMetricsCollectionService());

    JoranConfigurator configurator = new JoranConfigurator();
    configurator.setContext(appenderContext);
    // Call context.reset() to clear any previous configuration, e.g. default
    // configuration. For multi-step configuration, omit calling context.reset().
    appenderContext.reset();

    configurator.doConfigure(getClass().getResourceAsStream(""/rolling-appender-logback-test.xml""));
    StatusPrinter.printInCaseOfErrorsOrWarnings(appenderContext);

    RollingLocationLogAppender rollingAppender =
      (RollingLocationLogAppender) appenderContext.getLogger(RollingLocationLogAppenderTest.class)
        .getAppender(""rollingAppender"");

    addTagsToMdc(""testNamespace"", ""testApp"");
    Logger logger = appenderContext.getLogger(RollingLocationLogAppenderTest.class);
    ingestLogs(logger, 5);
    Map<LocationIdentifier, LocationOutputStream> activeFiles = rollingAppender.getLocationManager()
      .getActiveLocations();
    Assert.assertEquals(1, activeFiles.size());
    verifyFileOutput(activeFiles, 5);

    // different program should go to different directory
    addTagsToMdc(""testNamespace"", ""testApp1"");
    ingestLogs(logger, 5);
    activeFiles = rollingAppender.getLocationManager().getActiveLocations();
    Assert.assertEquals(2, activeFiles.size());
    verifyFileOutput(activeFiles, 5);

    // different program should go to different directory because namespace is different
    addTagsToMdc(""testNamespace1"", ""testApp1"");
    ingestLogs(logger, 5);
    activeFiles = rollingAppender.getLocationManager().getActiveLocations();
    Assert.assertEquals(3, activeFiles.size());
    verifyFileOutput(activeFiles, 5);
  }
",non-flaky,5
35727,cdapio_cdap,RollingLocationLogAppenderTest.testRollOver,"  @Test
  public void testRollOver() throws Exception {
    // assume SLF4J is bound to logback in the current environment
    AppenderContext appenderContext = new LocalAppenderContext(injector.getInstance(TransactionRunner.class),
                                                               injector.getInstance(LocationFactory.class),
                                                               new NoOpMetricsCollectionService());

    JoranConfigurator configurator = new JoranConfigurator();
    configurator.setContext(appenderContext);
    // Call context.reset() to clear any previous configuration, e.g. default
    // configuration. For multi-step configuration, omit calling context.reset().
    appenderContext.reset();

    configurator.doConfigure(getClass().getResourceAsStream(""/rolling-appender-logback-test.xml""));
    StatusPrinter.printInCaseOfErrorsOrWarnings(appenderContext);

    RollingLocationLogAppender rollingAppender =
      (RollingLocationLogAppender) appenderContext.getLogger(RollingLocationLogAppenderTest.class)
        .getAppender(""rollingAppender"");

    addTagsToMdc(""testNs"", ""testApp"");
    Logger logger = appenderContext.getLogger(RollingLocationLogAppenderTest.class);
    ingestLogs(logger, 20000);
    Map<LocationIdentifier, LocationOutputStream> activeFiles = rollingAppender.getLocationManager()
      .getActiveLocations();
    Assert.assertEquals(1, activeFiles.size());
    LocationOutputStream locationOutputStream = activeFiles.get(new LocationIdentifier(""testNs"", ""testApp""));
    Location parentDir = Locations.getParent(locationOutputStream.getLocation());
    Assert.assertEquals(10, parentDir.list().size());

    // different program should go to different directory
    addTagsToMdc(""testNs"", ""testApp1"");
    ingestLogs(logger, 20000);
    activeFiles = rollingAppender.getLocationManager().getActiveLocations();
    Assert.assertEquals(2, activeFiles.size());
    locationOutputStream = activeFiles.get(new LocationIdentifier(""testNs"", ""testApp1""));
    parentDir = Locations.getParent(locationOutputStream.getLocation());
    Assert.assertEquals(10, parentDir.list().size());

    // different program should go to different directory because namespace is different
    addTagsToMdc(""testNs1"", ""testApp1"");
    ingestLogs(logger, 20000);
    activeFiles = rollingAppender.getLocationManager().getActiveLocations();
    Assert.assertEquals(3, activeFiles.size());
    locationOutputStream = activeFiles.get(new LocationIdentifier(""testNs1"", ""testApp1""));
    parentDir = Locations.getParent(locationOutputStream.getLocation());
    Assert.assertEquals(10, parentDir.list().size());
  }
",non-flaky,5
35728,cdapio_cdap,RollingLocationLogAppenderTest.testFileClose,"  @Test
  public void testFileClose() throws Exception {
    // assume SLF4J is bound to logback in the current environment
    AppenderContext appenderContext = new LocalAppenderContext(injector.getInstance(TransactionRunner.class),
                                                               injector.getInstance(LocationFactory.class),
                                                               new NoOpMetricsCollectionService());

    JoranConfigurator configurator = new JoranConfigurator();
    configurator.setContext(appenderContext);
    // Call context.reset() to clear any previous configuration, e.g. default
    // configuration. For multi-step configuration, omit calling context.reset().
    appenderContext.reset();

    configurator.doConfigure(getClass().getResourceAsStream(""/rolling-appender-logback-test.xml""));
    StatusPrinter.printInCaseOfErrorsOrWarnings(appenderContext);

    RollingLocationLogAppender rollingAppender =
      (RollingLocationLogAppender) appenderContext.getLogger(RollingLocationLogAppenderTest.class)
        .getAppender(""rollingAppender"");

    addTagsToMdc(""testNs"", ""testApp"");
    Logger logger = appenderContext.getLogger(RollingLocationLogAppenderTest.class);
    ingestLogs(logger, 20);

    // wait for 500 ms so that file is eligible for closing
    Thread.sleep(500);
    // flush to make sure file is closed
    rollingAppender.flush();
    Assert.assertEquals(0, rollingAppender.getLocationManager().getActiveLocations().size());
  }
",non-flaky,5
35729,cdapio_cdap,MetricsAdminSubscriberServiceTest.test,"  @Test
  public void test() throws Exception {
    MetricsAdminSubscriberService adminService = injector.getInstance(MetricsAdminSubscriberService.class);
    adminService.startAndWait();

    // publish a metrics
    MetricsContext metricsContext = metricsCollectionService.getContext(
      Collections.singletonMap(Constants.Metrics.Tag.NAMESPACE, NamespaceId.SYSTEM.getNamespace()));
    metricsContext.increment(""test.increment"", 10L);
    metricsContext.gauge(""test.gauge"", 20L);

    MetricsSystemClient systemClient = injector.getInstance(RemoteMetricsSystemClient.class);

    // Search for metrics names
    Tasks.waitFor(true, () -> {
      Set<String> names = new HashSet<>(systemClient.search(metricsContext.getTags()));
      return names.contains(""system.test.increment"") && names.contains(""system.test.gauge"");
    }, 10, TimeUnit.SECONDS, 1, TimeUnit.SECONDS);

    // Query for metrics values
    Tasks.waitFor(true, () -> {
      Collection<MetricTimeSeries> values = systemClient.query(metricsContext.getTags(),
                                                               Arrays.asList(""system.test.increment"",
                                                                             ""system.test.gauge""));
      // Find and match the values for the increment and gauge
      boolean incMatched = values.stream()
        .filter(timeSeries -> timeSeries.getMetricName().equals(""system.test.increment""))
        .flatMap(timeSeries -> timeSeries.getTimeValues().stream())
        .findFirst()
        .filter(timeValue -> timeValue.getValue() == 10L)
        .isPresent();

      boolean gaugeMatched = values.stream()
        .filter(timeSeries -> timeSeries.getMetricName().equals(""system.test.gauge""))
        .flatMap(timeSeries -> timeSeries.getTimeValues().stream())
        .findFirst()
        .filter(timeValue -> timeValue.getValue() == 20L)
        .isPresent();

      return incMatched && gaugeMatched;
    }, 10, TimeUnit.SECONDS, 1, TimeUnit.SECONDS);

    // Emit more metrics
    metricsContext.increment(""test.increment"", 40L);
    metricsContext.gauge(""test.gauge"", 40L);

    // Query for metrics values. Should see the latest aggregates
    Tasks.waitFor(true, () -> {
      Collection<MetricTimeSeries> values = systemClient.query(metricsContext.getTags(),
                                                               Arrays.asList(""system.test.increment"",
                                                                             ""system.test.gauge""));
      // Find and match the values for the increment and gauge
      boolean incMatched = values.stream()
        .filter(timeSeries -> timeSeries.getMetricName().equals(""system.test.increment""))
        .flatMap(timeSeries -> timeSeries.getTimeValues().stream())
        .findFirst()
        .filter(timeValue -> timeValue.getValue() == 50L)
        .isPresent();

      boolean gaugeMatched = values.stream()
        .filter(timeSeries -> timeSeries.getMetricName().equals(""system.test.gauge""))
        .flatMap(timeSeries -> timeSeries.getTimeValues().stream())
        .findFirst()
        .filter(timeValue -> timeValue.getValue() == 40L)
        .isPresent();

      return incMatched && gaugeMatched;
    }, 10, TimeUnit.SECONDS, 1, TimeUnit.SECONDS);


    // Delete the increment metrics
    systemClient.delete(new MetricDeleteQuery(0, Integer.MAX_VALUE,
                                              Collections.emptySet(),
                                              metricsContext.getTags(),
                                              new ArrayList<>(metricsContext.getTags().keySet())));

    Tasks.waitFor(true, () -> {
      Collection<MetricTimeSeries> values = systemClient.query(metricsContext.getTags(),
                                                               Arrays.asList(""system.test.increment"",
                                                                             ""system.test.gauge""));
      // increment should be missing
      boolean foundInc = values.stream()
        .anyMatch(timeSeries -> timeSeries.getMetricName().equals(""system.test.increment""));

      // Find and match the values for gauge
      boolean foundGauge = values.stream()
        .anyMatch(timeSeries -> timeSeries.getMetricName().equals(""system.test.gauge""));

      return !foundInc && !foundGauge;
    }, 1000, TimeUnit.SECONDS, 1, TimeUnit.SECONDS);

    adminService.stopAndWait();
  }
",non-flaky,5
35730,cdapio_cdap,MessagingMetricsProcessorManagerServiceTest.persistMetricsTests,"  @Test
  public void persistMetricsTests() throws Exception {

    injector.getInstance(TransactionManager.class).startAndWait();
    StructuredTableRegistry structuredTableRegistry = injector.getInstance(StructuredTableRegistry.class);
    structuredTableRegistry.initialize();
    StoreDefinition.createAllTables(injector.getInstance(StructuredTableAdmin.class), structuredTableRegistry);
    injector.getInstance(DatasetOpExecutorService.class).startAndWait();
    injector.getInstance(DatasetService.class).startAndWait();

    Set<Integer> partitions = IntStream.range(0, cConf.getInt(Constants.Metrics.MESSAGING_TOPIC_NUM))
      .boxed().collect(Collectors.toSet());

    long startTime = TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis());

    for (int iteration = 0; iteration < 50; iteration++) {
      // First publish all metrics before MessagingMetricsProcessorManagerService starts, so that fetchers of
      // different topics
      // will fetch metrics concurrently.
      for (int i = 0; i < 50; i++) {
        // TOPIC_PREFIX + (i % PARTITION_SIZE) decides which topic the metric is published to
        publishMessagingMetrics(i, startTime, METRICS_CONTEXT, expected, """", MetricType.COUNTER);
      }
      for (int i = 50; i < 100; i++) {
        // TOPIC_PREFIX + (i % PARTITION_SIZE) decides which topic the metric is published to
        publishMessagingMetrics(i, startTime, METRICS_CONTEXT, expected, """", MetricType.GAUGE);
      }

      final MockMetricStore metricStore = new MockMetricStore();
      // Create new MessagingMetricsProcessorManagerService instance every time because the same instance cannot be
      // started
      // again after it's stopped
      MessagingMetricsProcessorManagerService messagingMetricsProcessorManagerService =
        new MessagingMetricsProcessorManagerService(cConf, injector.getInstance(MetricDatasetFactory.class),
                                                    messagingService,
                                                    injector.getInstance(SchemaGenerator.class),
                                                    injector.getInstance(DatumReaderFactory.class), metricStore,
                                                    injector.getInstance(MetricsWriterProvider.class),
                                                    partitions, new NoopMetricsContext(), 50, 0);
      messagingMetricsProcessorManagerService.startAndWait();

      // Wait for the 1 aggregated counter metric (with value 50) and 50 gauge metrics to be stored in the metricStore
      Tasks.waitFor(51, () -> metricStore.getAllMetrics().size(), 15, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);

      assertMetricsResult(expected, metricStore.getAllMetrics());

      // validate metrics processor metrics
      // 50 counter and 50 gauge metrics are emitted in each iteration above
      Tasks.waitFor(100L, () -> metricStore.getMetricsProcessedByMetricsProcessor(),
                    15, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);

      // publish a dummy metric
      // this is to force the metrics processor to publish delay metrics for all the topics
      publishMessagingMetrics(100, startTime, METRICS_CONTEXT, expected, """", MetricType.GAUGE);
      // validate the newly published metric
      Tasks.waitFor(101L, () -> metricStore.getMetricsProcessedByMetricsProcessor(),
                    15, TimeUnit.SECONDS, 100, TimeUnit.MILLISECONDS);

      // in MessagingMetricsProcessorManagerService, before persisting the metrics and topic metas, a copy of the
      // topic metas
      // containing the metrics processor delay metrics is made before making a copy of metric values.
      // Therefore, there can be a very small chance where all metric values are persisted but the corresponding
      // topic metas are not yet persisted. Wait for all topic metas to be persisted
      Tasks.waitFor(true, metricStore::isMetricsProcessorDelayEmitted, 15, TimeUnit.SECONDS);

      // Clear metricStore and expected results for the next iteration
      metricStore.deleteAll();
      expected.clear();
      // Stop messagingMetricsProcessorManagerService
      messagingMetricsProcessorManagerService.stopAndWait();
    }
  }
",non-flaky,5
35731,cdapio_cdap,MetricsProcessorServiceTest.call,"  @Test
  public void testMetricsProcessor() throws Exception {
    injector.getInstance(TransactionManager.class).startAndWait();
    StructuredTableRegistry structuredTableRegistry = injector.getInstance(StructuredTableRegistry.class);
    structuredTableRegistry.initialize();
    StoreDefinition.createAllTables(injector.getInstance(StructuredTableAdmin.class), structuredTableRegistry);
    injector.getInstance(DatasetOpExecutorService.class).startAndWait();
    injector.getInstance(DatasetService.class).startAndWait();

    final MetricStore metricStore = injector.getInstance(MetricStore.class);

    Set<Integer> partitions = new HashSet<>();
    for (int i = 0; i < cConf.getInt(Constants.Metrics.MESSAGING_TOPIC_NUM); i++) {
      partitions.add(i);
    }

    // Start KafkaMetricsProcessorService after metrics are published to Kafka

    // Intentionally set queue size to a small value, so that MessagingMetricsProcessorManagerService
    // internally can persist metrics when more messages are to be fetched
    MessagingMetricsProcessorManagerService messagingMetricsProcessorManagerService =
      new MessagingMetricsProcessorManagerService(cConf, injector.getInstance(MetricDatasetFactory.class),
                                                  messagingService, injector.getInstance(SchemaGenerator.class),
                                                  injector.getInstance(DatumReaderFactory.class),
                                                  metricStore, injector.getInstance(MetricsWriterProvider.class),
                                                  partitions, new NoopMetricsContext(), 50, 0);
    messagingMetricsProcessorManagerService.startAndWait();

    long startTime = TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis());
    // Publish metrics with messaging service and record expected metrics
    for (int i = 10; i < 20; i++) {
      publishMessagingMetrics(i, startTime, METRICS_CONTEXT, expected, SYSTEM_METRIC_PREFIX, MetricType.COUNTER);
    }

    Thread.sleep(500);
    // Stop and restart messagingMetricsProcessorManagerService
    messagingMetricsProcessorManagerService.stopAndWait();
    // Intentionally set queue size to a large value, so that MessagingMetricsProcessorManagerService
    // internally only persists metrics during terminating.
    messagingMetricsProcessorManagerService =
      new MessagingMetricsProcessorManagerService(cConf, injector.getInstance(MetricDatasetFactory.class),
                                                  messagingService, injector.getInstance(SchemaGenerator.class),
                                                  injector.getInstance(DatumReaderFactory.class),
                                                  metricStore, injector.getInstance(MetricsWriterProvider.class),
                                                  partitions, new NoopMetricsContext(), 50, 0);
    messagingMetricsProcessorManagerService.startAndWait();

    // Publish metrics after MessagingMetricsProcessorManagerService restarts and record expected metrics
    for (int i = 20; i < 30; i++) {
      publishMessagingMetrics(i, startTime, METRICS_CONTEXT, expected, SYSTEM_METRIC_PREFIX, MetricType.GAUGE);
    }

    final List<String> missingMetricNames = new ArrayList<>();
    // Wait until all expected metrics can be queried from the metric store. If not all expected metrics
    // are retrieved when timeout occurs, print out the missing metrics
    try {
      Tasks.waitFor(true, new Callable<Boolean>() {
        @Override
        public Boolean call() throws Exception {
          return canQueryAllMetrics(metricStore, METRICS_CONTEXT, expected, missingMetricNames);
        }
",non-flaky,5
35732,cdapio_cdap,MetricsQueryHelperTest.testGetResolution,"  @Test
  public void testGetResolution() {
    MetricsQueryHelper helper = new MetricsQueryHelper(null, CConfiguration.create());
    try {
      // test  start > end time
      helper.getResolution(""auto"", 10000L, 100L);
      Assert.fail();
    } catch (IllegalArgumentException e) {
      // expected
    }

    try {
      // test resolution is auto, but not both start and end time are provided
      helper.getResolution(""auto"", null, 200L);
      Assert.fail();
    } catch (IllegalArgumentException e) {
      // expected
    }

    try {
      // test resolution is auto, but not both start and end time are provided
      helper.getResolution(""auto"", 200L, null);
      Assert.fail();
    } catch (IllegalArgumentException e) {
      // expected
    }

    try {
      // test non-existing resolution
      helper.getResolution(""6s"", 400L, 600L);
      Assert.fail();
    } catch (IllegalArgumentException e) {
      // expected
    }

    // test specific resolution
    Assert.assertEquals(1, helper.getResolution(""1s"", 100L, 10000L).intValue());
    Assert.assertEquals(60, helper.getResolution(""1m"", 1000L, 100000L).intValue());
    Assert.assertEquals(3600, helper.getResolution(""1h"", 100L, 10000L).intValue());
    Assert.assertEquals(60, helper.getResolution(""60s"", 100L, 10000L).intValue());

    // test resolution is auto
    // if 0 < ts diff <= 600, second resolution will be used
    Assert.assertEquals(1, helper.getResolution(""auto"", 0L, 300L).intValue());
    Assert.assertEquals(1, helper.getResolution(""auto"", 10000L, 10300L).intValue());
    Assert.assertEquals(1, helper.getResolution(""auto"", 0L, 600L).intValue());
    Assert.assertEquals(1, helper.getResolution(""auto"", 1000L, 1600L).intValue());

    // if 600 < ts diff <= 36000, minute resolution will be used
    Assert.assertEquals(60, helper.getResolution(""auto"", 0L, 601L).intValue());
    Assert.assertEquals(60, helper.getResolution(""auto"", 10000L, 10601L).intValue());
    Assert.assertEquals(60, helper.getResolution(""auto"", 0L, 36000L).intValue());
    Assert.assertEquals(60, helper.getResolution(""auto"", 10000L, 46000L).intValue());

    // if ts > 36000, hour resolution will be used
    Assert.assertEquals(3600, helper.getResolution(""auto"", 0L, 36001L).intValue());
    Assert.assertEquals(3600, helper.getResolution(""auto"", 1000L, 10000000L).intValue());

    // if resolution is null, and both start and end time provided, the logic should be same as auto
    Assert.assertEquals(1, helper.getResolution(null, 0L, 300L).intValue());
    Assert.assertEquals(1, helper.getResolution(null, 10000L, 10300L).intValue());
    Assert.assertEquals(1, helper.getResolution(null, 0L, 600L).intValue());
    Assert.assertEquals(1, helper.getResolution(null, 1000L, 1600L).intValue());
    Assert.assertEquals(60, helper.getResolution(null, 0L, 601L).intValue());
    Assert.assertEquals(60, helper.getResolution(null, 10000L, 10601L).intValue());
    Assert.assertEquals(60, helper.getResolution(null, 0L, 36000L).intValue());
    Assert.assertEquals(60, helper.getResolution(null, 10000L, 46000L).intValue());
    Assert.assertEquals(3600, helper.getResolution(null, 0L, 36001L).intValue());
    Assert.assertEquals(3600, helper.getResolution(null, 1000L, 10000000L).intValue());

    // if resolution is null, and either timestamp is not specified, minimum resolution will be used
    Assert.assertEquals(1, helper.getResolution(null, 0L, null).intValue());
    Assert.assertEquals(1, helper.getResolution(null, null, 10000000L).intValue());
  }
",non-flaky,5
35733,cdapio_cdap,TimeseriesIdTest.testEquality,"  @Test
  public void testEquality() {
    TimeseriesId id1 = new TimeseriesId(""app.f.flow.flowlet.0"", ""process.events"", null, ""0"");
    TimeseriesId id2 = new TimeseriesId(""app.f.flow.flowlet.0"", ""process.events"", null, ""0"");
    Assert.assertTrue(id1.equals(id2));
    Assert.assertTrue(id2.equals(id1));
    Assert.assertEquals(id1.hashCode(), id2.hashCode());

    id1 = new TimeseriesId(""app.f.flow.flowlet.0"", ""process.events"", ""tag1"", ""0"");
    id2 = new TimeseriesId(""app.f.flow.flowlet.0"", ""process.events"", ""tag1"", ""0"");
    Assert.assertTrue(id1.equals(id2));
    Assert.assertTrue(id2.equals(id1));
    Assert.assertEquals(id1.hashCode(), id2.hashCode());
  }
",non-flaky,5
35734,cdapio_cdap,AggregatedMetricsCollectionServiceTest.testPublish,"  @Test
  public void testPublish() throws InterruptedException {
    final BlockingQueue<MetricValues> published = new LinkedBlockingQueue<>();

    AggregatedMetricsCollectionService service = new AggregatedMetricsCollectionService(1000L) {
      @Override
      protected void publish(Iterator<MetricValues> metrics) {
        Iterators.addAll(published, metrics);
      }
    };

    service.startAndWait();

    // non-empty tags.
    final Map<String, String> baseTags = ImmutableMap.of(Constants.Metrics.Tag.NAMESPACE, NAMESPACE,
                                                         Constants.Metrics.Tag.APP, APP,
                                                         Constants.Metrics.Tag.SERVICE, SERVICE,
                                                         Constants.Metrics.Tag.RUN_ID, RUNID);

    try {
      // The first section tests with empty tags.
      // Publish couple metrics with empty tags, they should be aggregated.
      service.getContext(EMPTY_TAGS).increment(METRIC, Integer.MAX_VALUE);
      service.getContext(EMPTY_TAGS).increment(METRIC, 2);
      service.getContext(EMPTY_TAGS).increment(METRIC, 3);
      service.getContext(EMPTY_TAGS).increment(METRIC, 4);

      verifyCounterMetricsValue(published, ImmutableMap.of(0, ImmutableMap.of(METRIC, 9L + Integer.MAX_VALUE)));

      // No publishing for 0 value metrics
      Assert.assertNull(published.poll(3, TimeUnit.SECONDS));

      //update the metrics multiple times with gauge.
      service.getContext(EMPTY_TAGS).gauge(GAUGE_METRIC, 1);
      service.getContext(EMPTY_TAGS).gauge(GAUGE_METRIC, 2);
      service.getContext(EMPTY_TAGS).gauge(GAUGE_METRIC, 3);

      // gauge just updates the value, so polling should return the most recent value written
      verifyGaugeMetricsValue(published, ImmutableMap.of(0, 3L));

      // define collectors for non-empty tags
      MetricsContext baseCollector = service.getContext(baseTags);
      MetricsContext metricsContext = baseCollector.childContext(Constants.Metrics.Tag.HANDLER, HANDLER)
        .childContext(Constants.Metrics.Tag.INSTANCE_ID, INSTANCE);

      // increment metrics for various collectors
      baseCollector.increment(METRIC, Integer.MAX_VALUE);
      metricsContext.increment(METRIC, 5);
      baseCollector.increment(METRIC, 10);
      baseCollector.increment(METRIC, 3);
      metricsContext.increment(METRIC, 2);
      metricsContext.increment(METRIC, 4);
      metricsContext.increment(METRIC, 3);
      metricsContext.increment(METRIC, 1);

      // there are two collectors, verify their metrics values
      verifyCounterMetricsValue(published, ImmutableMap.of(4, ImmutableMap.of(METRIC, 13L + Integer.MAX_VALUE),
                                                           6, ImmutableMap.of(METRIC, 15L)));

      // No publishing for 0 value metrics
      Assert.assertNull(published.poll(3, TimeUnit.SECONDS));

      // gauge metrics for various collectors
      baseCollector.gauge(GAUGE_METRIC, Integer.MAX_VALUE);
      baseCollector.gauge(GAUGE_METRIC, 3);
      metricsContext.gauge(GAUGE_METRIC, 6);
      metricsContext.gauge(GAUGE_METRIC, 2);
      baseCollector.gauge(GAUGE_METRIC, 1);
      metricsContext.gauge(GAUGE_METRIC, Integer.MAX_VALUE);

      // gauge just updates the value, so polling should return the most recent value written
      verifyGaugeMetricsValue(published, ImmutableMap.of(4, 1L, 6, (long) Integer.MAX_VALUE));

      metricsContext.gauge(GAUGE_METRIC, 0);
      verifyCounterMetricsValue(published, ImmutableMap.of(6, ImmutableMap.of(GAUGE_METRIC, 0L)));
    } finally {
      service.stopAndWait();
    }
  }
",non-flaky,5
35735,cdapio_cdap,MessagingMetricsCollectionServiceTest.testMessagingPublish,"  @Test
  public void testMessagingPublish() throws TopicNotFoundException {

    MetricsCollectionService collectionService = new MessagingMetricsCollectionService(CConfiguration.create(),
                                                                                       messagingService,
                                                                                       recordWriter);
    collectionService.startAndWait();

    // publish metrics for different context
    for (int i = 1; i <= 3; i++) {
      collectionService.getContext(ImmutableMap.of(""tag"", """" + i)).increment(""processed"", i);
    }

    collectionService.stopAndWait();

    // <Context, metricName, value>
    Table<String, String, Long> expected = HashBasedTable.create();
    expected.put(""tag.1"", ""processed"", 1L);
    expected.put(""tag.2"", ""processed"", 2L);
    expected.put(""tag.3"", ""processed"", 3L);

    ReflectionDatumReader<MetricValues> recordReader = new ReflectionDatumReader<>(schema, metricValueType);
    assertMetricsFromMessaging(schema, recordReader, expected);
  }
",non-flaky,5
35736,cdapio_cdap,RoutingToDataSetsTest.testTypeHandlerRequests,"  @Test
  public void testTypeHandlerRequests() throws Exception {
    Assert.assertEquals(""listModules"", doRequest(""/namespaces/myspace/data/modules"", ""GET""));
    Assert.assertEquals(""post:myModule"", doRequest(""/namespaces/myspace/data/modules/myModule"", ""POST""));
    Assert.assertEquals(""delete:myModule"", doRequest(""/namespaces/myspace/data/modules/myModule"", ""DELETE""));
    Assert.assertEquals(""get:myModule"", doRequest(""/namespaces/myspace/data/modules/myModule"", ""GET""));
    Assert.assertEquals(""listTypes"", doRequest(""/namespaces/myspace/data/types"", ""GET""));
    Assert.assertEquals(""getType:myType"", doRequest(""/namespaces/myspace/data/types/myType"", ""GET""));
  }
",non-flaky,5
35737,cdapio_cdap,RoutingToDataSetsTest.testInstanceHandlerRequests,"  @Test
  public void testInstanceHandlerRequests() throws Exception {
    Assert.assertEquals(""list"", doRequest(""/namespaces/myspace/data/datasets"", ""GET""));
    Assert.assertEquals(""post:myInstance"",
                        doRequest(""/namespaces/myspace/data/datasets/myInstance"", ""POST""));
    Assert.assertEquals(""delete:myInstance"",
                        doRequest(""/namespaces/myspace/data/datasets/myInstance"", ""DELETE""));
    Assert.assertEquals(""get:myInstance"",
                        doRequest(""/namespaces/myspace/data/datasets/myInstance"", ""GET""));
  }
",non-flaky,5
35738,cdapio_cdap,RouterAuditLookUpTest.testCorrectNumberInClassPath,"  @Test
  public void testCorrectNumberInClassPath() throws Exception {
    Assert.assertEquals(ExpectedNumberOfAuditPolicyPaths.EXPECTED_PATH_NUMBER, AUDIT_LOOK_UP.getNumberOfPaths());
  }
",non-flaky,5
35739,cdapio_cdap,RouterAuditLookUpTest.testDataFabricEndpoints,"  @Test
  public void testDataFabricEndpoints() throws Exception {
    // endpoints from DatasetInstanceHandler
    assertContent(""/v3/namespaces/default/data/datasets/myDataset"", DEFAULT_AUDIT);
    // endpoints from DatasetTypeHandler
    assertContent(""/v3/namespaces/default/data/modules/myModule"",
                  new AuditLogConfig(HttpMethod.PUT, false, false, ImmutableList.of(""X-Class-Name"")));
  }
",non-flaky,5
35740,cdapio_cdap,RouterAuditLookUpTest.testAppFabricEndpoints,"  @Test
  public void testAppFabricEndpoints() throws Exception {
    // endpoints from AppLifecycleHttpHandler
    assertContent(""/v3/namespaces/default/apps/myApp"", DEFAULT_AUDIT);
    assertContent(""/v3/namespaces/default/apps"",
                  new AuditLogConfig(HttpMethod.POST, false, true,
                                     ImmutableList.of(AbstractAppFabricHttpHandler.ARCHIVE_NAME_HEADER,
                                                       AbstractAppFabricHttpHandler.APP_CONFIG_HEADER,
                                                       AbstractAppFabricHttpHandler.PRINCIPAL_HEADER,
                                                       AbstractAppFabricHttpHandler.SCHEDULES_HEADER)));
    // endpoints from ArtifactHttpHandler
    assertContent(""/v3/namespaces/default/artifacts/myArtifact/versions/1.0/properties"", DEFAULT_AUDIT);
    assertContent(""/v3/namespaces/default/artifacts/myArtifact"",
                  new AuditLogConfig(HttpMethod.POST, false, false,
                                     ImmutableList.of(""Artifact-Version"", ""Artifact-Extends"", ""Artifact-Plugins"")));
    // endpoints from AuthorizationHandler
    assertContent(""/v3/security/authorization/privileges/grant"",
                  new AuditLogConfig(HttpMethod.POST, true, false, EMPTY_HEADERS));
    // endpoints from ConsoleSettingsHttpHandler
    assertContent(""/v3/configuration/user/"", DEFAULT_AUDIT);
    // endpoints from MetadataHttpHandler
    assertContent(""/v3/namespaces/default/apps/app1/metadata/properties"",
                  new AuditLogConfig(HttpMethod.POST, true, false, EMPTY_HEADERS));
    // endpoints from MonitorHttpHandler
    assertContent(""/v3/system/services/appfabric/instances"", DEFAULT_AUDIT);
    // endpoints from NamespaceHttpHandler
    assertContent(""/v3/namespaces/default"", DEFAULT_AUDIT);
    // endpoints from PreferencesHttpHandler
    assertContent(""/v3/preferences"", DEFAULT_AUDIT);
    // endpoints from ProgramLifecycleHttpHandler
    assertContent(""/v3/namespaces/default/stop"", new AuditLogConfig(HttpMethod.POST, true, true, EMPTY_HEADERS));
    // endpoints from SecureStoreHandler
    assertContent(""/v3/namespaces/default/securekeys/myKey"", DEFAULT_AUDIT);
    // endpoints from TransactionHttpHandler
    assertContent(""/v3/transactions/invalid/remove/until"",
                  new AuditLogConfig(HttpMethod.POST, true, false, EMPTY_HEADERS));
  }
",non-flaky,5
35741,cdapio_cdap,RouterAuditLookUpTest.testExploreEndpoints,"  @Test
  public void testExploreEndpoints() throws Exception {
    // endpoints from ExploreExecutorHttpHandler
    assertContent(""/v3/namespaces/default/data/explore/datasets/myDataset/update"",
                  new AuditLogConfig(HttpMethod.POST, true, false, EMPTY_HEADERS));
    // endpoints from NamespacedExploreMetadataHttpHandler
    assertContent(""/v3/namespaces/default/data/explore/jdbc/tables"",
                  new AuditLogConfig(HttpMethod.POST, true, false, EMPTY_HEADERS));
    // endpoints from NamespacedExploreQueryExecutorHttpHandler
    assertContent(""/v3/namespaces/default/data/explore/queries"",
                  new AuditLogConfig(HttpMethod.POST, true, false, EMPTY_HEADERS));
  }
",non-flaky,5
35742,cdapio_cdap,NettyRouterPipelineAuthTest.testRouterAuthBypass,"  @Test
  public void testRouterAuthBypass() throws Exception {
    // mock token validator passes for any token other than ""Bearer failme""
    testGet(200, ""hello"", ""/v1/echo/hello"", ImmutableMap.of(""Authorization"", ""Bearer x""));
    // so this should fail
    testGet(401, null, ""/v1/echo/hello"");
    testGet(401, null, ""/v1/echo/hello"", ImmutableMap.of(""Authorization"", ""Bearer failme""));
    // but /v1/echo/dontfail is configured to bypass auth
    testGet(200, ""dontfail"", ""/v1/echo/dontfail"", ImmutableMap.of(""Authorization"", ""Bearer failme""));
    // it only bypasses on exact match, not prefix match
    testGet(401, null, ""/v1/echo/dontfailme"", ImmutableMap.of(""Authorization"", ""Bearer failme""));

    // /v1/repeat is configured to bypass auth validation, on prefix match
    testGet(200, ""hello"", ""/v1/repeat/hello"");
    testGet(200, ""hello"", ""/v1/repeat/hello"", ImmutableMap.of(""Authorization"", ""Bearer x""));
    testGet(200, ""hello"", ""/v1/repeat/hello"", ImmutableMap.of(""Authorization"", ""Bearer failme""));
    // even with a token that fails validation, we get the correct status code 404
    testGet(404, null, ""/v1/repeat/dontfail/me"", ImmutableMap.of(""Authorization"", ""Bearer failme""));
  }
",non-flaky,5
35743,cdapio_cdap,AuditLogTest.testAuditLog,"  @Test
  public void testAuditLog() throws IOException {
    HttpURLConnection urlConn = createURLConnection(""/get"", HttpMethod.GET);
    Assert.assertEquals(200, urlConn.getResponseCode());
    urlConn.getInputStream().close();

    urlConn = createURLConnection(""/put"", HttpMethod.PUT);
    urlConn.getOutputStream().write(""Test Put"".getBytes(StandardCharsets.UTF_8));
    Assert.assertEquals(200, urlConn.getResponseCode());
    Assert.assertEquals(""Test Put"", new String(ByteStreams.toByteArray(urlConn.getInputStream()), ""UTF-8""));
    urlConn.getInputStream().close();

    urlConn = createURLConnection(""/post"", HttpMethod.POST);
    urlConn.getOutputStream().write(""Test Post"".getBytes(StandardCharsets.UTF_8));
    Assert.assertEquals(200, urlConn.getResponseCode());
    Assert.assertEquals(""Test Post"", new String(ByteStreams.toByteArray(urlConn.getInputStream()), ""UTF-8""));
    urlConn.getInputStream().close();

    urlConn = createURLConnection(""/postHeaders"", HttpMethod.POST);
    urlConn.setRequestProperty(""user-id"", ""cdap"");
    urlConn.getOutputStream().write(""Post Headers"".getBytes(StandardCharsets.UTF_8));
    Assert.assertEquals(200, urlConn.getResponseCode());
    Assert.assertEquals(""Post Headers"", new String(ByteStreams.toByteArray(urlConn.getInputStream()), ""UTF-8""));
    urlConn.getInputStream().close();

    List<String> loggedMessages = TestLogAppender.INSTANCE.getLoggedMessages();
    Assert.assertEquals(4, loggedMessages.size());

    Assert.assertTrue(loggedMessages.get(0).endsWith(""\""GET /get HTTP/1.1\"" - - 200 0 -""));
    Assert.assertTrue(loggedMessages.get(1).endsWith(""\""PUT /put HTTP/1.1\"" - Test Put 200 8 -""));
    Assert.assertTrue(loggedMessages.get(2).endsWith(""\""POST /post HTTP/1.1\"" - Test Post 200 9 Test Post""));
    Assert.assertTrue(
      loggedMessages.get(3).endsWith(""\""POST /postHeaders HTTP/1.1\"" {user-id=cdap} Post Headers 200 12 Post Headers""));
  }
",non-flaky,5
35744,cdapio_cdap,RouterMainTest.testGuiceInjection,"  @Test
  public void testGuiceInjection() {
    CConfiguration cConf = CConfiguration.create();

    Injector injector = RouterMain.createGuiceInjector(cConf);
    Assert.assertNotNull(injector);

    NettyRouter router = injector.getInstance(NettyRouter.class);
    Assert.assertNotNull(router);
  }
",non-flaky,5
35745,cdapio_cdap,NettyRouterPipelineTest.onCompleted,"  @Test
  public void testChunkRequestSuccess() throws Exception {

    AsyncHttpClientConfig.Builder configBuilder = new AsyncHttpClientConfig.Builder();

    final AsyncHttpClient asyncHttpClient = new AsyncHttpClient(
      new NettyAsyncHttpProvider(configBuilder.build()),
      configBuilder.build());

    byte [] requestBody = generatePostData();
    InetSocketAddress address = ROUTER.getRouterAddress();
    final Request request = new RequestBuilder(""POST"")
      .setUrl(String.format(""http://%s:%d%s"", address.getHostName(), address.getPort(), ""/v1/upload""))
      .setContentLength(requestBody.length)
      .setBody(new ByteEntityWriter(requestBody))
      .build();

    final ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream();
    Future<Void> future = asyncHttpClient.executeRequest(request, new AsyncCompletionHandler<Void>() {
      @Override
      public Void onCompleted(Response response) {
        return null;
      }
",non-flaky,5
35746,cdapio_cdap,NettyRouterPipelineTest.testDeployNTimes,"  @Test
  public void testDeployNTimes() throws Exception {
    // regression tests for race condition during multiple deploys.
    deploy(100);
  }
",non-flaky,5
35747,cdapio_cdap,NettyRouterPipelineTest.channelRead,"  @Test
  public void testHttpPipelining() throws Exception {
    final BlockingQueue<HttpResponseStatus> responseStatuses = new LinkedBlockingQueue<>();
    EventLoopGroup eventGroup = new NioEventLoopGroup();

    Bootstrap bootstrap = new Bootstrap()
      .channel(NioSocketChannel.class)
      .group(eventGroup)
      .handler(new ChannelInitializer<SocketChannel>() {
        @Override
        protected void initChannel(SocketChannel ch) {
          ChannelPipeline pipeline = ch.pipeline();
          pipeline.addLast(""codec"", new HttpClientCodec());
          pipeline.addLast(""aggregator"", new HttpObjectAggregator(1048576));
          pipeline.addLast(""handler"", new ChannelInboundHandlerAdapter() {
            @Override
            public void channelRead(ChannelHandlerContext ctx, Object msg) {
              if (msg instanceof HttpResponse) {
                responseStatuses.add(((HttpResponse) msg).status());
              }
              ReferenceCountUtil.release(msg);
            }
",non-flaky,5
35748,cdapio_cdap,AuthServerAnnounceTest.testEmptyAnnounceAddressURLsConfig,"  @Test
  public void testEmptyAnnounceAddressURLsConfig() throws Exception {
    HttpRouterService routerService = new AuthServerAnnounceTest.HttpRouterService(HOSTNAME, DISCOVERY_SERVICE);
    routerService.startUp();
    try {
      Assert.assertEquals(Collections.EMPTY_LIST, getAuthURI(routerService));
    } finally {
      routerService.shutDown();
    }
  }
",non-flaky,5
35749,cdapio_cdap,AuthServerAnnounceTest.testAnnounceURLsConfig,"  @Test
  public void testAnnounceURLsConfig() throws Exception {
    HttpRouterService routerService = new AuthServerAnnounceTest.HttpRouterService(HOSTNAME, DISCOVERY_SERVICE);
    routerService.cConf.set(Constants.Security.AUTH_SERVER_ANNOUNCE_URLS, ANNOUNCE_URLS);
    routerService.startUp();
    try {
      List<String> expected = Stream.of(ANNOUNCE_URLS.split("",""))
        .map(url -> String.format(""%s/%s"", url, GrantAccessToken.Paths.GET_TOKEN))
        .collect(Collectors.toList());
      Assert.assertEquals(expected, getAuthURI(routerService));
    } finally {
      routerService.shutDown();
    }
  }
",non-flaky,5
35750,cdapio_cdap,NettyRouterTestBase.testRouterSync,"  @Test
  public void testRouterSync() throws Exception {
    testSync(25);
    // sticky endpoint strategy used so the sum should be 25
    Assert.assertEquals(25, defaultServer1.getNumRequests() + defaultServer2.getNumRequests());
  }
",non-flaky,5
35751,cdapio_cdap,NettyRouterTestBase.onCompleted,"  @Test
  public void testRouterAsync() throws Exception {
    int numElements = 123;
    AsyncHttpClientConfig.Builder configBuilder = new AsyncHttpClientConfig.Builder();

    final AsyncHttpClient asyncHttpClient = new AsyncHttpClient(
      new NettyAsyncHttpProvider(configBuilder.build()),
      configBuilder.build());

    final CountDownLatch latch = new CountDownLatch(numElements);
    final AtomicInteger numSuccessfulRequests = new AtomicInteger(0);
    for (int i = 0; i < numElements; ++i) {
      final int elem = i;
      final Request request = new RequestBuilder(""GET"")
        .setUrl(resolveURI(String.format(""%s/%s-%d"", ""/v1/echo"", ""async"", i)))
        .build();
      asyncHttpClient.executeRequest(request, new AsyncCompletionHandler<Void>() {
        @Override
        public Void onCompleted(Response response) throws Exception {
          latch.countDown();
          Assert.assertEquals(HttpResponseStatus.OK.code(), response.getStatusCode());
          String responseBody = response.getResponseBody();
          LOG.trace(""Got response {}"", responseBody);
          Assert.assertEquals(""async-"" + elem, responseBody);
          numSuccessfulRequests.incrementAndGet();
          return null;
        }
",non-flaky,5
35752,cdapio_cdap,NettyRouterTestBase.testRouterOneServerDown,"  @Test
  public void testRouterOneServerDown() throws Exception {
    // Bring down defaultServer1
    defaultServer1.cancelRegistration();

    testSync(25);
    Assert.assertEquals(0, defaultServer1.getNumRequests());
    Assert.assertTrue(defaultServer2.getNumRequests() > 0);

    defaultServer1.registerServer();
  }
",non-flaky,5
35753,cdapio_cdap,NettyRouterTestBase.testRouterAllServersDown,"  @Test
  public void testRouterAllServersDown() throws Exception {
    // Bring down all servers
    defaultServer1.cancelRegistration();
    defaultServer2.cancelRegistration();

    testSyncServiceUnavailable();
    Assert.assertEquals(0, defaultServer1.getNumRequests());
    Assert.assertEquals(0, defaultServer2.getNumRequests());

    defaultServer1.registerServer();
    defaultServer2.registerServer();
  }
",non-flaky,5
35754,cdapio_cdap,NettyRouterTestBase.testHostForward,"  @Test
  public void testHostForward() throws Exception {
    // Test defaultService
    HttpResponse response = get(resolveURI(String.format(""%s/%s"", ""/v1/ping"", ""sync"")));
    Assert.assertEquals(HttpResponseStatus.OK.code(), response.getStatusLine().getStatusCode());
    Assert.assertEquals(APP_FABRIC_SERVICE, EntityUtils.toString(response.getEntity()));
  }
",non-flaky,5
35755,cdapio_cdap,NettyRouterTestBase.onCompleted,"  @Test
  public void testUpload() throws Exception {
    AsyncHttpClientConfig.Builder configBuilder = new AsyncHttpClientConfig.Builder();

    final AsyncHttpClient asyncHttpClient = new AsyncHttpClient(
      new NettyAsyncHttpProvider(configBuilder.build()),
      configBuilder.build());

    byte [] requestBody = generatePostData();
    final Request request = new RequestBuilder(""POST"")
      .setUrl(resolveURI(""/v1/upload""))
      .setContentLength(requestBody.length)
      .setBody(new ByteEntityWriter(requestBody))
      .build();

    final ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream();
    Future<Void> future = asyncHttpClient.executeRequest(request, new AsyncCompletionHandler<Void>() {
      @Override
      public Void onCompleted(Response response) {
        return null;
      }
",non-flaky,5
35756,cdapio_cdap,NettyRouterTestBase.testConnectionClose,"  @Test
  public void testConnectionClose() throws Exception {
    URL[] urls = new URL[] {
      new URL(resolveURI(""/abc/v1/status"")),
      new URL(resolveURI(""/def/v1/status""))
    };

    // Make bunch of requests to one service to 2 difference urls, with the first one keep-alive, second one not.
    // This make router creates two backend service connections on the same inbound connection
    // This is to verify on the close of the second one, it won't close the the inbound if there is an
    // in-flight request happening already (if reached another round of the following for-loop).
    int times = 1000;
    boolean keepAlive = true;
    for (int i = 0; i < times; i++) {
      HttpURLConnection urlConn = openURL(urls[i % urls.length]);
      try {
        urlConn.setRequestProperty(HttpHeaderNames.CONNECTION.toString(),
                                   (keepAlive ? HttpHeaderValues.KEEP_ALIVE : HttpHeaderValues.CLOSE).toString());
        Assert.assertEquals(HttpURLConnection.HTTP_OK, urlConn.getResponseCode());
      } finally {
        urlConn.getInputStream().close();
        keepAlive = !keepAlive;
        urlConn.disconnect();
      }
    }

    Assert.assertEquals(times, defaultServer1.getNumRequests() + defaultServer2.getNumRequests());
  }
",non-flaky,5
35757,cdapio_cdap,NettyRouterTestBase.testConnectionIdleTimeout,"  @Test
  public void testConnectionIdleTimeout() throws Exception {
    // Only use server1
    defaultServer2.cancelRegistration();

    String path = ""/v2/ping"";
    URI uri = new URI(resolveURI(path));
    Socket socket = getSocketFactory().createSocket(uri.getHost(), uri.getPort());
    PrintWriter out = new PrintWriter(socket.getOutputStream(), true);
    InputStream inputStream = socket.getInputStream();

    // make a request
    String firstLine = makeRequest(uri, out, inputStream);
    Assert.assertEquals(""HTTP/1.1 200 OK"", firstLine);

    // sleep for 500 ms below the configured idle timeout; the connection on server side should not get closed by then
    // Hence it should be reusing the same server side connection
    TimeUnit.MILLISECONDS.sleep(TimeUnit.SECONDS.toMillis(CONNECTION_IDLE_TIMEOUT_SECS) - 500);
    firstLine = makeRequest(uri, out, inputStream);
    Assert.assertEquals(""HTTP/1.1 200 OK"", firstLine);

    // sleep for 500 ms over the configured idle timeout; the connection on server side should get closed by then
    // Hence it should be create a new server side connection
    TimeUnit.MILLISECONDS.sleep(TimeUnit.SECONDS.toMillis(CONNECTION_IDLE_TIMEOUT_SECS) + 500);
    // Due to timeout the client connection will be closed, and hence this request should not go to the server
    firstLine = makeRequest(uri, out, inputStream);
    Assert.assertEquals(""HTTP/1.1 200 OK"", firstLine);

    // assert that the connection is closed on the server side
    Assert.assertEquals(3, defaultServer1.getNumRequests());
    Assert.assertEquals(2, defaultServer1.getNumConnectionsOpened());
    Assert.assertEquals(1, defaultServer1.getNumConnectionsClosed());
  }
",non-flaky,5
35758,cdapio_cdap,NettyRouterTestBase.testConnectionIdleTimeoutWithMultipleServers,"  @Test
  public void testConnectionIdleTimeoutWithMultipleServers() throws Exception {
    defaultServer2.cancelRegistration();

    URL url = new URL(resolveURI(""/v2/ping""));
    HttpURLConnection urlConnection = openURL(url);
    Assert.assertEquals(200, urlConnection.getResponseCode());
    urlConnection.getInputStream().close();
    urlConnection.disconnect();

    // requests past this point will go to defaultServer2
    defaultServer1.cancelRegistration();
    defaultServer2.registerServer();

    for (int i = 0; i < 4; i++) {
      // this is an assumption that CONNECTION_IDLE_TIMEOUT_SECS is more than 1 second
      TimeUnit.SECONDS.sleep(1);
      url = new URL(resolveURI(""/v1/ping/"" + i));
      urlConnection = openURL(url);
      Assert.assertEquals(200, urlConnection.getResponseCode());
      urlConnection.getInputStream().close();
      urlConnection.disconnect();
    }

    // for the past 4 seconds, we've been making requests to defaultServer2; therefore, defaultServer1 will have closed
    // its single connection
    Assert.assertEquals(1, defaultServer1.getNumConnectionsOpened());
    Assert.assertEquals(1, defaultServer1.getNumConnectionsClosed());

    // however, the connection to defaultServer2 is not timed out, because we've been making requests to it
    Assert.assertEquals(1, defaultServer2.getNumConnectionsOpened());
    Assert.assertEquals(0, defaultServer2.getNumConnectionsClosed());

    defaultServer2.registerServer();
    defaultServer1.cancelRegistration();
    url = new URL(resolveURI(""/v2/ping""));
    urlConnection = openURL(url);
    Assert.assertEquals(200, urlConnection.getResponseCode());
    urlConnection.getInputStream().close();
    urlConnection.disconnect();
  }
",non-flaky,5
35759,cdapio_cdap,NettyRouterTestBase.testExpectContinue,"  @Test (timeout = 5000L)
  public void testExpectContinue() throws Exception {
    URL url = new URL(resolveURI(""/v2/upload""));
    HttpURLConnection urlConn = openURL(url);
    urlConn.setRequestMethod(""POST"");
    urlConn.setRequestProperty(HttpHeaderNames.EXPECT.toString(), HttpHeaderValues.CONTINUE.toString());
    urlConn.setDoOutput(true);

    // Forces sending small chunks to have the netty server receives multiple chunks
    urlConn.setChunkedStreamingMode(10);
    String msg = Strings.repeat(""Message"", 100);
    urlConn.getOutputStream().write(msg.getBytes(StandardCharsets.UTF_8));

    Assert.assertEquals(200, urlConn.getResponseCode());
    String result = new String(ByteStreams.toByteArray(urlConn.getInputStream()), StandardCharsets.UTF_8);
    Assert.assertEquals(msg, result);
  }
",non-flaky,5
38608,apache_pulsar,PerformanceClientTest.testLoadArguments,"    @Test(timeOut = 5000)
    public void testLoadArguments() throws Exception {
        PerformanceClient client = new PerformanceClient();

        // ""--proxy-url"" has the highest priority
        PerformanceClient.Arguments arguments = client.loadArguments(
                getArgs(""ws://broker0.pulsar.apache.org:8080/"", ""./src/test/resources/websocket_client1.conf""));
        assertEquals(arguments.proxyURL, ""ws://broker0.pulsar.apache.org:8080/"");

        // ""webSocketServiceUrl"" written in the conf file has the second priority
        arguments = client.loadArguments(getArgs(null, ""./src/test/resources/websocket_client1.conf""));
        assertEquals(arguments.proxyURL, ""ws://broker1.pulsar.apache.org:8080/"");

        // ""webServiceUrl"" written in the conf file has the third priority
        arguments = client.loadArguments(getArgs(null, ""./src/test/resources/websocket_client2.conf""));
        assertEquals(arguments.proxyURL, ""ws://broker2.pulsar.apache.org:8080/"");

        // ""serviceUrl"" written in the conf file has the fourth priority
        arguments = client.loadArguments(getArgs(null, ""./src/test/resources/websocket_client3.conf""));
        assertEquals(arguments.proxyURL, ""wss://broker3.pulsar.apache.org:8443/"");

        // The default value is ""ws://localhost:8080/""
        arguments = client.loadArguments(getArgs(null, null));
        assertEquals(arguments.proxyURL, ""ws://localhost:8080/"");

        // If the URL does not end with ""/"", it will be added
        arguments = client.loadArguments(getArgs(""ws://broker0.pulsar.apache.org:8080"", null));
        assertEquals(arguments.proxyURL, ""ws://broker0.pulsar.apache.org:8080/"");
    }
",non-flaky,5
38609,apache_pulsar,GenerateDocumentionTest.testGenerateDocumention,"    @Test
    public void testGenerateDocumention() throws Exception {
        CmdGenerateDocumentation.main(new String[]{});
    }
",non-flaky,5
38610,apache_pulsar,GenerateDocumentionTest.testSpecifyModuleName,"    @Test
    public void testSpecifyModuleName() throws Exception {
        String[] args = new String[]{""-n"", ""produce"", ""-n"", ""consume""};
        CmdGenerateDocumentation.main(args);
    }
",non-flaky,5
38611,apache_pulsar,PerformanceProducerTest.testMsgKey,"    @Test(timeOut = 20000)
    public void testMsgKey() throws Exception {
        String argString = ""%s -r 10 -u %s -m 500"";
        String topic = testTopic + UUID.randomUUID().toString();
        String args = String.format(argString, topic, pulsar.getBrokerServiceUrl());
        Thread thread = new Thread(() -> {
            try {
                PerformanceProducer.main(args.split("" ""));
            } catch (Exception e) {
                e.printStackTrace();
            }
        });
        thread.start();
        Consumer<byte[]> consumer1 = pulsarClient.newConsumer().topic(topic).subscriptionName(""sub-1"")
                .subscriptionType(SubscriptionType.Key_Shared).subscribe();
        Consumer<byte[]> consumer2 = pulsarClient.newConsumer().topic(topic).subscriptionName(""sub-1"")
                .subscriptionType(SubscriptionType.Key_Shared).subscribe();

        int count1 = 0;
        int count2 = 0;
        for (int i = 0; i < 10; i++) {
            Message<byte[]> message = consumer1.receive(1, TimeUnit.SECONDS);
            if (message == null) {
                break;
            }
            count1++;
            consumer1.acknowledge(message);
        }
        for (int i = 0; i < 10; i++) {
            Message<byte[]> message = consumer2.receive(1, TimeUnit.SECONDS);
            if (message == null) {
                break;
            }
            count2++;
            consumer2.acknowledge(message);
        }
        //in key_share mode, only one consumer can get msg
        Assert.assertTrue(count1 == 0 || count2 == 0);

        consumer1.close();
        consumer2.close();
        thread.interrupt();
        while (thread.isAlive()) {
            Thread.sleep(1000);
        }

        //use msg key generator,so every consumer can get msg
        String newArgString = ""%s -r 10 -u %s -m 500 -mk autoIncrement"";
        String topic2 = testTopic + UUID.randomUUID().toString();
        String newArgs = String.format(newArgString, topic2, pulsar.getBrokerServiceUrl());
        Thread thread2 = new Thread(() -> {
            try {
                PerformanceProducer.main(newArgs.split("" ""));
            } catch (Exception e) {
                e.printStackTrace();
            }
        });
        thread2.start();

        Consumer newConsumer1 = pulsarClient.newConsumer().topic(topic2).subscriptionName(""sub-2"")
                .subscriptionType(SubscriptionType.Key_Shared).subscribe();
        Consumer newConsumer2 = pulsarClient.newConsumer().topic(topic2).subscriptionName(""sub-2"")
                .subscriptionType(SubscriptionType.Key_Shared).subscribe();
        count1 = 0;
        count2 = 0;
        for (int i = 0; i < 10; i++) {
            Message<byte[]> message = newConsumer1.receive(1, TimeUnit.SECONDS);
            if (message == null) {
                break;
            }
            count1++;
            newConsumer1.acknowledge(message);
        }
        for (int i = 0; i < 10; i++) {
            Message<byte[]> message = newConsumer2.receive(1, TimeUnit.SECONDS);
            if (message == null) {
                break;
            }
            count2++;
            newConsumer2.acknowledge(message);
        }

        Assert.assertTrue(count1 > 0 && count2 > 0);
        thread2.interrupt();
        newConsumer1.close();
        newConsumer2.close();
    }
",non-flaky,5
38612,apache_pulsar,PerformanceProducerTest.testCreatePartitions,"    @Test(timeOut = 20000)
    public void testCreatePartitions() throws Exception {
        String argString = ""%s -r 10 -u %s -au %s -m 5 -np 10"";
        String topic = testTopic + UUID.randomUUID().toString();
        String args = String.format(argString, topic, pulsar.getBrokerServiceUrl(), pulsar.getWebServiceAddress());
        Thread thread = new Thread(() -> {
            try {
                PerformanceProducer.main(args.split("" ""));
            } catch (Exception e) {
                e.printStackTrace();
            }
        });
        thread.start();
        thread.join();
        Assert.assertEquals(10, pulsar.getAdminClient().topics().getPartitionedTopicMetadata(topic).partitions);
    }
",non-flaky,5
38613,apache_pulsar,PerformanceProducerTest.testNotExistIMessageFormatter,"    @Test
    public void testNotExistIMessageFormatter() {
        IMessageFormatter msgFormatter = PerformanceProducer.getMessageFormatter(""org.apache.pulsar.testclient.NonExistentFormatter"");
        Assert.assertNull(msgFormatter);
    }
",non-flaky,5
38614,apache_pulsar,PerformanceProducerTest.testDefaultIMessageFormatter,"    @Test
    public void testDefaultIMessageFormatter() {
        IMessageFormatter msgFormatter = PerformanceProducer.getMessageFormatter(""org.apache.pulsar.testclient.DefaultMessageFormatter"");
        Assert.assertTrue(msgFormatter instanceof DefaultMessageFormatter);
    }
",non-flaky,5
38615,apache_pulsar,TestDefaultMessageFormatter.testFormatMessage,"    @Test
    public void testFormatMessage() {
        String producerName = ""producer-1"";
        long msgId = 3;
        byte[] message = ""{ \""producer\"": \""%p\"", \""msgId\"": %i, \""nanoTime\"": %t, \""float1\"": %5.2f, \""float2\"": %-5.2f, \""long1\"": %12l, \""long2\"": %l, \""int1\"": %d, \""int2\"": %1d , \""long3\"": %5l,  \""str\"": \""%5s\"" }"".getBytes();
        byte[] formatted = new DefaultMessageFormatter().formatMessage(producerName, msgId, message);
        String jsonString = new String(formatted, StandardCharsets.UTF_8);

        ObjectMapper objectMapper = new ObjectMapper();

        JsonNode obj = null;
        try {
            obj = objectMapper.readValue(jsonString, JsonNode.class);

        } catch(Exception jpe) {
            Assert.fail(""Exception parsing json"");
        }

        String prod = obj.get(""producer"").asText();
        int mid = obj.get(""msgId"").asInt();
        long nt = obj.get(""nanoTime"").asLong();
        float f1 = obj.get(""float1"").floatValue();
        float f2 = obj.get(""float2"").floatValue();
        long l1 = obj.get(""long1"").asLong();
        long l2 = obj.get(""long2"").asLong();
        long i1 = obj.get(""int1"").asInt();
        long i2 = obj.get(""int2"").asInt();
        String str = obj.get(""str"").asText();
        long l3 = obj.get(""long3"").asLong();
        Assert.assertEquals(producerName, prod);
        Assert.assertEquals(msgId, mid);
        Assert.assertTrue( nt > 0);
        Assert.assertNotEquals(f1, f2);
        Assert.assertNotEquals(l1, l2);
        Assert.assertNotEquals(i1, i2);
        Assert.assertTrue(l3 > 0);
        Assert.assertTrue(l3 <= 99999);
        Assert.assertTrue(i2 < 10);
        Assert.assertTrue(0 < i2, ""i2 was "" + i2);
        Assert.assertTrue(f2 < 100000);
        Assert.assertTrue( -100000 < f2);

    }
",non-flaky,5
38616,apache_pulsar,JdbcUtilsTest.TestGetTableId,"    @Test
    public void TestGetTableId() throws Exception {
        String tableName = ""TestGetTableId"";

        sqliteUtils.createTable(
            ""CREATE TABLE "" + tableName + ""("" +
                ""    firstName  TEXT,"" +
                ""    lastName  TEXT,"" +
                ""    age INTEGER,"" +
                ""    bool  NUMERIC,"" +
                ""    byte  INTEGER,"" +
                ""    short INTEGER NULL,"" +
                ""    long INTEGER,"" +
                ""    float NUMERIC,"" +
                ""    double NUMERIC,"" +
                ""    bytes BLOB, "" +
                ""PRIMARY KEY (firstName, lastName));""
        );

        Connection connection = sqliteUtils.getConnection();

        // Test getTableId
        log.info(""verify getTableId"");
        TableId id = JdbcUtils.getTableId(connection, tableName);
        Assert.assertEquals(id.getTableName(), tableName);

        // Test get getTableDefinition
        log.info(""verify getTableDefinition"");
        List<String> keyList = Lists.newArrayList();
        keyList.add(""firstName"");
        keyList.add(""lastName"");
        List<String> nonKeyList = Lists.newArrayList();
        nonKeyList.add(""age"");
        nonKeyList.add(""long"");
        TableDefinition table = JdbcUtils.getTableDefinition(connection, id, keyList, nonKeyList);
        Assert.assertEquals(table.getColumns().get(0).getName(), ""firstName"");
        Assert.assertEquals(table.getColumns().get(0).getTypeName(), ""TEXT"");
        Assert.assertEquals(table.getColumns().get(2).getName(), ""age"");
        Assert.assertEquals(table.getColumns().get(2).getTypeName(), ""INTEGER"");
        Assert.assertEquals(table.getColumns().get(7).getName(), ""float"");
        Assert.assertEquals(table.getColumns().get(7).getTypeName(), ""NUMERIC"");
        Assert.assertEquals(table.getKeyColumns().get(0).getName(), ""firstName"");
        Assert.assertEquals(table.getKeyColumns().get(0).getTypeName(), ""TEXT"");
        Assert.assertEquals(table.getKeyColumns().get(1).getName(), ""lastName"");
        Assert.assertEquals(table.getKeyColumns().get(1).getTypeName(), ""TEXT"");
        Assert.assertEquals(table.getNonKeyColumns().get(0).getName(), ""age"");
        Assert.assertEquals(table.getNonKeyColumns().get(0).getTypeName(), ""INTEGER"");
        Assert.assertEquals(table.getNonKeyColumns().get(1).getName(), ""long"");
        Assert.assertEquals(table.getNonKeyColumns().get(1).getTypeName(), ""INTEGER"");
        // Test get getTableDefinition
        log.info(""verify buildInsertSql"");
        String expctedInsertStatement = ""INSERT INTO "" + tableName +
            ""(firstName,lastName,age,bool,byte,short,long,float,double,bytes)"" +
            "" VALUES(?,?,?,?,?,?,?,?,?,?)"";
        String insertStatement = JdbcUtils.buildInsertSql(table);
        Assert.assertEquals(insertStatement, expctedInsertStatement);
        log.info(""verify buildUpdateSql"");
        String expectedUpdateStatement = ""UPDATE "" + tableName +
                "" SET age=? ,long=?  WHERE firstName=? AND lastName=?"";
        String updateStatement = JdbcUtils.buildUpdateSql(table);
        Assert.assertEquals(updateStatement, expectedUpdateStatement);
        log.info(""verify buildDeleteSql"");
        String expectedDeleteStatement = ""DELETE FROM "" + tableName +
                "" WHERE firstName=? AND lastName=?"";
        String deleteStatement = JdbcUtils.buildDeleteSql(table);
        Assert.assertEquals(deleteStatement, expectedDeleteStatement);
    }
",non-flaky,5
38617,apache_pulsar,SqliteJdbcSinkTest.TestInsertAction,"    @Test
    public void TestInsertAction() throws Exception {
        testOpenAndWriteSink(ImmutableMap.of(""ACTION"", ""INSERT""));
    }
",non-flaky,5
38618,apache_pulsar,SqliteJdbcSinkTest.TestNoAction,"    @Test
    public void TestNoAction() throws Exception {
        testOpenAndWriteSink(ImmutableMap.of());
    }
",non-flaky,5
38619,apache_pulsar,SqliteJdbcSinkTest.TestNoActionNullValue,"    @Test
    public void TestNoActionNullValue() throws Exception {
        testOpenAndWriteSinkNullValue(ImmutableMap.of(""ACTION"", ""INSERT""));
    }
",non-flaky,5
38620,apache_pulsar,SqliteJdbcSinkTest.TestNoActionNullValueJson,"    @Test
    public void TestNoActionNullValueJson() throws Exception {
        testOpenAndWriteSinkNullValueJson(ImmutableMap.of(""ACTION"", ""INSERT""));
    }
",non-flaky,5
38621,apache_pulsar,SqliteJdbcSinkTest.TestNoActionJson,"    @Test
    public void TestNoActionJson() throws Exception {
        testOpenAndWriteSinkJson(ImmutableMap.of(""ACTION"", ""INSERT""));
    }
",non-flaky,5
38622,apache_pulsar,SqliteJdbcSinkTest.TestUnknownAction,"    @Test
    public void TestUnknownAction() throws Exception {
        Record<GenericRecord> recordRecord = mock(Record.class);
        when(recordRecord.getProperties()).thenReturn(ImmutableMap.of(""ACTION"", ""UNKNOWN""));
        CompletableFuture<Void> future = new CompletableFuture<>();
        doAnswer(a -> future.complete(null)).when(recordRecord).fail();
        jdbcSink.write(recordRecord);
        future.get(1, TimeUnit.SECONDS);
    }
",non-flaky,5
38623,apache_pulsar,SqliteJdbcSinkTest.TestUpdateAction,"    @Test
    public void TestUpdateAction() throws Exception {

        AvroSchema<Foo> schema = AvroSchema.of(SchemaDefinition.<Foo>builder().withPojo(Foo.class).build());

        Foo updateObj = new Foo();
        updateObj.setField1(""ValueOfField3"");
        updateObj.setField2(""ValueOfField3"");
        updateObj.setField3(4);

        byte[] updateBytes = schema.encode(updateObj);
        Message<GenericRecord> updateMessage = mock(MessageImpl.class);
        CompletableFuture<Void> future = new CompletableFuture<>();
        Record<GenericRecord> updateRecord = PulsarRecord.<GenericRecord>builder()
                .message(updateMessage)
                .topicName(""fake_topic_name"")
                .ackFunction(() -> future.complete(null))
                .build();

        GenericSchema<GenericRecord> updateGenericAvroSchema;
        updateGenericAvroSchema = new GenericAvroSchema(schema.getSchemaInfo());

        Map<String, String> updateProperties = Maps.newHashMap();
        updateProperties.put(""ACTION"", ""UPDATE"");
        when(updateMessage.getValue()).thenReturn(updateGenericAvroSchema.decode(updateBytes));
        when(updateMessage.getProperties()).thenReturn(updateProperties);
        log.info(""foo:{}, Message.getValue: {}, record.getValue: {}"",
                updateObj.toString(),
                updateMessage.getValue().toString(),
                updateRecord.getValue().toString());

        jdbcSink.write(updateRecord);
        future.get(1, TimeUnit.SECONDS);

        // value has been written to db, read it out and verify.
        String updateQuerySql = ""SELECT * FROM "" + tableName + "" WHERE field3=4"";
        sqliteUtils.select(updateQuerySql, (resultSet) -> {
            Assert.assertEquals(updateObj.getField1(), resultSet.getString(1));
            Assert.assertEquals(updateObj.getField2(), resultSet.getString(2));
            Assert.assertEquals(updateObj.getField3(), resultSet.getInt(3));
        });
    }
",non-flaky,5
38624,apache_pulsar,SqliteJdbcSinkTest.TestDeleteAction,"    @Test
    public void TestDeleteAction() throws Exception {

        AvroSchema<Foo> schema = AvroSchema.of(SchemaDefinition.<Foo>builder().withPojo(Foo.class).build());

        Foo deleteObj = new Foo();
        deleteObj.setField3(5);

        byte[] deleteBytes = schema.encode(deleteObj);
        Message<GenericRecord> deleteMessage = mock(MessageImpl.class);
        CompletableFuture<Void> future = new CompletableFuture<>();
        Record<GenericRecord> deleteRecord = PulsarRecord.<GenericRecord>builder()
                .message(deleteMessage)
                .topicName(""fake_topic_name"")
                .ackFunction(() -> future.complete(null))
                .build();

        GenericSchema<GenericRecord> deleteGenericAvroSchema = new GenericAvroSchema(schema.getSchemaInfo());

        Map<String, String> deleteProperties = Maps.newHashMap();
        deleteProperties.put(""ACTION"", ""DELETE"");
        when(deleteMessage.getValue()).thenReturn(deleteGenericAvroSchema.decode(deleteBytes));
        when(deleteMessage.getProperties()).thenReturn(deleteProperties);
        log.info(""foo:{}, Message.getValue: {}, record.getValue: {}"",
                deleteObj.toString(),
                deleteMessage.getValue().toString(),
                deleteRecord.getValue().toString());

        jdbcSink.write(deleteRecord);
        future.get(1, TimeUnit.SECONDS);

        // value has been written to db, read it out and verify.
        String deleteQuerySql = ""SELECT * FROM "" + tableName + "" WHERE field3=5"";
        Assert.assertEquals(sqliteUtils.select(deleteQuerySql, (resultSet) -> {}), 0);
    }
",non-flaky,5
38625,apache_pulsar,TestAbstractConfigurationProvider.testDispoableChannel,"    @Test
    public void testDispoableChannel() throws Exception {
        String agentName = ""agent1"";
        Map<String, String> properties = getPropertiesForChannel(agentName,
                DisposableChannel.class.getName());
        MemoryConfigurationProvider provider =
                new MemoryConfigurationProvider(agentName, properties);
        MaterializedConfiguration config1 = provider.getConfiguration();
        Channel channel1 = config1.getChannels().values().iterator().next();
        assertTrue(channel1 instanceof DisposableChannel);
        MaterializedConfiguration config2 = provider.getConfiguration();
        Channel channel2 = config2.getChannels().values().iterator().next();
        assertTrue(channel2 instanceof DisposableChannel);
        assertNotSame(channel1, channel2);
    }
",non-flaky,5
38626,apache_pulsar,TestAbstractConfigurationProvider.testReusableChannel,"    @Test
    public void testReusableChannel() throws Exception {
        String agentName = ""agent1"";
        Map<String, String> properties = getPropertiesForChannel(agentName,
                RecyclableChannel.class.getName());
        MemoryConfigurationProvider provider =
                new MemoryConfigurationProvider(agentName, properties);

        MaterializedConfiguration config1 = provider.getConfiguration();
        Channel channel1 = config1.getChannels().values().iterator().next();
        assertTrue(channel1 instanceof RecyclableChannel);

        MaterializedConfiguration config2 = provider.getConfiguration();
        Channel channel2 = config2.getChannels().values().iterator().next();
        assertTrue(channel2 instanceof RecyclableChannel);

        assertSame(channel1, channel2);
    }
",non-flaky,5
38627,apache_pulsar,TestAbstractConfigurationProvider.testUnspecifiedChannel,"    @Test
    public void testUnspecifiedChannel() throws Exception {
        String agentName = ""agent1"";
        Map<String, String> properties = getPropertiesForChannel(agentName,
                UnspecifiedChannel.class.getName());
        MemoryConfigurationProvider provider =
                new MemoryConfigurationProvider(agentName, properties);

        MaterializedConfiguration config1 = provider.getConfiguration();
        Channel channel1 = config1.getChannels().values().iterator().next();
        assertTrue(channel1 instanceof UnspecifiedChannel);

        MaterializedConfiguration config2 = provider.getConfiguration();
        Channel channel2 = config2.getChannels().values().iterator().next();
        assertTrue(channel2 instanceof UnspecifiedChannel);

        assertSame(channel1, channel2);
    }
",non-flaky,5
38628,apache_pulsar,TestAbstractConfigurationProvider.testReusableChannelNotReusedLater,"    @Test
    public void testReusableChannelNotReusedLater() throws Exception {
        String agentName = ""agent1"";
        Map<String, String> propertiesReusable = getPropertiesForChannel(agentName,
                RecyclableChannel.class
                        .getName());
        Map<String, String> propertiesDispoable = getPropertiesForChannel(agentName,
                DisposableChannel.class
                        .getName());
        MemoryConfigurationProvider provider =
                new MemoryConfigurationProvider(agentName, propertiesReusable);
        MaterializedConfiguration config1 = provider.getConfiguration();
        Channel channel1 = config1.getChannels().values().iterator().next();
        assertTrue(channel1 instanceof RecyclableChannel);

        provider.setProperties(propertiesDispoable);
        MaterializedConfiguration config2 = provider.getConfiguration();
        Channel channel2 = config2.getChannels().values().iterator().next();
        assertTrue(channel2 instanceof DisposableChannel);

        provider.setProperties(propertiesReusable);
        MaterializedConfiguration config3 = provider.getConfiguration();
        Channel channel3 = config3.getChannels().values().iterator().next();
        assertTrue(channel3 instanceof RecyclableChannel);

        assertNotSame(channel1, channel3);
    }
",non-flaky,5
38629,apache_pulsar,TestAbstractConfigurationProvider.testSourceThrowsExceptionDuringConfiguration,"    @Test
    public void testSourceThrowsExceptionDuringConfiguration() throws Exception {
        String agentName = ""agent1"";
        String sourceType = UnconfigurableSource.class.getName();
        String channelType = ""memory"";
        String sinkType = ""null"";
        Map<String, String> properties = getProperties(agentName, sourceType,
                channelType, sinkType);
        MemoryConfigurationProvider provider =
                new MemoryConfigurationProvider(agentName, properties);
        MaterializedConfiguration config = provider.getConfiguration();
        assertEquals(config.getSourceRunners().size(), 0);
        assertEquals(config.getChannels().size(), 1);
        assertEquals(config.getSinkRunners().size(), 1);
    }
",non-flaky,5
38630,apache_pulsar,TestAbstractConfigurationProvider.testChannelThrowsExceptionDuringConfiguration,"    @Test
    public void testChannelThrowsExceptionDuringConfiguration() throws Exception {
        String agentName = ""agent1"";
        String sourceType = ""seq"";
        String channelType = UnconfigurableChannel.class.getName();
        String sinkType = ""null"";
        Map<String, String> properties = getProperties(agentName, sourceType,
                channelType, sinkType);
        MemoryConfigurationProvider provider =
                new MemoryConfigurationProvider(agentName, properties);
        MaterializedConfiguration config = provider.getConfiguration();
        assertEquals(config.getSourceRunners().size(), 0);
        assertEquals(config.getChannels().size(), 0);
        assertEquals(config.getSinkRunners().size(), 0);
    }
",non-flaky,5
38631,apache_pulsar,TestAbstractConfigurationProvider.testSinkThrowsExceptionDuringConfiguration,"    @Test
    public void testSinkThrowsExceptionDuringConfiguration() throws Exception {
        String agentName = ""agent1"";
        String sourceType = ""seq"";
        String channelType = ""memory"";
        String sinkType = UnconfigurableSink.class.getName();
        Map<String, String> properties = getProperties(agentName, sourceType,
                channelType, sinkType);
        MemoryConfigurationProvider provider =
                new MemoryConfigurationProvider(agentName, properties);
        MaterializedConfiguration config = provider.getConfiguration();
        assertEquals(config.getSourceRunners().size(), 1);
        assertEquals(config.getChannels().size(), 1);
        assertEquals(config.getSinkRunners().size(), 0);
    }
",non-flaky,5
38632,apache_pulsar,TestAbstractConfigurationProvider.testSourceAndSinkThrowExceptionDuringConfiguration,"    @Test
    public void testSourceAndSinkThrowExceptionDuringConfiguration()
            throws Exception {
        String agentName = ""agent1"";
        String sourceType = UnconfigurableSource.class.getName();
        String channelType = ""memory"";
        String sinkType = UnconfigurableSink.class.getName();
        Map<String, String> properties = getProperties(agentName, sourceType,
                channelType, sinkType);
        MemoryConfigurationProvider provider =
                new MemoryConfigurationProvider(agentName, properties);
        MaterializedConfiguration config = provider.getConfiguration();
        assertEquals(config.getSourceRunners().size(), 0);
        assertEquals(config.getChannels().size(), 0);
        assertEquals(config.getSinkRunners().size(), 0);
    }
",non-flaky,5
38633,apache_pulsar,TestAbstractConfigurationProvider.testSinkSourceMismatchDuringConfiguration,"    @Test
    public void testSinkSourceMismatchDuringConfiguration() throws Exception {
        String agentName = ""agent1"";
        String sourceType = ""seq"";
        String channelType = ""memory"";
        String sinkType = ""avro"";
        Map<String, String> properties = getProperties(agentName, sourceType,
                channelType, sinkType);
        properties.put(agentName + "".channels.channel1.capacity"", ""1000"");
        properties.put(agentName + "".channels.channel1.transactionCapacity"", ""1000"");
        properties.put(agentName + "".sources.source1.batchSize"", ""1000"");
        properties.put(agentName + "".sinks.sink1.batch-size"", ""1000"");
        properties.put(agentName + "".sinks.sink1.hostname"", ""10.10.10.10"");
        properties.put(agentName + "".sinks.sink1.port"", ""1010"");

        MemoryConfigurationProvider provider =
                new MemoryConfigurationProvider(agentName, properties);
        MaterializedConfiguration config = provider.getConfiguration();
        assertEquals(config.getSourceRunners().size(), 1);
        assertEquals(config.getChannels().size(), 1);
        assertEquals(config.getSinkRunners().size(), 1);

        properties.put(agentName + "".sources.source1.batchSize"", ""1001"");
        properties.put(agentName + "".sinks.sink1.batch-size"", ""1000"");

        provider = new MemoryConfigurationProvider(agentName, properties);
        config = provider.getConfiguration();
        assertEquals(config.getSourceRunners().size(), 0);
        assertEquals(config.getChannels().size(), 1);
        assertEquals(config.getSinkRunners().size(), 1);

        properties.put(agentName + "".sources.source1.batchSize"", ""1000"");
        properties.put(agentName + "".sinks.sink1.batch-size"", ""1001"");

        provider = new MemoryConfigurationProvider(agentName, properties);
        config = provider.getConfiguration();
        assertEquals(config.getSourceRunners().size(), 1);
        assertEquals(config.getChannels().size(), 1);
        assertEquals(config.getSinkRunners().size(), 0);

        properties.put(agentName + "".sources.source1.batchSize"", ""1001"");
        properties.put(agentName + "".sinks.sink1.batch-size"", ""1001"");

        provider = new MemoryConfigurationProvider(agentName, properties);
        config = provider.getConfiguration();
        assertEquals(config.getSourceRunners().size(), 0);
        assertEquals(config.getChannels().size(), 0);
        assertEquals(config.getSinkRunners().size(), 0);
    }
",non-flaky,5
38634,apache_pulsar,TestPollingPropertiesFileConfigurationProvider.testPolling,"    @Test(enabled = false)
    public void testPolling() throws Exception {

        // let first event fire
        Thread.sleep(2000L);

        final List<MaterializedConfiguration> events = Lists.newArrayList();

        Object eventHandler = new Object() {
            @Subscribe
            public synchronized void handleConfigurationEvent(MaterializedConfiguration event) {
                events.add(event);
            }
        };
        eventBus.register(eventHandler);
        configFile.setLastModified(System.currentTimeMillis());

        // now wait for second event to fire
        Thread.sleep(2000L);

        Assert.assertEquals(events.size(), 1, String.valueOf(events));

        MaterializedConfiguration materializedConfiguration = events.remove(0);

        Assert.assertEquals(materializedConfiguration.getSourceRunners().size(),1);
        Assert.assertEquals(materializedConfiguration.getSinkRunners().size(), 1);
        Assert.assertEquals(materializedConfiguration.getChannels().size(), 1);


    }
",non-flaky,5
38635,apache_pulsar,TestPollingZooKeeperConfigurationProvider.testPolling,"    @Test
    public void testPolling() throws Exception {
        es.awaitEvent();
        es.reset();

        FlumeConfiguration fc = cp.getFlumeConfiguration();
        Assert.assertTrue(fc.getConfigurationErrors().isEmpty());
        AgentConfiguration ac = fc.getConfigurationFor(AGENT_NAME);
        Assert.assertNull(ac);

        addData();
        es.awaitEvent();
        es.reset();

        verifyProperties(cp);
    }
",non-flaky,5
38636,apache_pulsar,TestApplication.testBasicConfiguration,"    @Test
    public void testBasicConfiguration() throws Exception {

        EventBus eventBus = new EventBus(""test-event-bus"");

        MaterializedConfiguration materializedConfiguration = new
                SimpleMaterializedConfiguration();

        SourceRunner sourceRunner = mockLifeCycle(SourceRunner.class);
        materializedConfiguration.addSourceRunner(""test"", sourceRunner);

        SinkRunner sinkRunner = mockLifeCycle(SinkRunner.class);
        materializedConfiguration.addSinkRunner(""test"", sinkRunner);

        Channel channel = mockLifeCycle(Channel.class);
        materializedConfiguration.addChannel(""test"", channel);


        ConfigurationProvider configurationProvider = mock(ConfigurationProvider.class);
        when(configurationProvider.getConfiguration()).thenReturn(materializedConfiguration);

        Application application = new Application();
        eventBus.register(application);
        eventBus.post(materializedConfiguration);
        application.start();

        Thread.sleep(1000L);

        verify(sourceRunner).start();
        verify(sinkRunner).start();
        verify(channel).start();

        application.stop();

        Thread.sleep(1000L);

        verify(sourceRunner).stop();
        verify(sinkRunner).stop();
        verify(channel).stop();
    }
",non-flaky,5
38637,apache_pulsar,TestApplication.testFLUME1854,"    @Test
    public void testFLUME1854() throws Exception {
        File configFile = new File(baseDir, ""flume-conf.properties"");
        Files.copy(new File(getClass().getClassLoader()
                .getResource(""flume-conf.properties"").getFile()), configFile);
        Random random = new Random();
        for (int i = 0; i < 3; i++) {
            EventBus eventBus = new EventBus(""test-event-bus"");
            PollingPropertiesFileConfigurationProvider configurationProvider =
                    new PollingPropertiesFileConfigurationProvider(""host1"",
                            configFile, eventBus, 1);
            List<LifecycleAware> components = Lists.newArrayList();
            components.add(configurationProvider);
            Application application = new Application(components);
            eventBus.register(application);
            application.start();
            Thread.sleep(random.nextInt(10000));
            application.stop();
        }
    }
",non-flaky,5
38638,apache_pulsar,TestApplication.answer,"    @Test(timeOut = 10000L)
    public void testFLUME2786() throws Exception {
        final String agentName = ""test"";
        final int interval = 1;
        final long intervalMs = 1000L;

        File configFile = new File(baseDir, ""flume-conf.properties"");
        Files.copy(new File(getClass().getClassLoader()
                .getResource(""flume-conf.properties.2786"").getFile()), configFile);
        File mockConfigFile = spy(configFile);
        when(mockConfigFile.lastModified()).then(new Answer<Long>() {
            @Override
            public Long answer(InvocationOnMock invocation) throws Throwable {
                Thread.sleep(intervalMs);
                return System.currentTimeMillis();
            }
",non-flaky,5
38639,apache_pulsar,TestEnvVarResolverProperties.resolveEnvVar,"    @Test
    public void resolveEnvVar() {
        environmentVariables.set(""VARNAME"", ""varvalue"");
        String resolved = EnvVarResolverProperties.resolveEnvVars(""padding ${VARNAME} padding"");
        Assert.assertEquals(""padding varvalue padding"", resolved);
    }
",non-flaky,5
38640,apache_pulsar,TestEnvVarResolverProperties.resolveEnvVars,"    @Test
    public void resolveEnvVars() {
        environmentVariables.set(""VARNAME1"", ""varvalue1"");
        environmentVariables.set(""VARNAME2"", ""varvalue2"");
        String resolved = EnvVarResolverProperties
                .resolveEnvVars(""padding ${VARNAME1} ${VARNAME2} padding"");
        Assert.assertEquals(""padding varvalue1 varvalue2 padding"", resolved);
    }
",non-flaky,5
38641,apache_pulsar,TestEnvVarResolverProperties.getProperty,"    @Test
    public void getProperty() {
        String NC_PORT = ""6667"";
        environmentVariables.set(""NC_PORT"", NC_PORT);
        System.setProperty(""propertiesImplementation"",
                ""org.apache.pulsar.io.flume.node.EnvVarResolverProperties"");

        Assert.assertEquals(NC_PORT, provider.getFlumeConfiguration()
                .getConfigurationFor(""a1"")
                .getSourceContext().get(""r1"").getParameters().get(""port""));
    }
",non-flaky,5
38642,apache_pulsar,TestStaticZooKeeperConfigurationProvider.testPropertyRead,"    @Test
    public void testPropertyRead() throws Exception {
        verifyProperties(configurationProvider);
    }
",non-flaky,5
38643,apache_pulsar,TestPropertiesFileConfigurationProvider.testPropertyRead,"    @Test
    public void testPropertyRead() {

        FlumeConfiguration configuration = provider.getFlumeConfiguration();
        assertNotNull(configuration);

    /*
     * Test the known errors in the file
     */
        List<String> expected = Lists.newArrayList();
        expected.add(""host5 CONFIG_ERROR"");
        expected.add(""host5 INVALID_PROPERTY"");
        expected.add(""host4 CONFIG_ERROR"");
        expected.add(""host4 CONFIG_ERROR"");
        expected.add(""host4 PROPERTY_VALUE_NULL"");
        expected.add(""host4 PROPERTY_VALUE_NULL"");
        expected.add(""host4 PROPERTY_VALUE_NULL"");
        expected.add(""host4 AGENT_CONFIGURATION_INVALID"");
        expected.add(""ch2 ATTRS_MISSING"");
        expected.add(""host3 CONFIG_ERROR"");
        expected.add(""host3 PROPERTY_VALUE_NULL"");
        expected.add(""host3 AGENT_CONFIGURATION_INVALID"");
        expected.add(""host2 PROPERTY_VALUE_NULL"");
        expected.add(""host2 AGENT_CONFIGURATION_INVALID"");
        List<String> actual = Lists.newArrayList();
        for (FlumeConfigurationError error : configuration.getConfigurationErrors()) {
            actual.add(error.getComponentName() + "" "" + error.getErrorType().toString());
        }
        Collections.sort(expected);
        Collections.sort(actual);
        assertEquals(actual, expected);

        AgentConfiguration agentConfiguration =
                configuration.getConfigurationFor(""host1"");
        assertNotNull(agentConfiguration);

        LOGGER.info(agentConfiguration.getPrevalidationConfig());
        LOGGER.info(agentConfiguration.getPostvalidationConfig());

        Set<String> sources = Sets.newHashSet(""source1"");
        Set<String> sinks = Sets.newHashSet(""sink1"");
        Set<String> channels = Sets.newHashSet(""channel1"");

        assertEquals(agentConfiguration.getSourceSet(), sources);
        assertEquals(agentConfiguration.getSinkSet(), sinks);
        assertEquals(agentConfiguration.getChannelSet(), channels);
    }
",non-flaky,5
38644,apache_pulsar,StringSinkTests.TestOpenAndWriteSink,"    @Test
    public void TestOpenAndWriteSink() throws Exception {
        Map<String, Object> conf = Maps.newHashMap();
        StringSink stringSink = new StringSink();
        conf.put(""name"", ""a1"");
        conf.put(""confFile"", ""./src/test/resources/flume/source.conf"");
        conf.put(""noReloadConf"", false);
        conf.put(""zkConnString"", """");
        conf.put(""zkBasePath"", """");
        stringSink.open(conf, mockSinkContext);
        send(stringSink, 100);

        Thread.sleep(3 * 1000);
        Transaction transaction = channel.getTransaction();
        transaction.begin();
        Event event = channel.take();

        Assert.assertNotNull(event);
        Assert.assertNotNull(mockRecord);

        verify(mockRecord, times(100)).ack();
        transaction.commit();
        transaction.close();
    }
",non-flaky,5
38645,apache_pulsar,StringSourceTests.TestOpenAndReadSource,"    @Test
    public void TestOpenAndReadSource() throws Exception {
        Map<String, Object> conf = Maps.newHashMap();
        StringSource stringSource = new StringSource();
        conf.put(""name"", ""a1"");
        conf.put(""confFile"", ""./src/test/resources/flume/sink.conf"");
        conf.put(""noReloadConf"", false);
        conf.put(""zkConnString"", """");
        conf.put(""zkBasePath"", """");
        Event event = EventBuilder.withBody(""test event 1"", Charsets.UTF_8);
        stringSource.open(conf, mockSourceContext);
        Thread.sleep(3 * 1000);
        sink.start();
        Transaction transaction = channel.getTransaction();

        transaction.begin();
        for (int i = 0; i < 10; i++) {
            channel.put(event);
        }
        transaction.commit();
        transaction.close();

        for (int i = 0; i < 5; i++) {
            Sink.Status status = sink.process();
            assertEquals(status, Sink.Status.READY);
        }

        assertEquals(sink.process(), Sink.Status.BACKOFF);
        stringSource.close();
    }
",non-flaky,5
38646,apache_pulsar,UtilsTest.testJsonSerialization,"    @Test
    public void testJsonSerialization() throws Exception {

        final String[] keyNames = { ""key1"", ""key2"" };
        final String key1Value = ""test1"";
        final String key2Value = ""test2"";
        final byte[][] keyValues = { key1Value.getBytes(), key2Value.getBytes() };
        final String param = ""param"";
        final String algo = ""algo"";
        int batchSize = 10;
        int compressionMsgSize = 10;

        // serialize to json
        byte[] data = ""payload"".getBytes();
        Map<String, String> properties = Maps.newHashMap();
        properties.put(""prop1"", ""value"");
        Map<String, String> metadata1 = Maps.newHashMap();
        metadata1.put(""version"", ""v1"");
        metadata1.put(""ckms"", ""cmks-1"");
        Map<String, String> metadata2 = Maps.newHashMap();
        metadata2.put(""version"", ""v2"");
        metadata2.put(""ckms"", ""cmks-2"");
        Record<byte[]> recordCtx = createRecord(data, algo, keyNames, keyValues, param.getBytes(), metadata1, metadata2,
                batchSize, compressionMsgSize, properties, true);
        String json = Utils.serializeRecordToJson(recordCtx);

        // deserialize from json and assert
        KinesisMessageResponse kinesisJsonResponse = deSerializeRecordFromJson(json);
        assertEquals(data, getDecoder().decode(kinesisJsonResponse.getPayloadBase64()));
        EncryptionCtx encryptionCtxDeser = kinesisJsonResponse.getEncryptionCtx();
        assertEquals(key1Value.getBytes(),
                getDecoder().decode(encryptionCtxDeser.getKeysMapBase64().get(keyNames[0])));
        assertEquals(key2Value.getBytes(),
                getDecoder().decode(encryptionCtxDeser.getKeysMapBase64().get(keyNames[1])));
        assertEquals(param.getBytes(), getDecoder().decode(encryptionCtxDeser.getEncParamBase64()));
        assertEquals(algo, encryptionCtxDeser.getAlgorithm());
        assertEquals(metadata1, encryptionCtxDeser.getKeysMetadataMap().get(keyNames[0]));
        assertEquals(metadata2, encryptionCtxDeser.getKeysMetadataMap().get(keyNames[1]));
        assertEquals(properties, kinesisJsonResponse.getProperties());

    }
",non-flaky,5
38647,apache_pulsar,UtilsTest.testFbSerialization,"    @Test(dataProvider=""encryption"")
    public void testFbSerialization(boolean isEncryption) throws Exception {

        final String[] keyNames = { ""key1"", ""key2"" };
        final String param = ""param"";
        final String algo = ""algo"";
        int batchSize = 10;
        int compressionMsgSize = 10;

        for (int k = 0; k < 5; k++) {
            String payloadString = RandomStringUtils.random(142342 * k, String.valueOf(System.currentTimeMillis()));
            final String key1Value = payloadString + ""test1"";
            final String key2Value = payloadString + ""test2"";
            final byte[][] keyValues = { key1Value.getBytes(), key2Value.getBytes() };
            byte[] data = payloadString.getBytes();
            Map<String, String> properties = Maps.newHashMap();
            properties.put(""prop1"", payloadString);
            Map<String, String> metadata1 = Maps.newHashMap();
            metadata1.put(""version"", ""v1"");
            metadata1.put(""ckms"", ""cmks-1"");
            Map<String, String> metadata2 = Maps.newHashMap();
            metadata2.put(""version"", ""v2"");
            metadata2.put(""ckms"", ""cmks-2"");
            Record<byte[]> record = createRecord(data, algo, keyNames, keyValues, param.getBytes(), metadata1,
                    metadata2, batchSize, compressionMsgSize, properties, isEncryption);
            ByteBuffer flatBuffer = Utils.serializeRecordToFlatBuffer(record);

            Message kinesisJsonResponse = Message.getRootAsMessage(flatBuffer);
            byte[] fbPayloadBytes = new byte[kinesisJsonResponse.payloadLength()];
            kinesisJsonResponse.payloadAsByteBuffer().get(fbPayloadBytes);
            assertEquals(data, fbPayloadBytes);

            if(isEncryption) {
                org.apache.pulsar.io.kinesis.fbs.EncryptionCtx encryptionCtxDeser = kinesisJsonResponse.encryptionCtx();
                byte compressionType = encryptionCtxDeser.compressionType();
                int fbBatchSize = encryptionCtxDeser.batchSize();
                boolean isBathcMessage = encryptionCtxDeser.isBatchMessage();
                int fbCompressionMsgSize = encryptionCtxDeser.uncompressedMessageSize();
                int totalKeys = encryptionCtxDeser.keysLength();
                Map<String, Map<String, String>> fbKeyMetadataResult = Maps.newHashMap();
                Map<String, byte[]> fbKeyValueResult = Maps.newHashMap();
                for (int i = 0; i < encryptionCtxDeser.keysLength(); i++) {
                    org.apache.pulsar.io.kinesis.fbs.EncryptionKey encryptionKey = encryptionCtxDeser.keys(i);
                    String keyName = encryptionKey.key();
                    byte[] keyValueBytes = new byte[encryptionKey.valueLength()];
                    encryptionKey.valueAsByteBuffer().get(keyValueBytes);
                    fbKeyValueResult.put(keyName, keyValueBytes);
                    Map<String, String> fbMetadata = Maps.newHashMap();
                    for (int j = 0; j < encryptionKey.metadataLength(); j++) {
                        KeyValue encMtdata = encryptionKey.metadata(j);
                        fbMetadata.put(encMtdata.key(), encMtdata.value());
                    }
                    fbKeyMetadataResult.put(keyName, fbMetadata);
                }
                byte[] paramBytes = new byte[encryptionCtxDeser.paramLength()];
                encryptionCtxDeser.paramAsByteBuffer().get(paramBytes);

                assertEquals(totalKeys, 2);
                assertEquals(batchSize, fbBatchSize);
                assertTrue(isBathcMessage);
                assertEquals(compressionMsgSize, fbCompressionMsgSize);
                assertEquals(keyValues[0], fbKeyValueResult.get(keyNames[0]));
                assertEquals(keyValues[1], fbKeyValueResult.get(keyNames[1]));
                assertEquals(metadata1, fbKeyMetadataResult.get(keyNames[0]));
                assertEquals(metadata2, fbKeyMetadataResult.get(keyNames[1]));
                assertEquals(compressionType, org.apache.pulsar.io.kinesis.fbs.CompressionType.LZ4);
                assertEquals(param.getBytes(), paramBytes);
                assertEquals(algo, encryptionCtxDeser.algo());
            }

            Map<String, String> fbproperties = Maps.newHashMap();
            for (int i = 0; i < kinesisJsonResponse.propertiesLength(); i++) {
                KeyValue property = kinesisJsonResponse.properties(i);
                fbproperties.put(property.key(), property.value());
            }
            assertEquals(properties, fbproperties);

        }
    }
",non-flaky,5
38648,apache_pulsar,KinesisSinkTest.testDefaultCredentialProvider,"    @Test
    public void testDefaultCredentialProvider() throws Exception {
        KinesisSink sink = new KinesisSink();
        Map<String, String> credentialParam = Maps.newHashMap();
        String awsCredentialPluginParam = new Gson().toJson(credentialParam);
        try {
            sink.defaultCredentialProvider(awsCredentialPluginParam);
            Assert.fail(""accessKey and SecretKey validation not applied"");
        } catch (IllegalArgumentException ie) {
            // Ok..
        }

        final String accesKey = ""ak"";
        final String secretKey = ""sk"";
        credentialParam.put(KinesisSink.ACCESS_KEY_NAME, accesKey);
        credentialParam.put(KinesisSink.SECRET_KEY_NAME, secretKey);
        awsCredentialPluginParam = new Gson().toJson(credentialParam);
        AWSCredentialsProvider credentialProvider = sink.defaultCredentialProvider(awsCredentialPluginParam)
                .getCredentialProvider();
        Assert.assertNotNull(credentialProvider);
        Assert.assertEquals(credentialProvider.getCredentials().getAWSAccessKeyId(), accesKey);
        Assert.assertEquals(credentialProvider.getCredentials().getAWSSecretKey(), secretKey);

        sink.close();
    }
",non-flaky,5
38649,apache_pulsar,KinesisSinkTest.testCredentialProvider,"    @Test
    public void testCredentialProvider() throws Exception {
        KinesisSink sink = new KinesisSink();

        final String accesKey = ""ak"";
        final String secretKey = ""sk"";
        Map<String, String> credentialParam = Maps.newHashMap();
        credentialParam.put(KinesisSink.ACCESS_KEY_NAME, accesKey);
        credentialParam.put(KinesisSink.SECRET_KEY_NAME, secretKey);
        String awsCredentialPluginParam = new Gson().toJson(credentialParam);
        AWSCredentialsProvider credentialProvider = sink.createCredentialProvider(null, awsCredentialPluginParam)
                .getCredentialProvider();
        Assert.assertEquals(credentialProvider.getCredentials().getAWSAccessKeyId(), accesKey);
        Assert.assertEquals(credentialProvider.getCredentials().getAWSSecretKey(), secretKey);

        credentialProvider = sink.createCredentialProvider(AwsCredentialProviderPluginImpl.class.getName(), ""{}"")
                .getCredentialProvider();
        Assert.assertNotNull(credentialProvider);
        Assert.assertEquals(credentialProvider.getCredentials().getAWSAccessKeyId(),
                AwsCredentialProviderPluginImpl.accessKey);
        Assert.assertEquals(credentialProvider.getCredentials().getAWSSecretKey(),
                AwsCredentialProviderPluginImpl.secretKey);
        Assert.assertEquals(((BasicSessionCredentials) credentialProvider.getCredentials()).getSessionToken(),
                AwsCredentialProviderPluginImpl.sessionToken);

        sink.close();
    }
",non-flaky,5
38650,apache_pulsar,KinesisSinkTest.testCredentialProviderPlugin,"    @Test
    public void testCredentialProviderPlugin() throws Exception {
        KinesisSink sink = new KinesisSink();

        AWSCredentialsProvider credentialProvider = sink
                .createCredentialProviderWithPlugin(AwsCredentialProviderPluginImpl.class.getName(), ""{}"")
                .getCredentialProvider();
        Assert.assertNotNull(credentialProvider);
        Assert.assertEquals(credentialProvider.getCredentials().getAWSAccessKeyId(),
                AwsCredentialProviderPluginImpl.accessKey);
        Assert.assertEquals(credentialProvider.getCredentials().getAWSSecretKey(),
                AwsCredentialProviderPluginImpl.secretKey);
        Assert.assertEquals(((BasicSessionCredentials) credentialProvider.getCredentials()).getSessionToken(),
                AwsCredentialProviderPluginImpl.sessionToken);

        sink.close();
    }
",non-flaky,5
38651,apache_pulsar,SolrGenericRecordSinkTest.TestOpenAndWriteSink,"    @Test
    public void TestOpenAndWriteSink() throws Exception {
        message = mock(MessageImpl.class);
        Map<String, Object> configs = new HashMap<>();
        configs.put(""solrUrl"", ""http://localhost:8983/solr"");
        configs.put(""solrMode"", ""Standalone"");
        configs.put(""solrCollection"", ""techproducts"");
        configs.put(""solrCommitWithinMs"", ""100"");
        configs.put(""username"", """");
        configs.put(""password"", """");
        GenericSchema<GenericRecord> genericAvroSchema;

        SolrGenericRecordSink sink = new SolrGenericRecordSink();

        // prepare a foo Record
        Foo obj = new Foo();
        obj.setField1(""FakeFiled1"");
        obj.setField2(""FakeFiled1"");
        AvroSchema<Foo> schema = AvroSchema.of(Foo.class);

        byte[] bytes = schema.encode(obj);
        AutoConsumeSchema autoConsumeSchema = new AutoConsumeSchema();
        autoConsumeSchema.setSchema(GenericSchemaImpl.of(schema.getSchemaInfo()));

        Record<GenericRecord> record = PulsarRecord.<GenericRecord>builder()
            .message(message)
            .topicName(""fake_topic_name"")
            .build();

        genericAvroSchema = new GenericAvroSchema(schema.getSchemaInfo());

        when(message.getValue())
                .thenReturn(genericAvroSchema.decode(bytes));

        log.info(""foo:{}, Message.getValue: {}, record.getValue: {}"",
            obj.toString(),
            message.getValue().toString(),
            record.getValue().toString());

        // open should success
        sink.open(configs, null);
    }
",non-flaky,5
38652,apache_pulsar,SinkTest.testSinkContext,"    @Test
    public void testSinkContext() throws Exception {
        SinkContext sinkContext = mock(SinkContext.class);

        Sink testSink = spy(TestSink.class);
        testSink.open(new HashMap<>(), sinkContext);

        verify(sinkContext, times(1)).recordMetric(""foo"", 1);
    }
",non-flaky,5
38653,apache_pulsar,BatchPushSourceTest.testNotifyErrors,"  @Test(expectedExceptions = RuntimeException.class, expectedExceptionsMessageRegExp = ""test exception"")
  public void testNotifyErrors() throws Exception {
    testBatchSource.notifyError(new RuntimeException(""test exception""));
    testBatchSource.readNext();
  }
",non-flaky,5
38654,apache_pulsar,SourceTest.testSinkContext,"    @Test
    public void testSinkContext() throws Exception {
        SourceContext sourceContext = mock(SourceContext.class);

        Source testSource = spy(TestSource.class);
        testSource.open(new HashMap<>(), sourceContext);

        verify(sourceContext, times(1)).recordMetric(""foo"", 1);
    }
",non-flaky,5
38655,apache_pulsar,ByteBufferSchemaWrapperTest.testGetBytesNoCopy,"    @Test
    public void testGetBytesNoCopy() throws Exception {
        byte[] originalArray = {1, 2, 3};
        ByteBuffer wrapped = ByteBuffer.wrap(originalArray);
        assertEquals(0, wrapped.arrayOffset());
        assertEquals(3, wrapped.remaining());
        assertSame(ByteBufferSchemaWrapper.getBytes(wrapped), originalArray);
    }
",non-flaky,5
38656,apache_pulsar,ByteBufferSchemaWrapperTest.testGetBytesOffsetZeroDifferentLen,"    @Test
    public void testGetBytesOffsetZeroDifferentLen() throws Exception {
        byte[] originalArray = {1, 2, 3};
        ByteBuffer wrapped = ByteBuffer.wrap(originalArray, 1, 2);
        assertEquals(0, wrapped.arrayOffset());
        assertEquals(2, wrapped.remaining());
        byte[] result = ByteBufferSchemaWrapper.getBytes(wrapped);
        assertNotSame(result, originalArray);
        assertArrayEquals(result, new byte[] {2,3});
    }
",non-flaky,5
38657,apache_pulsar,ByteBufferSchemaWrapperTest.testGetBytesOffsetNonZero,"    @Test
    public void testGetBytesOffsetNonZero() throws Exception {
        byte[] originalArray = {1, 2, 3};
        ByteBuffer wrapped = ByteBuffer.wrap(originalArray);
        wrapped.position(1);
        assertEquals(1, wrapped.position());
        wrapped = wrapped.slice();
        assertEquals(1, wrapped.arrayOffset());
        assertEquals(2, wrapped.remaining());
        byte[] result = ByteBufferSchemaWrapper.getBytes(wrapped);
        assertNotSame(result, originalArray);
        assertArrayEquals(result, new byte[] {2,3});
    }
",non-flaky,5
38658,apache_pulsar,ByteBufferSchemaWrapperTest.testGetBytesOffsetZero,"    @Test
    public void testGetBytesOffsetZero() throws Exception {
        byte[] originalArray = {1, 2, 3};
        ByteBuffer wrapped = ByteBuffer.wrap(originalArray, 0, 2);
        assertEquals(0, wrapped.arrayOffset());
        assertEquals(2, wrapped.remaining());
        byte[] result = ByteBufferSchemaWrapper.getBytes(wrapped);
        assertNotSame(result, originalArray);
        assertArrayEquals(result, new byte[] {1,2});
    }
",non-flaky,5
38659,apache_pulsar,KafkaBytesSourceTest.testNoKeyValueSchema,"    @Test
    public void testNoKeyValueSchema() throws Exception {

        validateSchemaNoKeyValue(StringDeserializer.class.getName(), Schema.STRING,
                StringDeserializer.class.getName(), Schema.STRING);

        validateSchemaNoKeyValue(StringDeserializer.class.getName(), Schema.STRING,
                ByteBufferDeserializer.class.getName(), Schema.BYTEBUFFER);

        validateSchemaNoKeyValue(StringDeserializer.class.getName(), Schema.STRING,
                BytesDeserializer.class.getName(), Schema.BYTEBUFFER);

        validateSchemaNoKeyValue(StringDeserializer.class.getName(), Schema.STRING,
                DoubleDeserializer.class.getName(), Schema.DOUBLE);

        validateSchemaNoKeyValue(StringDeserializer.class.getName(), Schema.STRING,
                FloatDeserializer.class.getName(), Schema.FLOAT);

        validateSchemaNoKeyValue(StringDeserializer.class.getName(), Schema.STRING,
                IntegerDeserializer.class.getName(), Schema.INT32);

        validateSchemaNoKeyValue(StringDeserializer.class.getName(), Schema.STRING,
                LongDeserializer.class.getName(), Schema.INT64);

        validateSchemaNoKeyValue(StringDeserializer.class.getName(), Schema.STRING,
                ShortDeserializer.class.getName(), Schema.INT16);

        validateSchemaNoKeyValue(StringDeserializer.class.getName(), Schema.STRING,
                KafkaAvroDeserializer.class.getName(), KafkaBytesSource.DeferredSchemaPlaceholder.INSTANCE);

    }
",non-flaky,5
38660,apache_pulsar,KafkaBytesSourceTest.testKeyValueSchema,"    @Test
    public void testKeyValueSchema() throws Exception {
        validateSchemaKeyValue(IntegerDeserializer.class.getName(), Schema.INT32,
                StringDeserializer.class.getName(), Schema.STRING,
                ByteBuffer.wrap(new IntegerSerializer().serialize(""test"", 10)),
                ByteBuffer.wrap(new StringSerializer().serialize(""test"", ""test"")));
    }
",non-flaky,5
38661,apache_pulsar,KafkaAbstractSinkTest.getInstanceId,"    @Test
    public void testInvalidConfigWillThrownException() throws Exception {
        KafkaAbstractSink sink = new DummySink();
        Map<String, Object> config = new HashMap<>();
        SinkContext sc = new SinkContext() {
            @Override
            public int getInstanceId() {
                return 0;
            }
",non-flaky,5
38662,apache_pulsar,KafkaAbstractSourceTest.testInvalidConfigWillThrownException,"    @Test
    public void testInvalidConfigWillThrownException() throws Exception {
        KafkaAbstractSource source = new DummySource();
        SourceContext ctx = mock(SourceContext.class);
        Map<String, Object> config = new HashMap<>();
        Assert.ThrowingRunnable openAndClose = ()->{
            try {
                source.open(config, ctx);
                fail();
            } finally {
                source.close();
            }
        };
        expectThrows(NullPointerException.class, openAndClose);
        config.put(""topic"", ""topic_1"");
        expectThrows(NullPointerException.class, openAndClose);
        config.put(""bootstrapServers"", ""localhost:8080"");
        expectThrows(NullPointerException.class, openAndClose);
        config.put(""groupId"", ""test-group"");
        config.put(""fetchMinBytes"", -1);
        expectThrows(IllegalArgumentException.class, openAndClose);
        config.put(""fetchMinBytes"", 1000);
        config.put(""autoCommitEnabled"", true);
        config.put(""autoCommitIntervalMs"", -1);
        expectThrows(IllegalArgumentException.class, openAndClose);
        config.put(""autoCommitIntervalMs"", 100);
        config.put(""sessionTimeoutMs"", -1);
        expectThrows(IllegalArgumentException.class, openAndClose);
        config.put(""sessionTimeoutMs"", 10000);
        config.put(""heartbeatIntervalMs"", -100);
        expectThrows(IllegalArgumentException.class, openAndClose);
        config.put(""heartbeatIntervalMs"", 20000);
        expectThrows(IllegalArgumentException.class, openAndClose);
        config.put(""heartbeatIntervalMs"", 5000);
        config.put(""autoOffsetReset"", ""some-value"");
        expectThrows(IllegalArgumentException.class, openAndClose);
        config.put(""autoOffsetReset"", ""earliest"");
        source.open(config, ctx);
        source.close();
    }
",non-flaky,5
38663,apache_pulsar,PulsarDatabaseHistoryTest.shouldStartWithEmptyTopicAndStoreDataAndRecoverAllState,"    @Test
    public void shouldStartWithEmptyTopicAndStoreDataAndRecoverAllState() throws Exception {
        // Create the empty topic ...
        testHistoryTopicContent(false, true);
    }
",non-flaky,5
38664,apache_pulsar,PulsarDatabaseHistoryTest.shouldIgnoreUnparseableMessages,"    @Test
    public void shouldIgnoreUnparseableMessages() throws Exception {
        try (final Producer<String> producer = pulsarClient.newProducer(Schema.STRING)
            .topic(topicName)
            .create()
        ) {
            producer.send("""");
            producer.send(""{\""position\"":{\""filename\"":\""my-txn-file.log\"",\""position\"":39},\""databaseName\"":\""db1\"",\""ddl\"":\""DROP TABLE foo;\""}"");
            producer.send(""{\""source\"":{\""server\"":\""my-server\""},\""databaseName\"":\""db1\"",\""ddl\"":\""DROP TABLE foo;\""}"");
            producer.send(""{\""source\"":{\""server\"":\""my-server\""},\""position\"":{\""filename\"":\""my-txn-file.log\"",\""position\"":39},\""databaseName\"":\""db1\"",\""ddl\"":\""DROP TABLE foo;\"""");
            producer.send(""\""source\"":{\""server\"":\""my-server\""},\""position\"":{\""filename\"":\""my-txn-file.log\"",\""position\"":39},\""databaseName\"":\""db1\"",\""ddl\"":\""DROP TABLE foo;\""}"");
            producer.send(""{\""source\"":{\""server\"":\""my-server\""},\""position\"":{\""filename\"":\""my-txn-file.log\"",\""position\"":39},\""databaseName\"":\""db1\"",\""ddl\"":\""xxxDROP TABLE foo;\""}"");
        }

        testHistoryTopicContent(true, true);
    }
",non-flaky,5
38665,apache_pulsar,PulsarDatabaseHistoryTest.shouldStopOnUnparseableSQL,"    @Test(expectedExceptions = ParsingException.class)
    public void shouldStopOnUnparseableSQL() throws Exception {
        try (final Producer<String> producer = pulsarClient.newProducer(Schema.STRING).topic(topicName).create()) {
            producer.send(""{\""source\"":{\""server\"":\""my-server\""},\""position\"":{\""filename\"":\""my-txn-file.log\"",\""position\"":39},\""databaseName\"":\""db1\"",\""ddl\"":\""xxxDROP TABLE foo;\""}"");
        }

        testHistoryTopicContent(false, false);
    }
",non-flaky,5
38666,apache_pulsar,PulsarDatabaseHistoryTest.testExists,"    @Test
    public void testExists() throws Exception {
        // happy path
        testHistoryTopicContent(true, false);
        assertTrue(history.exists());

        // Set history to use dummy topic
        Configuration config = Configuration.create()
            .with(PulsarDatabaseHistory.SERVICE_URL, brokerUrl.toString())
            .with(PulsarDatabaseHistory.TOPIC, ""persistent://my-property/my-ns/dummytopic"")
            .with(DatabaseHistory.NAME, ""my-db-history"")
            .with(DatabaseHistory.SKIP_UNPARSEABLE_DDL_STATEMENTS, true)
            .build();

        history.configure(config, null, DatabaseHistoryListener.NOOP, true);
        history.start();

        // dummytopic should not exist yet
        assertFalse(history.exists());
    }
",non-flaky,5
38667,apache_pulsar,InfluxDBGenericRecordSinkTest.openInfluxV1,"    @Test
    public void openInfluxV1() throws Exception {
        Map<String, Object> map = new HashMap<>();
        map.put(""influxdbUrl"", ""http://localhost:8086"");
        map.put(""database"", ""test_db"");

        InfluxDBGenericRecordSink sink = new InfluxDBGenericRecordSink();
        try {
            sink.open(map, mock(SinkContext.class));
        } catch (InfluxDBIOException e) {
            // Do nothing
        }
        assertTrue(sink.sink instanceof org.apache.pulsar.io.influxdb.v1.InfluxDBGenericRecordSink);
    }
",non-flaky,5
38668,apache_pulsar,InfluxDBGenericRecordSinkTest.openInfluxV2,"    @Test
    public void openInfluxV2() throws Exception {
        Map<String, Object> map = new HashMap();
        map.put(""influxdbUrl"", ""http://localhost:9999"");
        map.put(""token"", ""xxxx"");
        map.put(""organization"", ""example-org"");
        map.put(""bucket"", ""example-bucket"");

        InfluxDBGenericRecordSink sink = new InfluxDBGenericRecordSink();
        try {
            sink.open(map, mock(SinkContext.class));
        } catch (InfluxDBIOException e) {
            // Do nothing
        }
        assertTrue(sink.sink instanceof InfluxDBSink);
    }
",non-flaky,5
38669,apache_pulsar,InfluxDBGenericRecordSinkTest.openInvalidInfluxConfig,"    @Test(expectedExceptions = Exception.class,
    public void openInvalidInfluxConfig() throws Exception {
        InfluxDBGenericRecordSink sink = new InfluxDBGenericRecordSink();
        sink.open(new HashMap<>(), mock(SinkContext.class));
    }
",non-flaky,5
38670,apache_pulsar,InfluxDBSinkTest.testJsonSchema,"    @Test
    public void testJsonSchema() {
        JSONSchema<Cpu> schema = JSONSchema.of(Cpu.class);

        AutoConsumeSchema autoConsumeSchema = new AutoConsumeSchema();
        autoConsumeSchema.setSchema(GenericSchemaImpl.of(schema.getSchemaInfo()));
        GenericSchema<GenericRecord> genericSchema = GenericSchemaImpl.of(autoConsumeSchema.getSchemaInfo());

        assertFalse(genericSchema instanceof GenericAvroSchema);

        byte[] bytes = schema.encode(cpu);
        GenericRecord record = genericSchema.decode(bytes);

        assertEquals(record.getField(""measurement""), ""cpu"");

        // compare the String type
        assertEquals(record.getField(""timestamp"").toString(), timestamp + """");

        assertEquals(((GenericRecord)record.getField(""tags"")).getField(""host""), ""server-1"");
        assertEquals(((GenericRecord)record.getField(""fields"")).getField(""value""), 10);
    }
",non-flaky,5
38671,apache_pulsar,InfluxDBSinkTest.testAvroSchema,"    @Test
    public void testAvroSchema() {
        AvroSchema<Cpu> schema = AvroSchema.of(Cpu.class);

        AutoConsumeSchema autoConsumeSchema = new AutoConsumeSchema();
        autoConsumeSchema.setSchema(GenericSchemaImpl.of(schema.getSchemaInfo()));
        GenericSchema<GenericRecord> genericAvroSchema = GenericSchemaImpl.of(autoConsumeSchema.getSchemaInfo());

        assertTrue(genericAvroSchema instanceof GenericAvroSchema);

        byte[] bytes = schema.encode(cpu);
        GenericRecord record = genericAvroSchema.decode(bytes);

        assertEquals(record.getField(""measurement""), ""cpu"");
        assertEquals(record.getField(""timestamp""), timestamp);
        assertEquals(((Map)record.getField(""tags"")).get(new Utf8(""host"")).toString(), ""server-1"");
        assertEquals(((Map)record.getField(""fields"")).get(new Utf8(""value"")), 10);
    }
",non-flaky,5
38672,apache_pulsar,InfluxDBSinkTest.testOpenWriteCloseAvro,"    @Test
    public void testOpenWriteCloseAvro() throws Exception {
        AvroSchema<Cpu> avroSchema = AvroSchema.of(Cpu.class);
        openWriteClose(avroSchema);
    }
",non-flaky,5
38673,apache_pulsar,InfluxDBSinkTest.testOpenWriteCloseJson,"    @Test
    public void testOpenWriteCloseJson() throws Exception {
        JSONSchema<Cpu> jsonSchema = JSONSchema.of(Cpu.class);
        openWriteClose(jsonSchema);
    }
",non-flaky,5
38674,apache_pulsar,InfluxDBSinkConfigTest.testRequiredConfigMissing,"    @Test(expectedExceptions = NullPointerException.class,
    public void testRequiredConfigMissing() throws Exception {
        Map<String, Object> map = buildValidConfigMap();
        map.remove(""influxdbUrl"");
        InfluxDBSinkConfig config = InfluxDBSinkConfig.load(map);
        config.validate();
    }
",non-flaky,5
38675,apache_pulsar,InfluxDBSinkConfigTest.testBatchConfig,"    @Test(expectedExceptions = IllegalArgumentException.class,
    public void testBatchConfig() throws Exception {
        Map<String, Object> map = buildValidConfigMap();
        map.put(""batchSize"", -1);
        InfluxDBSinkConfig config = InfluxDBSinkConfig.load(map);
        config.validate();
    }
",non-flaky,5
38676,apache_pulsar,InfluxDBGenericRecordSinkTest.testOpenAndWrite,"    @Test
    public void testOpenAndWrite() throws Exception {
        message = mock(MessageImpl.class);
        GenericSchema<GenericRecord> genericAvroSchema;
        // prepare a cpu Record
        Cpu cpu = new Cpu();
        cpu.setMeasurement(""cpu"");
        cpu.setModel(""lenovo"");
        cpu.setValue(10);

        Map<String, String> tags = Maps.newHashMap();
        tags.put(""host"", ""server-1"");
        tags.put(""region"", ""us-west"");

        cpu.setTags(tags);
        AvroSchema<Cpu> schema = AvroSchema.of(Cpu.class);

        byte[] bytes = schema.encode(cpu);
        AutoConsumeSchema autoConsumeSchema = new AutoConsumeSchema();
        autoConsumeSchema.setSchema(GenericSchemaImpl.of(schema.getSchemaInfo()));

        Record<GenericRecord> record = PulsarRecord.<GenericRecord>builder()
            .message(message)
            .topicName(""influx_cpu"")
            .build();

        genericAvroSchema = new GenericAvroSchema(schema.getSchemaInfo());

        when(message.getValue())
                .thenReturn(genericAvroSchema.decode(bytes));

        log.info(""cpu:{}, Message.getValue: {}, record.getValue: {}"",
            cpu.toString(),
            message.getValue().toString(),
            record.getValue().toString());

        influxSink.open(configMap, mockSinkContext);

        verify(this.influxDB, times(1)).describeDatabases();
        verify(this.influxDB, times(1)).createDatabase(""testDB"");

        doAnswer(invocationOnMock -> {
            BatchPoints batchPoints = invocationOnMock.getArgument(0, BatchPoints.class);
            Assert.assertNotNull(batchPoints, ""batchPoints should not be null."");
            return null;
        }).when(influxDB).write(any(BatchPoints.class));

        influxSink.write(record);

        Thread.sleep(1000);

        verify(influxDB, times(1)).write(any(BatchPoints.class));
    }
",non-flaky,5
38677,apache_pulsar,RabbitMQSinkTest.TestOpenAndWriteSink,"    @Test
    public void TestOpenAndWriteSink() throws Exception {
        Map<String, Object> configs = new HashMap<>();
        configs.put(""host"", ""localhost"");
        configs.put(""port"", ""5673"");
        configs.put(""virtualHost"", ""default"");
        configs.put(""username"", ""guest"");
        configs.put(""password"", ""guest"");
        configs.put(""connectionName"", ""test-connection"");
        configs.put(""requestedChannelMax"", ""0"");
        configs.put(""requestedFrameMax"", ""0"");
        configs.put(""connectionTimeout"", ""60000"");
        configs.put(""handshakeTimeout"", ""10000"");
        configs.put(""requestedHeartbeat"", ""60"");
        configs.put(""exchangeName"", ""test-exchange"");
        configs.put(""exchangeType"", ""fanout"");

        RabbitMQSink sink = new RabbitMQSink();

        // open should success
        // rabbitmq service may need time to initialize
        Awaitility.await().ignoreExceptions().untilAsserted(() -> sink.open(configs, null));

        // write should success
        Record<byte[]> record = build(""test-topic"", ""fakeKey"", ""fakeValue"", ""fakeRoutingKey"");
        sink.write(record);

        sink.close();
    }
",non-flaky,5
38678,apache_pulsar,RabbitMQSourceTest.TestOpenAndWriteSink,"    @Test
    public void TestOpenAndWriteSink() {
        Map<String, Object> configs = new HashMap<>();
        configs.put(""host"", ""localhost"");
        configs.put(""port"", ""5672"");
        configs.put(""virtualHost"", ""default"");
        configs.put(""username"", ""guest"");
        configs.put(""password"", ""guest"");
        configs.put(""queueName"", ""test-queue"");
        configs.put(""connectionName"", ""test-connection"");
        configs.put(""requestedChannelMax"", ""0"");
        configs.put(""requestedFrameMax"", ""0"");
        configs.put(""connectionTimeout"", ""60000"");
        configs.put(""handshakeTimeout"", ""10000"");
        configs.put(""requestedHeartbeat"", ""60"");
        configs.put(""prefetchCount"", ""0"");
        configs.put(""prefetchGlobal"", ""false"");
        configs.put(""passive"", ""false"");

        RabbitMQSource source = new RabbitMQSource();

        // open should success
        // rabbitmq service may need time to initialize
        Awaitility.await().ignoreExceptions().untilAsserted(() -> source.open(configs, null));
    }
",non-flaky,5
38679,apache_pulsar,JsonConverterTests.testAvroToJson,"    @Test
    public void testAvroToJson() throws IOException {
        Schema schema = SchemaBuilder.record(""record"").fields()
                .name(""n"").type().longType().longDefault(10)
                .name(""l"").type().longType().longDefault(10)
                .name(""i"").type().intType().intDefault(10)
                .name(""b"").type().booleanType().booleanDefault(true)
                .name(""bb"").type().bytesType().bytesDefault(""10"")
                .name(""d"").type().doubleType().doubleDefault(10.0)
                .name(""f"").type().floatType().floatDefault(10.0f)
                .name(""s"").type().stringType().stringDefault(""titi"")
                .name(""array"").type().optional().array().items(SchemaBuilder.builder().stringType())
                .name(""map"").type().optional().map().values(SchemaBuilder.builder().intType())
                .endRecord();
        GenericRecord genericRecord = new GenericData.Record(schema);
        genericRecord.put(""n"", null);
        genericRecord.put(""l"", 1L);
        genericRecord.put(""i"", 1);
        genericRecord.put(""b"", true);
        genericRecord.put(""bb"", ""10"".getBytes(StandardCharsets.UTF_8));
        genericRecord.put(""d"", 10.0);
        genericRecord.put(""f"", 10.0f);
        genericRecord.put(""s"", ""toto"");
        genericRecord.put(""array"", new String[] {""toto""});
        genericRecord.put(""map"", ImmutableMap.of(""a"",10));
        JsonNode jsonNode = JsonConverter.toJson(genericRecord);
        assertEquals(jsonNode.get(""n""), NullNode.getInstance());
        assertEquals(jsonNode.get(""l"").asLong(), 1L);
        assertEquals(jsonNode.get(""i"").asInt(), 1);
        assertEquals(jsonNode.get(""b"").asBoolean(), true);
        assertEquals(jsonNode.get(""bb"").binaryValue(), ""10"".getBytes(StandardCharsets.UTF_8));
        assertEquals(jsonNode.get(""d"").asDouble(), 10.0);
        assertEquals(jsonNode.get(""f"").numberValue(), 10.0f);
        assertEquals(jsonNode.get(""s"").asText(), ""toto"");
        assertTrue(jsonNode.get(""array"").isArray());
        assertEquals(jsonNode.get(""array"").iterator().next().asText(), ""toto"");
        assertTrue(jsonNode.get(""map"").isObject());
        assertEquals(jsonNode.get(""map"").elements().next().asText(), ""10"");
        assertEquals(jsonNode.get(""map"").get(""a"").numberValue(), 10);
    }
",non-flaky,5
38680,apache_pulsar,JsonConverterTests.testLogicalTypesToJson,"    @Test
    public void testLogicalTypesToJson() {
        Schema decimalType = LogicalTypes.decimal(3,3).addToSchema(Schema.create(Schema.Type.BYTES));
        Schema dateType = LogicalTypes.date().addToSchema(Schema.create(Schema.Type.INT));
        Schema timestampMillisType = LogicalTypes.timestampMillis().addToSchema(Schema.create(Schema.Type.LONG));
        Schema timestampMicrosType = LogicalTypes.timestampMicros().addToSchema(Schema.create(Schema.Type.LONG));
        Schema timeMillisType = LogicalTypes.timeMillis().addToSchema(Schema.create(Schema.Type.INT));
        Schema timeMicrosType = LogicalTypes.timeMicros().addToSchema(Schema.create(Schema.Type.LONG));
        Schema uuidType = LogicalTypes.uuid().addToSchema(Schema.create(Schema.Type.STRING));
        Schema schema = SchemaBuilder.record(""record"")
                .fields()
                .name(""amount"").type(decimalType).noDefault()
                .name(""mydate"").type(dateType).noDefault()
                .name(""tsmillis"").type(timestampMillisType).noDefault()
                .name(""tsmicros"").type(timestampMicrosType).noDefault()
                .name(""timemillis"").type(timeMillisType).noDefault()
                .name(""timemicros"").type(timeMicrosType).noDefault()
                .name(""myuuid"").type(uuidType).noDefault()
                .endRecord();

        final long MILLIS_PER_DAY = 24 * 60 * 60 * 1000;
        BigDecimal myDecimal = new BigDecimal(""10.34"");
        UUID myUuid = UUID.randomUUID();
        Calendar calendar = new GregorianCalendar(TimeZone.getTimeZone(""Europe/Copenhagen""));
        GenericRecord genericRecord = new GenericData.Record(schema);
        genericRecord.put(""amount"", myDecimal);
        genericRecord.put(""mydate"", (int)calendar.toInstant().getEpochSecond());
        genericRecord.put(""tsmillis"", calendar.getTimeInMillis());
        genericRecord.put(""tsmicros"", calendar.getTimeInMillis() * 1000);
        genericRecord.put(""timemillis"", (int)(calendar.getTimeInMillis() % MILLIS_PER_DAY));
        genericRecord.put(""timemicros"", (calendar.getTimeInMillis() %MILLIS_PER_DAY) * 1000);
        genericRecord.put(""myuuid"", myUuid.toString());
        JsonNode jsonNode = JsonConverter.toJson(genericRecord);
        assertEquals(new BigDecimal(jsonNode.get(""amount"").asText()), myDecimal);
        assertEquals(jsonNode.get(""mydate"").asInt(), calendar.toInstant().getEpochSecond());
        assertEquals(jsonNode.get(""tsmillis"").asInt(), (int)calendar.getTimeInMillis());
        assertEquals(jsonNode.get(""tsmicros"").asLong(), calendar.getTimeInMillis() * 1000);
        assertEquals(jsonNode.get(""timemillis"").asInt(), (int)(calendar.getTimeInMillis() % MILLIS_PER_DAY));
        assertEquals(jsonNode.get(""timemicros"").asLong(), (calendar.getTimeInMillis() %MILLIS_PER_DAY) * 1000);
        assertEquals(UUID.fromString(jsonNode.get(""myuuid"").asText()), myUuid);
    }
",non-flaky,5
38681,apache_pulsar,RandomExponentialRetryTests.testExponentialWait,"    @Test
    public void testExponentialWait() {
        RandomExponentialRetry backoffRetry = new RandomExponentialRetry(5);
        assertEquals(backoffRetry.waitInMs(0, 100), 100L);
        assertEquals(backoffRetry.waitInMs(1, 100), 200L);
        assertEquals(backoffRetry.waitInMs(2, 100), 400L);
        assertEquals(backoffRetry.waitInMs(3, 100), 800L);
        assertEquals(backoffRetry.waitInMs(4, 100), 1600L);
        assertEquals(backoffRetry.waitInMs(5, 100), 3200L);
        assertEquals(backoffRetry.waitInMs(6, 100), 5000L);
    }
",non-flaky,5
38682,apache_pulsar,RandomExponentialRetryTests.callWithNoRetries,"    @Test
    public void callWithNoRetries() throws Exception {
        MockTime mockTime = new MockTime();
        RandomExponentialRetry backoffRetry = new RandomExponentialRetry();
        assertEquals(0, (int)backoffRetry.retry( () -> testFunction(0), 3, 100, ""NoRetries"", mockTime));
        assertEquals(0L, mockTime.totalMs.get());
        assertEquals(0L, mockTime.sleeps.size());
    }
",non-flaky,5
38683,apache_pulsar,RandomExponentialRetryTests.callWithExhaustedRetries,"    @Test(expectedExceptions = { IOException.class })
    public void callWithExhaustedRetries() throws Exception {
        MockTime mockTime = new MockTime();
        RandomExponentialRetry backoffRetry = new RandomExponentialRetry();
        assertEquals(4, (int)backoffRetry.retry( () -> testFunction(4), 3, 100, ""ExhautstedRetries"", mockTime));
    }
",non-flaky,5
38684,apache_pulsar,RandomExponentialRetryTests.callWithSomeRetries,"    @Test
    public void callWithSomeRetries() throws Exception {
        int N = 10;
        MockTime mockTime = new MockTime();
        RandomExponentialRetry backoffRetry = new RandomExponentialRetry();
        assertEquals(N, (int)backoffRetry.retry( () -> testFunction(N), N+1, 100, ""SomeRetries"", mockTime));
        assertEquals(N, mockTime.sleeps.size());
        for(int i = 0; i < N; i++) {
            assertTrue(mockTime.sleeps.get(i) <=  backoffRetry.waitInMs(i, 100));
        }
        System.out.println(""sleeps=""+mockTime.sleeps);
    }
",non-flaky,5
38685,apache_pulsar,ElasticSearchBWCTests.getSchemaType,"    @Test
    public void testGenericRecord() throws Exception {
        String json = ""{\""c\"":\""1\"",\""d\"":1,\""e\"":{\""a\"":\""a\"",\""b\"":true,\""d\"":1.0,\""f\"":1.0,\""i\"":1,\""l\"":10}}"";

        ElasticSearchSink elasticSearchSink = new ElasticSearchSink();
        elasticSearchSink.open(ImmutableMap.of(""elasticSearchUrl"", ""http://localhost:9200"", ""schemaEnable"", ""true""), null);
        Pair<String, String> pair = elasticSearchSink.extractIdAndDocument(new Record<GenericObject>() {
            @Override
            public GenericObject getValue() {
                return new GenericObject() {
                    @Override
                    public SchemaType getSchemaType() {
                        return SchemaType.BYTES;
                    }
",non-flaky,5
38686,apache_pulsar,ElasticSearchClientTests.testIndexDelete,"    @Test
    public void testIndexDelete() throws Exception {
        String index = ""myindex-"" + UUID.randomUUID();
        try (ElasticSearchClient client = new ElasticSearchClient(new ElasticSearchConfig()
                .setElasticSearchUrl(""http://"" + container.getHttpHostAddress())
                .setIndexName(index));) {
            assertTrue(client.createIndexIfNeeded(index));
            try {
                MockRecord<GenericObject> mockRecord = new MockRecord<>();
                client.indexDocument(mockRecord, Pair.of(""1"", ""{ \""a\"":1}""));
                assertEquals(mockRecord.acked, 1);
                assertEquals(mockRecord.failed, 0);
                assertEquals(client.totalHits(index), 1);

                client.deleteDocument(mockRecord, ""1"");
                assertEquals(mockRecord.acked, 2);
                assertEquals(mockRecord.failed, 0);
                assertEquals(client.totalHits(index), 0);
            } finally {
                client.delete(index);
            }
        }
    }
",non-flaky,5
38687,apache_pulsar,ElasticSearchClientTests.testIndexExists,"    @Test
    public void testIndexExists() throws IOException {
        String index = ""mynewindex-"" + UUID.randomUUID();
        try (ElasticSearchClient client = new ElasticSearchClient(new ElasticSearchConfig()
                .setElasticSearchUrl(""http://"" + container.getHttpHostAddress())
                .setIndexName(index));) {
            assertFalse(client.indexExists(index));
            assertTrue(client.createIndexIfNeeded(index));
            try {
                assertTrue(client.indexExists(index));
                assertFalse(client.createIndexIfNeeded(index));
            } finally {
                client.delete(index);
            }
        }
    }
",non-flaky,5
38688,apache_pulsar,ElasticSearchClientTests.testTopicToIndexName,"    @Test
    public void testTopicToIndexName() throws IOException {
        try (ElasticSearchClient client = new ElasticSearchClient(new ElasticSearchConfig()
                .setElasticSearchUrl(""http://"" + container.getHttpHostAddress())); ) {
            assertEquals(client.topicToIndexName(""data-ks1.table1""), ""data-ks1.table1"");
            assertEquals(client.topicToIndexName(""persistent://public/default/testesjson""), ""testesjson"");
            assertEquals(client.topicToIndexName(""default/testesjson""), ""testesjson"");
            assertEquals(client.topicToIndexName("".testesjson""), "".testesjson"");
            assertEquals(client.topicToIndexName(""TEST""), ""test"");

            assertThrows(RuntimeException.class, () -> client.topicToIndexName(""toto\\titi""));
            assertThrows(RuntimeException.class, () -> client.topicToIndexName(""_abc""));
            assertThrows(RuntimeException.class, () -> client.topicToIndexName(""-abc""));
            assertThrows(RuntimeException.class, () -> client.topicToIndexName(""+abc""));
        }
    }
",non-flaky,5
38689,apache_pulsar,ElasticSearchClientTests.testMalformedDocFails,"    @Test
    public void testMalformedDocFails() throws Exception {
        String index = ""indexmalformed-"" + UUID.randomUUID();
        ElasticSearchConfig config = new ElasticSearchConfig()
                .setElasticSearchUrl(""http://""+container.getHttpHostAddress())
                .setIndexName(index)
                .setBulkEnabled(true)
                .setMalformedDocAction(ElasticSearchConfig.MalformedDocAction.FAIL);
        try (ElasticSearchClient client = new ElasticSearchClient(config);) {
            MockRecord<GenericObject> mockRecord = new MockRecord<>();
            client.bulkIndex(mockRecord, Pair.of(""1"", ""{\""a\"":1}""));
            client.bulkIndex(mockRecord, Pair.of(""2"", ""{\""a\"":\""toto\""}""));
            client.flush();
            assertNotNull(client.irrecoverableError.get());
            assertTrue(client.irrecoverableError.get().getMessage().contains(""mapper_parsing_exception""));
            assertEquals(mockRecord.acked, 1);
            assertEquals(mockRecord.failed, 1);
            assertThrows(Exception.class, () -> client.bulkIndex(mockRecord, Pair.of(""3"", ""{\""a\"":3}"")));
            assertEquals(mockRecord.acked, 1);
            assertEquals(mockRecord.failed, 2);
        }
    }
",non-flaky,5
38690,apache_pulsar,ElasticSearchClientTests.testMalformedDocIgnore,"    @Test
    public void testMalformedDocIgnore() throws Exception {
        String index = ""indexmalformed2-"" + UUID.randomUUID();
        ElasticSearchConfig config = new ElasticSearchConfig()
                .setElasticSearchUrl(""http://""+container.getHttpHostAddress())
                .setIndexName(index)
                .setBulkEnabled(true)
                .setMalformedDocAction(ElasticSearchConfig.MalformedDocAction.IGNORE);
        try (ElasticSearchClient client = new ElasticSearchClient(config);) {
            MockRecord<GenericObject> mockRecord = new MockRecord<>();
            client.bulkIndex(mockRecord, Pair.of(""1"", ""{\""a\"":1}""));
            client.bulkIndex(mockRecord, Pair.of(""2"", ""{\""a\"":\""toto\""}""));
            client.flush();
            assertNull(client.irrecoverableError.get());
            assertEquals(mockRecord.acked, 1);
            assertEquals(mockRecord.failed, 1);
        }
    }
",non-flaky,5
38691,apache_pulsar,ElasticSearchClientTests.testBulkRetry,"    @Test
    public void testBulkRetry() throws Exception {
        final String index = ""indexbulktest-"" + UUID.randomUUID();
        ElasticSearchConfig config = new ElasticSearchConfig()
                .setElasticSearchUrl(""http://""+container.getHttpHostAddress())
                .setIndexName(index)
                .setBulkEnabled(true)
                .setMaxRetries(1000)
                .setBulkActions(2)
                .setRetryBackoffInMs(100)
                // disabled, we want to have full control over flush() method
                .setBulkFlushIntervalInMs(-1);

        try (ElasticSearchClient client = new ElasticSearchClient(config);) {
            try {
                assertTrue(client.createIndexIfNeeded(index));
                MockRecord<GenericObject> mockRecord = new MockRecord<>();
                client.bulkIndex(mockRecord, Pair.of(""1"", ""{\""a\"":1}""));
                client.bulkIndex(mockRecord, Pair.of(""2"", ""{\""a\"":2}""));
                assertEquals(mockRecord.acked, 2);
                assertEquals(mockRecord.failed, 0);
                assertEquals(client.totalHits(index), 2);

                ChaosContainer<?> chaosContainer = new ChaosContainer<>(container.getContainerName(), ""15s"");
                chaosContainer.start();

                client.bulkIndex(mockRecord, Pair.of(""3"", ""{\""a\"":3}""));
                assertEquals(mockRecord.acked, 2);
                assertEquals(mockRecord.failed, 0);
                assertEquals(client.totalHits(index), 2);

                chaosContainer.stop();
                client.flush();
                assertEquals(mockRecord.acked, 3);
                assertEquals(mockRecord.failed, 0);
                assertEquals(client.totalHits(index), 3);
            } finally {
                client.delete(index);
            }
        }
    }
",non-flaky,5
38692,apache_pulsar,ElasticSearchClientTests.testBulkBlocking,"    @Test
    public void testBulkBlocking() throws Exception {
        final String index = ""indexblocking-"" + UUID.randomUUID();
        ElasticSearchConfig config = new ElasticSearchConfig()
                .setElasticSearchUrl(""http://""+container.getHttpHostAddress())
                .setIndexName(index)
                .setBulkEnabled(true)
                .setMaxRetries(1000)
                .setBulkActions(2)
                .setBulkConcurrentRequests(2)
                .setRetryBackoffInMs(100)
                .setBulkFlushIntervalInMs(10000);
        try (ElasticSearchClient client = new ElasticSearchClient(config);) {
            assertTrue(client.createIndexIfNeeded(index));

            try {
                MockRecord<GenericObject> mockRecord = new MockRecord<>();
                for (int i = 1; i <= 5; i++) {
                    client.bulkIndex(mockRecord, Pair.of(Integer.toString(i), ""{\""a\"":"" + i + ""}""));
                }

                Awaitility.await().untilAsserted(() -> {
                    assertThat(""acked record"", mockRecord.acked, greaterThanOrEqualTo(4));
                    assertEquals(mockRecord.failed, 0);
                    assertThat(""totalHits"", client.totalHits(index), greaterThanOrEqualTo(4L));
                });
                client.flush();
                Awaitility.await().untilAsserted(() -> {
                    assertEquals(mockRecord.acked, 5);
                    assertEquals(mockRecord.failed, 0);
                    assertEquals(client.totalHits(index), 5);
                });

                ChaosContainer<?> chaosContainer = new ChaosContainer<>(container.getContainerName(), ""30s"");
                chaosContainer.start();
                Thread.sleep(1000L);

                // 11th bulkIndex is blocking because we have 2 pending requests, and the 3rd request is blocked.
                long start = System.currentTimeMillis();
                for (int i = 6; i <= 15; i++) {
                    client.bulkIndex(mockRecord, Pair.of(Integer.toString(i), ""{\""a\"":"" + i + ""}""));
                    log.info(""{} index {}"", System.currentTimeMillis(), i);
                }
                long elapsed = System.currentTimeMillis() - start;
                log.info(""elapsed = {}"", elapsed);
                assertTrue(elapsed > 29000); // bulkIndex was blocking while elasticsearch was down or busy

                Thread.sleep(1000L);
                assertEquals(mockRecord.acked, 15);
                assertEquals(mockRecord.failed, 0);
                assertEquals(client.records.size(), 0);

                chaosContainer.stop();
            } finally {
                client.delete(index);
            }
        }
    }
",non-flaky,5
38693,apache_pulsar,ElasticSearchSinkTests.getSchema,"    @Test
        public Schema getSchema() {
            return  kvSchema;
        }
",non-flaky,5
38694,apache_pulsar,ElasticSearchSinkTests.testStripNullNodes,"    @Test
    public void testStripNullNodes() throws Exception {
        map.put(""stripNulls"", true);
        sink.open(map, mockSinkContext);
        GenericRecord genericRecord = genericSchema.newRecordBuilder()
                .set(""name"", null)
                .set(""userName"", ""boby"")
                .set(""email"", null)
                .build();
        String json = sink.stringifyValue(valueSchema, genericRecord);
        assertEquals(json, ""{\""userName\"":\""boby\""}"");
    }
",non-flaky,5
38695,apache_pulsar,ElasticSearchSinkTests.testKeepNullNodes,"    @Test
    public void testKeepNullNodes() throws Exception {
        map.put(""stripNulls"", false);
        sink.open(map, mockSinkContext);
        GenericRecord genericRecord = genericSchema.newRecordBuilder()
                .set(""name"", null)
                .set(""userName"", ""boby"")
                .set(""email"", null)
                .build();
        String json = sink.stringifyValue(valueSchema, genericRecord);
        assertEquals(json, ""{\""name\"":null,\""userName\"":\""boby\"",\""email\"":null}"");
    }
",non-flaky,5
38696,apache_pulsar,ElasticSearchSinkTests.testNullValueFailure,"    @Test(expectedExceptions = PulsarClientException.InvalidMessageException.class)
    public void testNullValueFailure() throws Exception {
        String index = ""testnullvaluefail"";
        map.put(""indexName"", index);
        map.put(""keyIgnore"", ""false"");
        map.put(""nullValueAction"", ""FAIL"");
        sink.open(map, mockSinkContext);
        MockRecordNullValue mockRecordNullValue = new MockRecordNullValue();
        sink.write(mockRecordNullValue);
    }
",non-flaky,5
38697,apache_pulsar,ElasticSearchSinkTests.testNullValueIgnore,"    @Test
    public void testNullValueIgnore() throws Exception {
        testNullValue(ElasticSearchConfig.NullValueAction.IGNORE);
    }
",non-flaky,5
38698,apache_pulsar,ElasticSearchSinkTests.testNullValueDelete,"    @Test
    public void testNullValueDelete() throws Exception {
        testNullValue(ElasticSearchConfig.NullValueAction.DELETE);
    }
",non-flaky,5
38699,apache_pulsar,ElasticSearchExtractTests.getTopicName,"    @Test
    public void testGenericRecord() throws Exception {
        RecordSchemaBuilder valueSchemaBuilder = org.apache.pulsar.client.api.schema.SchemaBuilder.record(""value"");
        valueSchemaBuilder.field(""c"").type(SchemaType.STRING).optional().defaultValue(null);
        valueSchemaBuilder.field(""d"").type(SchemaType.INT32).optional().defaultValue(null);
        RecordSchemaBuilder udtSchemaBuilder = SchemaBuilder.record(""type1"");
        udtSchemaBuilder.field(""a"").type(SchemaType.STRING).optional().defaultValue(null);
        udtSchemaBuilder.field(""b"").type(SchemaType.BOOLEAN).optional().defaultValue(null);
        udtSchemaBuilder.field(""d"").type(SchemaType.DOUBLE).optional().defaultValue(null);
        udtSchemaBuilder.field(""f"").type(SchemaType.FLOAT).optional().defaultValue(null);
        udtSchemaBuilder.field(""i"").type(SchemaType.INT32).optional().defaultValue(null);
        udtSchemaBuilder.field(""l"").type(SchemaType.INT64).optional().defaultValue(null);
        GenericSchema<GenericRecord> udtGenericSchema = Schema.generic(udtSchemaBuilder.build(schemaType));
        valueSchemaBuilder.field(""e"", udtGenericSchema).type(schemaType).optional().defaultValue(null);
        GenericSchema<GenericRecord> valueSchema = Schema.generic(valueSchemaBuilder.build(schemaType));

        GenericRecord valueGenericRecord = valueSchema.newRecordBuilder()
                .set(""c"", ""1"")
                .set(""d"", 1)
                .set(""e"", udtGenericSchema.newRecordBuilder()
                        .set(""a"", ""a"")
                        .set(""b"", true)
                        .set(""d"", 1.0)
                        .set(""f"", 1.0f)
                        .set(""i"", 1)
                        .set(""l"", 10L)
                        .build())
                .build();

        Record<GenericObject> genericObjectRecord = new Record<GenericObject>() {
            @Override
            public Optional<String> getTopicName() {
                return Optional.of(""data-ks1.table1"");
            }
",non-flaky,5
38700,apache_pulsar,ElasticSearchExtractTests.getSchemaType,"    @Test
    public void testKeyValueGenericRecord() throws Exception {
        RecordSchemaBuilder keySchemaBuilder = org.apache.pulsar.client.api.schema.SchemaBuilder.record(""key"");
        keySchemaBuilder.field(""a"").type(SchemaType.STRING).optional().defaultValue(null);
        keySchemaBuilder.field(""b"").type(SchemaType.INT32).optional().defaultValue(null);
        GenericSchema<GenericRecord> keySchema = Schema.generic(keySchemaBuilder.build(schemaType));
        GenericRecord keyGenericRecord = keySchema.newRecordBuilder()
                .set(""a"", ""1"")
                .set(""b"", 1)
                .build();

        RecordSchemaBuilder valueSchemaBuilder = org.apache.pulsar.client.api.schema.SchemaBuilder.record(""value"");
        valueSchemaBuilder.field(""c"").type(SchemaType.STRING).optional().defaultValue(null);
        valueSchemaBuilder.field(""d"").type(SchemaType.INT32).optional().defaultValue(null);
        RecordSchemaBuilder udtSchemaBuilder = SchemaBuilder.record(""type1"");
        udtSchemaBuilder.field(""a"").type(SchemaType.STRING).optional().defaultValue(null);
        udtSchemaBuilder.field(""b"").type(SchemaType.BOOLEAN).optional().defaultValue(null);
        udtSchemaBuilder.field(""d"").type(SchemaType.DOUBLE).optional().defaultValue(null);
        udtSchemaBuilder.field(""f"").type(SchemaType.FLOAT).optional().defaultValue(null);
        udtSchemaBuilder.field(""i"").type(SchemaType.INT32).optional().defaultValue(null);
        udtSchemaBuilder.field(""l"").type(SchemaType.INT64).optional().defaultValue(null);
        GenericSchema<GenericRecord> udtGenericSchema = Schema.generic(udtSchemaBuilder.build(schemaType));
        valueSchemaBuilder.field(""e"", udtGenericSchema).type(schemaType).optional().defaultValue(null);
        GenericSchema<GenericRecord> valueSchema = Schema.generic(valueSchemaBuilder.build(schemaType));

        GenericRecord valueGenericRecord = valueSchema.newRecordBuilder()
                .set(""c"", ""1"")
                .set(""d"", 1)
                .set(""e"", udtGenericSchema.newRecordBuilder()
                        .set(""a"", ""a"")
                        .set(""b"", true)
                        .set(""d"", 1.0)
                        .set(""f"", 1.0f)
                        .set(""i"", 1)
                        .set(""l"", 10L)
                        .build())
                .build();

        Schema<KeyValue<GenericRecord, GenericRecord>> keyValueSchema = Schema.KeyValue(keySchema, valueSchema, KeyValueEncodingType.INLINE);
        KeyValue<GenericRecord, GenericRecord> keyValue = new KeyValue<>(keyGenericRecord, valueGenericRecord);
        GenericObject genericObject = new GenericObject() {
            @Override
            public SchemaType getSchemaType() {
                return SchemaType.KEY_VALUE;
            }
",non-flaky,5
38701,apache_pulsar,ElasticSearchClientSslTests.testSslBasic,"    @Test
    public void testSslBasic() throws IOException {
        try(ElasticsearchContainer container = new ElasticsearchContainer(ELASTICSEARCH_IMAGE)
                .withCreateContainerCmdModifier(c -> c.withName(""elasticsearch""))
                .withFileSystemBind(sslResourceDir, configDir + ""/ssl"")
                .withEnv(""ELASTIC_PASSWORD"",""elastic"")  // boostrap password
                .withEnv(""xpack.license.self_generated.type"", ""trial"")
                .withEnv(""xpack.security.enabled"", ""true"")
                .withEnv(""xpack.security.http.ssl.enabled"", ""true"")
                .withEnv(""xpack.security.http.ssl.client_authentication"", ""optional"")
                .withEnv(""xpack.security.http.ssl.key"", configDir + ""/ssl/elasticsearch.key"")
                .withEnv(""xpack.security.http.ssl.certificate"", configDir + ""/ssl/elasticsearch.crt"")
                .withEnv(""xpack.security.http.ssl.certificate_authorities"", configDir + ""/ssl/cacert.crt"")
                .withEnv(""xpack.security.transport.ssl.enabled"", ""true"")
                .withEnv(""xpack.security.transport.ssl.verification_mode"", ""certificate"")
                .withEnv(""xpack.security.transport.ssl.key"", configDir + ""/ssl/elasticsearch.key"")
                .withEnv(""xpack.security.transport.ssl.certificate"", configDir + ""/ssl/elasticsearch.crt"")
                .withEnv(""xpack.security.transport.ssl.certificate_authorities"", configDir + ""/ssl/cacert.crt"")
                .waitingFor(Wait.forLogMessage("".*(Security is enabled|Active license).*"", 1)
                        .withStartupTimeout(Duration.ofMinutes(2)))) {
            container.start();

            ElasticSearchConfig config = new ElasticSearchConfig()
                    .setElasticSearchUrl(""https://"" + container.getHttpHostAddress())
                    .setIndexName(INDEX)
                    .setUsername(""elastic"")
                    .setPassword(""elastic"")
                    .setSsl(new ElasticSearchSslConfig()
                            .setEnabled(true)
                            .setTruststorePath(sslResourceDir + ""/truststore.jks"")
                            .setTruststorePassword(""changeit""));
            ElasticSearchClient client = new ElasticSearchClient(config);
            testIndexExists(client);
        }
    }
",non-flaky,5
38702,apache_pulsar,ElasticSearchClientSslTests.testSslWithHostnameVerification,"    @Test
    public void testSslWithHostnameVerification() throws IOException {
        try(ElasticsearchContainer container = new ElasticsearchContainer(ELASTICSEARCH_IMAGE)
                .withCreateContainerCmdModifier(c -> c.withName(""elasticsearch""))
                .withFileSystemBind(sslResourceDir, configDir + ""/ssl"")
                .withEnv(""ELASTIC_PASSWORD"",""elastic"")  // boostrap password
                .withEnv(""xpack.license.self_generated.type"", ""trial"")
                .withEnv(""xpack.security.enabled"", ""true"")
                .withEnv(""xpack.security.http.ssl.enabled"", ""true"")
                .withEnv(""xpack.security.http.ssl.supported_protocols"", ""TLSv1.2,TLSv1.1"")
                .withEnv(""xpack.security.http.ssl.client_authentication"", ""optional"")
                .withEnv(""xpack.security.http.ssl.key"", configDir + ""/ssl/elasticsearch.key"")
                .withEnv(""xpack.security.http.ssl.certificate"", configDir + ""/ssl/elasticsearch.crt"")
                .withEnv(""xpack.security.http.ssl.certificate_authorities"", configDir + ""/ssl/cacert.crt"")
                .withEnv(""xpack.security.transport.ssl.enabled"", ""true"")
                .withEnv(""xpack.security.transport.ssl.verification_mode"", ""full"")
                .withEnv(""xpack.security.transport.ssl.key"", configDir + ""/ssl/elasticsearch.key"")
                .withEnv(""xpack.security.transport.ssl.certificate"", configDir + ""/ssl/elasticsearch.crt"")
                .withEnv(""xpack.security.transport.ssl.certificate_authorities"", configDir + ""/ssl/cacert.crt"")
                .waitingFor(Wait.forLogMessage("".*(Security is enabled|Active license).*"", 1)
                        .withStartupTimeout(Duration.ofMinutes(2)))) {
            container.start();

            ElasticSearchConfig config = new ElasticSearchConfig()
                    .setElasticSearchUrl(""https://"" + container.getHttpHostAddress())
                    .setIndexName(INDEX)
                    .setUsername(""elastic"")
                    .setPassword(""elastic"")
                    .setSsl(new ElasticSearchSslConfig()
                            .setEnabled(true)
                            .setProtocols(""TLSv1.2"")
                            .setHostnameVerification(true)
                            .setTruststorePath(sslResourceDir + ""/truststore.jks"")
                            .setTruststorePassword(""changeit""));
            ElasticSearchClient client = new ElasticSearchClient(config);
            testIndexExists(client);
        }
    }
",non-flaky,5
38703,apache_pulsar,ElasticSearchClientSslTests.testSslWithClientAuth,"    @Test
    public void testSslWithClientAuth() throws IOException {
        try(ElasticsearchContainer container = new ElasticsearchContainer(ELASTICSEARCH_IMAGE)
                .withCreateContainerCmdModifier(c -> c.withName(""elasticsearch""))
                .withFileSystemBind(sslResourceDir, configDir + ""/ssl"")
                .withEnv(""ELASTIC_PASSWORD"",""elastic"")  // boostrap password
                .withEnv(""xpack.license.self_generated.type"", ""trial"")
                .withEnv(""xpack.security.enabled"", ""true"")
                .withEnv(""xpack.security.http.ssl.enabled"", ""true"")
                .withEnv(""xpack.security.http.ssl.client_authentication"", ""required"")
                .withEnv(""xpack.security.http.ssl.key"", configDir + ""/ssl/elasticsearch.key"")
                .withEnv(""xpack.security.http.ssl.certificate"", configDir + ""/ssl/elasticsearch.crt"")
                .withEnv(""xpack.security.http.ssl.certificate_authorities"", configDir + ""/ssl/cacert.crt"")
                .withEnv(""xpack.security.transport.ssl.enabled"", ""true"")
                .withEnv(""xpack.security.transport.ssl.verification_mode"", ""full"")
                .withEnv(""xpack.security.transport.ssl.key"", configDir + ""/ssl/elasticsearch.key"")
                .withEnv(""xpack.security.transport.ssl.certificate"", configDir + ""/ssl/elasticsearch.crt"")
                .withEnv(""xpack.security.transport.ssl.certificate_authorities"", configDir + ""/ssl/cacert.crt"")
                .waitingFor(Wait.forLogMessage("".*(Security is enabled|Active license).*"", 1)
                        .withStartupTimeout(Duration.ofMinutes(3)))) {
            container.start();

            ElasticSearchConfig config = new ElasticSearchConfig()
                    .setElasticSearchUrl(""https://"" + container.getHttpHostAddress())
                    .setIndexName(INDEX)
                    .setUsername(""elastic"")
                    .setPassword(""elastic"")
                    .setSsl(new ElasticSearchSslConfig()
                            .setEnabled(true)
                            .setHostnameVerification(true)
                            .setTruststorePath(sslResourceDir + ""/truststore.jks"")
                            .setTruststorePassword(""changeit"")
                            .setKeystorePath(sslResourceDir + ""/keystore.jks"")
                            .setKeystorePassword(""changeit""));
            ElasticSearchClient client = new ElasticSearchClient(config);
            testIndexExists(client);
        }
    }
",non-flaky,5
38704,apache_pulsar,KafkaConnectSourceErrTest.testOpenAndRead,"    @Test
    public void testOpenAndRead() throws Exception {
        kafkaConnectSource = new KafkaConnectSource();
        kafkaConnectSource.open(config, context);

        // use FileStreamSourceConnector, each line is a record, need ""\n"" and end of each record.
        OutputStream os = Files.newOutputStream(tempFile.toPath());

        String line1 = ""This is the first line\n"";
        os.write(line1.getBytes());
        os.flush();
        log.info(""write 2 lines."");

        String line2 = ""This is the second line\n"";
        os.write(line2.getBytes());
        os.flush();

        log.info(""finish write, will read 2 lines"");

        // Note: FileStreamSourceTask read the whole line as Value, and set Key as null.
        Record<KeyValue<byte[], byte[]>> record = kafkaConnectSource.read();
        String readBack1 = new String(record.getValue().getValue());
        assertTrue(line1.contains(readBack1));
        assertNull(record.getValue().getKey());
        log.info(""read line1: {}"", readBack1);
        record.ack();

        record = kafkaConnectSource.read();
        String readBack2 = new String(record.getValue().getValue());
        assertTrue(line2.contains(readBack2));
        assertNull(record.getValue().getKey());
        assertTrue(record.getPartitionId().isPresent());
        assertFalse(record.getPartitionIndex().isPresent());
        log.info(""read line2: {}"", readBack2);
        record.ack();

        String line3 = ""This is the 3rd line\n"";
        os.write(line3.getBytes());
        os.flush();

        try {
            kafkaConnectSource.read();
            fail(""expected exception"");
        } catch (Exception e) {
            log.info(""got exception"", e);
            assertTrue(e.getCause().getCause() instanceof org.apache.kafka.connect.errors.ConnectException);
        }
    }
",non-flaky,5
38705,apache_pulsar,KafkaConnectSinkTest.smokeTest,"    @Test
    public void smokeTest() throws Exception {
        KafkaConnectSink sink = new KafkaConnectSink();
        sink.open(props, context);

        final GenericRecord rec = getGenericRecord(""value"", Schema.STRING);
        Message msg = mock(MessageImpl.class);
        when(msg.getValue()).thenReturn(rec);
        when(msg.getMessageId()).thenReturn(new MessageIdImpl(1, 0, 0));

        final AtomicInteger status = new AtomicInteger(0);
        Record<GenericObject> record = PulsarRecord.<String>builder()
                .topicName(""fake-topic"")
                .message(msg)
                .ackFunction(status::incrementAndGet)
                .failFunction(status::decrementAndGet)
                .schema(Schema.STRING)
                .build();

        sink.write(record);
        sink.flush();

        assertEquals(status.get(), 1);

        sink.close();

        List<String> lines = Files.readAllLines(file, StandardCharsets.US_ASCII);
        assertEquals(lines.get(0), ""value"");
    }
",non-flaky,5
38706,apache_pulsar,KafkaConnectSinkTest.seekPauseResumeTest,"    @Test
    public void seekPauseResumeTest() throws Exception {
        KafkaConnectSink sink = new KafkaConnectSink();
        sink.open(props, context);

        final GenericRecord rec = getGenericRecord(""value"", Schema.STRING);
        Message msg = mock(MessageImpl.class);
        when(msg.getValue()).thenReturn(rec);
        final MessageId msgId = new MessageIdImpl(10, 10, 0);
        when(msg.getMessageId()).thenReturn(msgId);

        final AtomicInteger status = new AtomicInteger(0);
        Record<GenericObject> record = PulsarRecord.<String>builder()
                .topicName(""fake-topic"")
                .message(msg)
                .ackFunction(status::incrementAndGet)
                .failFunction(status::decrementAndGet)
                .schema(Schema.STRING)
                .build();

        sink.write(record);
        sink.flush();

        assertEquals(status.get(), 1);

        final TopicPartition tp = new TopicPartition(""fake-topic"", 0);
        assertNotEquals(MessageIdUtils.getOffset(msgId), 0);
        assertEquals(sink.currentOffset(tp.topic(), tp.partition()), MessageIdUtils.getOffset(msgId));

        sink.taskContext.offset(tp, 0);
        verify(context, times(1)).seek(Mockito.anyString(), Mockito.anyInt(), any());
        assertEquals(sink.currentOffset(tp.topic(), tp.partition()), 0);

        sink.taskContext.pause(tp);
        verify(context, times(1)).pause(tp.topic(), tp.partition());
        sink.taskContext.resume(tp);
        verify(context, times(1)).resume(tp.topic(), tp.partition());

        sink.close();
    }
",non-flaky,5
38707,apache_pulsar,KafkaConnectSinkTest.subscriptionTypeTest,"    @Test
    public void subscriptionTypeTest() throws Exception {
        try (KafkaConnectSink sink = new KafkaConnectSink()) {
            log.info(""Failover is allowed"");
            sink.open(props, context);
        }

        when(context.getSubscriptionType()).thenReturn(SubscriptionType.Exclusive);
        try (KafkaConnectSink sink = new KafkaConnectSink()) {
            log.info(""Exclusive is allowed"");
            sink.open(props, context);
        }

        when(context.getSubscriptionType()).thenReturn(SubscriptionType.Key_Shared);
        try (KafkaConnectSink sink = new KafkaConnectSink()) {
            log.info(""Key_Shared is not allowed"");
            sink.open(props, context);
            fail(""expected exception"");
        } catch (IllegalArgumentException iae) {
            // pass
        }

        when(context.getSubscriptionType()).thenReturn(SubscriptionType.Shared);
        try (KafkaConnectSink sink = new KafkaConnectSink()) {
            log.info(""Shared is not allowed"");
            sink.open(props, context);
            fail(""expected exception"");
        } catch (IllegalArgumentException iae) {
            // pass
        }

        when(context.getSubscriptionType()).thenReturn(null);
        try (KafkaConnectSink sink = new KafkaConnectSink()) {
            log.info(""Type is required"");
            sink.open(props, context);
            fail(""expected exception"");
        } catch (IllegalArgumentException iae) {
            // pass
        }

    }
",non-flaky,5
42971,fabiomaffioletti_jsondoc,Issue151Test.testIssue151,"	@Test
	public void testIssue151() {
		JSONDoc jsonDoc = jsondocScanner.getJSONDoc("""", """", Lists.newArrayList(""org.jsondoc.core.issues.issue151""), true, MethodDisplay.URI);
		Assert.assertEquals(2, jsonDoc.getObjects().keySet().size());
		Assert.assertEquals(1, jsonDoc.getObjects().get(""bargroup"").size());
		Assert.assertEquals(1, jsonDoc.getObjects().get(""foogroup"").size());
	}
",non-flaky,5
42972,fabiomaffioletti_jsondoc,ApiDocTest.testApiErrorsDoc,"	@Test
	public void testApiErrorsDoc() throws Exception {

		final ApiDoc apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>>newHashSet(Test3Controller.class),
				MethodDisplay.URI).iterator().next();

		final Set<ApiMethodDoc> methods = apiDoc.getMethods();
		final ApiMethodDoc apiMethodDoc = methods.iterator().next();
		final List<ApiErrorDoc> apiErrors = apiMethodDoc.getApierrors();

		Assert.assertEquals(1, methods.size());
		Assert.assertEquals(3, apiErrors.size());
		Assert.assertEquals(""1000"", apiErrors.get(0).getCode());
		Assert.assertEquals(""method-level annotation should be applied"",
				""A test error #1"", apiErrors.get(0).getDescription());
		Assert.assertEquals(""2000"", apiErrors.get(1).getCode());
		Assert.assertEquals(""400"", apiErrors.get(2).getCode());

	}
",non-flaky,5
42973,fabiomaffioletti_jsondoc,ApiDocTest.testApiDoc,"	@Test
	public void testApiDoc() {
		Set<Class<?>> classes = new HashSet<Class<?>>();
		classes.add(TestController.class);
		ApiDoc apiDoc = jsondocScanner.getApiDocs(classes, MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""test-controller"", apiDoc.getName());
		Assert.assertEquals(""a-test-controller"", apiDoc.getDescription());
		Assert.assertEquals(""1.0"", apiDoc.getSupportedversions().getSince());
		Assert.assertEquals(""2.12"", apiDoc.getSupportedversions().getUntil());
		Assert.assertEquals(ApiAuthType.NONE.name(), apiDoc.getAuth().getType());
		Assert.assertEquals(DefaultJSONDocScanner.ANONYMOUS, apiDoc.getAuth().getRoles().get(0));

		for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
			
			if (apiMethodDoc.getPath().contains(""/name"")) {
				Assert.assertEquals(ApiVerb.GET, apiMethodDoc.getVerb().iterator().next());
				Assert.assertEquals(""string"", apiMethodDoc.getResponse().getJsondocType().getOneLineText());
				Assert.assertEquals(""string"", apiMethodDoc.getBodyobject().getJsondocType().getOneLineText());
				Assert.assertEquals(""200 - OK"", apiMethodDoc.getResponsestatuscode());
				for (ApiParamDoc apiParamDoc : apiMethodDoc.getPathparameters()) {
					if(apiParamDoc.getName().equals(""name"")) {
						Assert.assertEquals(""string"", apiParamDoc.getJsondocType().getOneLineText());
					}
				}
			}

			if (apiMethodDoc.getPath().contains(""/age"")) {
				Assert.assertEquals(ApiVerb.GET, apiMethodDoc.getVerb().iterator().next());
				Assert.assertEquals(""204"", apiMethodDoc.getResponsestatuscode());
				Assert.assertEquals(""integer"", apiMethodDoc.getResponse().getJsondocType().getOneLineText());
				Assert.assertEquals(""integer"", apiMethodDoc.getBodyobject().getJsondocType().getOneLineText());
				for (ApiParamDoc apiParamDoc : apiMethodDoc.getPathparameters()) {
					if(apiParamDoc.getName().equals(""age"")) {
						Assert.assertEquals(""integer"", apiParamDoc.getJsondocType().getOneLineText());
					}
				}
			}

			if (apiMethodDoc.getPath().contains(""/avg"")) {
				Assert.assertEquals(ApiVerb.GET, apiMethodDoc.getVerb().iterator().next());
				Assert.assertEquals(""long"", apiMethodDoc.getResponse().getJsondocType().getOneLineText());
				Assert.assertEquals(""long"", apiMethodDoc.getBodyobject().getJsondocType().getOneLineText());
				for (ApiParamDoc apiParamDoc : apiMethodDoc.getPathparameters()) {
					if(apiParamDoc.getName().equals(""avg"")) {
						Assert.assertEquals(""long"", apiParamDoc.getJsondocType().getOneLineText());
					}
				}
			}

			if (apiMethodDoc.getPath().contains(""/map"")) {
				Assert.assertEquals(ApiVerb.GET, apiMethodDoc.getVerb().iterator().next());
				Assert.assertEquals(""map[string, integer]"", apiMethodDoc.getResponse().getJsondocType().getOneLineText());
				Assert.assertEquals(""map[string, integer]"", apiMethodDoc.getBodyobject().getJsondocType().getOneLineText());
				for (ApiParamDoc apiParamDoc : apiMethodDoc.getPathparameters()) {
					if(apiParamDoc.getName().equals(""map"")) {
						Assert.assertEquals(""map[string, integer]"", apiParamDoc.getJsondocType().getOneLineText());
					}
				}
			}

			if (apiMethodDoc.getPath().contains(""/parametrizedList"")) {
				Assert.assertEquals(ApiVerb.GET, apiMethodDoc.getVerb().iterator().next());
				Assert.assertEquals(""list of string"", apiMethodDoc.getResponse().getJsondocType().getOneLineText());
				Assert.assertEquals(""list of string"", apiMethodDoc.getBodyobject().getJsondocType().getOneLineText());
				for (ApiParamDoc apiParamDoc : apiMethodDoc.getPathparameters()) {
					if(apiParamDoc.getName().equals(""parametrizedList"")) {
						Assert.assertEquals(""list of string"", apiParamDoc.getJsondocType().getOneLineText());
					}
				}
				
			}

			if (apiMethodDoc.getPath().contains(""/wildcardParametrizedList"")) {
				Assert.assertEquals(ApiVerb.GET, apiMethodDoc.getVerb().iterator().next());
				Assert.assertEquals(""list of wildcard"", apiMethodDoc.getResponse().getJsondocType().getOneLineText());
				Assert.assertEquals(""list of wildcard"", apiMethodDoc.getBodyobject().getJsondocType().getOneLineText());
				for (ApiParamDoc apiParamDoc : apiMethodDoc.getPathparameters()) {
					if(apiParamDoc.getName().equals(""wildcardParametrizedList"")) {
						Assert.assertEquals(""list of wildcard"", apiParamDoc.getJsondocType().getOneLineText());
					}
				}
			}

			if (apiMethodDoc.getPath().contains(""/LongArray"")) {
				Assert.assertEquals(ApiVerb.GET, apiMethodDoc.getVerb().iterator().next());
				Assert.assertEquals(""array of long"", apiMethodDoc.getResponse().getJsondocType().getOneLineText());
				Assert.assertEquals(""array of long"", apiMethodDoc.getBodyobject().getJsondocType().getOneLineText());
				for (ApiParamDoc apiParamDoc : apiMethodDoc.getPathparameters()) {
					if(apiParamDoc.getName().equals(""LongArray"")) {
						Assert.assertEquals(""array of long"", apiParamDoc.getJsondocType().getOneLineText());
					}
				}
			}

			if (apiMethodDoc.getPath().contains(""/longArray"")) {
				Assert.assertEquals(ApiVerb.GET, apiMethodDoc.getVerb().iterator().next());
				Assert.assertEquals(""array of long"", apiMethodDoc.getResponse().getJsondocType().getOneLineText());
				Assert.assertEquals(""array of long"", apiMethodDoc.getBodyobject().getJsondocType().getOneLineText());
				for (ApiParamDoc apiParamDoc : apiMethodDoc.getPathparameters()) {
					if(apiParamDoc.getName().equals(""longArray"")) {
						Assert.assertEquals(""array of long"", apiParamDoc.getJsondocType().getOneLineText());
					}
				}
			}
			
			if (apiMethodDoc.getPath().contains(""/version"")) {
				Assert.assertEquals(ApiVerb.GET, apiMethodDoc.getVerb().iterator().next());
				Assert.assertEquals(""1.0"", apiMethodDoc.getSupportedversions().getSince());
				Assert.assertEquals(""2.12"", apiMethodDoc.getSupportedversions().getUntil());
			}
			
			if (apiMethodDoc.getPath().contains(""/child"")) {
				Assert.assertEquals(ApiVerb.GET, apiMethodDoc.getVerb().iterator().next());
				Assert.assertEquals(""child"", apiMethodDoc.getResponse().getJsondocType().getOneLineText());
			}
			
			if (apiMethodDoc.getPath().contains(""/pizza"")) {
				Assert.assertEquals(ApiVerb.GET, apiMethodDoc.getVerb().iterator().next());
				Assert.assertEquals(""customPizzaObject"", apiMethodDoc.getResponse().getJsondocType().getOneLineText());
			}
			
			if (apiMethodDoc.getPath().contains(""/multiple-request-methods"")) {
				Assert.assertEquals(2, apiMethodDoc.getVerb().size());
				Iterator<ApiVerb> iterator = apiMethodDoc.getVerb().iterator();
				Assert.assertEquals(ApiVerb.GET, iterator.next());
				Assert.assertEquals(ApiVerb.POST, iterator.next());
			}
			
		}

		classes.clear();
		classes.add(TestControllerWithBasicAuth.class);
		apiDoc = jsondocScanner.getApiDocs(classes, MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""test-controller-with-basic-auth"", apiDoc.getName());
		Assert.assertEquals(ApiAuthType.BASIC_AUTH.name(), apiDoc.getAuth().getType());
		Assert.assertEquals(""ROLE_USER"", apiDoc.getAuth().getRoles().get(0));
		Assert.assertEquals(""ROLE_ADMIN"", apiDoc.getAuth().getRoles().get(1));
		Assert.assertTrue(apiDoc.getAuth().getTestusers().size() > 0);
		
		for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
			if (apiMethodDoc.getPath().contains(""/basicAuth"")) {
				Assert.assertEquals(ApiAuthType.BASIC_AUTH.name(), apiMethodDoc.getAuth().getType());
				Assert.assertEquals(""ROLE_USER"", apiMethodDoc.getAuth().getRoles().get(0));
				Assert.assertTrue(apiMethodDoc.getAuth().getTestusers().size() > 0);
			}
			
			if (apiMethodDoc.getPath().contains(""/noAuth"")) {
				Assert.assertEquals(ApiAuthType.NONE.name(), apiMethodDoc.getAuth().getType());
				Assert.assertEquals(DefaultJSONDocScanner.ANONYMOUS, apiMethodDoc.getAuth().getRoles().get(0));
			}
			
			if (apiMethodDoc.getPath().contains(""/undefinedAuthWithAuthOnClass"")) {
				Assert.assertEquals(ApiAuthType.BASIC_AUTH.name(), apiMethodDoc.getAuth().getType());
				Assert.assertEquals(""ROLE_USER"", apiMethodDoc.getAuth().getRoles().get(0));
				Assert.assertEquals(""ROLE_ADMIN"", apiMethodDoc.getAuth().getRoles().get(1));
			}
			
		}
		
		classes.clear();
		classes.add(TestControllerWithNoAuthAnnotation.class);
		apiDoc = jsondocScanner.getApiDocs(classes, MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""test-controller-with-no-auth-annotation"", apiDoc.getName());
		Assert.assertNull(apiDoc.getAuth());
		
		for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
			if (apiMethodDoc.getPath().contains(""/basicAuth"")) {
				Assert.assertEquals(ApiAuthType.BASIC_AUTH.name(), apiMethodDoc.getAuth().getType());
				Assert.assertEquals(""ROLE_USER"", apiMethodDoc.getAuth().getRoles().get(0));
				Assert.assertTrue(apiMethodDoc.getAuth().getTestusers().size() > 0);
			}
			
			if (apiMethodDoc.getPath().contains(""/noAuth"")) {
				Assert.assertEquals(ApiAuthType.NONE.name(), apiMethodDoc.getAuth().getType());
				Assert.assertEquals(DefaultJSONDocScanner.ANONYMOUS, apiMethodDoc.getAuth().getRoles().get(0));
			}
			
			if (apiMethodDoc.getPath().contains(""/undefinedAuthWithoutAuthOnClass"")) {
				Assert.assertNull(apiMethodDoc.getAuth());
			}
			
		}
		
		classes.clear();
		classes.add(TestOldStyleServlets.class);
		apiDoc = jsondocScanner.getApiDocs(classes, MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""test-old-style-servlets"", apiDoc.getName());
		
		for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
			if (apiMethodDoc.getPath().contains(""/oldStyle"")) {
				Assert.assertEquals(1, apiMethodDoc.getPathparameters().size());
			}
			
			if (apiMethodDoc.getPath().contains(""/oldStyleWithList"")) {
				Assert.assertEquals(1, apiMethodDoc.getPathparameters().size());
			}
			
			if (apiMethodDoc.getPath().contains(""/oldStyleWithMap"")) {
				Assert.assertEquals(1, apiMethodDoc.getPathparameters().size());
			}
			
			if (apiMethodDoc.getPath().contains(""/oldStyleMixed"")) {
				Assert.assertEquals(3, apiMethodDoc.getPathparameters().size());
				Assert.assertEquals(1, apiMethodDoc.getQueryparameters().size());
				Assert.assertEquals(""qTest"", apiMethodDoc.getQueryparameters().iterator().next().getDefaultvalue());
			}
			
			if (apiMethodDoc.getPath().contains(""/oldStyleResponseObject"")) {
				Assert.assertEquals(""list"", apiMethodDoc.getResponse().getJsondocType().getOneLineText());
			}
			
			if (apiMethodDoc.getPath().contains(""/oldStyleBodyObject"")) {
				Assert.assertEquals(""list"", apiMethodDoc.getBodyobject().getJsondocType().getOneLineText());
			}
		}
		
		classes.clear();
		classes.add(TestErrorsAndWarningsAndHints.class);
		apiDoc = jsondocScanner.getApiDocs(classes, MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""test-errors-warnings-hints"", apiDoc.getName());
		ApiMethodDoc apiMethodDoc = apiDoc.getMethods().iterator().next();
		Assert.assertEquals(1, apiMethodDoc.getJsondocerrors().size());
		Assert.assertEquals(1, apiMethodDoc.getJsondocwarnings().size());
		Assert.assertEquals(2, apiMethodDoc.getJsondochints().size());
		
		classes.clear();
		classes.add(TestErrorsAndWarningsAndHintsMethodSummary.class);
		apiDoc = jsondocScanner.getApiDocs(classes, MethodDisplay.SUMMARY).iterator().next();
		apiMethodDoc = apiDoc.getMethods().iterator().next();
		Assert.assertEquals(1, apiMethodDoc.getJsondocerrors().size());
		Assert.assertEquals(1, apiMethodDoc.getJsondocwarnings().size());
		Assert.assertEquals(3, apiMethodDoc.getJsondochints().size());
		
		classes.clear();
		classes.add(InterfaceController.class);
		apiDoc = jsondocScanner.getApiDocs(classes, MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""interface-controller"", apiDoc.getName());
		apiMethodDoc = apiDoc.getMethods().iterator().next();
		Assert.assertNotNull(apiMethodDoc);
		Assert.assertEquals(""/interface"", apiMethodDoc.getPath().iterator().next());
		
		classes.clear();
		classes.add(TestDeclaredMethods.class);
		apiDoc = jsondocScanner.getApiDocs(classes, MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""test-declared-methods"", apiDoc.getName());
		Assert.assertEquals(2, apiDoc.getMethods().size());
		
		
		classes.clear();
		classes.add(TestMultipleParamsWithSameMethod.class);
		apiDoc = jsondocScanner.getApiDocs(classes, MethodDisplay.URI).iterator().next();
		Assert.assertEquals(3, apiDoc.getMethods().size());
		
	}
",non-flaky,5
42974,fabiomaffioletti_jsondoc,ApiMethodDocTest.testNotEqual,"	@Test
	public void testNotEqual() {
		first = new ApiMethodDoc();
		first.setPath(Sets.newHashSet(""/first""));
		first.setVerb(Sets.newHashSet(ApiVerb.GET));
		second = new ApiMethodDoc();
		second.setPath(Sets.newHashSet(""/second""));
		second.setVerb(Sets.newHashSet(ApiVerb.GET));
		Assert.assertNotEquals(0, first.compareTo(second));
	}
",non-flaky,5
42975,fabiomaffioletti_jsondoc,ApiMethodDocTest.testEqual,"	@Test
	public void testEqual() {
		first = new ApiMethodDoc();
		first.setPath(Sets.newHashSet(""/test""));
		first.setVerb(Sets.newHashSet(ApiVerb.GET));
		second = new ApiMethodDoc();
		second.setPath(Sets.newHashSet(""/test""));
		second.setVerb(Sets.newHashSet(ApiVerb.GET));
		Assert.assertEquals(0, first.compareTo(second));
	}
",non-flaky,5
42976,fabiomaffioletti_jsondoc,ApiMethodDocTest.testNotEqualMultipleVerbs,"	@Test
	public void testNotEqualMultipleVerbs() {
		first = new ApiMethodDoc();
		first.setPath(Sets.newHashSet(""/first""));
		first.setVerb(Sets.newHashSet(ApiVerb.GET, ApiVerb.POST));
		second = new ApiMethodDoc();
		second.setPath(Sets.newHashSet(""/second""));
		second.setVerb(Sets.newHashSet(ApiVerb.GET, ApiVerb.POST));
		Assert.assertNotEquals(0, first.compareTo(second));
		
		second.setPath(Sets.newHashSet(""/first""));
		second.setVerb(Sets.newHashSet(ApiVerb.PUT, ApiVerb.POST));
		Assert.assertNotEquals(0, first.compareTo(second));
	}
",non-flaky,5
42977,fabiomaffioletti_jsondoc,ApiMethodDocTest.testEqualMultipleVerbs,"	@Test
	public void testEqualMultipleVerbs() {
		first = new ApiMethodDoc();
		first.setPath(Sets.newHashSet(""/test""));
		first.setVerb(Sets.newHashSet(ApiVerb.GET, ApiVerb.POST));
		second = new ApiMethodDoc();
		second.setPath(Sets.newHashSet(""/test""));
		second.setVerb(Sets.newHashSet(ApiVerb.GET, ApiVerb.POST));
		Assert.assertEquals(0, first.compareTo(second));
		
		second.setVerb(Sets.newHashSet(ApiVerb.POST, ApiVerb.GET));
		Assert.assertEquals(0, first.compareTo(second));
	}
",non-flaky,5
42978,fabiomaffioletti_jsondoc,ApiObjectDocTest.testUndefinedVisibilityAndStageDoc,"	@Test
	public void testUndefinedVisibilityAndStageDoc() {
		Set<Class<?>> classes = new HashSet<Class<?>>();
		classes.add(UndefinedVisibilityAndStage.class);
		ApiObjectDoc apiObjectDoc = jsondocScanner.getApiObjectDocs(classes).iterator().next();
		Assert.assertEquals(""undefinedvisibilityandstage"", apiObjectDoc.getName());
		Assert.assertEquals(ApiVisibility.UNDEFINED, apiObjectDoc.getVisibility());
		Assert.assertEquals(ApiStage.UNDEFINED, apiObjectDoc.getStage());
	}
",non-flaky,5
42979,fabiomaffioletti_jsondoc,ApiObjectDocTest.testTemplateApiObjectDoc,"	@Test
	public void testTemplateApiObjectDoc() {
		Set<Class<?>> classes = new HashSet<Class<?>>();
		classes.add(TemplateApiObject.class);
		ApiObjectDoc apiObjectDoc = jsondocScanner.getApiObjectDocs(classes).iterator().next();
		Assert.assertEquals(""templateapiobject"", apiObjectDoc.getName());
		Iterator<ApiObjectFieldDoc> iterator = apiObjectDoc.getFields().iterator();
		Assert.assertEquals(""id"", iterator.next().getName());
		Assert.assertEquals(""name"", iterator.next().getName());
	}
",non-flaky,5
42980,fabiomaffioletti_jsondoc,ApiObjectDocTest.testNoNameApiObjectDoc,"	@Test
	public void testNoNameApiObjectDoc() {
		Set<Class<?>> classes = new HashSet<Class<?>>();
		classes.add(NoNameApiObject.class);
		ApiObjectDoc apiObjectDoc = jsondocScanner.getApiObjectDocs(classes).iterator().next();
		Assert.assertEquals(""nonameapiobject"", apiObjectDoc.getName());
		Assert.assertEquals(""id"", apiObjectDoc.getFields().iterator().next().getName());
		Assert.assertEquals(1, apiObjectDoc.getJsondochints().size());
	}
",non-flaky,5
42981,fabiomaffioletti_jsondoc,ApiObjectDocTest.testEnumObjectDoc,"	@Test
	public void testEnumObjectDoc() {
		Set<Class<?>> classes = new HashSet<Class<?>>();
		classes.add(TestEnum.class);
		ApiObjectDoc childDoc = jsondocScanner.getApiObjectDocs(classes).iterator().next(); 
		Assert.assertEquals(""test-enum"", childDoc.getName());
		Assert.assertEquals(0, childDoc.getFields().size());
		Assert.assertEquals(TestEnum.TESTENUM1.name(), childDoc.getAllowedvalues()[0]);
		Assert.assertEquals(TestEnum.TESTENUM2.name(), childDoc.getAllowedvalues()[1]);
		Assert.assertEquals(TestEnum.TESTENUM3.name(), childDoc.getAllowedvalues()[2]);
	}
",non-flaky,5
42982,fabiomaffioletti_jsondoc,ApiObjectDocTest.testApiObjectDoc,"	@Test
	public void testApiObjectDoc() {
		Set<Class<?>> classes = new HashSet<Class<?>>();
		classes.add(TestObject.class);
		ApiObjectDoc childDoc = jsondocScanner.getApiObjectDocs(classes).iterator().next(); 
		Assert.assertEquals(""test-object"", childDoc.getName());
		Assert.assertEquals(14, childDoc.getFields().size());
		Assert.assertEquals(""1.0"", childDoc.getSupportedversions().getSince());
		Assert.assertEquals(""2.12"", childDoc.getSupportedversions().getUntil());
		Assert.assertEquals(ApiVisibility.PUBLIC, childDoc.getVisibility());
		Assert.assertEquals(ApiStage.PRE_ALPHA, childDoc.getStage());
		
		for (ApiObjectFieldDoc fieldDoc : childDoc.getFields()) {
			if(fieldDoc.getName().equals(""wildcardParametrized"")) {
				Assert.assertEquals(""list"", fieldDoc.getJsondocType().getType().get(0));
			}
			
			if(fieldDoc.getName().equals(""unparametrizedList"")) {
				Assert.assertEquals(""list"", fieldDoc.getJsondocType().getType().get(0));
			}
			
			if(fieldDoc.getName().equals(""parametrizedList"")) {
				Assert.assertEquals(""list of string"", fieldDoc.getJsondocType().getOneLineText());
			}
			
			if(fieldDoc.getName().equals(""name"")) {
				Assert.assertEquals(""string"", fieldDoc.getJsondocType().getType().get(0));
				Assert.assertEquals(""name"", fieldDoc.getName());
				Assert.assertEquals(""true"", fieldDoc.getRequired());
			}
			
			if(fieldDoc.getName().equals(""age"")) {
				Assert.assertEquals(""integer"", fieldDoc.getJsondocType().getType().get(0));
				Assert.assertEquals(""age"", fieldDoc.getName());
				Assert.assertEquals(""false"", fieldDoc.getRequired());
			}
			
			if(fieldDoc.getName().equals(""avg"")) {
				Assert.assertEquals(""long"", fieldDoc.getJsondocType().getType().get(0));
				Assert.assertEquals(""avg"", fieldDoc.getName());
				Assert.assertEquals(""false"", fieldDoc.getRequired());
			}
			
			if(fieldDoc.getName().equals(""map"")) {
				Assert.assertEquals(""map"", fieldDoc.getJsondocType().getType().get(0));
				Assert.assertEquals(""string"", fieldDoc.getJsondocType().getMapKey().getType().get(0));
				Assert.assertEquals(""integer"", fieldDoc.getJsondocType().getMapValue().getType().get(0));
			}
			
			if(fieldDoc.getName().equals(""LongArray"")) {
				Assert.assertEquals(""array of long"", fieldDoc.getJsondocType().getOneLineText());
				Assert.assertEquals(""LongArray"", fieldDoc.getName());
				Assert.assertEquals(""false"", fieldDoc.getRequired());
			}

			if(fieldDoc.getName().equals(""longArray"")) {
				Assert.assertEquals(""array of long"", fieldDoc.getJsondocType().getOneLineText());
				Assert.assertEquals(""longArray"", fieldDoc.getName());
				Assert.assertEquals(""false"", fieldDoc.getRequired());
			}
			
			if(fieldDoc.getName().equals(""fooBar"")) {
				Assert.assertEquals(""string"", fieldDoc.getJsondocType().getOneLineText());
				Assert.assertEquals(""foo_bar"", fieldDoc.getName());
				Assert.assertEquals(""false"", fieldDoc.getRequired());
			}
			
			if(fieldDoc.getName().equals(""version"")) {
				Assert.assertEquals(""string"", fieldDoc.getJsondocType().getOneLineText());
				Assert.assertEquals(""1.0"", fieldDoc.getSupportedversions().getSince());
				Assert.assertEquals(""2.12"", fieldDoc.getSupportedversions().getUntil());
			}
			
			if(fieldDoc.getName().equals(""test-enum"")) {
				Assert.assertEquals(""test-enum"", fieldDoc.getName());
				Assert.assertEquals(TestEnum.TESTENUM1.name(), fieldDoc.getAllowedvalues()[0]);
				Assert.assertEquals(TestEnum.TESTENUM2.name(), fieldDoc.getAllowedvalues()[1]);
				Assert.assertEquals(TestEnum.TESTENUM3.name(), fieldDoc.getAllowedvalues()[2]);
			}
			
			if(fieldDoc.getName().equals(""test-enum-with-allowed-values"")) {
				Assert.assertEquals(""A"", fieldDoc.getAllowedvalues()[0]);
				Assert.assertEquals(""B"", fieldDoc.getAllowedvalues()[1]);
				Assert.assertEquals(""C"", fieldDoc.getAllowedvalues()[2]);
			}

			if(fieldDoc.getName().equals(""orderedProperty"")) {
				Assert.assertEquals(""orderedProperty"", fieldDoc.getName());
				Assert.assertEquals(1, fieldDoc.getOrder().intValue());
			} else {
				Assert.assertEquals(Integer.MAX_VALUE, fieldDoc.getOrder().intValue());
			}

		}
	}
",non-flaky,5
42983,fabiomaffioletti_jsondoc,ApiHeadersDocTest.testApiHeadersOnClass,"	@Test
	public void testApiHeadersOnClass() {
		final ApiDoc apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>>newHashSet(ApiHeadersController.class), MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""ApiHeadersController"", apiDoc.getName());
		for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
			if(apiMethodDoc.getPath().contains(""/api-headers-controller-method-one"")) {
				Assert.assertEquals(2, apiMethodDoc.getHeaders().size());
			}
			if(apiMethodDoc.getPath().contains(""/api-headers-controller-method-two"")) {
				Assert.assertEquals(3, apiMethodDoc.getHeaders().size());
			}
		}
	}
",non-flaky,5
42984,fabiomaffioletti_jsondoc,ApiFlowDocTest.testApiDoc,"	@Test
	public void testApiDoc() {
		Set<Class<?>> classes = new HashSet<Class<?>>();
		classes.add(TestFlow.class);
		
		List<ApiMethodDoc> apiMethodDocs = new ArrayList<ApiMethodDoc>();
		ApiMethodDoc apiMethodDoc = new ApiMethodDoc();
		apiMethodDoc.setId(""F1"");
		apiMethodDocs.add(apiMethodDoc);
		
		Set<ApiFlowDoc> apiFlowDocs = jsondocScanner.getApiFlowDocs(classes, apiMethodDocs);
		for (ApiFlowDoc apiFlowDoc : apiFlowDocs) {
			if(apiFlowDoc.getName().equals(""flow"")) {
				Assert.assertEquals(""A test flow"", apiFlowDoc.getDescription());
				Assert.assertEquals(3, apiFlowDoc.getSteps().size());
				Assert.assertEquals(""F1"", apiFlowDoc.getSteps().get(0).getApimethodid());
				Assert.assertEquals(""F2"", apiFlowDoc.getSteps().get(1).getApimethodid());
				Assert.assertEquals(""Flows A"", apiFlowDoc.getGroup());
				Assert.assertNotNull(apiFlowDoc.getSteps().get(0).getApimethoddoc());
				Assert.assertEquals(""F1"", apiFlowDoc.getSteps().get(0).getApimethoddoc().getId());
			}
			
			if(apiFlowDoc.getName().equals(""flow2"")) {
				Assert.assertEquals(""A test flow 2"", apiFlowDoc.getDescription());
				Assert.assertEquals(3, apiFlowDoc.getSteps().size());
				Assert.assertEquals(""F4"", apiFlowDoc.getSteps().get(0).getApimethodid());
				Assert.assertEquals(""F5"", apiFlowDoc.getSteps().get(1).getApimethodid());
				Assert.assertEquals(""Flows B"", apiFlowDoc.getGroup());
			}
		}
	}
",non-flaky,5
42985,fabiomaffioletti_jsondoc,JSONDocApiObjectBuilderTest.testApiObjectDocWithHibernateValidator,"	@Test
	public void testApiObjectDocWithHibernateValidator() {
		Set<ApiObjectDoc> apiObjectDocs = jsondocScanner.getApiObjectDocs(Sets.<Class<?>>newHashSet(HibernateValidatorPojo.class));
		Iterator<ApiObjectDoc> iterator = apiObjectDocs.iterator();
		ApiObjectDoc next = iterator.next();
		Set<ApiObjectFieldDoc> fields = next.getFields();
		for (ApiObjectFieldDoc apiObjectFieldDoc : fields) {
			if(apiObjectFieldDoc.getName().equals(""id"")) {
				Iterator<String> formats = apiObjectFieldDoc.getFormat().iterator();
				Assert.assertEquals(""a not empty id"", formats.next());
				Assert.assertEquals(""length must be between 2 and 2147483647"", formats.next());
				Assert.assertEquals(""must be less than or equal to 9"", formats.next());
			}
		}
	}
",non-flaky,5
42986,fabiomaffioletti_jsondoc,JSONDocApiGlobalBuilderTest.testApiGlobalDoc,"	@Test
	public void testApiGlobalDoc() {
		ApiGlobalDoc apiGlobalDoc = jsondocScanner.getApiGlobalDoc(Sets.<Class<?>>newHashSet(Global.class), Sets.<Class<?>>newHashSet(), Sets.<Class<?>>newHashSet());
		Assert.assertNotNull(apiGlobalDoc);
		Assert.assertEquals(1, apiGlobalDoc.getSections().size());
		ApiGlobalSectionDoc sectionDoc = apiGlobalDoc.getSections().iterator().next();
		Assert.assertEquals(""title"", sectionDoc.getTitle());
		Assert.assertEquals(3, sectionDoc.getParagraphs().size());
		
		apiGlobalDoc = jsondocScanner.getApiGlobalDoc(Sets.<Class<?>>newHashSet(), Sets.<Class<?>>newHashSet(Changelog.class), Sets.<Class<?>>newHashSet());
		Assert.assertNotNull(apiGlobalDoc);
		Assert.assertEquals(1, apiGlobalDoc.getChangelogset().getChangelogs().size());

		apiGlobalDoc = jsondocScanner.getApiGlobalDoc(Sets.<Class<?>>newHashSet(MultipleGlobalSections.class), Sets.<Class<?>>newHashSet(), Sets.<Class<?>>newHashSet());
		Assert.assertNotNull(apiGlobalDoc);
		Assert.assertEquals(3, apiGlobalDoc.getSections().size());
		
		ApiGlobalSectionDoc[] apiGlobalSectionDocs = apiGlobalDoc.getSections().toArray(new ApiGlobalSectionDoc[apiGlobalDoc.getSections().size()]);
		Assert.assertEquals(""section1"", apiGlobalSectionDocs[0].getTitle());
		Assert.assertEquals(""abc"", apiGlobalSectionDocs[1].getTitle());
		Assert.assertEquals(""198xyz"", apiGlobalSectionDocs[2].getTitle());
		
		apiGlobalDoc = jsondocScanner.getApiGlobalDoc(Sets.<Class<?>>newHashSet(), Sets.<Class<?>>newHashSet(), Sets.<Class<?>>newHashSet(Migration.class));
		Assert.assertNotNull(apiGlobalDoc);
		Assert.assertEquals(1, apiGlobalDoc.getMigrationset().getMigrations().size());
		
		apiGlobalDoc = jsondocScanner.getApiGlobalDoc(Sets.<Class<?>>newHashSet(AllTogether.class), Sets.<Class<?>>newHashSet(AllTogether.class), Sets.<Class<?>>newHashSet(AllTogether.class));
		Assert.assertNotNull(apiGlobalDoc);
		Assert.assertEquals(1, apiGlobalDoc.getSections().size());
		Assert.assertEquals(1, apiGlobalDoc.getMigrationset().getMigrations().size());
		Assert.assertEquals(1, apiGlobalDoc.getChangelogset().getChangelogs().size());
	}
",non-flaky,5
42987,fabiomaffioletti_jsondoc,JSONDocApiVisibilityBuilderTest.testApiVisibility,"	@Test
	public void testApiVisibility() {
		ApiDoc apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(Controller.class), MethodDisplay.URI).iterator().next();
		Assert.assertEquals(ApiVisibility.PUBLIC, apiDoc.getVisibility());
		Assert.assertEquals(ApiStage.BETA, apiDoc.getStage());
		
		for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
			if(apiMethodDoc.getPath().contains(""/inherit"")) {
				Assert.assertEquals(ApiVisibility.PUBLIC, apiMethodDoc.getVisibility());
				Assert.assertEquals(ApiStage.BETA, apiMethodDoc.getStage());
			}
			if(apiMethodDoc.getPath().contains(""/override"")) {
				Assert.assertEquals(ApiVisibility.PRIVATE, apiMethodDoc.getVisibility());
				Assert.assertEquals(ApiStage.GA, apiMethodDoc.getStage());
			}
		}
		
		apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(Controller2.class), MethodDisplay.URI).iterator().next();
		Assert.assertEquals(ApiVisibility.UNDEFINED, apiDoc.getVisibility());
		Assert.assertEquals(ApiStage.UNDEFINED, apiDoc.getStage());
		
		for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
			if(apiMethodDoc.getPath().contains(""/only-method"")) {
				Assert.assertEquals(ApiVisibility.PRIVATE, apiMethodDoc.getVisibility());
				Assert.assertEquals(ApiStage.DEPRECATED, apiMethodDoc.getStage());
			}
		}
		
	}
",non-flaky,5
42988,fabiomaffioletti_jsondoc,JSONDocApiMethodPathBuilderTest.apply,"	@Test
	public void testPathWithMethodDisplayURI() {
		ApiDoc apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(Controller.class), MethodDisplay.URI).iterator().next();

		boolean allRight = FluentIterable.from(apiDoc.getMethods()).anyMatch(new Predicate<ApiMethodDoc>() {
			@Override
			public boolean apply(ApiMethodDoc input) {
				return 
						input.getPath().contains(""/path1"") && 
						input.getPath().contains(""/path2"") && 
						input.getDisplayedMethodString().contains(""/path1"") &&
						input.getDisplayedMethodString().contains(""/path2"");
			}
",non-flaky,5
42989,fabiomaffioletti_jsondoc,JSONDocApiMethodPathBuilderTest.apply,"	@Test
	public void testPathWithMethodDisplayMethod() {
		ApiDoc apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(Controller.class), MethodDisplay.METHOD).iterator().next();
		
		boolean allRight = FluentIterable.from(apiDoc.getMethods()).anyMatch(new Predicate<ApiMethodDoc>() {
			@Override
			public boolean apply(ApiMethodDoc input) {
				return 
						input.getPath().contains(""/path1"") && 
						input.getPath().contains(""/path2"") && 
						input.getDisplayedMethodString().contains(""path"") &&
						!input.getDisplayedMethodString().contains(""/path1"");
			}
",non-flaky,5
42990,fabiomaffioletti_jsondoc,JSONDocTypeBuilderTest.testReflex,"	@Test
	public void testReflex() throws NoSuchMethodException, SecurityException, ClassNotFoundException, JsonGenerationException, JsonMappingException, IOException {
		mapper.setSerializationInclusion(Include.NON_NULL);
		JSONDocType jsonDocType = new JSONDocType();
		
		Method method = JSONDocTypeBuilderTest.class.getMethod(""getString"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""string"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");
		
		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getInteger"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""integer"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");
		
		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getInt"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""int"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");
		
		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getLong"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""long"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");
		
		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getlong"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""long"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");
		
		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getListString"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""list of string"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");
		
		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getListSetString"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""list of set of string"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");
		
		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getStringArray"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""array of string"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");

		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getIntegerArray"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""array of integer"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");
		
		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getListOfStringArray"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""array of list of string"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");
		
		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getSetOfStringArray"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""array of set of string"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");
		
		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getList"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""list"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");
		
		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getListOfWildcard"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""list of wildcard"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");

		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getListOfWildcardArray"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""array of list of wildcard"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");
		
		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getListArray"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""array of list"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");

		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getSetArray"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""array of set"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");
		
		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getMap"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""map"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");
		
		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getHashMap"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""hashmap"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");

		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getMapStringInteger"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""map[string, integer]"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");

		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getMapListOfStringInteger"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""map[list of string, integer]"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");

		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getMapStringSetOfInteger"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""map[string, set of integer]"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");

		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getMapListOfStringSetOfInteger"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""map[list of string, set of integer]"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");

		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getMapListOfSetOfStringSetOfInteger"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""map[list of set of string, set of integer]"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");

		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getMapWildcardInteger"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""map[wildcard, integer]"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");

		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getMapWildcardWildcard"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""map[wildcard, wildcard]"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");

		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getMapListOfWildcardWildcard"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""map[list of wildcard, wildcard]"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");

		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getMapMapInteger"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""map[map, integer]"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");

		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getMapMapStringLongInteger"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""map[map[string, long], integer]"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");

		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getResponseEntityString"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""responseentity of string"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");

		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getResponseEntityListOfString"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""responseentity of list of string"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");
		
		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getParentPojoList"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""list of my_parent"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");
		
		jsonDocType = new JSONDocType();
		method = JSONDocTypeBuilderTest.class.getMethod(""getSpecializedWGenericsPojo"");
		JSONDocTypeBuilder.build(jsonDocType, method.getReturnType(), method.getGenericReturnType());
		System.out.println(mapper.writeValueAsString(jsonDocType));
		System.out.println(jsonDocType.getOneLineText());
		Assert.assertEquals(""fooPojo of T"", jsonDocType.getOneLineText());
		System.out.println(""---------------------------"");		
	}
",non-flaky,5
42991,fabiomaffioletti_jsondoc,JSONDocEnumTemplateBuilderTest.testTemplate,"	@Test
	public void testTemplate() throws IOException, IllegalArgumentException, IllegalAccessException, InstantiationException {
		ObjectMapper mapper = new ObjectMapper();
		Set<Class<?>> classes = Sets.<Class<?>>newHashSet(MyEnum.class);
		
		Map<String, Object> template = JSONDocTemplateBuilder.build(MyEnum.class, classes);
		System.out.println(mapper.writeValueAsString(template));
	}
",non-flaky,5
42992,fabiomaffioletti_jsondoc,JSONDocTemplateBuilderWithOrderTest.thatTemplateIsMappedToStringCorrectly,"	@Test
	public void thatTemplateIsMappedToStringCorrectly() throws Exception {
		final ObjectMapper mapper = new ObjectMapper();
		Set<Class<?>> classes = Sets.<Class<?>>newHashSet(Unordered.class, Ordered.class);

		Map<String, Object> unorderedTemplate = JSONDocTemplateBuilder.build(Unordered.class, classes);
		Assert.assertEquals(""{\""aField\"":\""\"",\""xField\"":\""\""}"", mapper.writeValueAsString(unorderedTemplate));

		Map<String, Object> orderedTemplate = JSONDocTemplateBuilder.build(Ordered.class, classes);
		Assert.assertEquals(""{\""xField\"":\""\"",\""aField\"":\""\"",\""bField\"":\""\""}"", mapper.writeValueAsString(orderedTemplate));
	}
",non-flaky,5
42993,fabiomaffioletti_jsondoc,DefaultJSONDocScannerTest.getJSONDoc,"    @Test
    public void getJSONDoc() throws IOException {
    	JSONDocScanner jsondocScanner = new DefaultJSONDocScanner();
        JSONDoc jsondoc = jsondocScanner.getJSONDoc(version, basePath, Lists.newArrayList(""org.jsondoc.core.util""), true, MethodDisplay.URI);
        assertEquals(1, jsondoc.getApis().size());

        int countApis = 0;
        for (String string : jsondoc.getApis().keySet()) {
            countApis += jsondoc.getApis().get(string).size();
        }
        assertEquals(4, countApis);

        assertEquals(3, jsondoc.getObjects().size());
        
        int countFlows = 0;
        for (String string : jsondoc.getFlows().keySet()) {
        	countFlows += jsondoc.getFlows().get(string).size();
        }
        assertEquals(2, countFlows);

        int countObjects = 0;
        for (String string : jsondoc.getObjects().keySet()) {
            countObjects += jsondoc.getObjects().get(string).size();
        }
        assertEquals(10, countObjects);

        Set<ApiVerb> apiVerbs = getAllTestedApiVerbs(jsondoc);
        assertEquals(ApiVerb.values().length, apiVerbs.size());

        log.debug(objectMapper.writeValueAsString(jsondoc));
    }
",non-flaky,5
42994,fabiomaffioletti_jsondoc,StackOverflowTemplateBuilderTest.testTemplate,"	@Test
	public void testTemplate() throws JsonGenerationException, JsonMappingException, IOException, IllegalArgumentException, IllegalAccessException, InstantiationException {
		Set<Class<?>> classes = Sets.<Class<?>>newHashSet(StackOverflowTemplateSelf.class, StackOverflowTemplateObjectOne.class, StackOverflowTemplateObjectTwo.class);
		
		StackOverflowTemplateSelf objectSelf = new StackOverflowTemplateSelf();
		Map<String, Object> template = JSONDocTemplateBuilder.build(objectSelf.getClass(), classes);
		System.out.println(mapper.writeValueAsString(template));
		
		StackOverflowTemplateObjectOne objectOne = new StackOverflowTemplateObjectOne();
		template = JSONDocTemplateBuilder.build(objectOne.getClass(), classes);
		System.out.println(mapper.writeValueAsString(template));
		
		StackOverflowTemplateObjectTwo objectTwo = new StackOverflowTemplateObjectTwo();
		template = JSONDocTemplateBuilder.build(objectTwo.getClass(), classes);
		System.out.println(mapper.writeValueAsString(template));
	}
",non-flaky,5
42995,fabiomaffioletti_jsondoc,StackOverflowTemplateBuilderTest.typeOneTwo,"	@Test
	public void typeOneTwo() throws JsonGenerationException, JsonMappingException, IOException {
		Set<Class<?>> classes = Sets.<Class<?>>newHashSet(NotAnnotatedStackOverflowObjectOne.class, NotAnnotatedStackOverflowObjectTwo.class);
		
		NotAnnotatedStackOverflowObjectOne typeOne = new NotAnnotatedStackOverflowObjectOne();
		Map<String, Object> template = JSONDocTemplateBuilder.build(typeOne.getClass(), classes);
		System.out.println(mapper.writeValueAsString(template));
	}
",non-flaky,5
42996,fabiomaffioletti_jsondoc,JSONDocTemplateBuilderTest.testTemplate,"	@Test
	public void testTemplate() throws IOException, IllegalArgumentException, IllegalAccessException, InstantiationException {
		ObjectMapper mapper = new ObjectMapper();
		Set<Class<?>> classes = Sets.<Class<?>>newHashSet(TemplateObject.class);
		
		Map<String, Object> template = JSONDocTemplateBuilder.build(TemplateObject.class, classes);

		Assert.assertEquals(0, template.get(""my_id""));
		Assert.assertEquals(0, template.get(""idint""));
		Assert.assertEquals(0, template.get(""idlong""));
		Assert.assertEquals("""", template.get(""name""));
		Assert.assertEquals("""", template.get(""gender""));
		Assert.assertEquals(true, template.get(""bool""));
		Assert.assertEquals(new ArrayList(), template.get(""intarrarr""));
		Assert.assertEquals(new JSONDocTemplate(), template.get(""sub_obj""));
		Assert.assertEquals(new ArrayList(), template.get(""untypedlist""));
		Assert.assertEquals(new ArrayList(), template.get(""subsubobjarr""));
		Assert.assertEquals(new ArrayList(), template.get(""stringlist""));
		Assert.assertEquals(new ArrayList(), template.get(""stringarrarr""));
		Assert.assertEquals(new ArrayList(), template.get(""integerarr""));
		Assert.assertEquals(new ArrayList(), template.get(""stringarr""));
		Assert.assertEquals(new ArrayList(), template.get(""intarr""));
		Assert.assertEquals(new ArrayList(), template.get(""subobjlist""));
		Assert.assertEquals(new ArrayList(), template.get(""wildcardlist""));
		Assert.assertEquals(new ArrayList(), template.get(""longlist""));
		Assert.assertEquals("""", template.get(""namechar""));
		Assert.assertEquals(new HashMap(), template.get(""map""));
		Assert.assertEquals(new HashMap(), template.get(""mapstringinteger""));
		Assert.assertEquals(new HashMap(), template.get(""mapsubobjinteger""));
		Assert.assertEquals(new HashMap(), template.get(""mapintegersubobj""));
		Assert.assertEquals(new HashMap(), template.get(""mapintegerlistsubsubobj""));
		
		System.out.println(mapper.writeValueAsString(template));
	}
",non-flaky,5
42997,fabiomaffioletti_jsondoc,JSONDocTemplateBuilderTest.testTemplateWithConstant,"	@Test
	public void testTemplateWithConstant() throws Exception {
        final ObjectMapper mapper = new ObjectMapper();
        final Set<Class<?>> classes = Sets.<Class<?>>newHashSet(ClassWithConstant.class);

        final Map<String, Object> template = JSONDocTemplateBuilder.build(ClassWithConstant.class, classes);
        Assert.assertEquals("""", template.get(""identifier""));
        Assert.assertEquals(null, template.get(THIS_IS_A_CONSTANT));

        final String serializedTemplate =
            ""{"" +
                ""\""identifier\"":\""\"""" +
            ""}"";

        assertThat(mapper.writeValueAsString(template), is(serializedTemplate));
	}
",non-flaky,5
42998,fabiomaffioletti_jsondoc,JSONDocNoAnnotationTemplateBuilderTest.testTemplate,"	@Test
	public void testTemplate() throws IOException, IllegalArgumentException, IllegalAccessException, InstantiationException {
		ObjectMapper mapper = new ObjectMapper();
		Set<Class<?>> classes = Sets.<Class<?>>newHashSet(NoAnnotationPojo.class);
		
		Map<String, Object> template = JSONDocTemplateBuilder.build(NoAnnotationPojo.class, classes);
		System.out.println(mapper.writeValueAsString(template));
	}
",non-flaky,5
42999,fabiomaffioletti_jsondoc,InterfaceApiObjectTest.testInvisible,"	@Test
	public void testInvisible() {
		JSONDoc jsonDoc = jsondocScanner.getJSONDoc(""version"", ""basePath"", Lists.newArrayList(""org.jsondoc.springmvc.issues.invisible""), true, MethodDisplay.URI);
		Assert.assertEquals(1, jsonDoc.getObjects().keySet().size());
		for (String string : jsonDoc.getObjects().keySet()) {
			Assert.assertEquals(2, jsonDoc.getObjects().get(string).size());
		}
		for (ApiDoc apiDoc : jsonDoc.getApis().get("""")) {
			for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
				Assert.assertEquals(""Resource Interface"", apiMethodDoc.getResponse().getJsondocType().getOneLineText());
			}
		}
		
	}
",non-flaky,5
43000,fabiomaffioletti_jsondoc,Issue151Test.testIssue151,"	@Test
	public void testIssue151() {
		JSONDoc jsonDoc = jsondocScanner.getJSONDoc(""version"", ""basePath"", Lists.newArrayList(""org.jsondoc.springmvc.issues.issue151""), true, MethodDisplay.URI);
		Assert.assertEquals(2, jsonDoc.getObjects().keySet().size());
		Assert.assertEquals(1, jsonDoc.getObjects().get(""bargroup"").size());
		Assert.assertEquals(1, jsonDoc.getObjects().get(""foogroup"").size());
	}
",non-flaky,5
43001,fabiomaffioletti_jsondoc,SpringResponseStatusBuilderTest.testApiVerb,"	@Test
	public void testApiVerb() {
		ApiDoc apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(SpringController.class), MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""SpringController"", apiDoc.getName());
		Assert.assertEquals(2, apiDoc.getMethods().size());
		for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
			if (apiMethodDoc.getPath().contains(""/status-one"")) {
				Assert.assertEquals(""201 - Created"", apiMethodDoc.getResponsestatuscode());
			}
			if (apiMethodDoc.getPath().contains(""/status-two"")) {
				Assert.assertEquals(""200 - OK"", apiMethodDoc.getResponsestatuscode());
			}
		}
	}
",non-flaky,5
43002,fabiomaffioletti_jsondoc,PlainSpringJSONDocScannerTest.testMergeApiDoc,"	@Test
	public void testMergeApiDoc() {
		Set<Class<?>> controllers = new LinkedHashSet<Class<?>>();
		controllers.add(SpringController.class);
		Set<ApiDoc> apiDocs = jsondocScanner.getApiDocs(controllers, MethodDisplay.URI);

		ApiDoc apiDoc = apiDocs.iterator().next();
		Assert.assertEquals(""SpringController"", apiDoc.getDescription());
		Assert.assertEquals(""SpringController"", apiDoc.getName());
		Assert.assertNotNull(apiDoc.getGroup());

		for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
			Assert.assertEquals(MethodDisplay.URI, apiMethodDoc.getDisplayMethodAs());
			Assert.assertNull(apiMethodDoc.getAuth());
			Assert.assertNull(apiMethodDoc.getSupportedversions());
			Assert.assertTrue(apiMethodDoc.getApierrors().isEmpty());
			Assert.assertNull(apiMethodDoc.getId());
			Assert.assertEquals("""", apiMethodDoc.getSummary());
			Assert.assertEquals("""", apiMethodDoc.getDescription());
			
			if (apiMethodDoc.getPath().contains(""/api/string/{name}"")) {
				Assert.assertEquals(2, apiMethodDoc.getHeaders().size());
				Set<ApiHeaderDoc> headers = apiMethodDoc.getHeaders();
				Iterator<ApiHeaderDoc> headersIterator = headers.iterator();
				ApiHeaderDoc headerTest = headersIterator.next();
				Assert.assertEquals(""header"", headerTest.getName());
				Assert.assertEquals(""test"", headerTest.getAllowedvalues()[0]);
				ApiHeaderDoc headerTwo = headersIterator.next();
				Assert.assertEquals(""header-two"", headerTwo.getName());
				Assert.assertEquals(""header-test"", headerTwo.getAllowedvalues()[0]);

				Assert.assertEquals(""string"", apiMethodDoc.getBodyobject().getJsondocType().getOneLineText());
				Assert.assertEquals(""string"", apiMethodDoc.getResponse().getJsondocType().getOneLineText());
				Assert.assertEquals(""POST"", apiMethodDoc.getVerb().iterator().next().name());
				Assert.assertEquals(""application/json"", apiMethodDoc.getProduces().iterator().next());
				Assert.assertEquals(""application/json"", apiMethodDoc.getConsumes().iterator().next());
				Assert.assertEquals(""201 - Created"", apiMethodDoc.getResponsestatuscode());

				Set<ApiParamDoc> queryparameters = apiMethodDoc.getQueryparameters();
				Assert.assertEquals(4, queryparameters.size());
				Iterator<ApiParamDoc> qpIterator = queryparameters.iterator();
				ApiParamDoc apiParamDoc = qpIterator.next();
				Assert.assertEquals(""delete"", apiParamDoc.getName());
				Assert.assertEquals(""true"", apiParamDoc.getRequired());
				Assert.assertEquals(null, apiParamDoc.getDefaultvalue());
				Assert.assertEquals(0, apiParamDoc.getAllowedvalues().length);
				apiParamDoc = qpIterator.next();
				Assert.assertEquals(""id"", apiParamDoc.getName());
				Assert.assertEquals(""true"", apiParamDoc.getRequired());
				Assert.assertTrue(apiParamDoc.getDefaultvalue().isEmpty());
				apiParamDoc = qpIterator.next();
				Assert.assertEquals("""", apiParamDoc.getName());
				Assert.assertEquals(""true"", apiParamDoc.getRequired());
				Assert.assertEquals("""", apiParamDoc.getDefaultvalue());

				apiParamDoc = qpIterator.next();
				Assert.assertEquals(""user"", apiParamDoc.getName());
				Assert.assertEquals(""false"", apiParamDoc.getRequired());
				Assert.assertEquals(""admin"", apiParamDoc.getDefaultvalue());

				Set<ApiParamDoc> pathparameters = apiMethodDoc.getPathparameters();
				Iterator<ApiParamDoc> ppIterator = pathparameters.iterator();
				apiParamDoc = ppIterator.next();
				apiParamDoc = apiMethodDoc.getPathparameters().iterator().next();
				Assert.assertEquals(""test"", apiParamDoc.getName());
			}
		}

	}
",non-flaky,5
43003,fabiomaffioletti_jsondoc,Spring3JSONDocGenericsObjectScannerTest.getJSONDoc,"	@Test
	public void getJSONDoc() throws IOException {
		JSONDocScanner jsondocScanner = new Spring3JSONDocScanner();
		JSONDoc jsondoc = jsondocScanner.getJSONDoc(version, basePath, Lists.newArrayList(""org.jsondoc.springmvc.issues.issue174""), true, MethodDisplay.URI);

		Map<String, Set<ApiObjectDoc>> objects = jsondoc.getObjects();
		for (Set<ApiObjectDoc> values : objects.values()) {
			for (ApiObjectDoc apiObjectDoc : values) {
				System.out.println(apiObjectDoc.getName());
			}
		}
	}
",non-flaky,5
43004,fabiomaffioletti_jsondoc,SpringObjectBuilderTest.testApiVerb,"	@Test
	public void testApiVerb() {
		ApiObjectDoc buildObject = SpringObjectBuilder.buildObject(MyObject.class);
		Assert.assertEquals(""MyObject"", buildObject.getName());
		Assert.assertEquals(3, buildObject.getFields().size());
	}
",non-flaky,5
43005,fabiomaffioletti_jsondoc,SpringPathBuilderTest.apply,"	@Test
	public void testPath() {
		ApiDoc apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(SpringController.class), MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""SpringController"", apiDoc.getName());

		boolean slashPath = FluentIterable.from(apiDoc.getMethods()).anyMatch(new Predicate<ApiMethodDoc>() {
			@Override
			public boolean apply(ApiMethodDoc input) {
				return input.getPath().contains(""/path"");
			}
",non-flaky,5
43006,fabiomaffioletti_jsondoc,SpringPathBuilderTest.apply,"	@Test
	public void testPath2() {
		ApiDoc apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(SpringController2.class), MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""SpringController2"", apiDoc.getName());

		boolean none = FluentIterable.from(apiDoc.getMethods()).anyMatch(new Predicate<ApiMethodDoc>() {
			
			@Override
			public boolean apply(ApiMethodDoc input) {
				System.out.println(input.getPath());
				return input.getPath().contains(""/"");
			}
",non-flaky,5
43007,fabiomaffioletti_jsondoc,SpringPathBuilderTest.apply,"	@Test
	public void testPath3() {
		ApiDoc apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(SpringController3.class), MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""SpringController3"", apiDoc.getName());

		boolean allRight = FluentIterable.from(apiDoc.getMethods()).anyMatch(new Predicate<ApiMethodDoc>() {
			@Override
			public boolean apply(ApiMethodDoc input) {
				boolean allRight =
								input.getPath().contains(""/path1/path3"") && 
								input.getPath().contains(""/path1/path4"") && 
								input.getPath().contains(""/path2/path3"") && 
								input.getPath().contains(""/path2/path4"");     
				return allRight;
			}
",non-flaky,5
43008,fabiomaffioletti_jsondoc,SpringPathBuilderTest.apply,"	@Test
	public void testPath4() {
		ApiDoc apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(SpringController4.class), MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""SpringController4"", apiDoc.getName());

		boolean allRight = FluentIterable.from(apiDoc.getMethods()).anyMatch(new Predicate<ApiMethodDoc>() {
			@Override
			public boolean apply(ApiMethodDoc input) {
				boolean allRight =
								input.getPath().contains(""/path""); 
				return allRight;
			}
",non-flaky,5
43009,fabiomaffioletti_jsondoc,SpringPathBuilderTest.apply,"	@Test
	public void testPath5() {
		ApiDoc apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(SpringController5.class), MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""SpringController5"", apiDoc.getName());
		
		boolean allRight = FluentIterable.from(apiDoc.getMethods()).anyMatch(new Predicate<ApiMethodDoc>() {
			@Override
			public boolean apply(ApiMethodDoc input) {
				boolean allRight = input.getPath().contains(""/path"") && input.getPath().contains(""/path2"");
				return allRight;
			}
",non-flaky,5
43010,fabiomaffioletti_jsondoc,SpringPathBuilderTest.apply,"	@Test
	public void testPath6() {
		ApiDoc apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(SpringController6.class), MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""SpringController6"", apiDoc.getName());

		boolean allRight = FluentIterable.from(apiDoc.getMethods()).anyMatch(new Predicate<ApiMethodDoc>() {
			@Override
			public boolean apply(ApiMethodDoc input) {
				return input.getPath().contains(""/api/widget/frame"");
			}
",non-flaky,5
43011,fabiomaffioletti_jsondoc,SpringPathBuilderTest.apply,"	@Test
	public void testPathWithMethodDisplayMethod() {
		ApiDoc apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(SpringController5.class), MethodDisplay.METHOD).iterator().next();
		boolean allRight = FluentIterable.from(apiDoc.getMethods()).anyMatch(new Predicate<ApiMethodDoc>() {
			@Override
			public boolean apply(ApiMethodDoc input) {
				boolean allRight = input.getPath().contains(""/path"") && input.getPath().contains(""/path2"") && input.getDisplayedMethodString().contains(""none"");
				return allRight;
			}
",non-flaky,5
43012,fabiomaffioletti_jsondoc,SpringConsumesBuilderTest.testApiVerb,"    @Test
    public void testApiVerb() {
	ApiDoc apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(SpringController.class), MethodDisplay.URI).iterator().next();
	Assert.assertEquals(""SpringController"", apiDoc.getName());
	Assert.assertEquals(3, apiDoc.getMethods().size());
	for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
	    if (apiMethodDoc.getPath().contains(""/consumes-one"")) {
		Assert.assertEquals(1, apiMethodDoc.getConsumes().size());
		Assert.assertEquals(MediaType.APPLICATION_JSON_VALUE, apiMethodDoc.getConsumes().iterator().next());
	    }
	    if (apiMethodDoc.getPath().contains(""/consumes-two"")) {
		Assert.assertEquals(2, apiMethodDoc.getConsumes().size());
		Iterator<String> iterator = apiMethodDoc.getConsumes().iterator();
		Assert.assertEquals(MediaType.APPLICATION_JSON_VALUE, iterator.next());
		Assert.assertEquals(MediaType.APPLICATION_XML_VALUE, iterator.next());
	    }
	    if (apiMethodDoc.getPath().contains(""/consumes-three"")) {
		Assert.assertEquals(1, apiMethodDoc.getConsumes().size());
		String consumes = apiMethodDoc.getConsumes().iterator().next();
		Assert.assertEquals(MediaType.APPLICATION_JSON_VALUE, consumes);
	    }
	}

	apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(SpringController2.class), MethodDisplay.URI).iterator().next();
	Assert.assertEquals(""SpringController2"", apiDoc.getName());
	Assert.assertEquals(3, apiDoc.getMethods().size());
	for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
	    if (apiMethodDoc.getPath().contains(""/consumes-one"")) {
		Assert.assertEquals(1, apiMethodDoc.getConsumes().size());
		Assert.assertEquals(MediaType.APPLICATION_JSON_VALUE, apiMethodDoc.getConsumes().iterator().next());
	    }
	    if (apiMethodDoc.getPath().contains(""/consumes-two"")) {
		Assert.assertEquals(2, apiMethodDoc.getConsumes().size());
		Iterator<String> iterator = apiMethodDoc.getConsumes().iterator();
		Assert.assertEquals(MediaType.APPLICATION_JSON_VALUE, iterator.next());
		Assert.assertEquals(MediaType.APPLICATION_XML_VALUE, iterator.next());
	    }
	    if (apiMethodDoc.getPath().contains(""/consumes-three"")) {
		Assert.assertEquals(1, apiMethodDoc.getConsumes().size());
		Assert.assertEquals(MediaType.APPLICATION_XML_VALUE, apiMethodDoc.getConsumes().iterator().next());
	    }
	}
    }
",non-flaky,5
43013,fabiomaffioletti_jsondoc,SpringQueryParamBuilderTest.testQueryParam,"	@Test
	public void testQueryParam() {
		ApiDoc apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(SpringController.class), MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""SpringController"", apiDoc.getName());
		Assert.assertEquals(3, apiDoc.getMethods().size());
		for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
			if (apiMethodDoc.getPath().contains(""/param-one"")) {
				Assert.assertEquals(1, apiMethodDoc.getQueryparameters().size());
			}
			if (apiMethodDoc.getPath().contains(""/param-two"")) {
				Assert.assertEquals(2, apiMethodDoc.getQueryparameters().size());
			}
			if (apiMethodDoc.getPath().contains(""/param-three"")) {
				Assert.assertEquals(2, apiMethodDoc.getQueryparameters().size());
				Iterator<ApiParamDoc> iterator = apiMethodDoc.getQueryparameters().iterator();
				ApiParamDoc param = iterator.next();
				Assert.assertEquals(""param"", param.getName());
				Assert.assertEquals(""value"", param.getAllowedvalues()[0]);
				ApiParamDoc param2 = iterator.next();
				Assert.assertEquals(""param2"", param2.getName());
				Assert.assertEquals(""value2"", param2.getAllowedvalues()[0]);
			}
		}
		
		apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(SpringController2.class), MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""SpringController2"", apiDoc.getName());
		Assert.assertEquals(2, apiDoc.getMethods().size());
		for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
			if (apiMethodDoc.getPath().contains(""/param-one"")) {
				Assert.assertEquals(2, apiMethodDoc.getQueryparameters().size());
			}
			if (apiMethodDoc.getPath().contains(""/param-two"")) {
				Assert.assertEquals(3, apiMethodDoc.getQueryparameters().size());
			}
		}
		
		apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(SpringController3.class), MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""SpringController3"", apiDoc.getName());
		Assert.assertEquals(4, apiDoc.getMethods().size());
		for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
			if (apiMethodDoc.getPath().contains(""/param-one"")) {
				Assert.assertEquals(2, apiMethodDoc.getQueryparameters().size());
				Iterator<ApiParamDoc> iterator = apiMethodDoc.getQueryparameters().iterator();
				ApiParamDoc param = iterator.next();
				ApiParamDoc queryParam = iterator.next();
				Assert.assertEquals(""name"", queryParam.getName());
				Assert.assertEquals(""true"", queryParam.getRequired());
				Assert.assertEquals(""string"", queryParam.getJsondocType().getOneLineText());
				Assert.assertEquals("""", queryParam.getDefaultvalue());
			}
			if (apiMethodDoc.getPath().contains(""/param-two"")) {
				Assert.assertEquals(2, apiMethodDoc.getQueryparameters().size());
				Iterator<ApiParamDoc> iterator = apiMethodDoc.getQueryparameters().iterator();
				ApiParamDoc param = iterator.next();
				ApiParamDoc queryParam = iterator.next();
				Assert.assertEquals(""name"", queryParam.getName());
				Assert.assertEquals(""false"", queryParam.getRequired());
				Assert.assertEquals(""string"", queryParam.getJsondocType().getOneLineText());
				Assert.assertEquals(""test"", queryParam.getDefaultvalue());
			}
			if (apiMethodDoc.getPath().contains(""/param-three"")) {
				Assert.assertEquals(2, apiMethodDoc.getQueryparameters().size());
				Iterator<ApiParamDoc> iterator = apiMethodDoc.getQueryparameters().iterator();
				ApiParamDoc param = iterator.next();
				ApiParamDoc queryParam = iterator.next();
				Assert.assertEquals("""", queryParam.getName());
				Assert.assertEquals(""true"", queryParam.getRequired());
				Assert.assertEquals(""string"", queryParam.getJsondocType().getOneLineText());
				Assert.assertEquals("""", queryParam.getDefaultvalue());
			}
			if (apiMethodDoc.getPath().contains(""/param-four"")) {
				Assert.assertEquals(2, apiMethodDoc.getQueryparameters().size());
				Iterator<ApiParamDoc> iterator = apiMethodDoc.getQueryparameters().iterator();
				ApiParamDoc param = iterator.next();
				ApiParamDoc queryParam = iterator.next();
				Assert.assertEquals(""value"", queryParam.getName());
				Assert.assertEquals(""false"", queryParam.getRequired());
				Assert.assertEquals(""string"", queryParam.getJsondocType().getOneLineText());
				Assert.assertEquals("""", queryParam.getDefaultvalue());
			}
		}
		
		apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(SpringController4.class), MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""SpringController4"", apiDoc.getName());
		Assert.assertEquals(2, apiDoc.getMethods().size());
		for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
			if (apiMethodDoc.getPath().contains(""/"")) {
				Assert.assertEquals(1, apiMethodDoc.getQueryparameters().size());
				ApiParamDoc param = apiMethodDoc.getQueryparameters().iterator().next();
				Assert.assertEquals(""name"", param.getName());
			}
			if (apiMethodDoc.getPath().contains(""/two"")) {
				Assert.assertEquals(2, apiMethodDoc.getQueryparameters().size());
			}
		}
		
		apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(SpringController5.class), MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""SpringController5"", apiDoc.getName());
		Assert.assertEquals(1, apiDoc.getMethods().size());
		for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
			if (apiMethodDoc.getPath().contains(""/"")) {
				Assert.assertEquals(1, apiMethodDoc.getQueryparameters().size());
				ApiParamDoc param = apiMethodDoc.getQueryparameters().iterator().next();
				Assert.assertEquals(""modelAttributePojo"", param.getName());
				Assert.assertEquals(""modelattributepojo"", param.getJsondocType().getOneLineText());
			}
		}
		
	}
",non-flaky,5
43014,fabiomaffioletti_jsondoc,SpringRequestMappingDerivativesTest.apply,"    @Test
    public void testGetMapping() {
        ApiDoc
            apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(RequestMappingController.class), MethodDisplay.URI).iterator().next();
        Assert.assertEquals(""RequestMappingController"", apiDoc.getName());

        boolean getMethodPresent = FluentIterable.from(apiDoc.getMethods()).anyMatch(new Predicate<ApiMethodDoc>() {
            @Override
            public boolean apply(ApiMethodDoc input) {
                return input.getMethod().equals(""get"");
            }
",non-flaky,5
43015,fabiomaffioletti_jsondoc,SpringApiHeadersDocTest.testApiHeadersOnClass,"	@Test
	public void testApiHeadersOnClass() {
		ApiDoc apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(SpringApiHeadersController.class), MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""SpringApiHeadersController"", apiDoc.getName());
		Assert.assertEquals(3, apiDoc.getMethods().size());
		for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
			if (apiMethodDoc.getPath().contains(""/spring-api-headers-controller-method-one"")) {
				Assert.assertEquals(2, apiMethodDoc.getHeaders().size());
			}
			if (apiMethodDoc.getPath().contains(""/spring-api-headers-controller-method-two"")) {
				Assert.assertEquals(3, apiMethodDoc.getHeaders().size());
			}
			if (apiMethodDoc.getPath().contains(""/spring-api-headers-controller-method-three"")) {
				Assert.assertEquals(4, apiMethodDoc.getHeaders().size());
				Iterator<ApiHeaderDoc> headers = apiMethodDoc.getHeaders().iterator();
				ApiHeaderDoc h1 = headers.next();
				ApiHeaderDoc h2 = headers.next();
				ApiHeaderDoc h4 = headers.next();
				Assert.assertEquals(""h4"", h4.getName());
				ApiHeaderDoc h5 = headers.next();
				Assert.assertEquals(""h5"", h5.getName());
			}
		}
	}
",non-flaky,5
43016,fabiomaffioletti_jsondoc,SpringPathVariableBuilderTest.testPathVariable,"	@Test
	public void testPathVariable() {
		ApiDoc apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(SpringController.class), MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""SpringController"", apiDoc.getName());
		Assert.assertEquals(2, apiDoc.getMethods().size());
		for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
			if (apiMethodDoc.getPath().contains(""/param-one/{id}/{string}"")) {
				Assert.assertEquals(2, apiMethodDoc.getPathparameters().size());
				Iterator<ApiParamDoc> iterator = apiMethodDoc.getPathparameters().iterator();
				ApiParamDoc id = iterator.next();
				Assert.assertEquals("""", id.getName());
				Assert.assertEquals(""long"", id.getJsondocType().getOneLineText());
				ApiParamDoc name = iterator.next();
				Assert.assertEquals(""name"", name.getName());
				Assert.assertEquals(""string"", name.getJsondocType().getOneLineText());
			}
			
			if (apiMethodDoc.getPath().contains(""/param-one/{id}/{string}/{test}"")) {
				Assert.assertEquals(3, apiMethodDoc.getPathparameters().size());
				Iterator<ApiParamDoc> iterator = apiMethodDoc.getPathparameters().iterator();
				ApiParamDoc id = iterator.next();
				Assert.assertEquals(""id"", id.getName());
				Assert.assertEquals(""long"", id.getJsondocType().getOneLineText());
				ApiParamDoc name = iterator.next();
				Assert.assertEquals(""name"", name.getName());
				Assert.assertEquals(""string"", name.getJsondocType().getOneLineText());
				ApiParamDoc test = iterator.next();
				Assert.assertEquals("""", test.getName());
				Assert.assertEquals(""long"", test.getJsondocType().getOneLineText());
			}
		}
		
	}
",non-flaky,5
43017,fabiomaffioletti_jsondoc,SpringPathVariableBuilderTest.testPathVariableWithJSONDoc,"	@Test
	public void testPathVariableWithJSONDoc() {
		ApiDoc apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(SpringController2.class), MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""SpringController2"", apiDoc.getName());
		Assert.assertEquals(1, apiDoc.getMethods().size());
		for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
			if (apiMethodDoc.getPath().contains(""/param-one/{id}/{string}"")) {
				Assert.assertEquals(2, apiMethodDoc.getPathparameters().size());
				Iterator<ApiParamDoc> iterator = apiMethodDoc.getPathparameters().iterator();
				ApiParamDoc id = iterator.next();
				Assert.assertEquals("""", id.getName());
				Assert.assertEquals(""long"", id.getJsondocType().getOneLineText());
				Assert.assertEquals(""description for id"", id.getDescription());
				ApiParamDoc name = iterator.next();
				Assert.assertEquals(""name"", name.getName());
				Assert.assertEquals(""string"", name.getJsondocType().getOneLineText());
			}
		}
		
	}
",non-flaky,5
43018,fabiomaffioletti_jsondoc,SpringRequestBodyBuilderTest.testBodyOne,"	@Test
	public void testBodyOne() {
		ApiDoc apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(SpringController.class), MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""SpringController"", apiDoc.getName());
		Assert.assertEquals(2, apiDoc.getMethods().size());
		for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
			if (apiMethodDoc.getPath().contains(""/body-one"")) {
				Assert.assertNotNull(apiMethodDoc.getBodyobject());
				Assert.assertEquals(""string"", apiMethodDoc.getBodyobject().getJsondocType().getOneLineText());
			}
			if (apiMethodDoc.getPath().contains(""/body-two"")) {
				Assert.assertNotNull(apiMethodDoc.getBodyobject());
				Assert.assertEquals(""body"", apiMethodDoc.getBodyobject().getJsondocType().getOneLineText());
			}
		}
	}
",non-flaky,5
43019,fabiomaffioletti_jsondoc,SpringResponseBuilderTest.testApiVerb,"	@Test
	public void testApiVerb() {
		ApiDoc apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(SpringController.class), MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""SpringController"", apiDoc.getName());
		Assert.assertEquals(3, apiDoc.getMethods().size());
		for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
			if (apiMethodDoc.getPath().contains(""/response-one"")) {
				Assert.assertEquals(""string"", apiMethodDoc.getResponse().getJsondocType().getOneLineText());
			}
			if (apiMethodDoc.getPath().contains(""/response-two"")) {
				Assert.assertEquals(""string"", apiMethodDoc.getResponse().getJsondocType().getOneLineText());
			}
			if (apiMethodDoc.getPath().contains(""/response-three"")) {
				Assert.assertEquals(""map[string, integer]"", apiMethodDoc.getResponse().getJsondocType().getOneLineText());
			}
		}
	}
",non-flaky,5
43020,fabiomaffioletti_jsondoc,SpringApiVerbDocTest.testApiVerb,"	@Test
	public void testApiVerb() {
		ApiDoc apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(SpringApiVerbController.class), MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""SpringApiVerbController"", apiDoc.getName());
		Assert.assertEquals(2, apiDoc.getMethods().size());
		for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
			if (apiMethodDoc.getPath().contains(""/api-verb/spring-api-verb-controller-method-one"")) {
				Assert.assertEquals(1, apiMethodDoc.getVerb().size());
				Assert.assertEquals(ApiVerb.GET, apiMethodDoc.getVerb().iterator().next());
			}
			if (apiMethodDoc.getPath().contains(""/api-verb/spring-api-verb-controller-method-two"")) {
				Assert.assertEquals(2, apiMethodDoc.getVerb().size());
			}
		}
		
		apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(SpringApiVerbController2.class), MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""SpringApiVerbController2"", apiDoc.getName());
		Assert.assertEquals(1, apiDoc.getMethods().size());
		for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
			if (apiMethodDoc.getPath().contains(""/api-verb-2/spring-api-verb-controller-method-one"")) {
				Assert.assertEquals(2, apiMethodDoc.getVerb().size());
			}
		}
		
	}
",non-flaky,5
43021,fabiomaffioletti_jsondoc,SpringProducesBuilderTest.testApiVerb,"	@Test
	public void testApiVerb() {
		ApiDoc apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(SpringController.class), MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""SpringController"", apiDoc.getName());
		Assert.assertEquals(3, apiDoc.getMethods().size());
		for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
			if (apiMethodDoc.getPath().contains(""/produces-one"")) {
				Assert.assertEquals(1, apiMethodDoc.getProduces().size());
				Assert.assertEquals(MediaType.APPLICATION_JSON_VALUE, apiMethodDoc.getProduces().iterator().next());
			}
			if (apiMethodDoc.getPath().contains(""/produces-two"")) {
				Assert.assertEquals(2, apiMethodDoc.getProduces().size());
				Iterator<String> iterator = apiMethodDoc.getProduces().iterator();
				Assert.assertEquals(MediaType.APPLICATION_JSON_VALUE, iterator.next());
				Assert.assertEquals(MediaType.APPLICATION_XML_VALUE, iterator.next());
			}
			if (apiMethodDoc.getPath().contains(""/produces-three"")) {
				Assert.assertEquals(1, apiMethodDoc.getProduces().size());
				String produces = apiMethodDoc.getProduces().iterator().next();
				Assert.assertEquals(""application/json"", produces);
			}
		}
		
		apiDoc = jsondocScanner.getApiDocs(Sets.<Class<?>> newHashSet(SpringController2.class), MethodDisplay.URI).iterator().next();
		Assert.assertEquals(""SpringController2"", apiDoc.getName());
		Assert.assertEquals(3, apiDoc.getMethods().size());
		for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
			if (apiMethodDoc.getPath().contains(""/produces-one"")) {
				Assert.assertEquals(1, apiMethodDoc.getProduces().size());
				Assert.assertEquals(MediaType.APPLICATION_JSON_VALUE, apiMethodDoc.getProduces().iterator().next());
			}
			if (apiMethodDoc.getPath().contains(""/produces-two"")) {
				Assert.assertEquals(2, apiMethodDoc.getProduces().size());
				Iterator<String> iterator = apiMethodDoc.getProduces().iterator();
				Assert.assertEquals(MediaType.APPLICATION_JSON_VALUE, iterator.next());
				Assert.assertEquals(MediaType.APPLICATION_XML_VALUE, iterator.next());
			}
			if (apiMethodDoc.getPath().contains(""/produces-three"")) {
				Assert.assertEquals(1, apiMethodDoc.getProduces().size());
				Assert.assertEquals(MediaType.APPLICATION_XML_VALUE, apiMethodDoc.getProduces().iterator().next());
			}
		}
	}
",non-flaky,5
43022,fabiomaffioletti_jsondoc,JSONDocSpringJSONDocScannerTest.testMergeApiDoc,"	@Test
	public void testMergeApiDoc() {
		Set<Class<?>> controllers = new LinkedHashSet<Class<?>>();
		controllers.add(SpringController.class);
		Set<ApiDoc> apiDocs = jsondocScanner.getApiDocs(controllers, MethodDisplay.URI);
		
		ApiDoc apiDoc = apiDocs.iterator().next();
		Assert.assertEquals(""A spring controller"", apiDoc.getDescription());
		Assert.assertEquals(""Spring controller"", apiDoc.getName());
		
		for (ApiMethodDoc apiMethodDoc : apiDoc.getMethods()) {
			if(apiMethodDoc.getPath().contains(""/api/string/{name}"")) {
				Assert.assertNotNull(apiMethodDoc.getAuth());
				Assert.assertNotNull(apiMethodDoc.getSupportedversions());
				Assert.assertFalse(apiMethodDoc.getApierrors().isEmpty());
				
				Assert.assertEquals(""string"", apiMethodDoc.getBodyobject().getJsondocType().getOneLineText());
				Assert.assertEquals(""string"", apiMethodDoc.getResponse().getJsondocType().getOneLineText());
				Assert.assertEquals(""/api/string/{name}"", apiMethodDoc.getPath().iterator().next());
				Assert.assertEquals(""POST"", apiMethodDoc.getVerb().iterator().next().name());
				Assert.assertEquals(""application/json"", apiMethodDoc.getProduces().iterator().next());
				Assert.assertEquals(""application/json"", apiMethodDoc.getConsumes().iterator().next());
				Assert.assertEquals(""201 - Created"", apiMethodDoc.getResponsestatuscode());
				
				Set<ApiHeaderDoc> headers = apiMethodDoc.getHeaders();
				ApiHeaderDoc header = headers.iterator().next();
				Assert.assertEquals(""header"", header.getName());
				Assert.assertEquals(""test"", header.getAllowedvalues()[0]);
				
				Set<ApiParamDoc> queryparameters = apiMethodDoc.getQueryparameters();
				Assert.assertEquals(3, queryparameters.size());
				Iterator<ApiParamDoc> qpIterator = queryparameters.iterator();
				ApiParamDoc apiParamDoc = qpIterator.next();
				Assert.assertEquals(""delete"", apiParamDoc.getName());
				Assert.assertEquals(""true"", apiParamDoc.getRequired());
				Assert.assertEquals(null, apiParamDoc.getDefaultvalue());
				Assert.assertEquals(0, apiParamDoc.getAllowedvalues().length);
				apiParamDoc = qpIterator.next();
				Assert.assertEquals(""id"", apiParamDoc.getName());
				Assert.assertEquals(""true"", apiParamDoc.getRequired());
				Assert.assertTrue(apiParamDoc.getDefaultvalue().isEmpty());
				apiParamDoc = qpIterator.next();
				Assert.assertEquals(""myquery"", apiParamDoc.getName());
				Assert.assertEquals(""true"", apiParamDoc.getRequired());
				Assert.assertEquals("""", apiParamDoc.getDefaultvalue());
				
				Set<ApiParamDoc> pathparameters = apiMethodDoc.getPathparameters();
				Iterator<ApiParamDoc> ppIterator = pathparameters.iterator();
				apiParamDoc = ppIterator.next();
				apiParamDoc = apiMethodDoc.getPathparameters().iterator().next();
				Assert.assertEquals(""test"", apiParamDoc.getName());
			}
		}
		
	}
",non-flaky,5
43023,fabiomaffioletti_jsondoc,Spring3JSONDocObjectScannerTest.getJSONDoc,"    @Test
    public void getJSONDoc() throws IOException {
        JSONDocScanner jsondocScanner = new Spring3JSONDocScanner();
        JSONDoc jsondoc = jsondocScanner.getJSONDoc(version, basePath, Lists.newArrayList(""org.jsondoc.springmvc.controller""), true, MethodDisplay.URI);

        Map<String, Set<ApiObjectDoc>> objects = jsondoc.getObjects();
        for (Set<ApiObjectDoc> values : objects.values()) {
            for (ApiObjectDoc apiObjectDoc : values) {
                System.out.println(apiObjectDoc.getName());
            }
        }

    }
",non-flaky,5
43024,fabiomaffioletti_jsondoc,Spring3JSONDocObjectScannerTest.findsNestedObject,"    @Test
    public void findsNestedObject() throws Exception {
        JSONDocScanner jsondocScanner = new Spring3JSONDocScanner();
        JSONDoc jsondoc = jsondocScanner.getJSONDoc(version, basePath, Lists.newArrayList(""org.jsondoc.springmvc.controller""), true, MethodDisplay.URI);

        Map<String, Set<ApiObjectDoc>> objects = jsondoc.getObjects();
        for (Set<ApiObjectDoc> values : objects.values()) {
            assertContainsDoc(values, ""NestedObject1"");
        }
    }
",non-flaky,5
43025,fabiomaffioletti_jsondoc,Spring3JSONDocObjectScannerTest.findsDeeplyNestedObjects,"    @Test
    public void findsDeeplyNestedObjects() throws Exception {
        JSONDocScanner jsondocScanner = new Spring3JSONDocScanner();
        JSONDoc jsondoc = jsondocScanner.getJSONDoc(version, basePath, Lists.newArrayList(""org.jsondoc.springmvc.controller""), true, MethodDisplay.URI);

        Map<String, Set<ApiObjectDoc>> objects = jsondoc.getObjects();
        for (Set<ApiObjectDoc> values : objects.values()) {
            assertContainsDoc(values, ""NestedObject2"");
            assertContainsDoc(values, ""NestedObject3"");
        }
    }
",non-flaky,5
57195,apache_ozone,TestReconUtils.testGetReconDbDir,"  @Test
  public void testGetReconDbDir() throws Exception {

    String filePath = folder.getRoot().getAbsolutePath();
    OzoneConfiguration configuration = new OzoneConfiguration();
    configuration.set(""TEST_DB_DIR"", filePath);

    File file = new ReconUtils().getReconDbDir(configuration,
        ""TEST_DB_DIR"");
    Assert.assertEquals(filePath, file.getAbsolutePath());
  }
",non-flaky,5
57196,apache_ozone,TestReconUtils.testCreateTarFile,"  @Test
  public void testCreateTarFile() throws Exception {

    File tempSnapshotDir = null;
    FileInputStream fis = null;
    FileOutputStream fos = null;
    File tarFile = null;

    try {
      String testDirName = System.getProperty(""java.io.tmpdir"");
      if (!testDirName.endsWith(""/"")) {
        testDirName += ""/"";
      }
      testDirName += ""TestCreateTarFile_Dir"" + System.currentTimeMillis();
      tempSnapshotDir = new File(testDirName);
      tempSnapshotDir.mkdirs();

      File file = new File(testDirName + ""/temp1.txt"");
      OutputStreamWriter writer = new OutputStreamWriter(
          new FileOutputStream(file), UTF_8);
      writer.write(""Test data 1"");
      writer.close();

      file = new File(testDirName + ""/temp2.txt"");
      writer = new OutputStreamWriter(
          new FileOutputStream(file), UTF_8);
      writer.write(""Test data 2"");
      writer.close();

      tarFile = createTarFile(Paths.get(testDirName));
      Assert.assertNotNull(tarFile);

    } finally {
      org.apache.hadoop.io.IOUtils.closeStream(fis);
      org.apache.hadoop.io.IOUtils.closeStream(fos);
      FileUtils.deleteDirectory(tempSnapshotDir);
      FileUtils.deleteQuietly(tarFile);
    }
  }
",non-flaky,5
57197,apache_ozone,TestReconUtils.testUntarCheckpointFile,"  @Test
  public void testUntarCheckpointFile() throws Exception {

    File newDir = folder.newFolder();

    File file1 = Paths.get(newDir.getAbsolutePath(), ""file1"")
        .toFile();
    String str = ""File1 Contents"";
    BufferedWriter writer = new BufferedWriter(new OutputStreamWriter(
        new FileOutputStream(file1.getAbsoluteFile()), UTF_8));
    writer.write(str);
    writer.close();

    File file2 = Paths.get(newDir.getAbsolutePath(), ""file2"")
        .toFile();
    str = ""File2 Contents"";
    writer = new BufferedWriter(new OutputStreamWriter(
        new FileOutputStream(file2.getAbsoluteFile()), UTF_8));
    writer.write(str);
    writer.close();

    //Create test tar file.
    File tarFile = createTarFile(newDir.toPath());
    File outputDir = folder.newFolder();
    new ReconUtils().untarCheckpointFile(tarFile, outputDir.toPath());

    assertTrue(outputDir.isDirectory());
    assertTrue(outputDir.listFiles().length == 2);
  }
",non-flaky,5
57198,apache_ozone,TestReconUtils.testMakeHttpCall,"  @Test
  public void testMakeHttpCall() throws Exception {
    String url = ""http://localhost:9874/dbCheckpoint"";
    File file1 = Paths.get(folder.getRoot().getPath(), ""file1"")
        .toFile();
    BufferedWriter writer = new BufferedWriter(new OutputStreamWriter(
        new FileOutputStream(file1.getAbsoluteFile()), UTF_8));
    writer.write(""File 1 Contents"");
    writer.close();
    InputStream fileInputStream = new FileInputStream(file1);

    String contents;
    URLConnectionFactory connectionFactoryMock =
        mock(URLConnectionFactory.class);
    HttpURLConnection urlConnectionMock = mock(HttpURLConnection.class);
    when(urlConnectionMock.getInputStream()).thenReturn(fileInputStream);
    when(connectionFactoryMock.openConnection(any(URL.class), anyBoolean()))
        .thenReturn(urlConnectionMock);
    try (InputStream inputStream = new ReconUtils()
        .makeHttpCall(connectionFactoryMock, url, false).getInputStream()) {
      contents = IOUtils.toString(inputStream, Charset.defaultCharset());
    }

    assertEquals(""File 1 Contents"", contents);
  }
",non-flaky,5
57199,apache_ozone,TestReconUtils.testGetLastKnownDB,"  @Test
  public void testGetLastKnownDB() throws IOException {
    File newDir = folder.newFolder();

    File file1 = Paths.get(newDir.getAbsolutePath(), ""valid_1"")
        .toFile();
    String str = ""File1 Contents"";
    BufferedWriter writer = new BufferedWriter(new OutputStreamWriter(
        new FileOutputStream(file1.getAbsoluteFile()), UTF_8));
    writer.write(str);
    writer.close();

    File file2 = Paths.get(newDir.getAbsolutePath(), ""valid_2"")
        .toFile();
    str = ""File2 Contents"";
    writer = new BufferedWriter(new OutputStreamWriter(
        new FileOutputStream(file2.getAbsoluteFile()), UTF_8));
    writer.write(str);
    writer.close();


    File file3 = Paths.get(newDir.getAbsolutePath(), ""invalid_3"")
        .toFile();
    str = ""File3 Contents"";
    writer = new BufferedWriter(new OutputStreamWriter(
        new FileOutputStream(file3.getAbsoluteFile()), UTF_8));
    writer.write(str);
    writer.close();

    ReconUtils reconUtils = new ReconUtils();
    File latestValidFile = reconUtils.getLastKnownDB(newDir, ""valid"");
    assertTrue(latestValidFile.getName().equals(""valid_2""));
  }
",non-flaky,5
57200,apache_ozone,TestReconOmMetadataManagerImpl.testStart,"  @Test
  public void testStart() throws Exception {

    OMMetadataManager omMetadataManager = getOMMetadataManager();

    //Take checkpoint of the above OM DB.
    DBCheckpoint checkpoint = omMetadataManager.getStore()
        .getCheckpoint(true);
    File snapshotFile = new File(
        checkpoint.getCheckpointLocation().getParent() + ""/"" +
            ""om.snapshot.db_"" + System.currentTimeMillis());
    checkpoint.getCheckpointLocation().toFile().renameTo(snapshotFile);

    //Create new Recon OM Metadata manager instance.
    File reconOmDbDir = temporaryFolder.newFolder();
    OzoneConfiguration configuration = new OzoneConfiguration();
    configuration.set(OZONE_RECON_OM_SNAPSHOT_DB_DIR, reconOmDbDir
        .getAbsolutePath());
    FileUtils.copyDirectory(snapshotFile.getParentFile(), reconOmDbDir);

    ReconOMMetadataManager reconOMMetadataManager =
        new ReconOmMetadataManagerImpl(configuration, new ReconUtils());
    reconOMMetadataManager.start(configuration);

    Assert.assertNotNull(reconOMMetadataManager.getBucketTable());
    Assert.assertNotNull(reconOMMetadataManager.getVolumeTable()
        .get(""/sampleVol""));
    Assert.assertNotNull(reconOMMetadataManager.getBucketTable()
        .get(""/sampleVol/bucketOne""));
    Assert.assertNotNull(reconOMMetadataManager.getKeyTable(getBucketLayout())
        .get(""/sampleVol/bucketOne/key_one""));
    Assert.assertNotNull(reconOMMetadataManager.getKeyTable(getBucketLayout())
        .get(""/sampleVol/bucketOne/key_two""));
  }
",non-flaky,5
57201,apache_ozone,TestReconOmMetadataManagerImpl.testUpdateOmDB,"  @Test
  public void testUpdateOmDB() throws Exception {

    OMMetadataManager omMetadataManager = getOMMetadataManager();
    //Make sure OM Metadata reflects the keys that were inserted.
    Assert.assertNotNull(omMetadataManager.getKeyTable(getBucketLayout())
        .get(""/sampleVol/bucketOne/key_one""));
    Assert.assertNotNull(omMetadataManager.getKeyTable(getBucketLayout())
        .get(""/sampleVol/bucketOne/key_two""));

    //Take checkpoint of OM DB.
    DBCheckpoint checkpoint = omMetadataManager.getStore()
        .getCheckpoint(true);
    Assert.assertNotNull(checkpoint.getCheckpointLocation());

    //Create new Recon OM Metadata manager instance.
    File reconOmDbDir = temporaryFolder.newFolder();
    OzoneConfiguration configuration = new OzoneConfiguration();
    configuration.set(OZONE_RECON_OM_SNAPSHOT_DB_DIR, reconOmDbDir
        .getAbsolutePath());
    ReconOMMetadataManager reconOMMetadataManager =
        new ReconOmMetadataManagerImpl(configuration, new ReconUtils());
    reconOMMetadataManager.start(configuration);

    //Before accepting a snapshot, the metadata should have null tables.
    Assert.assertNull(reconOMMetadataManager.getBucketTable());

    //Update Recon OM DB with the OM DB checkpoint location.
    reconOMMetadataManager.updateOmDB(
        checkpoint.getCheckpointLocation().toFile());

    //Now, the tables should have been initialized.
    Assert.assertNotNull(reconOMMetadataManager.getBucketTable());

    // Check volume and bucket entries.
    Assert.assertNotNull(reconOMMetadataManager.getVolumeTable()
        .get(""/sampleVol""));
    Assert.assertNotNull(reconOMMetadataManager.getBucketTable()
        .get(""/sampleVol/bucketOne""));

    //Verify Keys inserted in OM DB are available in Recon OM DB.
    Assert.assertNotNull(reconOMMetadataManager.getKeyTable(getBucketLayout())
        .get(""/sampleVol/bucketOne/key_one""));
    Assert.assertNotNull(reconOMMetadataManager.getKeyTable(getBucketLayout())
        .get(""/sampleVol/bucketOne/key_two""));

  }
",non-flaky,5
57202,apache_ozone,TestContainerKeyMapperTask.testReprocessOMDB,"  @Test
  public void testReprocessOMDB() throws Exception{

    Map<ContainerKeyPrefix, Integer> keyPrefixesForContainer =
        reconContainerMetadataManager.getKeyPrefixesForContainer(1);
    assertTrue(keyPrefixesForContainer.isEmpty());

    keyPrefixesForContainer = reconContainerMetadataManager
        .getKeyPrefixesForContainer(2);
    assertTrue(keyPrefixesForContainer.isEmpty());

    Pipeline pipeline = getRandomPipeline();

    List<OmKeyLocationInfo> omKeyLocationInfoList = new ArrayList<>();
    BlockID blockID1 = new BlockID(1, 1);
    OmKeyLocationInfo omKeyLocationInfo1 = getOmKeyLocationInfo(blockID1,
        pipeline);

    BlockID blockID2 = new BlockID(2, 1);
    OmKeyLocationInfo omKeyLocationInfo2
        = getOmKeyLocationInfo(blockID2, pipeline);

    omKeyLocationInfoList.add(omKeyLocationInfo1);
    omKeyLocationInfoList.add(omKeyLocationInfo2);

    OmKeyLocationInfoGroup omKeyLocationInfoGroup = new
        OmKeyLocationInfoGroup(0, omKeyLocationInfoList);

    writeDataToOm(reconOMMetadataManager,
        ""key_one"",
        ""bucketOne"",
        ""sampleVol"",
        Collections.singletonList(omKeyLocationInfoGroup));

    ContainerKeyMapperTask containerKeyMapperTask =
        new ContainerKeyMapperTask(reconContainerMetadataManager);
    containerKeyMapperTask.reprocess(reconOMMetadataManager);

    keyPrefixesForContainer =
        reconContainerMetadataManager.getKeyPrefixesForContainer(1);
    assertEquals(1, keyPrefixesForContainer.size());
    String omKey = omMetadataManager.getOzoneKey(""sampleVol"",
        ""bucketOne"", ""key_one"");
    ContainerKeyPrefix containerKeyPrefix = new ContainerKeyPrefix(1,
        omKey, 0);
    assertEquals(1,
        keyPrefixesForContainer.get(containerKeyPrefix).intValue());

    keyPrefixesForContainer =
        reconContainerMetadataManager.getKeyPrefixesForContainer(2);
    assertEquals(1, keyPrefixesForContainer.size());
    containerKeyPrefix = new ContainerKeyPrefix(2, omKey,
        0);
    assertEquals(1,
        keyPrefixesForContainer.get(containerKeyPrefix).intValue());

    // Test if container key counts are updated
    assertEquals(1, reconContainerMetadataManager.getKeyCountForContainer(1L));
    assertEquals(1, reconContainerMetadataManager.getKeyCountForContainer(2L));
    assertEquals(0, reconContainerMetadataManager.getKeyCountForContainer(3L));

    // Test if container count is updated
    assertEquals(2, reconContainerMetadataManager.getCountForContainers());
  }
",non-flaky,5
57203,apache_ozone,TestContainerKeyMapperTask.testProcessOMEvents,"  @Test
  public void testProcessOMEvents() throws IOException {
    Map<ContainerKeyPrefix, Integer> keyPrefixesForContainer =
        reconContainerMetadataManager.getKeyPrefixesForContainer(1);
    assertTrue(keyPrefixesForContainer.isEmpty());

    keyPrefixesForContainer = reconContainerMetadataManager
        .getKeyPrefixesForContainer(2);
    assertTrue(keyPrefixesForContainer.isEmpty());

    Pipeline pipeline = getRandomPipeline();

    List<OmKeyLocationInfo> omKeyLocationInfoList = new ArrayList<>();
    BlockID blockID1 = new BlockID(1, 1);
    OmKeyLocationInfo omKeyLocationInfo1 = getOmKeyLocationInfo(blockID1,
        pipeline);

    BlockID blockID2 = new BlockID(2, 1);
    OmKeyLocationInfo omKeyLocationInfo2
        = getOmKeyLocationInfo(blockID2, pipeline);

    omKeyLocationInfoList.add(omKeyLocationInfo1);
    omKeyLocationInfoList.add(omKeyLocationInfo2);

    OmKeyLocationInfoGroup omKeyLocationInfoGroup = new
        OmKeyLocationInfoGroup(0, omKeyLocationInfoList);

    String bucket = ""bucketOne"";
    String volume = ""sampleVol"";
    String key = ""key_one"";
    String omKey = omMetadataManager.getOzoneKey(volume, bucket, key);
    OmKeyInfo omKeyInfo = buildOmKeyInfo(volume, bucket, key,
        omKeyLocationInfoGroup);

    OMDBUpdateEvent keyEvent1 = new OMDBUpdateEvent.
        OMUpdateEventBuilder<String, OmKeyInfo>()
        .setKey(omKey)
        .setValue(omKeyInfo)
        .setTable(omMetadataManager.getKeyTable(getBucketLayout()).getName())
        .setAction(OMDBUpdateEvent.OMDBUpdateAction.PUT)
        .build();

    BlockID blockID3 = new BlockID(1, 2);
    OmKeyLocationInfo omKeyLocationInfo3 =
        getOmKeyLocationInfo(blockID3, pipeline);

    BlockID blockID4 = new BlockID(3, 1);
    OmKeyLocationInfo omKeyLocationInfo4
        = getOmKeyLocationInfo(blockID4, pipeline);

    omKeyLocationInfoList = new ArrayList<>();
    omKeyLocationInfoList.add(omKeyLocationInfo3);
    omKeyLocationInfoList.add(omKeyLocationInfo4);
    omKeyLocationInfoGroup = new OmKeyLocationInfoGroup(0,
        omKeyLocationInfoList);

    String key2 = ""key_two"";
    writeDataToOm(reconOMMetadataManager, key2, bucket, volume, Collections
        .singletonList(omKeyLocationInfoGroup));

    omKey = omMetadataManager.getOzoneKey(volume, bucket, key2);
    OMDBUpdateEvent keyEvent2 = new OMDBUpdateEvent.
        OMUpdateEventBuilder<String, OmKeyInfo>()
        .setKey(omKey)
        .setAction(OMDBUpdateEvent.OMDBUpdateAction.DELETE)
        .setTable(omMetadataManager.getKeyTable(getBucketLayout()).getName())
        .build();

    OMUpdateEventBatch omUpdateEventBatch = new OMUpdateEventBatch(new
        ArrayList<OMDBUpdateEvent>() {{
          add(keyEvent1);
          add(keyEvent2);
        }});

    ContainerKeyMapperTask containerKeyMapperTask =
        new ContainerKeyMapperTask(reconContainerMetadataManager);
    containerKeyMapperTask.reprocess(reconOMMetadataManager);

    keyPrefixesForContainer = reconContainerMetadataManager
        .getKeyPrefixesForContainer(1);
    assertEquals(1, keyPrefixesForContainer.size());

    keyPrefixesForContainer = reconContainerMetadataManager
        .getKeyPrefixesForContainer(2);
    assertTrue(keyPrefixesForContainer.isEmpty());

    keyPrefixesForContainer = reconContainerMetadataManager
        .getKeyPrefixesForContainer(3);
    assertEquals(1, keyPrefixesForContainer.size());

    assertEquals(1, reconContainerMetadataManager.getKeyCountForContainer(1L));
    assertEquals(0, reconContainerMetadataManager.getKeyCountForContainer(2L));
    assertEquals(1, reconContainerMetadataManager.getKeyCountForContainer(3L));

    // Process PUT & DELETE event.
    containerKeyMapperTask.process(omUpdateEventBatch);

    keyPrefixesForContainer = reconContainerMetadataManager
        .getKeyPrefixesForContainer(1);
    assertEquals(1, keyPrefixesForContainer.size());

    keyPrefixesForContainer = reconContainerMetadataManager
        .getKeyPrefixesForContainer(2);
    assertEquals(1, keyPrefixesForContainer.size());

    keyPrefixesForContainer = reconContainerMetadataManager
        .getKeyPrefixesForContainer(3);
    assertTrue(keyPrefixesForContainer.isEmpty());

    assertEquals(1, reconContainerMetadataManager.getKeyCountForContainer(1L));
    assertEquals(1, reconContainerMetadataManager.getKeyCountForContainer(2L));
    assertEquals(0, reconContainerMetadataManager.getKeyCountForContainer(3L));

    // Test if container count is updated
    assertEquals(3, reconContainerMetadataManager.getCountForContainers());
  }
",non-flaky,5
57204,apache_ozone,TestNSSummaryTask.testReprocess,"  @Test
  public void testReprocess() throws Exception {
    NSSummary nonExistentSummary =
            reconNamespaceSummaryManager.getNSSummary(BUCKET_ONE_OBJECT_ID);
    Assert.assertNull(nonExistentSummary);

    populateOMDB();

    // write a NSSummary prior to reprocess and verify it got cleaned up after.
    NSSummary staleNSSummary = new NSSummary();
    reconNamespaceSummaryManager.storeNSSummary(-1L, staleNSSummary);
    NSSummaryTask nsSummaryTask = new NSSummaryTask(
            reconNamespaceSummaryManager);
    nsSummaryTask.reprocess(reconOMMetadataManager);

    Assert.assertNull(reconNamespaceSummaryManager.getNSSummary(-1L));
    NSSummary nsSummaryForBucket1 =
            reconNamespaceSummaryManager.getNSSummary(BUCKET_ONE_OBJECT_ID);
    NSSummary nsSummaryForBucket2 =
            reconNamespaceSummaryManager.getNSSummary(BUCKET_TWO_OBJECT_ID);
    Assert.assertNotNull(nsSummaryForBucket1);
    Assert.assertNotNull(nsSummaryForBucket2);

    Assert.assertEquals(1, nsSummaryForBucket1.getNumOfFiles());
    Assert.assertEquals(2, nsSummaryForBucket2.getNumOfFiles());

    Assert.assertEquals(KEY_ONE_SIZE, nsSummaryForBucket1.getSizeOfFiles());
    Assert.assertEquals(KEY_TWO_OLD_SIZE + KEY_FOUR_SIZE,
            nsSummaryForBucket2.getSizeOfFiles());

    int[] fileDistBucket1 = nsSummaryForBucket1.getFileSizeBucket();
    int[] fileDistBucket2 = nsSummaryForBucket2.getFileSizeBucket();
    Assert.assertEquals(ReconConstants.NUM_OF_BINS, fileDistBucket1.length);
    Assert.assertEquals(ReconConstants.NUM_OF_BINS, fileDistBucket2.length);

    Assert.assertEquals(1, fileDistBucket1[0]);
    for (int i = 1; i < ReconConstants.NUM_OF_BINS; ++i) {
      Assert.assertEquals(0, fileDistBucket1[i]);
    }
    Assert.assertEquals(1, fileDistBucket2[1]);
    Assert.assertEquals(1, fileDistBucket2[2]);
    for (int i = 0; i < ReconConstants.NUM_OF_BINS; ++i) {
      if (i == 1 || i == 2) {
        continue;
      }
      Assert.assertEquals(0, fileDistBucket2[i]);
    }

    // Bucket one has one dir, bucket two has none.
    Set<Long> childDirBucketOne = nsSummaryForBucket1.getChildDir();
    Set<Long> childDirBucketTwo = nsSummaryForBucket2.getChildDir();
    Assert.assertEquals(1, childDirBucketOne.size());
    bucketOneAns.clear();
    bucketOneAns.add(DIR_ONE_OBJECT_ID);
    Assert.assertEquals(bucketOneAns, childDirBucketOne);
    Assert.assertEquals(0, childDirBucketTwo.size());

    // Dir 1 has two dir: dir2 and dir3.
    NSSummary nsSummaryInDir1 = reconNamespaceSummaryManager
            .getNSSummary(DIR_ONE_OBJECT_ID);
    Assert.assertNotNull(nsSummaryInDir1);
    Set<Long> childDirForDirOne = nsSummaryInDir1.getChildDir();
    Assert.assertEquals(2, childDirForDirOne.size());
    dirOneAns.clear();
    dirOneAns.add(DIR_TWO_OBJECT_ID);
    dirOneAns.add(DIR_THREE_OBJECT_ID);
    Assert.assertEquals(dirOneAns, childDirForDirOne);

    NSSummary nsSummaryInDir2 = reconNamespaceSummaryManager
            .getNSSummary(DIR_TWO_OBJECT_ID);
    Assert.assertEquals(1, nsSummaryInDir2.getNumOfFiles());
    Assert.assertEquals(KEY_THREE_SIZE, nsSummaryInDir2.getSizeOfFiles());

    int[] fileDistForDir2 = nsSummaryInDir2.getFileSizeBucket();
    Assert.assertEquals(ReconConstants.NUM_OF_BINS, fileDistForDir2.length);
    Assert.assertEquals(1, fileDistForDir2[fileDistForDir2.length - 1]);
    for (int i = 0; i < ReconConstants.NUM_OF_BINS - 1; ++i) {
      Assert.assertEquals(0, fileDistForDir2[i]);
    }
    Assert.assertEquals(0, nsSummaryInDir2.getChildDir().size());

    // bucket should have empty dirName
    Assert.assertEquals(0, nsSummaryForBucket1.getDirName().length());
    Assert.assertEquals(0, nsSummaryForBucket2.getDirName().length());
    // check dirName is correctly written
    Assert.assertEquals(DIR_ONE, nsSummaryInDir1.getDirName());
    Assert.assertEquals(DIR_TWO, nsSummaryInDir2.getDirName());
  }
",non-flaky,5
57205,apache_ozone,TestNSSummaryTask.testProcess,"  @Test
  public void testProcess() throws Exception {
    NSSummary nonExistentSummary =
            reconNamespaceSummaryManager.getNSSummary(BUCKET_ONE_OBJECT_ID);
    Assert.assertNull(nonExistentSummary);

    populateOMDB();

    // Events for keyTable change:
    // put file5 under bucket 2
    String omPutKey = BUCKET_TWO_OBJECT_ID + OM_KEY_PREFIX + FILE_FIVE;
    OmKeyInfo omPutKeyInfo = buildOmKeyInfo(VOL, BUCKET_TWO, KEY_FIVE,
            FILE_FIVE, KEY_FIVE_OBJECT_ID, BUCKET_TWO_OBJECT_ID, KEY_FIVE_SIZE);
    OMDBUpdateEvent keyEvent1 = new OMDBUpdateEvent.
            OMUpdateEventBuilder<String, OmKeyInfo>()
            .setKey(omPutKey)
            .setValue(omPutKeyInfo)
            .setTable(omMetadataManager.getKeyTable(getBucketLayout())
            .getName())
            .setAction(OMDBUpdateEvent.OMDBUpdateAction.PUT)
            .build();

    // delete file 1 under bucket 1
    String omDeleteKey = BUCKET_ONE_OBJECT_ID + OM_KEY_PREFIX + FILE_ONE;
    OmKeyInfo omDeleteInfo = buildOmKeyInfo(VOL, BUCKET_ONE, KEY_ONE, FILE_ONE,
            KEY_ONE_OBJECT_ID, BUCKET_ONE_OBJECT_ID);
    OMDBUpdateEvent keyEvent2 = new OMDBUpdateEvent.
            OMUpdateEventBuilder<String, OmKeyInfo>()
            .setKey(omDeleteKey)
            .setValue(omDeleteInfo)
            .setTable(omMetadataManager.getKeyTable(getBucketLayout())
            .getName())
            .setAction(OMDBUpdateEvent.OMDBUpdateAction.DELETE)
            .build();

    // update file 2's size under bucket 2
    String omUpdateKey = BUCKET_TWO_OBJECT_ID + OM_KEY_PREFIX + FILE_TWO;
    OmKeyInfo omOldInfo = buildOmKeyInfo(VOL, BUCKET_TWO, KEY_TWO, FILE_TWO,
            KEY_TWO_OBJECT_ID, BUCKET_TWO_OBJECT_ID, KEY_TWO_OLD_SIZE);
    OmKeyInfo omUpdateInfo = buildOmKeyInfo(VOL, BUCKET_TWO, KEY_TWO, FILE_TWO,
            KEY_TWO_OBJECT_ID, BUCKET_TWO_OBJECT_ID, KEY_TWO_UPDATE_SIZE);
    OMDBUpdateEvent keyEvent3 = new OMDBUpdateEvent.
            OMUpdateEventBuilder<String, OmKeyInfo>()
            .setKey(omUpdateKey)
            .setValue(omUpdateInfo)
            .setOldValue(omOldInfo)
            .setTable(omMetadataManager.getKeyTable(getBucketLayout())
            .getName())
            .setAction(OMDBUpdateEvent.OMDBUpdateAction.UPDATE)
            .build();

    // Events for DirectoryTable change:
    // add dir 4 under bucket 1
    String omDirPutKey1 = BUCKET_ONE_OBJECT_ID + OM_KEY_PREFIX + DIR_FOUR;
    OmDirectoryInfo omDirPutValue1 = buildOmDirInfo(DIR_FOUR,
            DIR_FOUR_OBJECT_ID, BUCKET_ONE_OBJECT_ID);
    OMDBUpdateEvent keyEvent4 = new OMDBUpdateEvent.
            OMUpdateEventBuilder<String, OmDirectoryInfo>()
            .setKey(omDirPutKey1)
            .setValue(omDirPutValue1)
            .setAction(OMDBUpdateEvent.OMDBUpdateAction.PUT)
            .setTable(omMetadataManager.getDirectoryTable().getName())
            .build();

    // add dir 5 under bucket 2
    String omDirPutKey2 = BUCKET_TWO_OBJECT_ID + OM_KEY_PREFIX + DIR_FIVE;
    OmDirectoryInfo omDirPutValue2 = buildOmDirInfo(DIR_FIVE,
            DIR_FIVE_OBJECT_ID, BUCKET_TWO_OBJECT_ID);
    OMDBUpdateEvent keyEvent5 = new OMDBUpdateEvent.
            OMUpdateEventBuilder<String, OmDirectoryInfo>()
            .setKey(omDirPutKey2)
            .setValue(omDirPutValue2)
            .setAction(OMDBUpdateEvent.OMDBUpdateAction.PUT)
            .setTable(omMetadataManager.getDirectoryTable().getName())
            .build();

    // delete dir 3 under dir 1
    String omDirDeleteKey = DIR_ONE_OBJECT_ID + OM_KEY_PREFIX + DIR_THREE;
    OmDirectoryInfo omDirDeleteValue = buildOmDirInfo(DIR_FIVE,
            DIR_THREE_OBJECT_ID, DIR_ONE_OBJECT_ID);
    OMDBUpdateEvent keyEvent6 = new OMDBUpdateEvent.
            OMUpdateEventBuilder<String, OmDirectoryInfo>()
            .setKey(omDirDeleteKey)
            .setValue(omDirDeleteValue)
            .setAction(OMDBUpdateEvent.OMDBUpdateAction.DELETE)
            .setTable(omMetadataManager.getDirectoryTable().getName())
            .build();

    // rename dir1
    String omDirUpdateKey = BUCKET_ONE_OBJECT_ID + OM_KEY_PREFIX + DIR_ONE;
    OmDirectoryInfo omDirOldValue = buildOmDirInfo(DIR_ONE,
            DIR_ONE_OBJECT_ID, BUCKET_ONE_OBJECT_ID);
    OmDirectoryInfo omDirUpdateValue = buildOmDirInfo(DIR_ONE_RENAME,
            DIR_ONE_OBJECT_ID, BUCKET_ONE_OBJECT_ID);
    OMDBUpdateEvent keyEvent7 = new OMDBUpdateEvent.
            OMUpdateEventBuilder<String, OmDirectoryInfo>()
            .setKey(omDirUpdateKey)
            .setValue(omDirUpdateValue)
            .setOldValue(omDirOldValue)
            .setAction(OMDBUpdateEvent.OMDBUpdateAction.UPDATE)
            .setTable(omMetadataManager.getDirectoryTable().getName())
            .build();

    OMUpdateEventBatch omUpdateEventBatch = new OMUpdateEventBatch(
            new ArrayList<OMDBUpdateEvent>() {{
              add(keyEvent1);
              add(keyEvent2);
              add(keyEvent3);
              add(keyEvent4);
              add(keyEvent5);
              add(keyEvent6);
              add(keyEvent7);
          }});

    NSSummaryTask nsSummaryTask = new NSSummaryTask(
            reconNamespaceSummaryManager);
    nsSummaryTask.reprocess(reconOMMetadataManager);
    nsSummaryTask.process(omUpdateEventBatch);

    // file 5 is added under bucket 2, so bucket 2 has 3 keys now
    // file 1 is gone, so bucket 1 is empty now
    // file 2 is updated with new datasize,
    // so file size dist for bucket 2 should be updated
    NSSummary nsSummaryForBucket1 =
            reconNamespaceSummaryManager.getNSSummary(BUCKET_ONE_OBJECT_ID);
    Assert.assertNotNull(nsSummaryForBucket1);
    Assert.assertEquals(0, nsSummaryForBucket1.getNumOfFiles());

    Set<Long> childDirBucket1 = nsSummaryForBucket1.getChildDir();
    // after put dir4, bucket1 now has two child dirs: dir1 and dir4
    Assert.assertEquals(2, childDirBucket1.size());
    bucketOneAns.clear();
    bucketOneAns.add(DIR_ONE_OBJECT_ID);
    bucketOneAns.add(DIR_FOUR_OBJECT_ID);
    Assert.assertEquals(bucketOneAns, childDirBucket1);

    NSSummary nsSummaryForBucket2 =
            reconNamespaceSummaryManager.getNSSummary(BUCKET_TWO_OBJECT_ID);
    Assert.assertNotNull(nsSummaryForBucket2);
    Assert.assertEquals(3, nsSummaryForBucket2.getNumOfFiles());
    // key 4 + key 5 + updated key 2
    Assert.assertEquals(KEY_FOUR_SIZE + KEY_FIVE_SIZE + KEY_TWO_UPDATE_SIZE,
            nsSummaryForBucket2.getSizeOfFiles());

    int[] fileSizeDist = nsSummaryForBucket2.getFileSizeBucket();
    Assert.assertEquals(ReconConstants.NUM_OF_BINS, fileSizeDist.length);
    // 1023L and 100L
    Assert.assertEquals(2, fileSizeDist[0]);
    // 2050L
    Assert.assertEquals(1, fileSizeDist[2]);
    for (int i = 0; i < ReconConstants.NUM_OF_BINS; ++i) {
      if (i == 0 || i == 2) {
        continue;
      }
      Assert.assertEquals(0, fileSizeDist[i]);
    }

    // after put dir5, bucket 2 now has one dir
    Set<Long> childDirBucket2 = nsSummaryForBucket2.getChildDir();
    Assert.assertEquals(1, childDirBucket2.size());
    bucketTwoAns.add(DIR_FIVE_OBJECT_ID);
    Assert.assertEquals(bucketTwoAns, childDirBucket2);

    // after delete dir 3, dir 1 now has only one dir: dir2
    NSSummary nsSummaryForDir1 = reconNamespaceSummaryManager
            .getNSSummary(DIR_ONE_OBJECT_ID);
    Assert.assertNotNull(nsSummaryForDir1);
    Set<Long> childDirForDir1 = nsSummaryForDir1.getChildDir();
    Assert.assertEquals(1, childDirForDir1.size());
    dirOneAns.clear();
    dirOneAns.add(DIR_TWO_OBJECT_ID);
    Assert.assertEquals(dirOneAns, childDirForDir1);

    // after renaming dir1, check its new name
    Assert.assertEquals(DIR_ONE_RENAME, nsSummaryForDir1.getDirName());
  }
",non-flaky,5
57206,apache_ozone,TestFileSizeCountTask.testReprocess,"  @Test
  public void testReprocess() throws IOException {
    OmKeyInfo omKeyInfo1 = mock(OmKeyInfo.class);
    given(omKeyInfo1.getKeyName()).willReturn(""key1"");
    given(omKeyInfo1.getVolumeName()).willReturn(""vol1"");
    given(omKeyInfo1.getBucketName()).willReturn(""bucket1"");
    given(omKeyInfo1.getDataSize()).willReturn(1000L);

    OmKeyInfo omKeyInfo2 = mock(OmKeyInfo.class);
    given(omKeyInfo2.getKeyName()).willReturn(""key2"");
    given(omKeyInfo2.getVolumeName()).willReturn(""vol1"");
    given(omKeyInfo2.getBucketName()).willReturn(""bucket1"");
    given(omKeyInfo2.getDataSize()).willReturn(100000L);

    OmKeyInfo omKeyInfo3 = mock(OmKeyInfo.class);
    given(omKeyInfo3.getKeyName()).willReturn(""key3"");
    given(omKeyInfo3.getVolumeName()).willReturn(""vol1"");
    given(omKeyInfo3.getBucketName()).willReturn(""bucket1"");
    given(omKeyInfo3.getDataSize()).willReturn(1125899906842624L * 4); // 4PB

    OMMetadataManager omMetadataManager = mock(OmMetadataManagerImpl.class);
    TypedTable<String, OmKeyInfo> keyTable = mock(TypedTable.class);

    TypedTable.TypedTableIterator mockKeyIter = mock(TypedTable
        .TypedTableIterator.class);
    TypedTable.TypedKeyValue mockKeyValue = mock(
        TypedTable.TypedKeyValue.class);

    when(keyTable.iterator()).thenReturn(mockKeyIter);
    when(omMetadataManager.getKeyTable(getBucketLayout())).thenReturn(keyTable);
    when(mockKeyIter.hasNext())
        .thenReturn(true)
        .thenReturn(true)
        .thenReturn(true)
        .thenReturn(false);
    when(mockKeyIter.next()).thenReturn(mockKeyValue);
    when(mockKeyValue.getValue())
        .thenReturn(omKeyInfo1)
        .thenReturn(omKeyInfo2)
        .thenReturn(omKeyInfo3);

    // Reprocess could be called from table having existing entries. Adding
    // an entry to simulate that.
    fileCountBySizeDao.insert(
        new FileCountBySize(""vol1"", ""bucket1"", 1024L, 10L));

    Pair<String, Boolean> result =
        fileSizeCountTask.reprocess(omMetadataManager);
    assertTrue(result.getRight());

    assertEquals(3, fileCountBySizeDao.count());
    Record3<String, String, Long> recordToFind = dslContext
        .newRecord(FILE_COUNT_BY_SIZE.VOLUME,
        FILE_COUNT_BY_SIZE.BUCKET,
        FILE_COUNT_BY_SIZE.FILE_SIZE)
        .value1(""vol1"")
        .value2(""bucket1"")
        .value3(1024L);
    assertEquals(1L,
        fileCountBySizeDao.findById(recordToFind).getCount().longValue());
    // file size upper bound for 100000L is 131072L (next highest power of 2)
    recordToFind.value3(131072L);
    assertEquals(1L,
        fileCountBySizeDao.findById(recordToFind).getCount().longValue());
    // file size upper bound for 4PB is Long.MAX_VALUE
    recordToFind.value3(Long.MAX_VALUE);
    assertEquals(1L,
        fileCountBySizeDao.findById(recordToFind).getCount().longValue());
  }
",non-flaky,5
57207,apache_ozone,TestFileSizeCountTask.testProcess,"  @Test
  public void testProcess() {
    // Write 2 keys.
    OmKeyInfo toBeDeletedKey = mock(OmKeyInfo.class);
    given(toBeDeletedKey.getVolumeName()).willReturn(""vol1"");
    given(toBeDeletedKey.getBucketName()).willReturn(""bucket1"");
    given(toBeDeletedKey.getKeyName()).willReturn(""deletedKey"");
    given(toBeDeletedKey.getDataSize()).willReturn(2000L); // Bin 1
    OMDBUpdateEvent event = new OMUpdateEventBuilder()
        .setAction(PUT)
        .setKey(""deletedKey"")
        .setValue(toBeDeletedKey)
        .setTable(OmMetadataManagerImpl.KEY_TABLE)
        .build();

    OmKeyInfo toBeUpdatedKey = mock(OmKeyInfo.class);
    given(toBeUpdatedKey.getVolumeName()).willReturn(""vol1"");
    given(toBeUpdatedKey.getBucketName()).willReturn(""bucket1"");
    given(toBeUpdatedKey.getKeyName()).willReturn(""updatedKey"");
    given(toBeUpdatedKey.getDataSize()).willReturn(10000L); // Bin 4
    OMDBUpdateEvent event2 = new OMUpdateEventBuilder()
        .setAction(PUT)
        .setKey(""updatedKey"")
        .setValue(toBeUpdatedKey)
        .setTable(OmMetadataManagerImpl.KEY_TABLE)
        .build();

    OMUpdateEventBatch omUpdateEventBatch =
        new OMUpdateEventBatch(Arrays.asList(event, event2));
    fileSizeCountTask.process(omUpdateEventBatch);

    // Verify 2 keys are in correct bins.
    assertEquals(2, fileCountBySizeDao.count());
    Record3<String, String, Long> recordToFind = dslContext
        .newRecord(FILE_COUNT_BY_SIZE.VOLUME,
            FILE_COUNT_BY_SIZE.BUCKET,
            FILE_COUNT_BY_SIZE.FILE_SIZE)
        .value1(""vol1"")
        .value2(""bucket1"")
        .value3(2048L);
    assertEquals(1L,
        fileCountBySizeDao.findById(recordToFind).getCount().longValue());
    // file size upper bound for 10000L is 16384L (next highest power of 2)
    recordToFind.value3(16384L);
    assertEquals(1L,
        fileCountBySizeDao.findById(recordToFind).getCount().longValue());

    // Add new key.
    OmKeyInfo newKey = mock(OmKeyInfo.class);
    given(newKey.getVolumeName()).willReturn(""vol1"");
    given(newKey.getBucketName()).willReturn(""bucket1"");
    given(newKey.getKeyName()).willReturn(""newKey"");
    given(newKey.getDataSize()).willReturn(1000L); // Bin 0
    OMDBUpdateEvent putEvent = new OMUpdateEventBuilder()
        .setAction(PUT)
        .setKey(""newKey"")
        .setValue(newKey)
        .setTable(OmMetadataManagerImpl.KEY_TABLE)
        .build();

    // Update existing key.
    OmKeyInfo updatedKey = mock(OmKeyInfo.class);
    given(updatedKey.getVolumeName()).willReturn(""vol1"");
    given(updatedKey.getBucketName()).willReturn(""bucket1"");
    given(updatedKey.getKeyName()).willReturn(""updatedKey"");
    given(updatedKey.getDataSize()).willReturn(50000L); // Bin 6
    OMDBUpdateEvent updateEvent = new OMUpdateEventBuilder()
        .setAction(UPDATE)
        .setKey(""updatedKey"")
        .setValue(updatedKey)
        .setOldValue(toBeUpdatedKey)
        .setTable(OmMetadataManagerImpl.KEY_TABLE)
        .build();

    // Delete another existing key.
    OMDBUpdateEvent deleteEvent = new OMUpdateEventBuilder()
        .setAction(DELETE)
        .setKey(""deletedKey"")
        .setValue(toBeDeletedKey)
        .setTable(OmMetadataManagerImpl.KEY_TABLE)
        .build();

    omUpdateEventBatch = new OMUpdateEventBatch(
        Arrays.asList(updateEvent, putEvent, deleteEvent));
    fileSizeCountTask.process(omUpdateEventBatch);

    assertEquals(4, fileCountBySizeDao.count());
    recordToFind.value3(1024L);
    assertEquals(1, fileCountBySizeDao.findById(recordToFind)
        .getCount().longValue());
    recordToFind.value3(2048L);
    assertEquals(0, fileCountBySizeDao.findById(recordToFind)
        .getCount().longValue());
    recordToFind.value3(16384L);
    assertEquals(0, fileCountBySizeDao.findById(recordToFind)
        .getCount().longValue());
    recordToFind.value3(65536L);
    assertEquals(1, fileCountBySizeDao.findById(recordToFind)
        .getCount().longValue());
  }
",non-flaky,5
57208,apache_ozone,TestFileSizeCountTask.testReprocessAtScale,"  @Test
  public void testReprocessAtScale() throws IOException {
    // generate mocks for 2 volumes, 500 buckets each volume
    // and 42 keys in each bucket.
    List<OmKeyInfo> omKeyInfoList = new ArrayList<>();
    List<Boolean> hasNextAnswer = new ArrayList<>();
    for (int volIndex = 1; volIndex <= 2; volIndex++) {
      for (int bktIndex = 1; bktIndex <= 500; bktIndex++) {
        for (int keyIndex = 1; keyIndex <= 42; keyIndex++) {
          OmKeyInfo omKeyInfo = mock(OmKeyInfo.class);
          given(omKeyInfo.getKeyName()).willReturn(""key"" + keyIndex);
          given(omKeyInfo.getVolumeName()).willReturn(""vol"" + volIndex);
          given(omKeyInfo.getBucketName()).willReturn(""bucket"" + bktIndex);
          // Place keys in each bin
          long fileSize = (long)Math.pow(2, keyIndex + 9) - 1L;
          given(omKeyInfo.getDataSize()).willReturn(fileSize);
          omKeyInfoList.add(omKeyInfo);
          hasNextAnswer.add(true);
        }
      }
    }
    hasNextAnswer.add(false);

    OMMetadataManager omMetadataManager = mock(OmMetadataManagerImpl.class);
    TypedTable<String, OmKeyInfo> keyTable = mock(TypedTable.class);

    TypedTable.TypedTableIterator mockKeyIter = mock(TypedTable
        .TypedTableIterator.class);
    TypedTable.TypedKeyValue mockKeyValue = mock(
        TypedTable.TypedKeyValue.class);

    when(keyTable.iterator()).thenReturn(mockKeyIter);
    when(omMetadataManager.getKeyTable(getBucketLayout())).thenReturn(keyTable);
    when(mockKeyIter.hasNext())
        .thenAnswer(AdditionalAnswers.returnsElementsOf(hasNextAnswer));
    when(mockKeyIter.next()).thenReturn(mockKeyValue);
    when(mockKeyValue.getValue())
        .thenAnswer(AdditionalAnswers.returnsElementsOf(omKeyInfoList));

    Pair<String, Boolean> result =
        fileSizeCountTask.reprocess(omMetadataManager);
    assertTrue(result.getRight());

    // 2 volumes * 500 buckets * 42 bins = 42000 rows
    assertEquals(42000, fileCountBySizeDao.count());
    Record3<String, String, Long> recordToFind = dslContext
        .newRecord(FILE_COUNT_BY_SIZE.VOLUME,
            FILE_COUNT_BY_SIZE.BUCKET,
            FILE_COUNT_BY_SIZE.FILE_SIZE)
        .value1(""vol1"")
        .value2(""bucket1"")
        .value3(1024L);
    assertEquals(1L,
        fileCountBySizeDao.findById(recordToFind).getCount().longValue());
    // file size upper bound for 100000L is 131072L (next highest power of 2)
    recordToFind.value1(""vol1"");
    recordToFind.value3(131072L);
    assertEquals(1L,
        fileCountBySizeDao.findById(recordToFind).getCount().longValue());
    recordToFind.value2(""bucket500"");
    recordToFind.value3(Long.MAX_VALUE);
    assertEquals(1L,
        fileCountBySizeDao.findById(recordToFind).getCount().longValue());
  }
",non-flaky,5
57209,apache_ozone,TestFileSizeCountTask.testProcessAtScale,"  @Test
  public void testProcessAtScale() {
    // Write 10000 keys.
    List<OMDBUpdateEvent> omDbEventList = new ArrayList<>();
    List<OmKeyInfo> omKeyInfoList = new ArrayList<>();
    for (int volIndex = 1; volIndex <= 10; volIndex++) {
      for (int bktIndex = 1; bktIndex <= 100; bktIndex++) {
        for (int keyIndex = 1; keyIndex <= 10; keyIndex++) {
          OmKeyInfo omKeyInfo = mock(OmKeyInfo.class);
          given(omKeyInfo.getKeyName()).willReturn(""key"" + keyIndex);
          given(omKeyInfo.getVolumeName()).willReturn(""vol"" + volIndex);
          given(omKeyInfo.getBucketName()).willReturn(""bucket"" + bktIndex);
          // Place keys in each bin
          long fileSize = (long)Math.pow(2, keyIndex + 9) - 1L;
          given(omKeyInfo.getDataSize()).willReturn(fileSize);
          omKeyInfoList.add(omKeyInfo);
          omDbEventList.add(new OMUpdateEventBuilder()
              .setAction(PUT)
              .setKey(""key"" + keyIndex)
              .setValue(omKeyInfo)
              .setTable(OmMetadataManagerImpl.KEY_TABLE)
              .build());
        }
      }
    }

    OMUpdateEventBatch omUpdateEventBatch =
        new OMUpdateEventBatch(omDbEventList);
    fileSizeCountTask.process(omUpdateEventBatch);

    // Verify 2 keys are in correct bins.
    assertEquals(10000, fileCountBySizeDao.count());
    Record3<String, String, Long> recordToFind = dslContext
        .newRecord(FILE_COUNT_BY_SIZE.VOLUME,
            FILE_COUNT_BY_SIZE.BUCKET,
            FILE_COUNT_BY_SIZE.FILE_SIZE)
        .value1(""vol1"")
        .value2(""bucket1"")
        .value3(2048L);
    assertEquals(1L,
        fileCountBySizeDao.findById(recordToFind).getCount().longValue());
    recordToFind.value1(""vol10"");
    recordToFind.value2(""bucket100"");
    // file size upper bound for 10000L is 16384L (next highest power of 2)
    recordToFind.value3(16384L);
    assertEquals(1L,
        fileCountBySizeDao.findById(recordToFind).getCount().longValue());

    // Process 500 deletes and 500 updates
    omDbEventList = new ArrayList<>();
    for (int volIndex = 1; volIndex <= 1; volIndex++) {
      for (int bktIndex = 1; bktIndex <= 100; bktIndex++) {
        for (int keyIndex = 1; keyIndex <= 10; keyIndex++) {
          OmKeyInfo omKeyInfo = mock(OmKeyInfo.class);
          given(omKeyInfo.getKeyName()).willReturn(""key"" + keyIndex);
          given(omKeyInfo.getVolumeName()).willReturn(""vol"" + volIndex);
          given(omKeyInfo.getBucketName()).willReturn(""bucket"" + bktIndex);
          if (keyIndex <= 5) {
            long fileSize = (long)Math.pow(2, keyIndex + 9) - 1L;
            given(omKeyInfo.getDataSize()).willReturn(fileSize);
            omDbEventList.add(new OMUpdateEventBuilder()
                .setAction(DELETE)
                .setKey(""key"" + keyIndex)
                .setValue(omKeyInfo)
                .setTable(OmMetadataManagerImpl.KEY_TABLE)
                .build());
          } else {
            // update all the files with keyIndex > 5 to filesize 1023L
            // so that they get into first bin
            given(omKeyInfo.getDataSize()).willReturn(1023L);
            omDbEventList.add(new OMUpdateEventBuilder()
                .setAction(UPDATE)
                .setKey(""key"" + keyIndex)
                .setValue(omKeyInfo)
                .setTable(OmMetadataManagerImpl.KEY_TABLE)
                .setOldValue(
                    omKeyInfoList.get((volIndex * bktIndex) + keyIndex))
                .build());
          }
        }
      }
    }

    omUpdateEventBatch = new OMUpdateEventBatch(omDbEventList);
    fileSizeCountTask.process(omUpdateEventBatch);

    assertEquals(10000, fileCountBySizeDao.count());
    recordToFind = dslContext
        .newRecord(FILE_COUNT_BY_SIZE.VOLUME,
            FILE_COUNT_BY_SIZE.BUCKET,
            FILE_COUNT_BY_SIZE.FILE_SIZE)
        .value1(""vol1"")
        .value2(""bucket1"")
        .value3(1024L);
    // The update events on keys 6-10 should now put them under first bin 1024L
    assertEquals(5, fileCountBySizeDao.findById(recordToFind)
        .getCount().longValue());
    recordToFind.value2(""bucket100"");
    assertEquals(5, fileCountBySizeDao.findById(recordToFind)
        .getCount().longValue());
    recordToFind.value3(2048L);
    assertEquals(0, fileCountBySizeDao.findById(recordToFind)
        .getCount().longValue());
    // Volumes 2 - 10 should not be affected by this process
    recordToFind.value1(""vol2"");
    assertEquals(1, fileCountBySizeDao.findById(recordToFind)
        .getCount().longValue());
  }
",non-flaky,5
57210,apache_ozone,TestReconTaskControllerImpl.testRegisterTask,"  @Test
  public void testRegisterTask() {
    String taskName = ""Dummy_"" + System.currentTimeMillis();
    DummyReconDBTask dummyReconDBTask =
        new DummyReconDBTask(taskName, DummyReconDBTask.TaskType.ALWAYS_PASS);
    reconTaskController.registerTask(dummyReconDBTask);
    assertTrue(reconTaskController.getRegisteredTasks().size() == 1);
    assertTrue(reconTaskController.getRegisteredTasks()
        .get(dummyReconDBTask.getTaskName()) == dummyReconDBTask);
  }
",non-flaky,5
57211,apache_ozone,TestReconTaskControllerImpl.testConsumeOMEvents,"  @Test
  public void testConsumeOMEvents() throws Exception {
    ReconOmTask reconOmTaskMock = getMockTask(""MockTask"");
    when(reconOmTaskMock.process(any(OMUpdateEventBatch.class)))
        .thenReturn(new ImmutablePair<>(""MockTask"", true));
    reconTaskController.registerTask(reconOmTaskMock);
    OMUpdateEventBatch omUpdateEventBatchMock = mock(OMUpdateEventBatch.class);
    when(omUpdateEventBatchMock.getLastSequenceNumber()).thenReturn(100L);
    when(omUpdateEventBatchMock.isEmpty()).thenReturn(false);

    long startTime = System.currentTimeMillis();
    reconTaskController.consumeOMEvents(
        omUpdateEventBatchMock,
        mock(OMMetadataManager.class));

    verify(reconOmTaskMock, times(1))
        .process(any());
    long endTime = System.currentTimeMillis();

    reconTaskStatusDao = getDao(ReconTaskStatusDao.class);
    ReconTaskStatus reconTaskStatus = reconTaskStatusDao.findById(""MockTask"");
    long taskTimeStamp = reconTaskStatus.getLastUpdatedTimestamp();
    long seqNumber = reconTaskStatus.getLastUpdatedSeqNumber();

    Assert.assertTrue(startTime <= taskTimeStamp
        && taskTimeStamp <= endTime);
    Assert.assertEquals(seqNumber,
        omUpdateEventBatchMock.getLastSequenceNumber());
  }
",non-flaky,5
57212,apache_ozone,TestReconTaskControllerImpl.testFailedTaskRetryLogic,"  @Test
  public void testFailedTaskRetryLogic() throws Exception {
    String taskName = ""Dummy_"" + System.currentTimeMillis();

    DummyReconDBTask dummyReconDBTask =
        new DummyReconDBTask(taskName, DummyReconDBTask.TaskType.FAIL_ONCE);
    reconTaskController.registerTask(dummyReconDBTask);

    long currentTime = System.currentTimeMillis();
    OMUpdateEventBatch omUpdateEventBatchMock = mock(OMUpdateEventBatch.class);
    when(omUpdateEventBatchMock.isEmpty()).thenReturn(false);
    when(omUpdateEventBatchMock.getLastSequenceNumber()).thenReturn(100L);

    reconTaskController.consumeOMEvents(omUpdateEventBatchMock,
        mock(OMMetadataManager.class));
    assertFalse(reconTaskController.getRegisteredTasks().isEmpty());
    assertEquals(dummyReconDBTask, reconTaskController.getRegisteredTasks()
        .get(dummyReconDBTask.getTaskName()));

    reconTaskStatusDao = getDao(ReconTaskStatusDao.class);
    ReconTaskStatus dbRecord = reconTaskStatusDao.findById(taskName);

    Assert.assertEquals(taskName, dbRecord.getTaskName());
    Assert.assertTrue(
        dbRecord.getLastUpdatedTimestamp() > currentTime);

    Assert.assertEquals(Long.valueOf(100L), dbRecord.getLastUpdatedSeqNumber());
  }
",non-flaky,5
57213,apache_ozone,TestReconTaskControllerImpl.testBadBehavedTaskIsIgnored,"  @Test
  public void testBadBehavedTaskIsIgnored() throws Exception {
    String taskName = ""Dummy_"" + System.currentTimeMillis();
    DummyReconDBTask dummyReconDBTask =
        new DummyReconDBTask(taskName, DummyReconDBTask.TaskType.ALWAYS_FAIL);
    reconTaskController.registerTask(dummyReconDBTask);

    OMUpdateEventBatch omUpdateEventBatchMock = mock(OMUpdateEventBatch.class);
    when(omUpdateEventBatchMock.isEmpty()).thenReturn(false);
    when(omUpdateEventBatchMock.getLastSequenceNumber()).thenReturn(100L);

    OMMetadataManager omMetadataManagerMock = mock(OMMetadataManager.class);
    for (int i = 0; i < 2; i++) {
      reconTaskController.consumeOMEvents(omUpdateEventBatchMock,
          omMetadataManagerMock);

      assertFalse(reconTaskController.getRegisteredTasks().isEmpty());
      assertEquals(dummyReconDBTask, reconTaskController.getRegisteredTasks()
          .get(dummyReconDBTask.getTaskName()));
    }

    //Should be ignored now.
    reconTaskController.consumeOMEvents(omUpdateEventBatchMock,
        omMetadataManagerMock);
    assertTrue(reconTaskController.getRegisteredTasks().isEmpty());

    reconTaskStatusDao = getDao(ReconTaskStatusDao.class);
    ReconTaskStatus dbRecord = reconTaskStatusDao.findById(taskName);

    Assert.assertEquals(taskName, dbRecord.getTaskName());
    Assert.assertEquals(Long.valueOf(0L), dbRecord.getLastUpdatedTimestamp());
    Assert.assertEquals(Long.valueOf(0L), dbRecord.getLastUpdatedSeqNumber());
  }
",non-flaky,5
57214,apache_ozone,TestReconTaskControllerImpl.testReInitializeTasks,"  @Test
  public void testReInitializeTasks() throws Exception {

    ReconOMMetadataManager omMetadataManagerMock = mock(
        ReconOMMetadataManager.class);
    ReconOmTask reconOmTaskMock =
        getMockTask(""MockTask2"");
    when(reconOmTaskMock.reprocess(omMetadataManagerMock))
        .thenReturn(new ImmutablePair<>(""MockTask2"", true));
    when(omMetadataManagerMock.getLastSequenceNumberFromDB()
    ).thenReturn(100L);

    long startTime = System.currentTimeMillis();
    reconTaskController.registerTask(reconOmTaskMock);
    reconTaskController.reInitializeTasks(omMetadataManagerMock);
    long endTime = System.currentTimeMillis();

    verify(reconOmTaskMock, times(1))
        .reprocess(omMetadataManagerMock);

    verify(omMetadataManagerMock, times(1)
    ).getLastSequenceNumberFromDB();

    ReconTaskStatus reconTaskStatus = reconTaskStatusDao.findById(""MockTask2"");
    long taskTimeStamp = reconTaskStatus.getLastUpdatedTimestamp();
    long seqNumber = reconTaskStatus.getLastUpdatedSeqNumber();

    Assert.assertTrue(startTime <= taskTimeStamp
        && taskTimeStamp <= endTime);
    Assert.assertEquals(seqNumber,
        omMetadataManagerMock.getLastSequenceNumberFromDB());
  }
",non-flaky,5
57215,apache_ozone,TestOMDBUpdatesHandler.testPut,"  @Test
  public void testPut() throws Exception {
    OzoneConfiguration configuration = createNewTestPath();
    OmMetadataManagerImpl metaMgr = new OmMetadataManagerImpl(configuration);

    // Create 1 volume, 2 keys and write to source OM DB.
    String volumeKey = metaMgr.getVolumeKey(""sampleVol"");
    OmVolumeArgs args =
        OmVolumeArgs.newBuilder()
            .setVolume(""sampleVol"")
            .setAdminName(""bilbo"")
            .setOwnerName(""bilbo"")
            .build();
    metaMgr.getVolumeTable().put(volumeKey, args);

    OmKeyInfo firstKey = getOmKeyInfo(""sampleVol"", ""bucketOne"", ""key_one"");
    metaMgr.getKeyTable(getBucketLayout())
        .put(""/sampleVol/bucketOne/key_one"", firstKey);

    OmKeyInfo secondKey = getOmKeyInfo(""sampleVol"", ""bucketOne"", ""key_two"");
    metaMgr.getKeyTable(getBucketLayout())
        .put(""/sampleVol/bucketOne/key_two"", secondKey);

    // Write the secondKey to the target OM DB.
    OzoneConfiguration conf2 = createNewTestPath();
    OmMetadataManagerImpl reconOmmetaMgr = new OmMetadataManagerImpl(conf2);
    reconOmmetaMgr.getKeyTable(getBucketLayout())
        .put(""/sampleVol/bucketOne/key_two"", secondKey);

    RDBStore rdbStore = (RDBStore) metaMgr.getStore();
    RocksDB rocksDB = rdbStore.getDb();
    // Get all updates from source DB. (3 PUTs)
    TransactionLogIterator transactionLogIterator =
        rocksDB.getUpdatesSince(0);
    List<byte[]> writeBatches = new ArrayList<>();

    while(transactionLogIterator.isValid()) {
      TransactionLogIterator.BatchResult result =
          transactionLogIterator.getBatch();
      result.writeBatch().markWalTerminationPoint();
      WriteBatch writeBatch = result.writeBatch();
      writeBatches.add(writeBatch.data());
      transactionLogIterator.next();
    }

    // OMDBUpdatesHandler has access to target DB. Hence it has only the
    // ""secondKey"".
    OMDBUpdatesHandler omdbUpdatesHandler =
        new OMDBUpdatesHandler(reconOmmetaMgr);
    for (byte[] data : writeBatches) {
      WriteBatch writeBatch = new WriteBatch(data);
      // Capture the 3 PUT events from source DB.
      writeBatch.iterate(omdbUpdatesHandler);
    }

    List<OMDBUpdateEvent> events = omdbUpdatesHandler.getEvents();
    assertEquals(3, events.size());

    OMDBUpdateEvent volEvent = events.get(0);
    assertEquals(PUT, volEvent.getAction());
    assertEquals(volumeKey, volEvent.getKey());
    assertEquals(args.getVolume(), ((OmVolumeArgs)volEvent.getValue())
        .getVolume());

    OMDBUpdateEvent keyEvent = events.get(1);
    assertEquals(PUT, keyEvent.getAction());
    assertEquals(""/sampleVol/bucketOne/key_one"", keyEvent.getKey());
    assertNull(keyEvent.getOldValue());

    OMDBUpdateEvent updateEvent = events.get(2);
    assertEquals(UPDATE, updateEvent.getAction());
    assertEquals(""/sampleVol/bucketOne/key_two"", updateEvent.getKey());
    assertNotNull(updateEvent.getOldValue());
    assertEquals(secondKey.getKeyName(),
        ((OmKeyInfo)updateEvent.getOldValue()).getKeyName());
  }
",non-flaky,5
57216,apache_ozone,TestOMDBUpdatesHandler.testDelete,"  @Test
  public void testDelete() throws Exception {

    OzoneConfiguration configuration = createNewTestPath();
    OmMetadataManagerImpl metaMgr = new OmMetadataManagerImpl(configuration);

    OzoneConfiguration conf2 = createNewTestPath();
    OmMetadataManagerImpl metaMgrCopy = new OmMetadataManagerImpl(conf2);

    // Write 1 volume, 1 key into source and target OM DBs.
    String volumeKey = metaMgr.getVolumeKey(""sampleVol"");
    String nonExistVolumeKey = metaMgr.getVolumeKey(""nonExistingVolume"");
    OmVolumeArgs args =
        OmVolumeArgs.newBuilder()
            .setVolume(""sampleVol"")
            .setAdminName(""bilbo"")
            .setOwnerName(""bilbo"")
            .build();
    metaMgr.getVolumeTable().put(volumeKey, args);
    metaMgrCopy.getVolumeTable().put(volumeKey, args);

    OmKeyInfo omKeyInfo = getOmKeyInfo(""sampleVol"", ""bucketOne"", ""key_one"");
    metaMgr.getKeyTable(getBucketLayout())
        .put(""/sampleVol/bucketOne/key_one"", omKeyInfo);
    metaMgrCopy.getKeyTable(getBucketLayout())
        .put(""/sampleVol/bucketOne/key_one"", omKeyInfo);

    // Delete the volume and key from target DB.
    metaMgr.getKeyTable(getBucketLayout())
        .delete(""/sampleVol/bucketOne/key_one"");
    metaMgr.getVolumeTable().delete(volumeKey);
    // Delete a non-existing volume and key
    metaMgr.getKeyTable(getBucketLayout())
        .delete(""/sampleVol/bucketOne/key_two"");
    metaMgr.getVolumeTable().delete(metaMgr.getVolumeKey(""nonExistingVolume""));

    RDBStore rdbStore = (RDBStore) metaMgr.getStore();
    RocksDB rocksDB = rdbStore.getDb();
    TransactionLogIterator transactionLogIterator =
        rocksDB.getUpdatesSince(3);
    List<byte[]> writeBatches = new ArrayList<>();

    while(transactionLogIterator.isValid()) {
      TransactionLogIterator.BatchResult result =
          transactionLogIterator.getBatch();
      result.writeBatch().markWalTerminationPoint();
      WriteBatch writeBatch = result.writeBatch();
      writeBatches.add(writeBatch.data());
      transactionLogIterator.next();
    }

    // OMDBUpdatesHandler has access to target DB. So it has the volume and
    // key.
    OMDBUpdatesHandler omdbUpdatesHandler =
        new OMDBUpdatesHandler(metaMgrCopy);
    for (byte[] data : writeBatches) {
      WriteBatch writeBatch = new WriteBatch(data);
      writeBatch.iterate(omdbUpdatesHandler);
    }

    List<OMDBUpdateEvent> events = omdbUpdatesHandler.getEvents();
    assertEquals(4, events.size());

    OMDBUpdateEvent keyEvent = events.get(0);
    assertEquals(OMDBUpdateEvent.OMDBUpdateAction.DELETE, keyEvent.getAction());
    assertEquals(""/sampleVol/bucketOne/key_one"", keyEvent.getKey());
    assertEquals(omKeyInfo, keyEvent.getValue());

    OMDBUpdateEvent volEvent = events.get(1);
    assertEquals(OMDBUpdateEvent.OMDBUpdateAction.DELETE, volEvent.getAction());
    assertEquals(volumeKey, volEvent.getKey());
    assertNotNull(volEvent.getValue());
    OmVolumeArgs volumeInfo = (OmVolumeArgs) volEvent.getValue();
    assertEquals(""sampleVol"", volumeInfo.getVolume());

    // Assert the values of non existent keys are set to null.
    OMDBUpdateEvent nonExistKey = events.get(2);
    assertEquals(OMDBUpdateEvent.OMDBUpdateAction.DELETE,
        nonExistKey.getAction());
    assertEquals(""/sampleVol/bucketOne/key_two"", nonExistKey.getKey());
    assertNull(nonExistKey.getValue());

    OMDBUpdateEvent nonExistVolume = events.get(3);
    assertEquals(OMDBUpdateEvent.OMDBUpdateAction.DELETE,
        nonExistVolume.getAction());
    assertEquals(nonExistVolumeKey, nonExistVolume.getKey());
    assertNull(nonExistVolume.getValue());
  }
",non-flaky,5
57217,apache_ozone,TestOMDBUpdatesHandler.testGetKeyType,"  @Test
  public void testGetKeyType() throws IOException {
    OzoneConfiguration configuration = createNewTestPath();
    OmMetadataManagerImpl metaMgr = new OmMetadataManagerImpl(configuration);

    assertEquals(String.class, omdbDefinition.getKeyType(
        metaMgr.getKeyTable(getBucketLayout()).getName()).get());
    assertEquals(OzoneTokenIdentifier.class, omdbDefinition.getKeyType(
        metaMgr.getDelegationTokenTable().getName()).get());
  }
",non-flaky,5
57218,apache_ozone,TestOMDBUpdatesHandler.testGetValueType,"  @Test
  public void testGetValueType() throws IOException {
    OzoneConfiguration configuration = createNewTestPath();
    OmMetadataManagerImpl metaMgr = new OmMetadataManagerImpl(configuration);

    assertEquals(OmKeyInfo.class, omdbDefinition.getValueType(
        metaMgr.getKeyTable(getBucketLayout()).getName()).get());
    assertEquals(OmVolumeArgs.class, omdbDefinition.getValueType(
        metaMgr.getVolumeTable().getName()).get());
    assertEquals(OmBucketInfo.class, omdbDefinition.getValueType(
        metaMgr.getBucketTable().getName()).get());
  }
",non-flaky,5
57219,apache_ozone,TestTableCountTask.testReprocess,"  @Test
  public void testReprocess() {
    OMMetadataManager omMetadataManager = mock(OmMetadataManagerImpl.class);
    // Mock 5 rows in each table and test the count
    for (String tableName: tableCountTask.getTaskTables()) {
      TypedTable<String, Object> table = mock(TypedTable.class);
      TypedTable.TypedTableIterator mockIter = mock(TypedTable
          .TypedTableIterator.class);
      when(table.iterator()).thenReturn(mockIter);
      when(omMetadataManager.getTable(tableName)).thenReturn(table);
      when(mockIter.hasNext())
          .thenReturn(true)
          .thenReturn(true)
          .thenReturn(true)
          .thenReturn(true)
          .thenReturn(true)
          .thenReturn(false);
    }

    Pair<String, Boolean> result = tableCountTask.reprocess(omMetadataManager);
    assertTrue(result.getRight());

    assertEquals(5L, getCountForTable(KEY_TABLE));
    assertEquals(5L, getCountForTable(VOLUME_TABLE));
    assertEquals(5L, getCountForTable(BUCKET_TABLE));
    assertEquals(5L, getCountForTable(OPEN_KEY_TABLE));
    assertEquals(5L, getCountForTable(DELETED_TABLE));
  }
",non-flaky,5
57220,apache_ozone,TestTableCountTask.testProcess,"  @Test
  public void testProcess() {
    ArrayList<OMDBUpdateEvent> events = new ArrayList<>();
    // Create 5 put, 1 delete and 1 update event for each table
    for (String tableName: tableCountTask.getTaskTables()) {
      for (int i=0; i<5; i++) {
        events.add(getOMUpdateEvent(""item"" + i, null, tableName, PUT));
      }
      // for delete event, if value is set to null, the counter will not be
      // decremented. This is because the value will be null if item does not
      // exist in the database and there is no need to delete.
      events.add(getOMUpdateEvent(""item0"", mock(OmKeyInfo.class), tableName,
          DELETE));
      events.add(getOMUpdateEvent(""item1"", null, tableName, UPDATE));
    }
    OMUpdateEventBatch omUpdateEventBatch = new OMUpdateEventBatch(events);
    tableCountTask.process(omUpdateEventBatch);

    // Verify 4 items in each table. (5 puts - 1 delete + 0 update)
    assertEquals(4L, getCountForTable(KEY_TABLE));
    assertEquals(4L, getCountForTable(VOLUME_TABLE));
    assertEquals(4L, getCountForTable(BUCKET_TABLE));
    assertEquals(4L, getCountForTable(OPEN_KEY_TABLE));
    assertEquals(4L, getCountForTable(DELETED_TABLE));

    // add a new key and simulate delete on non-existing item (value: null)
    ArrayList<OMDBUpdateEvent> newEvents = new ArrayList<>();
    for (String tableName: tableCountTask.getTaskTables()) {
      newEvents.add(getOMUpdateEvent(""item5"", null, tableName, PUT));
      // This delete event should be a noop since value is null
      newEvents.add(getOMUpdateEvent(""item0"", null, tableName, DELETE));
    }

    omUpdateEventBatch = new OMUpdateEventBatch(newEvents);
    tableCountTask.process(omUpdateEventBatch);

    // Verify 5 items in each table. (1 new put + 0 delete)
    assertEquals(5L, getCountForTable(KEY_TABLE));
    assertEquals(5L, getCountForTable(VOLUME_TABLE));
    assertEquals(5L, getCountForTable(BUCKET_TABLE));
    assertEquals(5L, getCountForTable(OPEN_KEY_TABLE));
    assertEquals(5L, getCountForTable(DELETED_TABLE));
  }
",non-flaky,5
57221,apache_ozone,TestUtilizationSchemaDefinition.testReconSchemaCreated,"  @Test
  public void testReconSchemaCreated() throws Exception {
    Connection connection = getConnection();
    // Verify table definition
    DatabaseMetaData metaData = connection.getMetaData();
    ResultSet resultSet = metaData.getColumns(null, null,
        CLUSTER_GROWTH_DAILY_TABLE_NAME, null);

    List<Pair<String, Integer>> expectedPairs = new ArrayList<>();

    expectedPairs.add(new ImmutablePair<>(""timestamp"", Types.TIMESTAMP));
    expectedPairs.add(new ImmutablePair<>(""datanode_id"", Types.INTEGER));
    expectedPairs.add(new ImmutablePair<>(""datanode_host"", Types.VARCHAR));
    expectedPairs.add(new ImmutablePair<>(""rack_id"", Types.VARCHAR));
    expectedPairs.add(new ImmutablePair<>(""available_size"", Types.BIGINT));
    expectedPairs.add(new ImmutablePair<>(""used_size"", Types.BIGINT));
    expectedPairs.add(new ImmutablePair<>(""container_count"", Types.INTEGER));
    expectedPairs.add(new ImmutablePair<>(""block_count"", Types.INTEGER));

    List<Pair<String, Integer>> actualPairs = new ArrayList<>();

    while (resultSet.next()) {
      actualPairs.add(new ImmutablePair<>(resultSet.getString(""COLUMN_NAME""),
          resultSet.getInt(""DATA_TYPE"")));
    }

    Assert.assertEquals(8, actualPairs.size());
    Assert.assertEquals(expectedPairs, actualPairs);

    ResultSet resultSetFileCount = metaData.getColumns(null, null,
        FILE_COUNT_BY_SIZE_TABLE_NAME, null);

    List<Pair<String, Integer>> expectedPairsFileCount = new ArrayList<>();
    expectedPairsFileCount.add(
        new ImmutablePair<>(""volume"", Types.VARCHAR));
    expectedPairsFileCount.add(
        new ImmutablePair<>(""bucket"", Types.VARCHAR));
    expectedPairsFileCount.add(
        new ImmutablePair<>(""file_size"", Types.BIGINT));
    expectedPairsFileCount.add(
        new ImmutablePair<>(""count"", Types.BIGINT));

    List<Pair<String, Integer>> actualPairsFileCount = new ArrayList<>();
    while(resultSetFileCount.next()) {
      actualPairsFileCount.add(new ImmutablePair<>(resultSetFileCount.getString(
          ""COLUMN_NAME""), resultSetFileCount.getInt(
              ""DATA_TYPE"")));
    }
    assertEquals(""Unexpected number of columns"",
        4, actualPairsFileCount.size());
    assertEquals(""Columns Do not Match "",
        expectedPairsFileCount, actualPairsFileCount);
  }
",non-flaky,5
57222,apache_ozone,TestUtilizationSchemaDefinition.testClusterGrowthDailyCRUDOperations,"  @Test
  public void testClusterGrowthDailyCRUDOperations() throws Exception {
    // Verify table exists
    Connection connection = getConnection();

    DatabaseMetaData metaData = connection.getMetaData();
    ResultSet resultSet = metaData.getTables(null, null,
        CLUSTER_GROWTH_DAILY_TABLE_NAME, null);

    while (resultSet.next()) {
      Assert.assertEquals(CLUSTER_GROWTH_DAILY_TABLE_NAME,
          resultSet.getString(""TABLE_NAME""));
    }

    ClusterGrowthDailyDao dao = getDao(ClusterGrowthDailyDao.class);
    long now = System.currentTimeMillis();
    ClusterGrowthDaily newRecord = new ClusterGrowthDaily();
    newRecord.setTimestamp(new Timestamp(now));
    newRecord.setDatanodeId(10);
    newRecord.setDatanodeHost(""host1"");
    newRecord.setRackId(""rack1"");
    newRecord.setAvailableSize(1024L);
    newRecord.setUsedSize(512L);
    newRecord.setContainerCount(10);
    newRecord.setBlockCount(25);

    // Create
    dao.insert(newRecord);

    // Read
    ClusterGrowthDaily dbRecord =
        dao.findById(getDslContext().newRecord(CLUSTER_GROWTH_DAILY.TIMESTAMP,
            CLUSTER_GROWTH_DAILY.DATANODE_ID)
            .value1(new Timestamp(now)).value2(10));

    Assert.assertEquals(""host1"", dbRecord.getDatanodeHost());
    Assert.assertEquals(""rack1"", dbRecord.getRackId());
    Assert.assertEquals(Long.valueOf(1024), dbRecord.getAvailableSize());
    Assert.assertEquals(Long.valueOf(512), dbRecord.getUsedSize());
    Assert.assertEquals(Integer.valueOf(10), dbRecord.getContainerCount());
    Assert.assertEquals(Integer.valueOf(25), dbRecord.getBlockCount());

    // Update
    dbRecord.setUsedSize(700L);
    dbRecord.setBlockCount(30);
    dao.update(dbRecord);

    // Read updated
    dbRecord =
        dao.findById(getDslContext().newRecord(CLUSTER_GROWTH_DAILY.TIMESTAMP,
            CLUSTER_GROWTH_DAILY.DATANODE_ID)
            .value1(new Timestamp(now)).value2(10));

    Assert.assertEquals(Long.valueOf(700), dbRecord.getUsedSize());
    Assert.assertEquals(Integer.valueOf(30), dbRecord.getBlockCount());

    // Delete
    dao.deleteById(getDslContext().newRecord(CLUSTER_GROWTH_DAILY.TIMESTAMP,
        CLUSTER_GROWTH_DAILY.DATANODE_ID)
        .value1(new Timestamp(now)).value2(10));

    // Verify
    dbRecord =
        dao.findById(getDslContext().newRecord(CLUSTER_GROWTH_DAILY.TIMESTAMP,
            CLUSTER_GROWTH_DAILY.DATANODE_ID)
            .value1(new Timestamp(now)).value2(10));

    Assert.assertNull(dbRecord);
  }
",non-flaky,5
57223,apache_ozone,TestUtilizationSchemaDefinition.testFileCountBySizeCRUDOperations,"  @Test
  public void testFileCountBySizeCRUDOperations() throws SQLException {
    Connection connection = getConnection();

    DatabaseMetaData metaData = connection.getMetaData();
    ResultSet resultSet = metaData.getTables(null, null,
        FILE_COUNT_BY_SIZE_TABLE_NAME, null);

    while (resultSet.next()) {
      Assert.assertEquals(FILE_COUNT_BY_SIZE_TABLE_NAME,
          resultSet.getString(""TABLE_NAME""));
    }

    FileCountBySizeDao fileCountBySizeDao = getDao(FileCountBySizeDao.class);
    UtilizationSchemaDefinition utilizationSchemaDefinition =
        getSchemaDefinition(UtilizationSchemaDefinition.class);

    FileCountBySize newRecord = new FileCountBySize();
    newRecord.setVolume(""vol1"");
    newRecord.setBucket(""bucket1"");
    newRecord.setFileSize(1024L);
    newRecord.setCount(1L);

    fileCountBySizeDao.insert(newRecord);

    Record3<String, String, Long> recordToFind = utilizationSchemaDefinition
        .getDSLContext().newRecord(FILE_COUNT_BY_SIZE.VOLUME,
            FILE_COUNT_BY_SIZE.BUCKET,
            FILE_COUNT_BY_SIZE.FILE_SIZE)
        .value1(""vol1"")
        .value2(""bucket1"")
        .value3(1024L);
    FileCountBySize dbRecord = fileCountBySizeDao.findById(recordToFind);
    assertEquals(Long.valueOf(1), dbRecord.getCount());

    dbRecord.setCount(2L);
    fileCountBySizeDao.update(dbRecord);

    dbRecord = fileCountBySizeDao.findById(recordToFind);
    assertEquals(Long.valueOf(2), dbRecord.getCount());

    Table<FileCountBySizeRecord> fileCountBySizeRecordTable =
        fileCountBySizeDao.getTable();
    List<UniqueKey<FileCountBySizeRecord>> tableKeys =
        fileCountBySizeRecordTable.getKeys();
    for (UniqueKey key : tableKeys) {
      String name = key.getName();
    }
  }
",non-flaky,5
57224,apache_ozone,TestReconInternalSchemaDefinition.testSchemaCreated,"  @Test
  public void testSchemaCreated() throws Exception {

    Connection connection = getConnection();
    // Verify table definition
    DatabaseMetaData metaData = connection.getMetaData();
    ResultSet resultSet = metaData.getColumns(null, null,
        RECON_TASK_STATUS_TABLE_NAME, null);

    List<Pair<String, Integer>> expectedPairs = new ArrayList<>();

    expectedPairs.add(new ImmutablePair<>(""task_name"", Types.VARCHAR));
    expectedPairs.add(new ImmutablePair<>(""last_updated_timestamp"",
        Types.BIGINT));
    expectedPairs.add(new ImmutablePair<>(""last_updated_seq_number"",
        Types.BIGINT));

    List<Pair<String, Integer>> actualPairs = new ArrayList<>();

    while (resultSet.next()) {
      actualPairs.add(new ImmutablePair<>(
          resultSet.getString(""COLUMN_NAME""),
          resultSet.getInt(""DATA_TYPE"")));
    }

    Assert.assertEquals(3, actualPairs.size());
    Assert.assertEquals(expectedPairs, actualPairs);
  }
",non-flaky,5
57225,apache_ozone,TestReconInternalSchemaDefinition.testReconTaskStatusCRUDOperations,"  @Test
  public void testReconTaskStatusCRUDOperations() throws Exception {
    // Verify table exists
    Connection connection = getConnection();
    DatabaseMetaData metaData = connection.getMetaData();
    ResultSet resultSet = metaData.getTables(null, null,
        RECON_TASK_STATUS_TABLE_NAME, null);

    while (resultSet.next()) {
      Assert.assertEquals(RECON_TASK_STATUS_TABLE_NAME,
          resultSet.getString(""TABLE_NAME""));
    }

    ReconTaskStatusDao dao = getDao(ReconTaskStatusDao.class);
    long now = System.currentTimeMillis();
    ReconTaskStatus newRecord = new ReconTaskStatus();
    newRecord.setTaskName(""HelloWorldTask"");
    newRecord.setLastUpdatedTimestamp(now);
    newRecord.setLastUpdatedSeqNumber(100L);

    // Create
    dao.insert(newRecord);

    ReconTaskStatus newRecord2 = new ReconTaskStatus();
    newRecord2.setTaskName(""GoodbyeWorldTask"");
    newRecord2.setLastUpdatedTimestamp(now);
    newRecord2.setLastUpdatedSeqNumber(200L);
    // Create
    dao.insert(newRecord2);

    // Read
    ReconTaskStatus dbRecord = dao.findById(""HelloWorldTask"");

    Assert.assertEquals(""HelloWorldTask"", dbRecord.getTaskName());
    Assert.assertEquals(Long.valueOf(now), dbRecord.getLastUpdatedTimestamp());
    Assert.assertEquals(Long.valueOf(100), dbRecord.getLastUpdatedSeqNumber());

    // Update
    dbRecord.setLastUpdatedSeqNumber(150L);
    dao.update(dbRecord);

    // Read updated
    dbRecord = dao.findById(""HelloWorldTask"");
    Assert.assertEquals(Long.valueOf(150), dbRecord.getLastUpdatedSeqNumber());

    // Delete
    dao.deleteById(""GoodbyeWorldTask"");

    // Verify
    dbRecord = dao.findById(""GoodbyeWorldTask"");

    Assert.assertNull(dbRecord);
  }
",non-flaky,5
57226,apache_ozone,TestReconWithDifferentSqlDBs.testSchemaSetup,"  @Test
  public void testSchemaSetup() throws SQLException {
    assertNotNull(getInjector());
    assertNotNull(getConfiguration());
    assertNotNull(getDslContext());
    assertNotNull(getConnection());
    RECON_DAO_LIST.forEach(dao -> {
      assertNotNull(getDao(dao));
    });
    ReconTaskStatusDao dao = getDao(ReconTaskStatusDao.class);
    dao.insert(new ReconTaskStatus(""TestTask"", 1L, 2L));
    assertEquals(1, dao.findAll().size());

    int numRows = getDslContext().delete(RECON_TASK_STATUS).execute();
    assertEquals(1, numRows);
    assertEquals(0, dao.findAll().size());
  }
",non-flaky,5
57227,apache_ozone,TestSqlSchemaSetup.testSchemaSetup,"  @Test
  public void testSchemaSetup() throws SQLException {
    assertNotNull(getInjector());
    assertNotNull(getConfiguration());
    assertNotNull(getDslContext());
    assertNotNull(getConnection());
    RECON_DAO_LIST.forEach(dao -> {
      assertNotNull(getDao(dao));
    });
    ReconTaskStatusDao dao = getDao(ReconTaskStatusDao.class);
    dao.insert(new ReconTaskStatus(""TestTask"", 1L, 2L));
    assertEquals(1, dao.findAll().size());
  }
",non-flaky,5
57228,apache_ozone,TestStatsSchemaDefinition.testIfStatsSchemaCreated,"  @Test
  public void testIfStatsSchemaCreated() throws Exception {
    Connection connection = getConnection();
    // Verify table definition
    DatabaseMetaData metaData = connection.getMetaData();
    ResultSet resultSet = metaData.getColumns(null, null,
        GLOBAL_STATS_TABLE_NAME, null);

    List<Pair<String, Integer>> expectedPairs = new ArrayList<>();

    expectedPairs.add(new ImmutablePair<>(""key"", Types.VARCHAR));
    expectedPairs.add(new ImmutablePair<>(""value"", Types.BIGINT));
    expectedPairs.add(new ImmutablePair<>(""last_updated_timestamp"",
        Types.TIMESTAMP));

    List<Pair<String, Integer>> actualPairs = new ArrayList<>();

    while (resultSet.next()) {
      actualPairs.add(new ImmutablePair<>(resultSet.getString(""COLUMN_NAME""),
          resultSet.getInt(""DATA_TYPE"")));
    }

    Assert.assertEquals(3, actualPairs.size());
    Assert.assertEquals(expectedPairs, actualPairs);
  }
",non-flaky,5
57229,apache_ozone,TestStatsSchemaDefinition.testGlobalStatsCRUDOperations,"  @Test
  public void testGlobalStatsCRUDOperations() throws Exception {
    Connection connection = getConnection();

    DatabaseMetaData metaData = connection.getMetaData();
    ResultSet resultSet = metaData.getTables(null, null,
        GLOBAL_STATS_TABLE_NAME, null);

    while (resultSet.next()) {
      Assert.assertEquals(GLOBAL_STATS_TABLE_NAME,
          resultSet.getString(""TABLE_NAME""));
    }

    GlobalStatsDao dao = getDao(GlobalStatsDao.class);

    long now = System.currentTimeMillis();
    GlobalStats newRecord = new GlobalStats();
    newRecord.setLastUpdatedTimestamp(new Timestamp(now));
    newRecord.setKey(""key1"");
    newRecord.setValue(500L);

    // Create
    dao.insert(newRecord);
    GlobalStats newRecord2 = new GlobalStats();
    newRecord2.setLastUpdatedTimestamp(new Timestamp(now + 1000L));
    newRecord2.setKey(""key2"");
    newRecord2.setValue(10L);
    dao.insert(newRecord2);

    // Read
    GlobalStats dbRecord = dao.findById(""key1"");

    Assert.assertEquals(""key1"", dbRecord.getKey());
    Assert.assertEquals(Long.valueOf(500), dbRecord.getValue());
    Assert.assertEquals(new Timestamp(now), dbRecord.getLastUpdatedTimestamp());

    dbRecord = dao.findById(""key2"");
    Assert.assertEquals(""key2"", dbRecord.getKey());
    Assert.assertEquals(Long.valueOf(10), dbRecord.getValue());
    Assert.assertEquals(new Timestamp(now + 1000L),
        dbRecord.getLastUpdatedTimestamp());

    // Update
    dbRecord.setValue(100L);
    dbRecord.setLastUpdatedTimestamp(new Timestamp(now + 2000L));
    dao.update(dbRecord);

    // Read updated
    dbRecord = dao.findById(""key2"");

    Assert.assertEquals(new Timestamp(now + 2000L),
        dbRecord.getLastUpdatedTimestamp());
    Assert.assertEquals(Long.valueOf(100L), dbRecord.getValue());

    // Delete
    dao.deleteById(""key1"");

    // Verify
    dbRecord = dao.findById(""key1"");

    Assert.assertNull(dbRecord);
  }
",non-flaky,5
57230,apache_ozone,TestContainerHealthTaskRecordGenerator.testMissingRecordRetained,"  @Test
  public void testMissingRecordRetained() {
    Set<ContainerReplica> replicas = new HashSet<>();
    ContainerHealthStatus status =
        new ContainerHealthStatus(container, replicas, placementPolicy);
    // Missing record should be retained
    assertTrue(ContainerHealthTask.ContainerHealthRecords
        .retainOrUpdateRecord(status, missingRecord()));
    // Under / Over / Mis replicated should not be retained as if a container is
    // missing then it is not in any other category.
    assertFalse(ContainerHealthTask.ContainerHealthRecords
        .retainOrUpdateRecord(status, underReplicatedRecord()));
    assertFalse(ContainerHealthTask.ContainerHealthRecords
        .retainOrUpdateRecord(status, overReplicatedRecord()));
    assertFalse(ContainerHealthTask.ContainerHealthRecords
        .retainOrUpdateRecord(status, misReplicatedRecord()));

    replicas = generateReplicas(container, CLOSED, CLOSED, CLOSED);
    status = new ContainerHealthStatus(container, replicas, placementPolicy);
    assertFalse(ContainerHealthTask.ContainerHealthRecords
        .retainOrUpdateRecord(status, missingRecord()));
  }
",non-flaky,5
57231,apache_ozone,TestContainerHealthTaskRecordGenerator.testUnderReplicatedRecordRetainedAndUpdated,"  @Test
  public void testUnderReplicatedRecordRetainedAndUpdated() {
    // under replicated container
    Set<ContainerReplica> replicas =
        generateReplicas(container, CLOSED, CLOSED);
    ContainerHealthStatus status =
        new ContainerHealthStatus(container, replicas, placementPolicy);

    UnhealthyContainersRecord rec = underReplicatedRecord();
    assertTrue(ContainerHealthTask.ContainerHealthRecords
        .retainOrUpdateRecord(status, rec));
    // The record actual count should be updated from 1 -> 2
    assertEquals(2, rec.getActualReplicaCount().intValue());
    assertEquals(1, rec.getReplicaDelta().intValue());

    // Missing / Over / Mis replicated should not be retained
    assertFalse(ContainerHealthTask.ContainerHealthRecords
        .retainOrUpdateRecord(status, missingRecord()));
    assertFalse(ContainerHealthTask.ContainerHealthRecords
        .retainOrUpdateRecord(status, overReplicatedRecord()));
    assertFalse(ContainerHealthTask.ContainerHealthRecords
        .retainOrUpdateRecord(status, misReplicatedRecord()));

    // Container is now replicated OK - should be removed.
    replicas = generateReplicas(container, CLOSED, CLOSED, CLOSED);
    status = new ContainerHealthStatus(container, replicas, placementPolicy);
    assertFalse(ContainerHealthTask.ContainerHealthRecords
        .retainOrUpdateRecord(status, rec));
  }
",non-flaky,5
57232,apache_ozone,TestContainerHealthTaskRecordGenerator.testOverReplicatedRecordRetainedAndUpdated,"  @Test
  public void testOverReplicatedRecordRetainedAndUpdated() {
    // under replicated container
    Set<ContainerReplica> replicas =
        generateReplicas(container, CLOSED, CLOSED, CLOSED, CLOSED);
    ContainerHealthStatus status =
        new ContainerHealthStatus(container, replicas, placementPolicy);

    UnhealthyContainersRecord rec = overReplicatedRecord();
    assertTrue(ContainerHealthTask.ContainerHealthRecords
        .retainOrUpdateRecord(status, rec));
    // The record actual count should be updated from 5 -> 4
    assertEquals(4, rec.getActualReplicaCount().intValue());
    assertEquals(-1, rec.getReplicaDelta().intValue());

    // Missing / Over / Mis replicated should not be retained
    assertFalse(ContainerHealthTask.ContainerHealthRecords
        .retainOrUpdateRecord(status, missingRecord()));
    assertFalse(ContainerHealthTask.ContainerHealthRecords
        .retainOrUpdateRecord(status, underReplicatedRecord()));
    assertFalse(ContainerHealthTask.ContainerHealthRecords
        .retainOrUpdateRecord(status, misReplicatedRecord()));

    // Container is now replicated OK - should be removed.
    replicas = generateReplicas(container, CLOSED, CLOSED, CLOSED);
    status = new ContainerHealthStatus(container, replicas, placementPolicy);
    assertFalse(ContainerHealthTask.ContainerHealthRecords
        .retainOrUpdateRecord(status, rec));
  }
",non-flaky,5
57233,apache_ozone,TestContainerHealthTaskRecordGenerator.testMisReplicatedRecordRetainedAndUpdated,"  @Test
  public void testMisReplicatedRecordRetainedAndUpdated() {
    // under replicated container
    Set<ContainerReplica> replicas =
        generateReplicas(container, CLOSED, CLOSED, CLOSED);
    when(placementPolicy.validateContainerPlacement(
        Mockito.anyList(), Mockito.anyInt()))
        .thenReturn(new ContainerPlacementStatusDefault(2, 3, 5));
    ContainerHealthStatus status =
        new ContainerHealthStatus(container, replicas, placementPolicy);

    UnhealthyContainersRecord rec = misReplicatedRecord();
    assertTrue(ContainerHealthTask.ContainerHealthRecords
        .retainOrUpdateRecord(status, rec));
    // The record actual count should be updated from 1 -> 2
    assertEquals(2, rec.getActualReplicaCount().intValue());
    assertEquals(1, rec.getReplicaDelta().intValue());
    assertNotNull(rec.getReason());

    // Missing / Over / Mis replicated should not be retained
    assertFalse(ContainerHealthTask.ContainerHealthRecords
        .retainOrUpdateRecord(status, missingRecord()));
    assertFalse(ContainerHealthTask.ContainerHealthRecords
        .retainOrUpdateRecord(status, underReplicatedRecord()));
    assertFalse(ContainerHealthTask.ContainerHealthRecords
        .retainOrUpdateRecord(status, overReplicatedRecord()));

    // Container is now placed OK - should be removed.
    when(placementPolicy.validateContainerPlacement(
        Mockito.anyList(), Mockito.anyInt()))
        .thenReturn(new ContainerPlacementStatusDefault(3, 3, 5));
    status = new ContainerHealthStatus(container, replicas, placementPolicy);
    assertFalse(ContainerHealthTask.ContainerHealthRecords
        .retainOrUpdateRecord(status, rec));
  }
",non-flaky,5
57234,apache_ozone,TestContainerHealthTaskRecordGenerator.testCorrectRecordsGenerated,"  @Test
  public void testCorrectRecordsGenerated() {
    Set<ContainerReplica> replicas =
        generateReplicas(container, CLOSED, CLOSED, CLOSED);

    // HEALTHY container - no records generated.
    ContainerHealthStatus status =
        new ContainerHealthStatus(container, replicas, placementPolicy);
    List<UnhealthyContainers> records =
        ContainerHealthTask.ContainerHealthRecords
            .generateUnhealthyRecords(status, (long)1234567);
    assertEquals(0, records.size());

    // Over-replicated - expect 1 over replicated record
    replicas =
        generateReplicas(container, CLOSED, CLOSED, CLOSED, CLOSED, CLOSED);
    status =
        new ContainerHealthStatus(container, replicas, placementPolicy);
    records = ContainerHealthTask.ContainerHealthRecords
        .generateUnhealthyRecords(status, (long)1234567);
    assertEquals(1, records.size());
    UnhealthyContainers rec = records.get(0);
    assertEquals(UnHealthyContainerStates.OVER_REPLICATED.toString(),
        rec.getContainerState());
    assertEquals(3, rec.getExpectedReplicaCount().intValue());
    assertEquals(5, rec.getActualReplicaCount().intValue());
    assertEquals(-2, rec.getReplicaDelta().intValue());

    // Under and Mis Replicated - expect 2 records - mis and under replicated
    replicas =
        generateReplicas(container, CLOSED, CLOSED);
    when(placementPolicy.validateContainerPlacement(
        Mockito.anyList(), Mockito.anyInt()))
        .thenReturn(new ContainerPlacementStatusDefault(1, 2, 5));
    status =
        new ContainerHealthStatus(container, replicas, placementPolicy);
    records = ContainerHealthTask.ContainerHealthRecords
        .generateUnhealthyRecords(status, (long)1234567);
    assertEquals(2, records.size());

    rec = findRecordForState(records, UnHealthyContainerStates.MIS_REPLICATED);
    assertEquals(UnHealthyContainerStates.MIS_REPLICATED.toString(),
        rec.getContainerState());
    assertEquals(2, rec.getExpectedReplicaCount().intValue());
    assertEquals(1, rec.getActualReplicaCount().intValue());
    assertEquals(1, rec.getReplicaDelta().intValue());
    assertNotNull(rec.getReason());

    rec = findRecordForState(records,
        UnHealthyContainerStates.UNDER_REPLICATED);
    assertEquals(UnHealthyContainerStates.UNDER_REPLICATED.toString(),
        rec.getContainerState());
    assertEquals(3, rec.getExpectedReplicaCount().intValue());
    assertEquals(2, rec.getActualReplicaCount().intValue());
    assertEquals(1, rec.getReplicaDelta().intValue());

    // Missing Record - expect just a single missing record even though
    // it is mis-replicated too
    replicas.clear();
    when(placementPolicy.validateContainerPlacement(
        Mockito.anyList(), Mockito.anyInt()))
        .thenReturn(new ContainerPlacementStatusDefault(1, 2, 5));
    status =
        new ContainerHealthStatus(container, replicas, placementPolicy);
    records = ContainerHealthTask.ContainerHealthRecords
        .generateUnhealthyRecords(status, (long)1234567);
    assertEquals(1, records.size());
    rec = records.get(0);
    assertEquals(UnHealthyContainerStates.MISSING.toString(),
        rec.getContainerState());
    assertEquals(3, rec.getExpectedReplicaCount().intValue());
    assertEquals(0, rec.getActualReplicaCount().intValue());
    assertEquals(3, rec.getReplicaDelta().intValue());
  }
",non-flaky,5
57235,apache_ozone,TestContainerHealthTaskRecordGenerator.testRecordNotGeneratedIfAlreadyExists,"  @Test
  public void testRecordNotGeneratedIfAlreadyExists() {
    Set<String> existingRec = new HashSet<>();
    for (UnHealthyContainerStates s : UnHealthyContainerStates.values()) {
      existingRec.add(s.toString());
    }

    // Over-replicated
    Set<ContainerReplica> replicas = generateReplicas(
        container, CLOSED, CLOSED, CLOSED, CLOSED, CLOSED);
    ContainerHealthStatus status =
        new ContainerHealthStatus(container, replicas, placementPolicy);
    List<UnhealthyContainers> records =
        ContainerHealthTask.ContainerHealthRecords
            .generateUnhealthyRecords(status, existingRec, (long)1234567);
    assertEquals(0, records.size());

    // Missing
    replicas.clear();
    status = new ContainerHealthStatus(container, replicas, placementPolicy);
    records = ContainerHealthTask.ContainerHealthRecords
        .generateUnhealthyRecords(status, existingRec, (long)1234567);
    assertEquals(0, records.size());

    // Under and Mis-Replicated
    replicas = generateReplicas(container, CLOSED, CLOSED);
    when(placementPolicy.validateContainerPlacement(
        Mockito.anyList(), Mockito.anyInt()))
        .thenReturn(new ContainerPlacementStatusDefault(1, 2, 5));
    status = new ContainerHealthStatus(container, replicas, placementPolicy);
    records = ContainerHealthTask.ContainerHealthRecords
        .generateUnhealthyRecords(status, existingRec, (long)1234567);
    assertEquals(0, records.size());
  }
",non-flaky,5
57236,apache_ozone,TestContainerHealthStatus.testHealthyContainer,"  @Test
  public void testHealthyContainer() {
    Set<ContainerReplica> replicas = generateReplicas(container,
        ContainerReplicaProto.State.CLOSED,
        ContainerReplicaProto.State.CLOSED,
        ContainerReplicaProto.State.CLOSED);
    ContainerHealthStatus status =
        new ContainerHealthStatus(container, replicas, placementPolicy);
    assertTrue(status.isHealthy());
    assertFalse(status.isOverReplicated());
    assertFalse(status.isUnderReplicated());
    assertEquals(0, status.replicaDelta());
    assertFalse(status.isMissing());
    assertEquals(false, status.isMisReplicated());
    assertEquals(0, status.misReplicatedDelta());

    assertEquals(container, status.getContainer());
    assertEquals((long)123456, status.getContainerID());
    assertEquals(3, status.getReplicationFactor());
    assertEquals(3, status.getReplicaCount());
  }
",non-flaky,5
57237,apache_ozone,TestContainerHealthStatus.testHealthyContainerWithExtraUnhealthyReplica,"  @Test
  public void testHealthyContainerWithExtraUnhealthyReplica() {
    Set<ContainerReplica> replicas = generateReplicas(container,
        ContainerReplicaProto.State.CLOSED,
        ContainerReplicaProto.State.CLOSED,
        ContainerReplicaProto.State.CLOSED,
        ContainerReplicaProto.State.UNHEALTHY);
    ContainerHealthStatus status =
        new ContainerHealthStatus(container, replicas, placementPolicy);
    assertTrue(status.isHealthy());
    assertFalse(status.isOverReplicated());
    assertFalse(status.isUnderReplicated());
    assertEquals(0, status.replicaDelta());
    assertFalse(status.isMissing());
    assertEquals(false, status.isMisReplicated());
    assertEquals(0, status.misReplicatedDelta());
  }
",non-flaky,5
57238,apache_ozone,TestContainerHealthStatus.testMissingContainer,"  @Test
  public void testMissingContainer() {
    Set<ContainerReplica> replicas = new HashSet<>();
    ContainerHealthStatus status =
        new ContainerHealthStatus(container, replicas, placementPolicy);
    assertFalse(status.isHealthy());
    assertFalse(status.isOverReplicated());
    assertFalse(status.isUnderReplicated());
    assertEquals(3, status.replicaDelta());
    assertTrue(status.isMissing());
    assertEquals(false, status.isMisReplicated());
    assertEquals(0, status.misReplicatedDelta());
  }
",non-flaky,5
57239,apache_ozone,TestContainerHealthStatus.testUnderReplicatedContainer,"  @Test
  public void testUnderReplicatedContainer() {
    Set<ContainerReplica> replicas = generateReplicas(container,
        ContainerReplicaProto.State.CLOSED);
    ContainerHealthStatus status =
        new ContainerHealthStatus(container, replicas, placementPolicy);
    assertFalse(status.isHealthy());
    assertFalse(status.isMissing());
    assertFalse(status.isOverReplicated());
    assertTrue(status.isUnderReplicated());
    assertEquals(2, status.replicaDelta());
    assertEquals(false, status.isMisReplicated());
    assertEquals(0, status.misReplicatedDelta());
  }
",non-flaky,5
57240,apache_ozone,TestContainerHealthStatus.testOverReplicatedContainer,"  @Test
  public void testOverReplicatedContainer() {
    Set<ContainerReplica> replicas = generateReplicas(container,
        ContainerReplicaProto.State.CLOSED,
        ContainerReplicaProto.State.CLOSED,
        ContainerReplicaProto.State.CLOSED,
        ContainerReplicaProto.State.CLOSED);
    ContainerHealthStatus status =
        new ContainerHealthStatus(container, replicas, placementPolicy);
    assertFalse(status.isHealthy());
    assertFalse(status.isMissing());
    assertFalse(status.isUnderReplicated());
    assertTrue(status.isOverReplicated());
    assertEquals(-1, status.replicaDelta());
    assertEquals(false, status.isMisReplicated());
    assertEquals(0, status.misReplicatedDelta());
  }
",non-flaky,5
57241,apache_ozone,TestContainerHealthStatus.testMisReplicated,"  @Test
  public void testMisReplicated() {
    Set<ContainerReplica> replicas = generateReplicas(container,
        ContainerReplicaProto.State.CLOSED,
        ContainerReplicaProto.State.CLOSED,
        ContainerReplicaProto.State.CLOSED);
    when(placementPolicy.validateContainerPlacement(
        Mockito.anyList(), Mockito.anyInt()))
        .thenReturn(new ContainerPlacementStatusDefault(1, 2, 5));
    ContainerHealthStatus status =
        new ContainerHealthStatus(container, replicas, placementPolicy);
    assertFalse(status.isHealthy());
    assertFalse(status.isMissing());
    assertFalse(status.isUnderReplicated());
    assertFalse(status.isOverReplicated());
    assertEquals(0, status.replicaDelta());
    assertTrue(status.isMisReplicated());
    assertEquals(1, status.misReplicatedDelta());
  }
",non-flaky,5
57242,apache_ozone,TestContainerHealthTask.testRun,"  @Test
  public void testRun() throws Exception {
    UnhealthyContainersDao unHealthyContainersTableHandle =
        getDao(UnhealthyContainersDao.class);

    ContainerHealthSchemaManager containerHealthSchemaManager =
        new ContainerHealthSchemaManager(
            getSchemaDefinition(ContainerSchemaDefinition.class),
            unHealthyContainersTableHandle);
    ReconStorageContainerManagerFacade scmMock =
        mock(ReconStorageContainerManagerFacade.class);
    MockPlacementPolicy placementMock = new MockPlacementPolicy();
    ContainerManager containerManagerMock = mock(ContainerManager.class);
    StorageContainerServiceProvider scmClientMock =
        mock(StorageContainerServiceProvider.class);
    ContainerReplica unhealthyReplicaMock = mock(ContainerReplica.class);
    when(unhealthyReplicaMock.getState()).thenReturn(State.UNHEALTHY);
    ContainerReplica healthyReplicaMock = mock(ContainerReplica.class);
    when(healthyReplicaMock.getState()).thenReturn(State.CLOSED);

    // Create 6 containers. The first 5 will have various unhealthy states
    // defined below. The container with ID=6 will be healthy.
    List<ContainerInfo> mockContainers = getMockContainers(6);
    when(scmMock.getScmServiceProvider()).thenReturn(scmClientMock);
    when(scmMock.getContainerManager()).thenReturn(containerManagerMock);
    when(containerManagerMock.getContainers()).thenReturn(mockContainers);
    for (ContainerInfo c : mockContainers) {
      when(containerManagerMock.getContainer(c.containerID())).thenReturn(c);
      when(scmClientMock.getContainerWithPipeline(c.getContainerID()))
          .thenReturn(new ContainerWithPipeline(c, null));
    }
    // Under replicated
    when(containerManagerMock.getContainerReplicas(ContainerID.valueOf(1L)))
        .thenReturn(getMockReplicas(1L, State.CLOSED, State.UNHEALTHY));

    // return all UNHEALTHY replicas for container ID 2 -> UNDER_REPLICATED
    when(containerManagerMock.getContainerReplicas(ContainerID.valueOf(2L)))
        .thenReturn(getMockReplicas(2L, State.UNHEALTHY));

    // return 0 replicas for container ID 3 -> Missing
    when(containerManagerMock.getContainerReplicas(ContainerID.valueOf(3L)))
        .thenReturn(Collections.emptySet());

    // Return 5 Healthy -> Over replicated
    when(containerManagerMock.getContainerReplicas(ContainerID.valueOf(4L)))
        .thenReturn(getMockReplicas(4L, State.CLOSED, State.CLOSED,
        State.CLOSED, State.CLOSED, State.CLOSED));

    // Mis-replicated
    Set<ContainerReplica> misReplicas = getMockReplicas(5L,
        State.CLOSED, State.CLOSED, State.CLOSED);
    placementMock.setMisRepWhenDnPresent(
        misReplicas.iterator().next().getDatanodeDetails().getUuid());
    when(containerManagerMock.getContainerReplicas(ContainerID.valueOf(5L)))
        .thenReturn(misReplicas);

    // Return 3 Healthy -> Healthy container
    when(containerManagerMock.getContainerReplicas(ContainerID.valueOf(6L)))
        .thenReturn(getMockReplicas(6L,
            State.CLOSED, State.CLOSED, State.CLOSED));

    List<UnhealthyContainers> all = unHealthyContainersTableHandle.findAll();
    Assert.assertTrue(all.isEmpty());

    long currentTime = System.currentTimeMillis();
    ReconTaskStatusDao reconTaskStatusDao = getDao(ReconTaskStatusDao.class);
    ReconTaskConfig reconTaskConfig = new ReconTaskConfig();
    reconTaskConfig.setMissingContainerTaskInterval(Duration.ofSeconds(2));
    ContainerHealthTask containerHealthTask =
        new ContainerHealthTask(scmMock.getContainerManager(),
            scmMock.getScmServiceProvider(),
            reconTaskStatusDao, containerHealthSchemaManager,
            placementMock, reconTaskConfig);
    containerHealthTask.start();
    LambdaTestUtils.await(6000, 1000, () ->
        (unHealthyContainersTableHandle.count() == 5));
    UnhealthyContainers rec =
        unHealthyContainersTableHandle.fetchByContainerId(1L).get(0);
    assertEquals(""UNDER_REPLICATED"", rec.getContainerState());
    assertEquals(2, rec.getReplicaDelta().intValue());

    rec = unHealthyContainersTableHandle.fetchByContainerId(2L).get(0);
    assertEquals(""UNDER_REPLICATED"", rec.getContainerState());
    assertEquals(3, rec.getReplicaDelta().intValue());

    List<UnhealthyContainers> unhealthyContainers =
        containerHealthSchemaManager.getUnhealthyContainers(
            ALL_REPLICAS_UNHEALTHY, 0, Integer.MAX_VALUE);
    assertEquals(1, unhealthyContainers.size());
    assertEquals(2L,
        unhealthyContainers.get(0).getContainerId().longValue());
    assertEquals(0,
        unhealthyContainers.get(0).getActualReplicaCount().intValue());

    rec = unHealthyContainersTableHandle.fetchByContainerId(3L).get(0);
    assertEquals(""MISSING"", rec.getContainerState());
    assertEquals(3, rec.getReplicaDelta().intValue());

    rec = unHealthyContainersTableHandle.fetchByContainerId(4L).get(0);
    assertEquals(""OVER_REPLICATED"", rec.getContainerState());
    assertEquals(-2, rec.getReplicaDelta().intValue());

    rec = unHealthyContainersTableHandle.fetchByContainerId(5L).get(0);
    assertEquals(""MIS_REPLICATED"", rec.getContainerState());
    assertEquals(1, rec.getReplicaDelta().intValue());
    assertEquals(2, rec.getExpectedReplicaCount().intValue());
    assertEquals(1, rec.getActualReplicaCount().intValue());
    assertNotNull(rec.getReason());

    ReconTaskStatus taskStatus =
        reconTaskStatusDao.findById(containerHealthTask.getTaskName());
    Assert.assertTrue(taskStatus.getLastUpdatedTimestamp() >
        currentTime);

    // Now run the job again, to check that relevant records are updated or
    // removed as appropriate. Need to adjust the return value for all the mocks
    // Under replicated -> Delta goes from 2 to 1
    when(containerManagerMock.getContainerReplicas(ContainerID.valueOf(1L)))
        .thenReturn(getMockReplicas(1L, State.CLOSED, State.CLOSED));

    // ID 2 was missing - make it healthy now
    when(containerManagerMock.getContainerReplicas(ContainerID.valueOf(2L)))
        .thenReturn(getMockReplicas(2L,
            State.CLOSED, State.CLOSED, State.CLOSED));

    // return 0 replicas for container ID 3 -> Still Missing
    when(containerManagerMock.getContainerReplicas(ContainerID.valueOf(3L)))
        .thenReturn(Collections.emptySet());

    // Return 4 Healthy -> Delta changes from -2 to -1
    when(containerManagerMock.getContainerReplicas(ContainerID.valueOf(4L)))
        .thenReturn(getMockReplicas(4L, State.CLOSED, State.CLOSED,
            State.CLOSED, State.CLOSED));

    // Was mis-replicated - make it healthy now
    placementMock.setMisRepWhenDnPresent(null);

    LambdaTestUtils.await(6000, 1000, () ->
        (unHealthyContainersTableHandle.count() == 3));
    rec = unHealthyContainersTableHandle.fetchByContainerId(1L).get(0);
    assertEquals(""UNDER_REPLICATED"", rec.getContainerState());
    assertEquals(1, rec.getReplicaDelta().intValue());

    // This container is now healthy, it should not be in the table any more
    assertEquals(0,
        unHealthyContainersTableHandle.fetchByContainerId(2L).size());

    rec = unHealthyContainersTableHandle.fetchByContainerId(3L).get(0);

    assertEquals(""MISSING"", rec.getContainerState());
    assertEquals(3, rec.getReplicaDelta().intValue());

    rec = unHealthyContainersTableHandle.fetchByContainerId(4L).get(0);
    assertEquals(""OVER_REPLICATED"", rec.getContainerState());
    assertEquals(-1, rec.getReplicaDelta().intValue());

    // This container is now healthy, it should not be in the table any more
    assertEquals(0,
        unHealthyContainersTableHandle.fetchByContainerId(5L).size());
  }
",non-flaky,5
57243,apache_ozone,TestContainerHealthTask.testDeletedContainer,"  @Test
  public void testDeletedContainer() throws Exception {
    UnhealthyContainersDao unHealthyContainersTableHandle =
        getDao(UnhealthyContainersDao.class);

    ContainerHealthSchemaManager containerHealthSchemaManager =
        new ContainerHealthSchemaManager(
            getSchemaDefinition(ContainerSchemaDefinition.class),
            unHealthyContainersTableHandle);
    ReconStorageContainerManagerFacade scmMock =
        mock(ReconStorageContainerManagerFacade.class);
    MockPlacementPolicy placementMock = new MockPlacementPolicy();
    ContainerManager containerManagerMock = mock(ContainerManager.class);
    StorageContainerServiceProvider scmClientMock =
        mock(StorageContainerServiceProvider.class);

    // Create 2 containers. The first is OPEN will no replicas, the second is
    // CLOSED with no replicas.
    List<ContainerInfo> mockContainers = getMockContainers(2);
    when(scmMock.getScmServiceProvider()).thenReturn(scmClientMock);
    when(scmMock.getContainerManager()).thenReturn(containerManagerMock);
    when(containerManagerMock.getContainers()).thenReturn(mockContainers);
    for (ContainerInfo c : mockContainers) {
      when(containerManagerMock.getContainer(c.containerID())).thenReturn(c);
      when(scmClientMock.getContainerWithPipeline(c.getContainerID()))
          .thenReturn(new ContainerWithPipeline(c, null));
    }
    // Container State OPEN with no replicas
    when(containerManagerMock.getContainer(ContainerID.valueOf(1L)).getState())
        .thenReturn(HddsProtos.LifeCycleState.OPEN);
    when(containerManagerMock.getContainerReplicas(ContainerID.valueOf(1L)))
        .thenReturn(Collections.emptySet());
    when(scmClientMock.getContainerWithPipeline(1))
        .thenReturn(new ContainerWithPipeline(mockContainers.get(0), null));

    // Container State CLOSED with no replicas
    when(containerManagerMock.getContainer(ContainerID.valueOf(2L)).getState())
        .thenReturn(HddsProtos.LifeCycleState.CLOSED);
    when(containerManagerMock.getContainerReplicas(ContainerID.valueOf(2L)))
        .thenReturn(Collections.emptySet());
    ContainerInfo mockDeletedContainer = getMockDeletedContainer(2);
    when(scmClientMock.getContainerWithPipeline(2))
        .thenReturn(new ContainerWithPipeline(mockDeletedContainer, null));

    List<UnhealthyContainers> all = unHealthyContainersTableHandle.findAll();
    Assert.assertTrue(all.isEmpty());

    long currentTime = System.currentTimeMillis();
    ReconTaskStatusDao reconTaskStatusDao = getDao(ReconTaskStatusDao.class);
    ReconTaskConfig reconTaskConfig = new ReconTaskConfig();
    reconTaskConfig.setMissingContainerTaskInterval(Duration.ofSeconds(2));
    ContainerHealthTask containerHealthTask =
        new ContainerHealthTask(scmMock.getContainerManager(),
            scmMock.getScmServiceProvider(),
            reconTaskStatusDao, containerHealthSchemaManager,
            placementMock, reconTaskConfig);
    containerHealthTask.start();
    LambdaTestUtils.await(6000, 1000, () ->
        (unHealthyContainersTableHandle.count() == 1));
    UnhealthyContainers rec =
        unHealthyContainersTableHandle.fetchByContainerId(1L).get(0);
    assertEquals(""MISSING"", rec.getContainerState());
    assertEquals(3, rec.getReplicaDelta().intValue());

    ReconTaskStatus taskStatus =
        reconTaskStatusDao.findById(containerHealthTask.getTaskName());
    Assert.assertTrue(taskStatus.getLastUpdatedTimestamp() >
        currentTime);
  }
",non-flaky,5
57244,apache_ozone,TestReconPipelineReportHandler.testProcessPipelineReport,"  @Test
  public void testProcessPipelineReport() throws IOException {

    // Check with pipeline which does not exist in Recon.
    Pipeline pipeline = getRandomPipeline();
    PipelineID pipelineID = pipeline.getId();
    HddsProtos.PipelineID pipelineIDProto =  pipelineID.getProtobuf();

    ReconPipelineManager reconPipelineManagerMock = mock(
        ReconPipelineManager.class);
    when(reconPipelineManagerMock.getPipeline(pipelineID)).thenReturn(pipeline);

    StorageContainerServiceProvider scmServiceProviderMock = mock(
        StorageContainerServiceProvider.class);
    when(scmServiceProviderMock.getPipeline(pipelineIDProto))
        .thenReturn(pipeline);

    OzoneConfiguration configuration = new OzoneConfiguration();

    ReconPipelineReportHandler handler =
        new ReconPipelineReportHandler(new ReconSafeModeManager(),
            reconPipelineManagerMock, SCMContext.emptyContext(),
            configuration, scmServiceProviderMock);

    EventPublisher eventPublisherMock = mock(EventPublisher.class);
    PipelineReport report = mock(PipelineReport.class);
    when(report.getPipelineID()).thenReturn(pipelineIDProto);

    handler.processPipelineReport(report, pipeline.getNodes().get(0),
        eventPublisherMock);

    // Verify that the new pipeline was added to pipeline manager.
    verify(reconPipelineManagerMock, times(1))
        .addPipeline(pipeline);
    verify(reconPipelineManagerMock, times(1))
        .getPipeline(pipelineID);

    // Check with pipeline which already exists in Recon.
    pipeline = getRandomPipeline();
    pipelineID = pipeline.getId();
    pipelineIDProto =  pipelineID.getProtobuf();

    when(reconPipelineManagerMock.containsPipeline(pipelineID))
        .thenReturn(true);
    when(reconPipelineManagerMock.getPipeline(pipelineID))
        .thenReturn(pipeline);
    when(report.getPipelineID()).thenReturn(pipelineIDProto);

    handler.processPipelineReport(report, pipeline.getNodes().get(0),
        eventPublisherMock);

    // Verify that the pipeline was not added to pipeline manager.
    verify(reconPipelineManagerMock, times(0))
        .addPipeline(pipeline);
    verify(reconPipelineManagerMock, times(1))
        .getPipeline(pipelineID);
  }
",non-flaky,5
57245,apache_ozone,TestReconIncrementalContainerReportHandler.testProcessICR,"  @Test
  public void testProcessICR() throws IOException, NodeNotFoundException {

    ContainerID containerID = ContainerID.valueOf(100L);
    DatanodeDetails datanodeDetails = randomDatanodeDetails();
    IncrementalContainerReportFromDatanode reportMock =
        mock(IncrementalContainerReportFromDatanode.class);
    when(reportMock.getDatanodeDetails()).thenReturn(datanodeDetails);
    IncrementalContainerReportProto containerReport =
        getIncrementalContainerReportProto(containerID,
            State.OPEN,
            datanodeDetails.getUuidString());
    when(reportMock.getReport()).thenReturn(containerReport);

    final String path =
        GenericTestUtils.getTempPath(UUID.randomUUID().toString());
    Path scmPath = Paths.get(path, ""scm-meta"");
    final OzoneConfiguration conf = new OzoneConfiguration();
    conf.set(HddsConfigKeys.OZONE_METADATA_DIRS, scmPath.toString());
    NetworkTopology clusterMap = new NetworkTopologyImpl(conf);
    EventQueue eventQueue = new EventQueue();
    SCMStorageConfig storageConfig = new SCMStorageConfig(conf);
    this.versionManager =
        Mockito.mock(HDDSLayoutVersionManager.class);
    Mockito.when(versionManager.getMetadataLayoutVersion())
        .thenReturn(maxLayoutVersion());
    Mockito.when(versionManager.getSoftwareLayoutVersion())
        .thenReturn(maxLayoutVersion());

    NodeManager nodeManager = new SCMNodeManager(conf, storageConfig,
        eventQueue, clusterMap, SCMContext.emptyContext(), versionManager);

    nodeManager.register(datanodeDetails, null, null);

    ReconContainerManager containerManager = getContainerManager();
    ReconIncrementalContainerReportHandler reconIcr =
        new ReconIncrementalContainerReportHandler(nodeManager,
            containerManager, SCMContext.emptyContext());
    EventPublisher eventPublisherMock = mock(EventPublisher.class);

    reconIcr.onMessage(reportMock, eventPublisherMock);
    nodeManager.addContainer(datanodeDetails, containerID);
    assertTrue(containerManager.containerExist(containerID));
    assertEquals(1, containerManager.getContainerReplicas(containerID).size());
    assertEquals(OPEN, containerManager.getContainer(containerID).getState());
  }
",non-flaky,5
57246,apache_ozone,TestReconIncrementalContainerReportHandler.testProcessICRStateMismatch,"  @Test
  public void testProcessICRStateMismatch() throws IOException {

    // Recon container state is ""OPEN"".
    // Replica state could be any Non OPEN state.
    long containerId = 11;
    for (State state : Arrays.asList(State.CLOSING, State.QUASI_CLOSED,
        State.CLOSED)) {
      ContainerWithPipeline containerWithPipeline = getTestContainer(
          containerId++, OPEN);
      ContainerID containerID =
          containerWithPipeline.getContainerInfo().containerID();

      ReconContainerManager containerManager = getContainerManager();
      containerManager.addNewContainer(containerWithPipeline);

      DatanodeDetails datanodeDetails =
          containerWithPipeline.getPipeline().getFirstNode();
      NodeManager nodeManagerMock = mock(NodeManager.class);
      when(nodeManagerMock.getNodeByUuid(any())).thenReturn(datanodeDetails);
      IncrementalContainerReportFromDatanode reportMock =
          mock(IncrementalContainerReportFromDatanode.class);
      when(reportMock.getDatanodeDetails())
          .thenReturn(containerWithPipeline.getPipeline().getFirstNode());

      IncrementalContainerReportProto containerReport =
          getIncrementalContainerReportProto(containerID, state,
              datanodeDetails.getUuidString());
      when(reportMock.getReport()).thenReturn(containerReport);
      ReconIncrementalContainerReportHandler reconIcr =
          new ReconIncrementalContainerReportHandler(nodeManagerMock,
              containerManager, SCMContext.emptyContext());

      reconIcr.onMessage(reportMock, mock(EventPublisher.class));
      assertTrue(containerManager.containerExist(containerID));
      assertEquals(1,
          containerManager.getContainerReplicas(containerID).size());
      LifeCycleState expectedState = getContainerStateFromReplicaState(state);
      LifeCycleState actualState =
          containerManager.getContainer(containerID).getState();
      assertEquals(String.format(""Expecting %s in "" +
              ""container state for replica state %s"", expectedState,
          state), expectedState, actualState);
    }
  }
",non-flaky,5
57247,apache_ozone,TestReconNodeManager.testReconNodeDB,"  @Test
  public void testReconNodeDB() throws IOException, NodeNotFoundException {
    ReconStorageConfig scmStorageConfig = new ReconStorageConfig(conf);
    EventQueue eventQueue = new EventQueue();
    NetworkTopology clusterMap = new NetworkTopologyImpl(conf);
    Table<UUID, DatanodeDetails> nodeTable =
        ReconSCMDBDefinition.NODES.getTable(store);
    ReconNodeManager reconNodeManager = new ReconNodeManager(conf,
        scmStorageConfig, eventQueue, clusterMap, nodeTable, versionManager);
    ReconNewNodeHandler reconNewNodeHandler =
        new ReconNewNodeHandler(reconNodeManager);
    assertTrue(reconNodeManager.getAllNodes().isEmpty());

    DatanodeDetails datanodeDetails = randomDatanodeDetails();
    String uuidString = datanodeDetails.getUuidString();

    // Register a random datanode.
    reconNodeManager.register(datanodeDetails, null, null);
    reconNewNodeHandler.onMessage(reconNodeManager.getNodeByUuid(uuidString),
        null);

    assertEquals(1, reconNodeManager.getAllNodes().size());
    assertNotNull(reconNodeManager.getNodeByUuid(uuidString));

    // If any commands are added to the eventQueue without using the onMessage
    // interface, then they should be filtered out and not returned to the DN
    // when it heartbeats.
    // This command should never be returned by Recon
    reconNodeManager.addDatanodeCommand(datanodeDetails.getUuid(),
        new SetNodeOperationalStateCommand(1234,
        DECOMMISSIONING, 0));

    // This one should be returned
    reconNodeManager.addDatanodeCommand(datanodeDetails.getUuid(),
        new ReregisterCommand());

    // OperationalState sanity check
    final DatanodeDetails dnDetails =
        reconNodeManager.getNodeByUuid(datanodeDetails.getUuidString());
    assertEquals(HddsProtos.NodeOperationalState.IN_SERVICE,
        dnDetails.getPersistedOpState());
    assertEquals(dnDetails.getPersistedOpState(),
        reconNodeManager.getNodeStatus(dnDetails)
            .getOperationalState());
    assertEquals(dnDetails.getPersistedOpStateExpiryEpochSec(),
        reconNodeManager.getNodeStatus(dnDetails)
            .getOpStateExpiryEpochSeconds());

    // Upon processing the heartbeat, the illegal command should be filtered out
    List<SCMCommand> returnedCmds =
        reconNodeManager.processHeartbeat(datanodeDetails,
            defaultLayoutVersionProto());
    assertEquals(1, returnedCmds.size());
    assertEquals(SCMCommandProto.Type.reregisterCommand,
        returnedCmds.get(0).getType());

    // Now feed a DECOMMISSIONED heartbeat of the same DN
    datanodeDetails.setPersistedOpState(
        HddsProtos.NodeOperationalState.DECOMMISSIONED);
    datanodeDetails.setPersistedOpStateExpiryEpochSec(12345L);
    reconNodeManager.processHeartbeat(datanodeDetails,
        defaultLayoutVersionProto());
    // Check both persistedOpState and NodeStatus#operationalState
    assertEquals(HddsProtos.NodeOperationalState.DECOMMISSIONED,
        dnDetails.getPersistedOpState());
    assertEquals(dnDetails.getPersistedOpState(),
        reconNodeManager.getNodeStatus(dnDetails)
            .getOperationalState());
    assertEquals(12345L, dnDetails.getPersistedOpStateExpiryEpochSec());
    assertEquals(dnDetails.getPersistedOpStateExpiryEpochSec(),
        reconNodeManager.getNodeStatus(dnDetails)
            .getOpStateExpiryEpochSeconds());

    // Close the DB, and recreate the instance of Recon Node Manager.
    eventQueue.close();
    reconNodeManager.close();
    reconNodeManager = new ReconNodeManager(conf, scmStorageConfig, eventQueue,
        clusterMap, nodeTable, versionManager);

    // Verify that the node information was persisted and loaded back.
    assertEquals(1, reconNodeManager.getAllNodes().size());
    assertNotNull(
        reconNodeManager.getNodeByUuid(datanodeDetails.getUuidString()));
  }
",non-flaky,5
57248,apache_ozone,TestReconNodeManager.testUpdateNodeOperationalStateFromScm,"  @Test
  public void testUpdateNodeOperationalStateFromScm() throws Exception {
    ReconStorageConfig scmStorageConfig = new ReconStorageConfig(conf);
    EventQueue eventQueue = new EventQueue();
    NetworkTopology clusterMap = new NetworkTopologyImpl(conf);
    Table<UUID, DatanodeDetails> nodeTable =
        ReconSCMDBDefinition.NODES.getTable(store);
    ReconNodeManager reconNodeManager = new ReconNodeManager(conf,
        scmStorageConfig, eventQueue, clusterMap, nodeTable, versionManager);


    DatanodeDetails datanodeDetails = randomDatanodeDetails();
    HddsProtos.Node node = mock(HddsProtos.Node.class);

    LambdaTestUtils.intercept(NodeNotFoundException.class, () -> {
      reconNodeManager.updateNodeOperationalStateFromScm(node, datanodeDetails);
    });

    reconNodeManager.register(datanodeDetails, null, null);
    assertEquals(IN_SERVICE, reconNodeManager
        .getNodeByUuid(datanodeDetails.getUuidString()).getPersistedOpState());

    when(node.getNodeOperationalStates(eq(0)))
        .thenReturn(DECOMMISSIONING);
    reconNodeManager.updateNodeOperationalStateFromScm(node, datanodeDetails);
    assertEquals(DECOMMISSIONING, reconNodeManager
        .getNodeByUuid(datanodeDetails.getUuidString()).getPersistedOpState());
    List<DatanodeDetails> nodes =
        reconNodeManager.getNodes(DECOMMISSIONING, null);
    assertEquals(1, nodes.size());
    assertEquals(datanodeDetails.getUuid(), nodes.get(0).getUuid());
  }
",non-flaky,5
57249,apache_ozone,TestReconNodeManager.testDatanodeUpdate,"  @Test
  public void testDatanodeUpdate() throws IOException {
    ReconStorageConfig scmStorageConfig = new ReconStorageConfig(conf);
    EventQueue eventQueue = new EventQueue();
    NetworkTopology clusterMap = new NetworkTopologyImpl(conf);
    Table<UUID, DatanodeDetails> nodeTable =
        ReconSCMDBDefinition.NODES.getTable(store);
    ReconNodeManager reconNodeManager = new ReconNodeManager(conf,
        scmStorageConfig, eventQueue, clusterMap, nodeTable, versionManager);
    ReconNewNodeHandler reconNewNodeHandler =
        new ReconNewNodeHandler(reconNodeManager);
    assertTrue(reconNodeManager.getAllNodes().isEmpty());

    DatanodeDetails datanodeDetails = randomDatanodeDetails();
    datanodeDetails.setHostName(""hostname1"");
    String uuidString = datanodeDetails.getUuidString();

    // Register ""hostname1"" datanode.
    reconNodeManager.register(datanodeDetails, null, null);
    reconNewNodeHandler.onMessage(reconNodeManager.getNodeByUuid(uuidString),
        null);

    assertEquals(1, reconNodeManager.getAllNodes().size());
    assertNotNull(reconNodeManager.getNodeByUuid(uuidString));
    assertEquals(""hostname1"",
        reconNodeManager.getNodeByUuid(uuidString).getHostName());

    datanodeDetails.setHostName(""hostname2"");
    // Upon processing the heartbeat, the illegal command should be filtered out
    List<SCMCommand> returnedCmds =
        reconNodeManager.processHeartbeat(datanodeDetails,
            defaultLayoutVersionProto());
    assertEquals(1, returnedCmds.size());
    assertEquals(SCMCommandProto.Type.reregisterCommand,
        returnedCmds.get(0).getType());

  }
",non-flaky,5
57250,apache_ozone,TestReconContainerManager.testAddNewOpenContainer,"  @Test
  public void testAddNewOpenContainer() throws IOException {
    ContainerWithPipeline containerWithPipeline =
        getTestContainer(LifeCycleState.OPEN);
    ContainerID containerID =
        containerWithPipeline.getContainerInfo().containerID();
    ContainerInfo containerInfo = containerWithPipeline.getContainerInfo();

    ReconContainerManager containerManager = getContainerManager();
    assertFalse(containerManager.containerExist(containerID));
    assertFalse(getContainerTable().isExist(containerID));

    containerManager.addNewContainer(containerWithPipeline);

    assertTrue(containerManager.containerExist(containerID));

    List<ContainerInfo> containers =
        containerManager.getContainers(LifeCycleState.OPEN);
    assertEquals(1, containers.size());
    assertEquals(containerInfo, containers.get(0));
    NavigableSet<ContainerID> containersInPipeline =
        getPipelineManager().getContainersInPipeline(
            containerWithPipeline.getPipeline().getId());
    assertEquals(1, containersInPipeline.size());
    assertEquals(containerID, containersInPipeline.first());

    // Verify container DB.
    SCMHAManager scmhaManager = containerManager.getSCMHAManager();
    scmhaManager.getDBTransactionBuffer().close();
    assertTrue(getContainerTable().isExist(containerID));
  }
",non-flaky,5
57251,apache_ozone,TestReconContainerManager.testAddNewClosedContainer,"  @Test
  public void testAddNewClosedContainer() throws IOException {
    ContainerWithPipeline containerWithPipeline = getTestContainer(CLOSED);
    ContainerID containerID =
        containerWithPipeline.getContainerInfo().containerID();
    ContainerInfo containerInfo = containerWithPipeline.getContainerInfo();

    ReconContainerManager containerManager = getContainerManager();
    assertFalse(containerManager.containerExist(containerID));
    assertFalse(getContainerTable().isExist(containerID));

    containerManager.addNewContainer(containerWithPipeline);

    assertTrue(containerManager.containerExist(containerID));

    List<ContainerInfo> containers = containerManager.getContainers(CLOSED);
    assertEquals(1, containers.size());
    assertEquals(containerInfo, containers.get(0));
    // Verify container DB.
    SCMHAManager scmhaManager = containerManager.getSCMHAManager();
    scmhaManager.getDBTransactionBuffer().close();
    assertTrue(getContainerTable().isExist(containerID));
  }
",non-flaky,5
57252,apache_ozone,TestReconContainerManager.testCheckAndAddNewContainer,"  @Test
  public void testCheckAndAddNewContainer() throws Exception {
    ContainerID containerID = ContainerID.valueOf(100L);
    ReconContainerManager containerManager = getContainerManager();
    assertFalse(containerManager.containerExist(containerID));
    DatanodeDetails datanodeDetails = randomDatanodeDetails();
    containerManager.checkAndAddNewContainer(containerID,
        OPEN, datanodeDetails);
    assertTrue(containerManager.containerExist(containerID));

    // Doing it one more time should not change any state.
    containerManager.checkAndAddNewContainer(containerID, OPEN,
        datanodeDetails);
    assertTrue(containerManager.containerExist(containerID));
    assertEquals(LifeCycleState.OPEN,
        getContainerManager().getContainer(containerID).getState());
  }
",non-flaky,5
57253,apache_ozone,TestReconContainerManager.testCheckAndAddNewContainerBatch,"  @Test
  public void testCheckAndAddNewContainerBatch() throws IOException {
    List<ContainerReplicaProto> containerReplicaProtoList = new LinkedList<>();
    ReconContainerManager containerManager = getContainerManager();
    State[] stateTypes = State.values();
    LifeCycleState[] lifeCycleStateTypes = LifeCycleState.values();
    int lifeCycleStateCount = lifeCycleStateTypes.length;
    for (int i = 200; i < 300; i++) {
      assertFalse(containerManager.containerExist(ContainerID.valueOf(i)));
      ContainerReplicaProto.Builder ciBuilder =
          ContainerReplicaProto.newBuilder();
      ContainerReplicaProto crp = ciBuilder.
          setContainerID(i).
          setState(stateTypes[i % lifeCycleStateCount]).build();
      containerReplicaProtoList.add(crp);
    }

    containerManager.checkAndAddNewContainerBatch(containerReplicaProtoList);
    for (long i = 200L; i < 300L; i++) {
      assertTrue(containerManager.containerExist(ContainerID.valueOf(i)));
    }

    // Doing it one more time should not change any state.
    containerManager.checkAndAddNewContainerBatch(containerReplicaProtoList);
    for (int i = 200; i < 300; i++) {
      assertTrue(containerManager.containerExist(ContainerID.valueOf(i)));
      assertEquals(lifeCycleStateTypes[i % lifeCycleStateCount],
          getContainerManager().
            getContainer(ContainerID.valueOf(i)).getState());
    }
  }
",non-flaky,5
57254,apache_ozone,TestReconContainerManager.testUpdateContainerStateFromOpen,"  @Test
  public void testUpdateContainerStateFromOpen() throws Exception {
    ContainerWithPipeline containerWithPipeline =
        getTestContainer(LifeCycleState.OPEN);
    ContainerID containerID =
        containerWithPipeline.getContainerInfo().containerID();

    // Adding container #100.
    getContainerManager().addNewContainer(containerWithPipeline);
    assertEquals(LifeCycleState.OPEN,
        getContainerManager().getContainer(containerID).getState());

    DatanodeDetails datanodeDetails = randomDatanodeDetails();

    // First report with ""CLOSED"" replica state moves container state to
    // ""CLOSING"".
    getContainerManager().checkAndAddNewContainer(containerID, State.CLOSED,
        datanodeDetails);
    assertEquals(CLOSING,
        getContainerManager().getContainer(containerID).getState());
  }
",non-flaky,5
57255,apache_ozone,TestReconContainerManager.testUpdateAndRemoveContainerReplica,"  @Test
  public void testUpdateAndRemoveContainerReplica() throws IOException {
    // Sanity checking updateContainerReplica and ContainerReplicaHistory

    // Init Container 1
    final long cIDlong1 = 1L;
    final ContainerID containerID1 = ContainerID.valueOf(cIDlong1);

    // Init DN01
    final UUID uuid1 = UUID.randomUUID();
    final DatanodeDetails datanodeDetails1 = DatanodeDetails.newBuilder()
        .setUuid(uuid1).setHostName(""host1"").setIpAddress(""127.0.0.1"").build();
    ContainerReplica containerReplica1 = ContainerReplica.newBuilder()
        .setContainerID(containerID1).setContainerState(State.OPEN)
        .setDatanodeDetails(datanodeDetails1).setSequenceId(1001L).build();

    final ReconContainerManager containerManager = getContainerManager();
    final Map<Long, Map<UUID, ContainerReplicaHistory>> repHistMap =
        containerManager.getReplicaHistoryMap();
    // Should be empty at the beginning
    Assert.assertEquals(0, repHistMap.size());

    // Put a replica info and call updateContainerReplica
    Pipeline pipeline = getRandomPipeline();
    getPipelineManager().addPipeline(pipeline);
    for (int i = 1; i <= 10; i++) {
      final ContainerInfo info = newContainerInfo(i, pipeline);
      containerManager.addNewContainer(
          new ContainerWithPipeline(info, pipeline));
    }

    containerManager.updateContainerReplica(containerID1, containerReplica1);
    // Should have 1 container entry in the replica history map
    Assert.assertEquals(1, repHistMap.size());
    // Should only have 1 entry for this replica (on DN01)
    Assert.assertEquals(1, repHistMap.get(cIDlong1).size());
    ContainerReplicaHistory repHist1 = repHistMap.get(cIDlong1).get(uuid1);
    Assert.assertEquals(uuid1, repHist1.getUuid());
    // Because this is a new entry, first seen time equals last seen time
    assertEquals(repHist1.getLastSeenTime(), repHist1.getFirstSeenTime());
    assertEquals(containerReplica1.getSequenceId().longValue(),
        repHist1.getBcsId());

    // Let's update the entry again
    containerReplica1 = ContainerReplica.newBuilder()
        .setContainerID(containerID1).setContainerState(State.OPEN)
        .setDatanodeDetails(datanodeDetails1).setSequenceId(1051L).build();
    containerManager.updateContainerReplica(containerID1, containerReplica1);
    // Should still have 1 entry in the replica history map
    Assert.assertEquals(1, repHistMap.size());
    // Now last seen time should be larger than first seen time
    Assert.assertTrue(repHist1.getLastSeenTime() > repHist1.getFirstSeenTime());
    assertEquals(1051L, repHist1.getBcsId());

    // Init DN02
    final UUID uuid2 = UUID.randomUUID();
    final DatanodeDetails datanodeDetails2 = DatanodeDetails.newBuilder()
        .setUuid(uuid2).setHostName(""host2"").setIpAddress(""127.0.0.2"").build();
    final ContainerReplica containerReplica2 = ContainerReplica.newBuilder()
        .setContainerID(containerID1).setContainerState(State.OPEN)
        .setDatanodeDetails(datanodeDetails2).setSequenceId(1051L).build();

    // Add replica to DN02
    containerManager.updateContainerReplica(containerID1, containerReplica2);

    // Should still have 1 container entry in the replica history map
    Assert.assertEquals(1, repHistMap.size());
    // Should have 2 entries for this replica (on DN01 and DN02)
    Assert.assertEquals(2, repHistMap.get(cIDlong1).size());
    ContainerReplicaHistory repHist2 = repHistMap.get(cIDlong1).get(uuid2);
    Assert.assertEquals(uuid2, repHist2.getUuid());
    // Because this is a new entry, first seen time equals last seen time
    assertEquals(repHist2.getLastSeenTime(), repHist2.getFirstSeenTime());
    assertEquals(1051L, repHist2.getBcsId());

    // Remove replica from DN01
    containerManager.removeContainerReplica(containerID1, containerReplica1);
    // Should still have 1 container entry in the replica history map
    Assert.assertEquals(1, repHistMap.size());
    // Should have 1 entry for this replica
    Assert.assertEquals(1, repHistMap.get(cIDlong1).size());
    // And the only entry should match DN02
    Assert.assertEquals(uuid2,
        repHistMap.get(cIDlong1).keySet().iterator().next());
  }
",non-flaky,5
57256,apache_ozone,TestReconPipelineManager.testInitialize,"  @Test
  public void testInitialize() throws IOException {

    // Get 3 OPEN pipelines from SCM.
    List<Pipeline> pipelinesFromScm = getPipelines(3);

    // Recon has 2 pipelines in ALLOCATED state. (1 is valid and 1 is obsolete)

    // Valid pipeline in Allocated state.
    Pipeline validPipeline = Pipeline.newBuilder()
        .setReplicationConfig(
            new StandaloneReplicationConfig(ReplicationFactor.ONE))
        .setId(pipelinesFromScm.get(0).getId())
        .setNodes(pipelinesFromScm.get(0).getNodes())
        .setState(Pipeline.PipelineState.ALLOCATED)

        .build();

    // Invalid pipeline.
    Pipeline invalidPipeline = Pipeline.newBuilder()
        .setReplicationConfig(
            new StandaloneReplicationConfig(ReplicationFactor.ONE))
        .setId(PipelineID.randomId())
        .setNodes(Collections.singletonList(randomDatanodeDetails()))
        .setState(Pipeline.PipelineState.CLOSED)
        .build();

    NetworkTopology clusterMap = new NetworkTopologyImpl(conf);
    EventQueue eventQueue = new EventQueue();
    this.versionManager =
        Mockito.mock(HDDSLayoutVersionManager.class);
    Mockito.when(versionManager.getMetadataLayoutVersion())
        .thenReturn(maxLayoutVersion());
    Mockito.when(versionManager.getSoftwareLayoutVersion())
        .thenReturn(maxLayoutVersion());
    NodeManager nodeManager = new SCMNodeManager(conf, scmStorageConfig,
        eventQueue, clusterMap, SCMContext.emptyContext(), versionManager);

    try (ReconPipelineManager reconPipelineManager =
             ReconPipelineManager.newReconPipelineManager(
                 conf,
                 nodeManager,
                 ReconSCMDBDefinition.PIPELINES.getTable(store),
                 eventQueue,
                 scmhaManager,
                 scmContext)) {
      scmContext = new SCMContext.Builder().setIsInSafeMode(true)
              .setLeader(true).setIsPreCheckComplete(true)
              .setSCM(mock(StorageContainerManager.class)).build();
      reconPipelineManager.setScmContext(scmContext);
      reconPipelineManager.addPipeline(validPipeline);
      reconPipelineManager.addPipeline(invalidPipeline);

      reconPipelineManager.initializePipelines(pipelinesFromScm);
      List<Pipeline> newReconPipelines = reconPipelineManager.getPipelines();

      // Test if the number of pipelines in SCM is as expected.
      assertEquals(3, newReconPipelines.size());

      // Test if new pipelines from SCM are picked up.
      for (Pipeline pipeline : pipelinesFromScm) {
        assertTrue(reconPipelineManager.containsPipeline(pipeline.getId()));
      }

      // Test if existing pipeline state is updated.
      assertEquals(Pipeline.PipelineState.OPEN, reconPipelineManager
          .getPipeline(validPipeline.getId()).getPipelineState());

      // Test if obsolete pipelines in Recon are removed.
      assertFalse(reconPipelineManager.containsPipeline(
          invalidPipeline.getId()));
    }
  }
",non-flaky,5
57257,apache_ozone,TestReconPipelineManager.testAddPipeline,"  @Test
  public void testAddPipeline() throws IOException {

    Pipeline pipeline = getRandomPipeline();
    NetworkTopology clusterMap = new NetworkTopologyImpl(conf);
    EventQueue eventQueue = new EventQueue();
    this.versionManager =
        Mockito.mock(HDDSLayoutVersionManager.class);
    Mockito.when(versionManager.getMetadataLayoutVersion())
        .thenReturn(maxLayoutVersion());
    Mockito.when(versionManager.getSoftwareLayoutVersion())
        .thenReturn(maxLayoutVersion());
    NodeManager nodeManager = new SCMNodeManager(conf, scmStorageConfig,
        eventQueue, clusterMap, SCMContext.emptyContext(), versionManager);

    ReconPipelineManager reconPipelineManager =
        ReconPipelineManager.newReconPipelineManager(
            conf,
            nodeManager,
            ReconSCMDBDefinition.PIPELINES.getTable(store),
            eventQueue,
            scmhaManager,
            scmContext);

    assertFalse(reconPipelineManager.containsPipeline(pipeline.getId()));
    reconPipelineManager.addPipeline(pipeline);
    assertTrue(reconPipelineManager.containsPipeline(pipeline.getId()));
  }
",non-flaky,5
57258,apache_ozone,TestReconPipelineManager.testStubbedReconPipelineFactory,"  @Test
  public void testStubbedReconPipelineFactory() throws IOException {

    NodeManager nodeManagerMock = mock(NodeManager.class);

    ReconPipelineManager reconPipelineManager =
        ReconPipelineManager.newReconPipelineManager(
            conf,
            nodeManagerMock,
            ReconSCMDBDefinition.PIPELINES.getTable(store),
            new EventQueue(),
            scmhaManager,
            scmContext);

    PipelineFactory pipelineFactory = reconPipelineManager.getPipelineFactory();
    assertTrue(pipelineFactory instanceof ReconPipelineFactory);
    ReconPipelineFactory reconPipelineFactory =
        (ReconPipelineFactory) pipelineFactory;
    assertTrue(reconPipelineFactory.getProviders().isEmpty());
    for (ReplicationType type  : reconPipelineFactory.getProviders().keySet()) {
      PipelineProvider pipelineProvider =
          reconPipelineFactory.getProviders().get(type);
      assertTrue(pipelineProvider instanceof ReconPipelineProvider);
    }
  }
",non-flaky,5
57259,apache_ozone,TestAdminFilter.testAdminOnlyEndpoints,"  @Test
  public void testAdminOnlyEndpoints() {
    // Get all classes with @Path annotation anywhere in recon.
    Reflections reflections = new Reflections(
        ""org.apache.hadoop.ozone.recon"",
        new TypeAnnotationsScanner(),
        new SubTypesScanner());
    Set<Class<?>> allEndpoints =
        reflections.getTypesAnnotatedWith(Path.class);

    Assert.assertFalse(allEndpoints.isEmpty());

    // If an endpoint is added, it must be explicitly added to this set or be
    // marked with @AdminOnly for this test to pass.
    Set<Class<?>> nonAdminEndpoints = new HashSet<>();
    nonAdminEndpoints.add(UtilizationEndpoint.class);
    nonAdminEndpoints.add(ClusterStateEndpoint.class);
    nonAdminEndpoints.add(MetricsProxyEndpoint.class);
    nonAdminEndpoints.add(NodeEndpoint.class);
    nonAdminEndpoints.add(PipelineEndpoint.class);
    nonAdminEndpoints.add(TaskStatusService.class);

    Assert.assertTrue(allEndpoints.containsAll(nonAdminEndpoints));

    Set<Class<?>> adminEndpoints = Sets.difference(allEndpoints,
        nonAdminEndpoints);

    for (Class<?> endpoint: nonAdminEndpoints) {
      Assert.assertFalse(String.format(""Endpoint class %s has been "" +
              ""declared as non admin in this test, but is marked as "" +
              ""@AdminOnly."", endpoint),
          endpoint.isAnnotationPresent(AdminOnly.class));
    }

    for (Class<?> endpoint: adminEndpoints) {
      Assert.assertTrue(String.format(""Endpoint class %s must be marked as "" +
              ""@AdminOnly or explicitly declared as non admin in this test."",
          endpoint),
          endpoint.isAnnotationPresent(AdminOnly.class));
    }
  }
",non-flaky,5
57260,apache_ozone,TestAdminFilter.testAdminFilterOzoneAdminsOnly,"  @Test
  public void testAdminFilterOzoneAdminsOnly() throws Exception {
    OzoneConfiguration conf = new OzoneConfiguration();
    conf.setStrings(OzoneConfigKeys.OZONE_ADMINISTRATORS, ""ozone"");
    testAdminFilterWithPrincipal(conf, ""ozone"", true);
    testAdminFilterWithPrincipal(conf, ""reject"", false);

    conf.setStrings(OzoneConfigKeys.OZONE_ADMINISTRATORS,
        OzoneConfigKeys.OZONE_ADMINISTRATORS_WILDCARD);
    testAdminFilterWithPrincipal(conf, ""other"", true);
  }
",non-flaky,5
57261,apache_ozone,TestAdminFilter.testAdminFilterReconAdminsOnly,"  @Test
  public void testAdminFilterReconAdminsOnly() throws Exception {
    OzoneConfiguration conf = new OzoneConfiguration();
    conf.setStrings(ReconConfigKeys.OZONE_RECON_ADMINISTRATORS, ""recon"");
    testAdminFilterWithPrincipal(conf, ""recon"", true);
    testAdminFilterWithPrincipal(conf, ""reject"", false);

    conf.setStrings(ReconConfigKeys.OZONE_RECON_ADMINISTRATORS,
        OzoneConfigKeys.OZONE_ADMINISTRATORS_WILDCARD);
    testAdminFilterWithPrincipal(conf, ""other"", true);
  }
",non-flaky,5
57262,apache_ozone,TestAdminFilter.testAdminFilterOzoneAndReconAdmins,"  @Test
  public void testAdminFilterOzoneAndReconAdmins() throws Exception {
    OzoneConfiguration conf = new OzoneConfiguration();
    conf.setStrings(OzoneConfigKeys.OZONE_ADMINISTRATORS, ""ozone"");
    conf.setStrings(ReconConfigKeys.OZONE_RECON_ADMINISTRATORS, ""recon"");
    testAdminFilterWithPrincipal(conf, ""ozone"", true);
    testAdminFilterWithPrincipal(conf, ""recon"", true);
    testAdminFilterWithPrincipal(conf, ""reject"", false);

    conf.setStrings(OzoneConfigKeys.OZONE_ADMINISTRATORS,
        OzoneConfigKeys.OZONE_ADMINISTRATORS_WILDCARD);
    conf.setStrings(ReconConfigKeys.OZONE_RECON_ADMINISTRATORS,
        OzoneConfigKeys.OZONE_ADMINISTRATORS_WILDCARD);
    testAdminFilterWithPrincipal(conf, ""other"", true);
  }
",non-flaky,5
57263,apache_ozone,TestAdminFilter.testAdminFilterNoAdmins,"  @Test
  public void testAdminFilterNoAdmins() throws Exception {
    testAdminFilterWithPrincipal(new OzoneConfiguration(), ""reject"", false);
  }
",non-flaky,5
57264,apache_ozone,TestNSSummaryEndpoint.testUtility,"  @Test
  public void testUtility() {
    String[] names = NSSummaryEndpoint.parseRequestPath(TEST_PATH_UTILITY);
    Assert.assertArrayEquals(TEST_NAMES, names);
    String keyName = NSSummaryEndpoint.getKeyName(names);
    Assert.assertEquals(TEST_KEY_NAMES, keyName);
    String subpath = NSSummaryEndpoint.buildSubpath(PARENT_DIR, ""file1.txt"");
    Assert.assertEquals(TEST_PATH_UTILITY, subpath);
  }
",non-flaky,5
57265,apache_ozone,TestNSSummaryEndpoint.testBasic,"  @Test
  public void testBasic() throws Exception {
    // Test volume basics
    Response volResponse = nsSummaryEndpoint.getBasicInfo(VOL_PATH);
    NamespaceSummaryResponse volResponseObj =
            (NamespaceSummaryResponse) volResponse.getEntity();
    Assert.assertEquals(EntityType.VOLUME, volResponseObj.getEntityType());
    Assert.assertEquals(2, volResponseObj.getNumBucket());
    Assert.assertEquals(4, volResponseObj.getNumTotalDir());
    Assert.assertEquals(6, volResponseObj.getNumTotalKey());

    // Test bucket 1's basics
    Response bucketOneResponse =
            nsSummaryEndpoint.getBasicInfo(BUCKET_ONE_PATH);
    NamespaceSummaryResponse bucketOneObj =
            (NamespaceSummaryResponse) bucketOneResponse.getEntity();
    Assert.assertEquals(EntityType.BUCKET, bucketOneObj.getEntityType());
    Assert.assertEquals(4, bucketOneObj.getNumTotalDir());
    Assert.assertEquals(4, bucketOneObj.getNumTotalKey());

    // Test bucket 2's basics
    Response bucketTwoResponse =
            nsSummaryEndpoint.getBasicInfo(BUCKET_TWO_PATH);
    NamespaceSummaryResponse bucketTwoObj =
            (NamespaceSummaryResponse) bucketTwoResponse.getEntity();
    Assert.assertEquals(EntityType.BUCKET, bucketTwoObj.getEntityType());
    Assert.assertEquals(0, bucketTwoObj.getNumTotalDir());
    Assert.assertEquals(2, bucketTwoObj.getNumTotalKey());

    // Test intermediate directory basics
    Response dirOneResponse = nsSummaryEndpoint.getBasicInfo(DIR_ONE_PATH);
    NamespaceSummaryResponse dirOneObj =
            (NamespaceSummaryResponse) dirOneResponse.getEntity();
    Assert.assertEquals(EntityType.DIRECTORY, dirOneObj.getEntityType());
    Assert.assertEquals(3, dirOneObj.getNumTotalDir());
    Assert.assertEquals(3, dirOneObj.getNumTotalKey());

    // Test invalid path
    Response invalidResponse = nsSummaryEndpoint.getBasicInfo(INVALID_PATH);
    NamespaceSummaryResponse invalidObj =
            (NamespaceSummaryResponse) invalidResponse.getEntity();
    Assert.assertEquals(ResponseStatus.PATH_NOT_FOUND,
            invalidObj.getStatus());

    // Test key
    Response keyResponse = nsSummaryEndpoint.getBasicInfo(KEY_PATH);
    NamespaceSummaryResponse keyResObj =
            (NamespaceSummaryResponse) keyResponse.getEntity();
    Assert.assertEquals(EntityType.KEY, keyResObj.getEntityType());
  }
",non-flaky,5
57266,apache_ozone,TestNSSummaryEndpoint.testDiskUsage,"  @Test
  public void testDiskUsage() throws Exception {
    // volume level DU
    Response volResponse = nsSummaryEndpoint.getDiskUsage(VOL_PATH,
            false, false);
    DUResponse duVolRes = (DUResponse) volResponse.getEntity();
    Assert.assertEquals(2, duVolRes.getCount());
    List<DUResponse.DiskUsage> duData = duVolRes.getDuData();
    // sort based on subpath
    Collections.sort(duData,
            Comparator.comparing(DUResponse.DiskUsage::getSubpath));
    DUResponse.DiskUsage duBucket1 = duData.get(0);
    DUResponse.DiskUsage duBucket2 = duData.get(1);
    Assert.assertEquals(BUCKET_ONE_PATH, duBucket1.getSubpath());
    Assert.assertEquals(BUCKET_TWO_PATH, duBucket2.getSubpath());
    Assert.assertEquals(BUCKET_ONE_DATA_SIZE, duBucket1.getSize());
    Assert.assertEquals(BUCKET_TWO_DATA_SIZE, duBucket2.getSize());

    // bucket level DU
    Response bucketResponse = nsSummaryEndpoint.getDiskUsage(BUCKET_ONE_PATH,
            false, false);
    DUResponse duBucketResponse = (DUResponse) bucketResponse.getEntity();
    Assert.assertEquals(1, duBucketResponse.getCount());
    DUResponse.DiskUsage duDir1 = duBucketResponse.getDuData().get(0);
    Assert.assertEquals(DIR_ONE_PATH, duDir1.getSubpath());
    Assert.assertEquals(DIR_ONE_DATA_SIZE, duDir1.getSize());

    // dir level DU
    Response dirResponse = nsSummaryEndpoint.getDiskUsage(DIR_ONE_PATH,
            false, false);
    DUResponse duDirReponse = (DUResponse) dirResponse.getEntity();
    Assert.assertEquals(3, duDirReponse.getCount());
    List<DUResponse.DiskUsage> duSubDir = duDirReponse.getDuData();
    Collections.sort(duSubDir,
            Comparator.comparing(DUResponse.DiskUsage::getSubpath));
    DUResponse.DiskUsage duDir2 = duSubDir.get(0);
    DUResponse.DiskUsage duDir3 = duSubDir.get(1);
    DUResponse.DiskUsage duDir4 = duSubDir.get(2);
    Assert.assertEquals(DIR_TWO_PATH, duDir2.getSubpath());
    Assert.assertEquals(KEY_TWO_SIZE, duDir2.getSize());

    Assert.assertEquals(DIR_THREE_PATH, duDir3.getSubpath());
    Assert.assertEquals(KEY_THREE_SIZE, duDir3.getSize());

    Assert.assertEquals(DIR_FOUR_PATH, duDir4.getSubpath());
    Assert.assertEquals(KEY_SIX_SIZE, duDir4.getSize());

    // key level DU
    Response keyResponse = nsSummaryEndpoint.getDiskUsage(KEY_PATH,
            false, false);
    DUResponse keyObj = (DUResponse) keyResponse.getEntity();
    Assert.assertEquals(0, keyObj.getCount());
    Assert.assertEquals(KEY_FOUR_SIZE, keyObj.getSize());

    // invalid path check
    Response invalidResponse = nsSummaryEndpoint.getDiskUsage(INVALID_PATH,
            false, false);
    DUResponse invalidObj = (DUResponse) invalidResponse.getEntity();
    Assert.assertEquals(ResponseStatus.PATH_NOT_FOUND,
            invalidObj.getStatus());
  }
",non-flaky,5
57267,apache_ozone,TestNSSummaryEndpoint.testDiskUsageWithReplication,"  @Test
  public void testDiskUsageWithReplication() throws Exception {
    setUpMultiBlockKey();
    Response keyResponse = nsSummaryEndpoint.getDiskUsage(MULTI_BLOCK_KEY_PATH,
            false, true);
    DUResponse replicaDUResponse = (DUResponse) keyResponse.getEntity();
    Assert.assertEquals(ResponseStatus.OK, replicaDUResponse.getStatus());
    Assert.assertEquals(MULTI_BLOCK_KEY_SIZE_WITH_REPLICA,
            replicaDUResponse.getSizeWithReplica());
  }
",non-flaky,5
57268,apache_ozone,TestNSSummaryEndpoint.testQuotaUsage,"  @Test
  public void testQuotaUsage() throws Exception {
    // volume level quota usage
    Response volResponse = nsSummaryEndpoint.getQuotaUsage(VOL_PATH);
    QuotaUsageResponse quVolRes = (QuotaUsageResponse) volResponse.getEntity();
    Assert.assertEquals(VOL_QUOTA, quVolRes.getQuota());
    Assert.assertEquals(TOTAL_DATA_SIZE, quVolRes.getQuotaUsed());

    // bucket level quota usage
    Response bucketRes = nsSummaryEndpoint.getQuotaUsage(BUCKET_ONE_PATH);
    QuotaUsageResponse quBucketRes = (QuotaUsageResponse) bucketRes.getEntity();
    Assert.assertEquals(BUCKET_ONE_QUOTA, quBucketRes.getQuota());
    Assert.assertEquals(BUCKET_ONE_DATA_SIZE, quBucketRes.getQuotaUsed());

    Response bucketRes2 = nsSummaryEndpoint.getQuotaUsage(BUCKET_TWO_PATH);
    QuotaUsageResponse quBucketRes2 =
            (QuotaUsageResponse) bucketRes2.getEntity();
    Assert.assertEquals(BUCKET_TWO_QUOTA, quBucketRes2.getQuota());
    Assert.assertEquals(BUCKET_TWO_DATA_SIZE, quBucketRes2.getQuotaUsed());

    // other level not applicable
    Response naResponse1 = nsSummaryEndpoint.getQuotaUsage(DIR_ONE_PATH);
    QuotaUsageResponse quotaUsageResponse1 =
            (QuotaUsageResponse) naResponse1.getEntity();
    Assert.assertEquals(ResponseStatus.TYPE_NOT_APPLICABLE,
            quotaUsageResponse1.getResponseCode());

    Response naResponse2 = nsSummaryEndpoint.getQuotaUsage(KEY_PATH);
    QuotaUsageResponse quotaUsageResponse2 =
            (QuotaUsageResponse) naResponse2.getEntity();
    Assert.assertEquals(ResponseStatus.TYPE_NOT_APPLICABLE,
            quotaUsageResponse2.getResponseCode());

    // invalid path request
    Response invalidRes = nsSummaryEndpoint.getQuotaUsage(INVALID_PATH);
    QuotaUsageResponse invalidResObj =
            (QuotaUsageResponse) invalidRes.getEntity();
    Assert.assertEquals(ResponseStatus.PATH_NOT_FOUND,
            invalidResObj.getResponseCode());
  }
",non-flaky,5
57269,apache_ozone,TestNSSummaryEndpoint.testFileSizeDist,"  @Test
  public void testFileSizeDist() throws Exception {
    Response volRes = nsSummaryEndpoint.getFileSizeDistribution(VOL_PATH);
    FileSizeDistributionResponse volFileSizeDistResObj =
            (FileSizeDistributionResponse) volRes.getEntity();
    // If the volume has the correct file size distribution,
    // other lower level should be correct as well, given all
    // other previous tests have passed.
    int[] volFileSizeDist = volFileSizeDistResObj.getFileSizeDist();
    for (int i = 0; i < ReconConstants.NUM_OF_BINS; ++i) {
      if (i == 0 || i == 2) {
        Assert.assertEquals(2, volFileSizeDist[i]);
      } else if (i == 1 || i == 3) {
        Assert.assertEquals(1, volFileSizeDist[i]);
      } else {
        Assert.assertEquals(0, volFileSizeDist[i]);
      }
    }
  }
",non-flaky,5
57270,apache_ozone,TestEndpoints.testGetDatanodes,"  @Test
  public void testGetDatanodes() throws Exception {
    Response response = nodeEndpoint.getDatanodes();
    DatanodesResponse datanodesResponse =
        (DatanodesResponse) response.getEntity();
    Assert.assertEquals(2, datanodesResponse.getTotalCount());
    Assert.assertEquals(2, datanodesResponse.getDatanodes().size());

    datanodesResponse.getDatanodes().forEach(datanodeMetadata -> {
      try {
        testDatanodeResponse(datanodeMetadata);
      } catch (IOException e) {
        Assert.fail(e.getMessage());
      }
    });

    waitAndCheckConditionAfterHeartbeat(() -> {
      Response response1 = nodeEndpoint.getDatanodes();
      DatanodesResponse datanodesResponse1 =
          (DatanodesResponse) response1.getEntity();
      DatanodeMetadata datanodeMetadata1 =
          datanodesResponse1.getDatanodes().stream().filter(datanodeMetadata ->
              datanodeMetadata.getHostname().equals(""host1.datanode""))
              .findFirst().orElse(null);
      return (datanodeMetadata1 != null &&
          datanodeMetadata1.getContainers() == 1 &&
          datanodeMetadata1.getOpenContainers() == 1 &&
          reconScm.getPipelineManager()
              .getContainersInPipeline(pipeline.getId()).size() == 1);
    });

    // Change Node OperationalState with NodeManager
    final NodeManager nodeManager = reconScm.getScmNodeManager();
    final DatanodeDetails dnDetailsInternal =
        nodeManager.getNodeByUuid(datanodeDetails.getUuidString());
    // Backup existing state and sanity check
    final NodeStatus nStatus = nodeManager.getNodeStatus(dnDetailsInternal);
    final NodeOperationalState backupOpState =
        dnDetailsInternal.getPersistedOpState();
    final long backupOpStateExpiry =
        dnDetailsInternal.getPersistedOpStateExpiryEpochSec();
    assertEquals(backupOpState, nStatus.getOperationalState());
    assertEquals(backupOpStateExpiry, nStatus.getOpStateExpiryEpochSeconds());

    dnDetailsInternal.setPersistedOpState(NodeOperationalState.DECOMMISSIONING);
    dnDetailsInternal.setPersistedOpStateExpiryEpochSec(666L);
    nodeManager.setNodeOperationalState(dnDetailsInternal,
        NodeOperationalState.DECOMMISSIONING, 666L);
    // Check if the endpoint response reflects the change
    response = nodeEndpoint.getDatanodes();
    datanodesResponse = (DatanodesResponse) response.getEntity();
    // Order of datanodes in the response is random
    AtomicInteger count = new AtomicInteger();
    datanodesResponse.getDatanodes().forEach(metadata -> {
      if (metadata.getUuid().equals(dnDetailsInternal.getUuidString())) {
        count.incrementAndGet();
        assertEquals(NodeOperationalState.DECOMMISSIONING,
            metadata.getOperationalState());
      }
    });
    assertEquals(1, count.get());

    // Restore state
    dnDetailsInternal.setPersistedOpState(backupOpState);
    dnDetailsInternal.setPersistedOpStateExpiryEpochSec(backupOpStateExpiry);
    nodeManager.setNodeOperationalState(dnDetailsInternal,
        backupOpState, backupOpStateExpiry);
  }
",non-flaky,5
57271,apache_ozone,TestEndpoints.testGetPipelines,"  @Test
  public void testGetPipelines() throws Exception {
    Response response = pipelineEndpoint.getPipelines();
    PipelinesResponse pipelinesResponse =
        (PipelinesResponse) response.getEntity();
    Assert.assertEquals(1, pipelinesResponse.getTotalCount());
    Assert.assertEquals(1, pipelinesResponse.getPipelines().size());
    PipelineMetadata pipelineMetadata =
        pipelinesResponse.getPipelines().iterator().next();
    Assert.assertEquals(1, pipelineMetadata.getDatanodes().size());
    Assert.assertEquals(pipeline.getType().toString(),
        pipelineMetadata.getReplicationType());
    Assert.assertEquals(pipeline.getReplicationConfig().getRequiredNodes(),
        pipelineMetadata.getReplicationFactor());
    Assert.assertEquals(datanodeDetails.getHostName(),
        pipelineMetadata.getLeaderNode());
    Assert.assertEquals(pipeline.getId().getId(),
        pipelineMetadata.getPipelineId());
    Assert.assertEquals(5, pipelineMetadata.getLeaderElections());

    waitAndCheckConditionAfterHeartbeat(() -> {
      Response response1 = pipelineEndpoint.getPipelines();
      PipelinesResponse pipelinesResponse1 =
          (PipelinesResponse) response1.getEntity();
      PipelineMetadata pipelineMetadata1 =
          pipelinesResponse1.getPipelines().iterator().next();
      return (pipelineMetadata1.getContainers() == 1);
    });
  }
",non-flaky,5
57272,apache_ozone,TestEndpoints.testGetMetricsResponse,"  @Test
  public void testGetMetricsResponse() throws Exception {
    HttpServletResponse responseMock = mock(HttpServletResponse.class);
    ServletOutputStream outputStreamMock = mock(ServletOutputStream.class);
    when(responseMock.getOutputStream()).thenReturn(outputStreamMock);
    UriInfo uriInfoMock = mock(UriInfo.class);
    URI uriMock = mock(URI.class);
    when(uriMock.getQuery()).thenReturn("""");
    when(uriInfoMock.getRequestUri()).thenReturn(uriMock);

    // Mock makeHttpCall to send a json response
    // when the prometheus endpoint is queried.
    ClassLoader classLoader = Thread.currentThread().getContextClassLoader();
    InputStream inputStream = classLoader
        .getResourceAsStream(PROMETHEUS_TEST_RESPONSE_FILE);
    HttpURLConnection urlConnectionMock = mock(HttpURLConnection.class);
    when(urlConnectionMock.getResponseCode())
        .thenReturn(HttpServletResponse.SC_OK);
    when(urlConnectionMock.getInputStream()).thenReturn(inputStream);
    when(reconUtilsMock.makeHttpCall(any(URLConnectionFactory.class),
        anyString(), anyBoolean())).thenReturn(urlConnectionMock);

    metricsProxyEndpoint.getMetricsResponse(PROMETHEUS_INSTANT_QUERY_API,
        uriInfoMock, responseMock);

    byte[] fileBytes = FileUtils.readFileToByteArray(
        new File(classLoader.getResource(PROMETHEUS_TEST_RESPONSE_FILE)
            .getFile())
        );
    verify(outputStreamMock).write(fileBytes, 0, fileBytes.length);
  }
",non-flaky,5
57273,apache_ozone,TestEndpoints.testGetClusterState,"  @Test
  public void testGetClusterState() throws Exception {
    Response response = clusterStateEndpoint.getClusterState();
    ClusterStateResponse clusterStateResponse =
        (ClusterStateResponse) response.getEntity();

    Assert.assertEquals(1, clusterStateResponse.getPipelines());
    Assert.assertEquals(0, clusterStateResponse.getVolumes());
    Assert.assertEquals(0, clusterStateResponse.getBuckets());
    Assert.assertEquals(0, clusterStateResponse.getKeys());
    Assert.assertEquals(2, clusterStateResponse.getTotalDatanodes());
    Assert.assertEquals(2, clusterStateResponse.getHealthyDatanodes());

    waitAndCheckConditionAfterHeartbeat(() -> {
      Response response1 = clusterStateEndpoint.getClusterState();
      ClusterStateResponse clusterStateResponse1 =
          (ClusterStateResponse) response1.getEntity();
      return (clusterStateResponse1.getContainers() == 1);
    });

    // check volume, bucket and key count after running table count task
    Pair<String, Boolean> result =
        tableCountTask.reprocess(reconOMMetadataManager);
    assertTrue(result.getRight());
    response = clusterStateEndpoint.getClusterState();
    clusterStateResponse = (ClusterStateResponse) response.getEntity();
    Assert.assertEquals(2, clusterStateResponse.getVolumes());
    Assert.assertEquals(2, clusterStateResponse.getBuckets());
    Assert.assertEquals(3, clusterStateResponse.getKeys());
  }
",non-flaky,5
57274,apache_ozone,TestEndpoints.testGetFileCounts,"  @Test
  public void testGetFileCounts() throws Exception {
    OmKeyInfo omKeyInfo1 = mock(OmKeyInfo.class);
    given(omKeyInfo1.getKeyName()).willReturn(""key1"");
    given(omKeyInfo1.getVolumeName()).willReturn(""vol1"");
    given(omKeyInfo1.getBucketName()).willReturn(""bucket1"");
    given(omKeyInfo1.getDataSize()).willReturn(1000L);

    OmKeyInfo omKeyInfo2 = mock(OmKeyInfo.class);
    given(omKeyInfo2.getKeyName()).willReturn(""key2"");
    given(omKeyInfo2.getVolumeName()).willReturn(""vol1"");
    given(omKeyInfo2.getBucketName()).willReturn(""bucket1"");
    given(omKeyInfo2.getDataSize()).willReturn(100000L);

    OmKeyInfo omKeyInfo3 = mock(OmKeyInfo.class);
    given(omKeyInfo3.getKeyName()).willReturn(""key1"");
    given(omKeyInfo3.getVolumeName()).willReturn(""vol2"");
    given(omKeyInfo3.getBucketName()).willReturn(""bucket1"");
    given(omKeyInfo3.getDataSize()).willReturn(1000L);

    OMMetadataManager omMetadataManager = mock(OmMetadataManagerImpl.class);
    TypedTable<String, OmKeyInfo> keyTable = mock(TypedTable.class);

    TypedTable.TypedTableIterator mockKeyIter = mock(TypedTable
        .TypedTableIterator.class);
    TypedTable.TypedKeyValue mockKeyValue = mock(
        TypedTable.TypedKeyValue.class);

    when(keyTable.iterator()).thenReturn(mockKeyIter);
    when(omMetadataManager.getKeyTable(getBucketLayout())).thenReturn(keyTable);
    when(mockKeyIter.hasNext())
        .thenReturn(true)
        .thenReturn(true)
        .thenReturn(true)
        .thenReturn(false);
    when(mockKeyIter.next()).thenReturn(mockKeyValue);
    when(mockKeyValue.getValue())
        .thenReturn(omKeyInfo1)
        .thenReturn(omKeyInfo2)
        .thenReturn(omKeyInfo3);

    Pair<String, Boolean> result =
        fileSizeCountTask.reprocess(omMetadataManager);
    assertTrue(result.getRight());

    assertEquals(3, fileCountBySizeDao.count());
    Response response = utilizationEndpoint.getFileCounts(null, null, 0);
    List<FileCountBySize> resultSet =
        (List<FileCountBySize>) response.getEntity();
    assertEquals(3, resultSet.size());
    assertTrue(resultSet.stream().anyMatch(o -> o.getVolume().equals(""vol1"") &&
        o.getBucket().equals(""bucket1"") && o.getFileSize() == 1024L &&
        o.getCount() == 1L));
    assertTrue(resultSet.stream().anyMatch(o -> o.getVolume().equals(""vol1"") &&
        o.getBucket().equals(""bucket1"") && o.getFileSize() == 131072 &&
        o.getCount() == 1L));
    assertTrue(resultSet.stream().anyMatch(o -> o.getVolume().equals(""vol2"") &&
        o.getBucket().equals(""bucket1"") && o.getFileSize() == 1024L &&
        o.getCount() == 1L));

    // Test for ""volume"" query param
    response = utilizationEndpoint.getFileCounts(""vol1"", null, 0);
    resultSet = (List<FileCountBySize>) response.getEntity();
    assertEquals(2, resultSet.size());
    assertTrue(resultSet.stream().allMatch(o -> o.getVolume().equals(""vol1"")));

    // Test for non-existent volume
    response = utilizationEndpoint.getFileCounts(""vol"", null, 0);
    resultSet = (List<FileCountBySize>) response.getEntity();
    assertEquals(0, resultSet.size());

    // Test for ""volume"" + ""bucket"" query param
    response = utilizationEndpoint.getFileCounts(""vol1"", ""bucket1"", 0);
    resultSet = (List<FileCountBySize>) response.getEntity();
    assertEquals(2, resultSet.size());
    assertTrue(resultSet.stream().allMatch(o -> o.getVolume().equals(""vol1"") &&
        o.getBucket().equals(""bucket1"")));

    // Test for non-existent bucket
    response = utilizationEndpoint.getFileCounts(""vol1"", ""bucket"", 0);
    resultSet = (List<FileCountBySize>) response.getEntity();
    assertEquals(0, resultSet.size());

    // Test for ""volume"" + ""bucket"" + ""fileSize"" query params
    response = utilizationEndpoint.getFileCounts(""vol1"", ""bucket1"", 131072);
    resultSet = (List<FileCountBySize>) response.getEntity();
    assertEquals(1, resultSet.size());
    FileCountBySize o = resultSet.get(0);
    assertTrue(o.getVolume().equals(""vol1"") && o.getBucket().equals(
        ""bucket1"") && o.getFileSize() == 131072);

    // Test for non-existent fileSize
    response = utilizationEndpoint.getFileCounts(""vol1"", ""bucket1"", 1310725);
    resultSet = (List<FileCountBySize>) response.getEntity();
    assertEquals(0, resultSet.size());
  }
",non-flaky,5
57275,apache_ozone,TestOpenContainerCount.testOpenContainerCount,"  @Test
  public void testOpenContainerCount() throws Exception {
    // In case of pipeline doesn't exist
    waitAndCheckConditionAfterHeartbeat(() -> {

      DatanodeMetadata datanodeMetadata1 = getDatanodeMetadata();
      return datanodeMetadata1.getContainers() == 10
              && datanodeMetadata1.getPipelines().size() == 2;
    });

    DatanodeMetadata datanodeMetadata = getDatanodeMetadata();

    int expectedCnt = datanodeMetadata.getOpenContainers();

    // check if open container's count decrement according
    for (long id = 1L; id <= 10L; ++id) {
      --expectedCnt;
      closeContainer(id);
      DatanodeMetadata metadata = getDatanodeMetadata();
      Assert.assertEquals(expectedCnt, metadata.getOpenContainers());
    }
  }
",non-flaky,5
57276,apache_ozone,TestTaskStatusService.testGetTaskTimes,"  @Test
  public void testGetTaskTimes() {
    ReconTaskStatusDao reconTaskStatusDao = getDao(ReconTaskStatusDao.class);

    ReconTaskStatus reconTaskStatusRecord = new ReconTaskStatus(
        ""Dummy_Task"", System.currentTimeMillis(), 0L);
    reconTaskStatusDao.insert(reconTaskStatusRecord);

    List<ReconTaskStatus> resultList = new ArrayList<>();
    resultList.add(reconTaskStatusRecord);

    Response response = taskStatusService.getTaskTimes();

    List<ReconTaskStatus> responseList = (List<ReconTaskStatus>)
        response.getEntity();

    Assert.assertEquals(resultList.size(), responseList.size());
    for(ReconTaskStatus r : responseList) {
      Assert.assertEquals(reconTaskStatusRecord.getTaskName(), r.getTaskName());
      Assert.assertEquals(reconTaskStatusRecord.getLastUpdatedTimestamp(),
          r.getLastUpdatedTimestamp());
    }
  }
",non-flaky,5
57277,apache_ozone,TestContainerEndpoint.testGetKeysForContainer,"  @Test
  public void testGetKeysForContainer() {
    Response response = containerEndpoint.getKeysForContainer(1L, -1, """");

    KeysResponse data = (KeysResponse) response.getEntity();
    Collection<KeyMetadata> keyMetadataList = data.getKeys();

    assertEquals(3, data.getTotalCount());
    assertEquals(2, keyMetadataList.size());

    Iterator<KeyMetadata> iterator = keyMetadataList.iterator();

    KeyMetadata keyMetadata = iterator.next();
    assertEquals(""key_one"", keyMetadata.getKey());
    assertEquals(1, keyMetadata.getVersions().size());
    assertEquals(1, keyMetadata.getBlockIds().size());
    Map<Long, List<KeyMetadata.ContainerBlockMetadata>> blockIds =
        keyMetadata.getBlockIds();
    assertEquals(101, blockIds.get(0L).iterator().next().getLocalID());

    keyMetadata = iterator.next();
    assertEquals(""key_two"", keyMetadata.getKey());
    assertEquals(2, keyMetadata.getVersions().size());
    assertTrue(keyMetadata.getVersions().contains(0L) && keyMetadata
        .getVersions().contains(1L));
    assertEquals(2, keyMetadata.getBlockIds().size());
    blockIds = keyMetadata.getBlockIds();
    assertEquals(103, blockIds.get(0L).iterator().next().getLocalID());
    assertEquals(104, blockIds.get(1L).iterator().next().getLocalID());

    response = containerEndpoint.getKeysForContainer(3L, -1, """");
    data = (KeysResponse) response.getEntity();
    keyMetadataList = data.getKeys();
    assertTrue(keyMetadataList.isEmpty());
    assertEquals(0, data.getTotalCount());

    // test if limit works as expected
    response = containerEndpoint.getKeysForContainer(1L, 1, """");
    data = (KeysResponse) response.getEntity();
    keyMetadataList = data.getKeys();
    assertEquals(1, keyMetadataList.size());
    assertEquals(3, data.getTotalCount());
  }
",non-flaky,5
57278,apache_ozone,TestContainerEndpoint.testGetKeysForContainerWithPrevKey,"  @Test
  public void testGetKeysForContainerWithPrevKey() {
    // test if prev-key param works as expected
    Response response = containerEndpoint.getKeysForContainer(
        1L, -1, ""/sampleVol/bucketOne/key_one"");

    KeysResponse data =
        (KeysResponse) response.getEntity();

    assertEquals(3, data.getTotalCount());

    Collection<KeyMetadata> keyMetadataList = data.getKeys();
    assertEquals(1, keyMetadataList.size());

    Iterator<KeyMetadata> iterator = keyMetadataList.iterator();
    KeyMetadata keyMetadata = iterator.next();

    assertEquals(""key_two"", keyMetadata.getKey());
    assertEquals(2, keyMetadata.getVersions().size());
    assertEquals(2, keyMetadata.getBlockIds().size());

    response = containerEndpoint.getKeysForContainer(
        1L, -1, StringUtils.EMPTY);
    data = (KeysResponse) response.getEntity();
    keyMetadataList = data.getKeys();

    assertEquals(3, data.getTotalCount());
    assertEquals(2, keyMetadataList.size());
    iterator = keyMetadataList.iterator();
    keyMetadata = iterator.next();
    assertEquals(""key_one"", keyMetadata.getKey());

    // test for negative cases
    response = containerEndpoint.getKeysForContainer(
        1L, -1, ""/sampleVol/bucketOne/invalid_key"");
    data = (KeysResponse) response.getEntity();
    keyMetadataList = data.getKeys();
    assertEquals(3, data.getTotalCount());
    assertEquals(0, keyMetadataList.size());

    response = containerEndpoint.getKeysForContainer(
        5L, -1, """");
    data = (KeysResponse) response.getEntity();
    keyMetadataList = data.getKeys();
    assertEquals(0, keyMetadataList.size());
    assertEquals(0, data.getTotalCount());
  }
",non-flaky,5
57279,apache_ozone,TestContainerEndpoint.testGetContainers,"  @Test
  public void testGetContainers() {
    Response response = containerEndpoint.getContainers(-1, 0L);

    ContainersResponse responseObject =
        (ContainersResponse) response.getEntity();

    ContainersResponse.ContainersResponseData data =
        responseObject.getContainersResponseData();
    assertEquals(2, data.getTotalCount());

    List<ContainerMetadata> containers = new ArrayList<>(data.getContainers());

    Iterator<ContainerMetadata> iterator = containers.iterator();

    ContainerMetadata containerMetadata = iterator.next();
    assertEquals(1L, containerMetadata.getContainerID());
    // Number of keys for CID:1 should be 3 because of two different versions
    // of key_two stored in CID:1
    assertEquals(3L, containerMetadata.getNumberOfKeys());

    containerMetadata = iterator.next();
    assertEquals(2L, containerMetadata.getContainerID());
    assertEquals(2L, containerMetadata.getNumberOfKeys());

    // test if limit works as expected
    response = containerEndpoint.getContainers(1, 0L);
    responseObject = (ContainersResponse) response.getEntity();
    data = responseObject.getContainersResponseData();
    containers = new ArrayList<>(data.getContainers());
    assertEquals(1, containers.size());
    assertEquals(2, data.getTotalCount());
  }
",non-flaky,5
57280,apache_ozone,TestContainerEndpoint.testGetContainersWithPrevKey,"  @Test
  public void testGetContainersWithPrevKey() {

    Response response = containerEndpoint.getContainers(1, 1L);

    ContainersResponse responseObject =
        (ContainersResponse) response.getEntity();

    ContainersResponse.ContainersResponseData data =
        responseObject.getContainersResponseData();
    assertEquals(2, data.getTotalCount());

    List<ContainerMetadata> containers = new ArrayList<>(data.getContainers());

    Iterator<ContainerMetadata> iterator = containers.iterator();

    ContainerMetadata containerMetadata = iterator.next();

    assertEquals(1, containers.size());
    assertEquals(2L, containerMetadata.getContainerID());

    response = containerEndpoint.getContainers(-1, 0L);
    responseObject = (ContainersResponse) response.getEntity();
    data = responseObject.getContainersResponseData();
    containers = new ArrayList<>(data.getContainers());
    assertEquals(2, containers.size());
    assertEquals(2, data.getTotalCount());
    iterator = containers.iterator();
    containerMetadata = iterator.next();
    assertEquals(1L, containerMetadata.getContainerID());

    // test for negative cases
    response = containerEndpoint.getContainers(-1, 5L);
    responseObject = (ContainersResponse) response.getEntity();
    data = responseObject.getContainersResponseData();
    containers = new ArrayList<>(data.getContainers());
    assertEquals(0, containers.size());
    assertEquals(2, data.getTotalCount());

    response = containerEndpoint.getContainers(-1, -1L);
    responseObject = (ContainersResponse) response.getEntity();
    data = responseObject.getContainersResponseData();
    containers = new ArrayList<>(data.getContainers());
    assertEquals(2, containers.size());
    assertEquals(2, data.getTotalCount());
  }
",non-flaky,5
57281,apache_ozone,TestContainerEndpoint.testGetMissingContainers,"  @Test
  public void testGetMissingContainers() throws IOException {
    Response response = containerEndpoint.getMissingContainers();

    MissingContainersResponse responseObject =
        (MissingContainersResponse) response.getEntity();

    assertEquals(0, responseObject.getTotalCount());
    assertEquals(Collections.EMPTY_LIST, responseObject.getContainers());

    // Add missing containers to the database
    long missingSince = System.currentTimeMillis();
    UnhealthyContainers missing = new UnhealthyContainers();
    missing.setContainerId(1L);
    missing.setInStateSince(missingSince);
    missing.setActualReplicaCount(0);
    missing.setExpectedReplicaCount(3);
    missing.setReplicaDelta(3);
    missing.setContainerState(
        ContainerSchemaDefinition.UnHealthyContainerStates.MISSING.toString());
    ArrayList<UnhealthyContainers> missingList =
        new ArrayList<UnhealthyContainers>();
    missingList.add(missing);
    containerHealthSchemaManager.insertUnhealthyContainerRecords(missingList);

    putContainerInfos(1);
    // Add container history for id 1
    final UUID u1 = newDatanode(""host1"", ""127.0.0.1"");
    final UUID u2 = newDatanode(""host2"", ""127.0.0.2"");
    final UUID u3 = newDatanode(""host3"", ""127.0.0.3"");
    final UUID u4 = newDatanode(""host4"", ""127.0.0.4"");
    reconContainerManager.upsertContainerHistory(1L, u1, 1L, 1L);
    reconContainerManager.upsertContainerHistory(1L, u2, 2L, 1L);
    reconContainerManager.upsertContainerHistory(1L, u3, 3L, 1L);
    reconContainerManager.upsertContainerHistory(1L, u4, 4L, 1L);

    response = containerEndpoint.getMissingContainers();
    responseObject = (MissingContainersResponse) response.getEntity();
    assertEquals(1, responseObject.getTotalCount());
    MissingContainerMetadata container =
        responseObject.getContainers().stream().findFirst().orElse(null);
    Assert.assertNotNull(container);

    assertEquals(containerID.getId(), container.getContainerID());
    assertEquals(keyCount, container.getKeys());
    assertEquals(pipelineID.getId(), container.getPipelineID());
    assertEquals(3, container.getReplicas().size());
    assertEquals(missingSince, container.getMissingSince());

    Set<String> datanodes = Collections.unmodifiableSet(
        new HashSet<>(Arrays.asList(""host2"", ""host3"", ""host4"")));
    List<ContainerHistory> containerReplicas = container.getReplicas();
    containerReplicas.forEach(history -> {
      Assert.assertTrue(datanodes.contains(history.getDatanodeHost()));
    });
  }
",non-flaky,5
57282,apache_ozone,TestContainerEndpoint.testUnhealthyContainers,"  @Test
  public void testUnhealthyContainers() throws IOException {
    Response response = containerEndpoint.getUnhealthyContainers(1000, 1);

    UnhealthyContainersResponse responseObject =
        (UnhealthyContainersResponse) response.getEntity();

    assertEquals(0, responseObject.getMissingCount());
    assertEquals(0, responseObject.getOverReplicatedCount());
    assertEquals(0, responseObject.getUnderReplicatedCount());
    assertEquals(0, responseObject.getMisReplicatedCount());

    assertEquals(Collections.EMPTY_LIST, responseObject.getContainers());

    putContainerInfos(14);
    uuid1 = newDatanode(""host1"", ""127.0.0.1"");
    uuid2 = newDatanode(""host2"", ""127.0.0.2"");
    uuid3 = newDatanode(""host3"", ""127.0.0.3"");
    uuid4 = newDatanode(""host4"", ""127.0.0.4"");
    createUnhealthyRecords(5, 4, 3, 2);

    response = containerEndpoint.getUnhealthyContainers(1000, 1);

    responseObject = (UnhealthyContainersResponse) response.getEntity();
    assertEquals(5, responseObject.getMissingCount());
    assertEquals(4, responseObject.getOverReplicatedCount());
    assertEquals(3, responseObject.getUnderReplicatedCount());
    assertEquals(2, responseObject.getMisReplicatedCount());

    Collection<UnhealthyContainerMetadata> records
        = responseObject.getContainers();
    List<UnhealthyContainerMetadata> missing = records
        .stream()
        .filter(r -> r.getContainerState()
            .equals(UnHealthyContainerStates.MISSING.toString()))
        .collect(Collectors.toList());
    assertEquals(5, missing.size());
    assertEquals(3, missing.get(0).getExpectedReplicaCount());
    assertEquals(0, missing.get(0).getActualReplicaCount());
    assertEquals(3, missing.get(0).getReplicaDeltaCount());
    assertEquals(12345L, missing.get(0).getUnhealthySince());
    assertEquals(1L, missing.get(0).getContainerID());
    assertEquals(keyCount, missing.get(0).getKeys());
    assertEquals(pipelineID.getId(), missing.get(0).getPipelineID());
    assertEquals(3, missing.get(0).getReplicas().size());
    assertNull(missing.get(0).getReason());

    Set<String> datanodes = Collections.unmodifiableSet(
        new HashSet<>(Arrays.asList(""host2"", ""host3"", ""host4"")));
    List<ContainerHistory> containerReplicas = missing.get(0).getReplicas();
    containerReplicas.forEach(history -> {
      Assert.assertTrue(datanodes.contains(history.getDatanodeHost()));
    });

    List<UnhealthyContainerMetadata> overRep = records
        .stream()
        .filter(r -> r.getContainerState()
            .equals(UnHealthyContainerStates.OVER_REPLICATED.toString()))
        .collect(Collectors.toList());
    assertEquals(4, overRep.size());
    assertEquals(3, overRep.get(0).getExpectedReplicaCount());
    assertEquals(5, overRep.get(0).getActualReplicaCount());
    assertEquals(-2, overRep.get(0).getReplicaDeltaCount());
    assertEquals(12345L, overRep.get(0).getUnhealthySince());
    assertEquals(6L, overRep.get(0).getContainerID());
    assertNull(overRep.get(0).getReason());

    List<UnhealthyContainerMetadata> underRep = records
        .stream()
        .filter(r -> r.getContainerState()
            .equals(UnHealthyContainerStates.UNDER_REPLICATED.toString()))
        .collect(Collectors.toList());
    assertEquals(3, underRep.size());
    assertEquals(3, underRep.get(0).getExpectedReplicaCount());
    assertEquals(1, underRep.get(0).getActualReplicaCount());
    assertEquals(2, underRep.get(0).getReplicaDeltaCount());
    assertEquals(12345L, underRep.get(0).getUnhealthySince());
    assertEquals(10L, underRep.get(0).getContainerID());
    assertNull(underRep.get(0).getReason());

    List<UnhealthyContainerMetadata> misRep = records
        .stream()
        .filter(r -> r.getContainerState()
            .equals(UnHealthyContainerStates.MIS_REPLICATED.toString()))
        .collect(Collectors.toList());
    assertEquals(2, misRep.size());
    assertEquals(2, misRep.get(0).getExpectedReplicaCount());
    assertEquals(1, misRep.get(0).getActualReplicaCount());
    assertEquals(1, misRep.get(0).getReplicaDeltaCount());
    assertEquals(12345L, misRep.get(0).getUnhealthySince());
    assertEquals(13L, misRep.get(0).getContainerID());
    assertEquals(""some reason"", misRep.get(0).getReason());
  }
",non-flaky,5
57283,apache_ozone,TestContainerEndpoint.testUnhealthyContainersFilteredResponse,"  @Test
  public void testUnhealthyContainersFilteredResponse() throws IOException {
    String missing =  UnHealthyContainerStates.MISSING.toString();

    Response response = containerEndpoint
        .getUnhealthyContainers(missing, 1000, 1);

    UnhealthyContainersResponse responseObject =
        (UnhealthyContainersResponse) response.getEntity();

    assertEquals(0, responseObject.getMissingCount());
    assertEquals(0, responseObject.getOverReplicatedCount());
    assertEquals(0, responseObject.getUnderReplicatedCount());
    assertEquals(0, responseObject.getMisReplicatedCount());
    assertEquals(Collections.EMPTY_LIST, responseObject.getContainers());

    putContainerInfos(5);
    uuid1 = newDatanode(""host1"", ""127.0.0.1"");
    uuid2 = newDatanode(""host2"", ""127.0.0.2"");
    uuid3 = newDatanode(""host3"", ""127.0.0.3"");
    uuid4 = newDatanode(""host4"", ""127.0.0.4"");
    createUnhealthyRecords(5, 4, 3, 2);

    response = containerEndpoint.getUnhealthyContainers(missing, 1000, 1);

    responseObject = (UnhealthyContainersResponse) response.getEntity();
    // Summary should have the count for all unhealthy:
    assertEquals(5, responseObject.getMissingCount());
    assertEquals(4, responseObject.getOverReplicatedCount());
    assertEquals(3, responseObject.getUnderReplicatedCount());
    assertEquals(2, responseObject.getMisReplicatedCount());

    Collection<UnhealthyContainerMetadata> records
        = responseObject.getContainers();

    // There should only be 5 missing containers and no others as we asked for
    // only missing.
    assertEquals(5, records.size());
    for (UnhealthyContainerMetadata r : records) {
      assertEquals(missing, r.getContainerState());
    }
  }
",non-flaky,5
57284,apache_ozone,TestContainerEndpoint.testUnhealthyContainersInvalidState,"  @Test
  public void testUnhealthyContainersInvalidState() {
    try {
      containerEndpoint.getUnhealthyContainers(""invalid"", 1000, 1);
      fail(""Expected exception to be raised"");
    } catch (WebApplicationException e) {
      assertEquals(""HTTP 400 Bad Request"", e.getMessage());
    }
  }
",non-flaky,5
57285,apache_ozone,TestContainerEndpoint.testUnhealthyContainersPaging,"  @Test
  public void testUnhealthyContainersPaging() throws IOException {
    putContainerInfos(6);
    uuid1 = newDatanode(""host1"", ""127.0.0.1"");
    uuid2 = newDatanode(""host2"", ""127.0.0.2"");
    uuid3 = newDatanode(""host3"", ""127.0.0.3"");
    uuid4 = newDatanode(""host4"", ""127.0.0.4"");
    createUnhealthyRecords(5, 4, 3, 2);
    UnhealthyContainersResponse firstBatch =
        (UnhealthyContainersResponse) containerEndpoint.getUnhealthyContainers(
            3, 1).getEntity();

    UnhealthyContainersResponse secondBatch =
        (UnhealthyContainersResponse) containerEndpoint.getUnhealthyContainers(
            3, 2).getEntity();

    ArrayList<UnhealthyContainerMetadata> records
        = new ArrayList<>(firstBatch.getContainers());
    assertEquals(3, records.size());
    assertEquals(1L, records.get(0).getContainerID());
    assertEquals(2L, records.get(1).getContainerID());
    assertEquals(3L, records.get(2).getContainerID());

    records
        = new ArrayList<>(secondBatch.getContainers());
    assertEquals(3, records.size());
    assertEquals(4L, records.get(0).getContainerID());
    assertEquals(5L, records.get(1).getContainerID());
    assertEquals(6L, records.get(2).getContainerID());
  }
",non-flaky,5
57286,apache_ozone,TestContainerEndpoint.testGetReplicaHistoryForContainer,"  @Test
  public void testGetReplicaHistoryForContainer() throws IOException {
    // Add container history for container id 1
    final UUID u1 = newDatanode(""host1"", ""127.0.0.1"");
    final UUID u2 = newDatanode(""host2"", ""127.0.0.2"");
    final UUID u3 = newDatanode(""host3"", ""127.0.0.3"");
    final UUID u4 = newDatanode(""host4"", ""127.0.0.4"");
    reconContainerManager.upsertContainerHistory(1L, u1, 1L, 1L);
    reconContainerManager.upsertContainerHistory(1L, u2, 2L, 1L);
    reconContainerManager.upsertContainerHistory(1L, u3, 3L, 1L);
    reconContainerManager.upsertContainerHistory(1L, u4, 4L, 1L);

    reconContainerManager.upsertContainerHistory(1L, u1, 5L, 1L);

    Response response = containerEndpoint.getReplicaHistoryForContainer(1L);
    List<ContainerHistory> histories =
        (List<ContainerHistory>) response.getEntity();
    Set<String> datanodes = Collections.unmodifiableSet(
        new HashSet<>(Arrays.asList(
            u1.toString(), u2.toString(), u3.toString(), u4.toString())));
    Assert.assertEquals(4, histories.size());
    histories.forEach(history -> {
      Assert.assertTrue(datanodes.contains(history.getDatanodeUuid()));
      if (history.getDatanodeUuid().equals(u1.toString())) {
        Assert.assertEquals(""host1"", history.getDatanodeHost());
        Assert.assertEquals(1L, history.getFirstSeenTime());
        Assert.assertEquals(5L, history.getLastSeenTime());
      }
    });

    // Check getLatestContainerHistory
    List<ContainerHistory> hist1 = reconContainerManager
        .getLatestContainerHistory(1L, 10);
    Assert.assertTrue(hist1.size() <= 10);
    // Descending order by last report timestamp
    for (int i = 0; i < hist1.size() - 1; i++) {
      Assert.assertTrue(hist1.get(i).getLastSeenTime()
          >= hist1.get(i + 1).getLastSeenTime());
    }
  }
",non-flaky,5
57287,apache_ozone,TestReconCodecs.testContainerKeyPrefixCodec,"  @Test
  public void testContainerKeyPrefixCodec() throws IOException {
    ContainerKeyPrefix containerKeyPrefix = new ContainerKeyPrefix(
        System.currentTimeMillis(), ""TestKeyPrefix"", 0);

    Codec<ContainerKeyPrefix> codec = new ContainerKeyPrefixCodec();
    byte[] persistedFormat = codec.toPersistedFormat(containerKeyPrefix);
    Assert.assertTrue(persistedFormat != null);
    ContainerKeyPrefix fromPersistedFormat =
        codec.fromPersistedFormat(persistedFormat);
    Assert.assertEquals(containerKeyPrefix, fromPersistedFormat);
  }
",non-flaky,5
57288,apache_ozone,TestReconCodecs.testIntegerCodec,"  @Test
  public void testIntegerCodec() throws IOException {
    Integer i = 1000;
    Codec<Integer> codec = new IntegerCodec();
    byte[] persistedFormat = codec.toPersistedFormat(i);
    Assert.assertTrue(persistedFormat != null);
    Integer fromPersistedFormat =
        codec.fromPersistedFormat(persistedFormat);
    Assert.assertEquals(i, fromPersistedFormat);
  }
",non-flaky,5
57289,apache_ozone,TestReconDBProvider.testGet,"  @Test
  public void testGet() throws Exception {
    ReconDBProvider reconDBProvider = injector.getInstance(
        ReconDBProvider.class);
    assertNotNull(reconDBProvider.getDbStore());
  }
",non-flaky,5
57290,apache_ozone,TestStorageContainerServiceProviderImpl.testGetPipelines,"  @Test
  public void testGetPipelines() throws IOException {
    StorageContainerServiceProvider scmProvider =
        injector.getInstance(StorageContainerServiceProvider.class);
    StorageContainerLocationProtocol scmClient =
        injector.getInstance(StorageContainerLocationProtocol.class);
    scmProvider.getPipelines();
    verify(scmClient, times(1)).listPipelines();
  }
",non-flaky,5
57291,apache_ozone,TestStorageContainerServiceProviderImpl.testGetPipeline,"  @Test
  public void testGetPipeline() throws IOException {
    StorageContainerServiceProvider scmProvider =
        injector.getInstance(StorageContainerServiceProvider.class);
    StorageContainerLocationProtocol scmClient =
        injector.getInstance(StorageContainerLocationProtocol.class);
    Pipeline pipeline = scmProvider.getPipeline(pipelineID);
    assertNotNull(pipeline);
    verify(scmClient, times(1))
        .getPipeline(pipelineID);
  }
",non-flaky,5
57292,apache_ozone,TestReconContainerMetadataManagerImpl.testInitNewContainerDB,"  @Test
  public void testInitNewContainerDB() throws Exception {
    long containerId = System.currentTimeMillis();
    Map<ContainerKeyPrefix, Integer> prefixCounts = new HashMap<>();

    ContainerKeyPrefix ckp1 = new ContainerKeyPrefix(containerId,
        ""V1/B1/K1"", 0);
    prefixCounts.put(ckp1, 1);

    ContainerKeyPrefix ckp2 = new ContainerKeyPrefix(containerId,
        ""V1/B1/K2"", 0);
    prefixCounts.put(ckp2, 2);

    ContainerKeyPrefix ckp3 = new ContainerKeyPrefix(containerId,
        ""V1/B2/K3"", 0);
    prefixCounts.put(ckp3, 3);

    for (Map.Entry<ContainerKeyPrefix, Integer> entry :
        prefixCounts.entrySet()) {
      reconContainerMetadataManager.storeContainerKeyMapping(
          entry.getKey(), prefixCounts.get(entry.getKey()));
    }

    assertEquals(1, reconContainerMetadataManager
        .getCountForContainerKeyPrefix(ckp1).intValue());

    prefixCounts.clear();
    prefixCounts.put(ckp2, 12);
    prefixCounts.put(ckp3, 13);
    ContainerKeyPrefix ckp4 = new ContainerKeyPrefix(containerId,
        ""V1/B3/K1"", 0);
    prefixCounts.put(ckp4, 14);
    ContainerKeyPrefix ckp5 = new ContainerKeyPrefix(containerId,
        ""V1/B3/K2"", 0);
    prefixCounts.put(ckp5, 15);

    reconContainerMetadataManager
            .reinitWithNewContainerDataFromOm(prefixCounts);
    Map<ContainerKeyPrefix, Integer> keyPrefixesForContainer =
        reconContainerMetadataManager.getKeyPrefixesForContainer(containerId);

    assertEquals(4, keyPrefixesForContainer.size());
    assertEquals(12, keyPrefixesForContainer.get(ckp2).intValue());
    assertEquals(13, keyPrefixesForContainer.get(ckp3).intValue());
    assertEquals(14, keyPrefixesForContainer.get(ckp4).intValue());
    assertEquals(15, keyPrefixesForContainer.get(ckp5).intValue());

    assertEquals(0, reconContainerMetadataManager
        .getCountForContainerKeyPrefix(ckp1).intValue());
  }
",non-flaky,5
57293,apache_ozone,TestReconContainerMetadataManagerImpl.testStoreContainerKeyMapping,"  @Test
  public void testStoreContainerKeyMapping() throws Exception {

    long containerId = System.currentTimeMillis();
    Map<String, Integer> prefixCounts = new HashMap<>();
    prefixCounts.put(keyPrefix1, 1);
    prefixCounts.put(keyPrefix2, 2);
    prefixCounts.put(keyPrefix3, 3);

    for (Map.Entry<String, Integer> entry : prefixCounts.entrySet()) {
      ContainerKeyPrefix containerKeyPrefix = new ContainerKeyPrefix(
          containerId, entry.getKey(), 0);
      reconContainerMetadataManager.storeContainerKeyMapping(
          containerKeyPrefix, prefixCounts.get(entry.getKey()));
    }

    Assert.assertEquals(1,
        reconContainerMetadataManager.getCountForContainerKeyPrefix(
            new ContainerKeyPrefix(containerId, keyPrefix1,
                0)).longValue());
    Assert.assertEquals(2,
        reconContainerMetadataManager.getCountForContainerKeyPrefix(
            new ContainerKeyPrefix(containerId, keyPrefix2,
                0)).longValue());
    Assert.assertEquals(3,
        reconContainerMetadataManager.getCountForContainerKeyPrefix(
            new ContainerKeyPrefix(containerId, keyPrefix3,
                0)).longValue());
  }
",non-flaky,5
57294,apache_ozone,TestReconContainerMetadataManagerImpl.testStoreContainerKeyCount,"  @Test
  public void testStoreContainerKeyCount() throws Exception {
    long containerId = 1L;
    long nextContainerId = 2L;
    reconContainerMetadataManager.storeContainerKeyCount(containerId, 2L);
    reconContainerMetadataManager.storeContainerKeyCount(nextContainerId, 3L);

    assertEquals(2,
        reconContainerMetadataManager.getKeyCountForContainer(containerId));
    assertEquals(3,
        reconContainerMetadataManager.getKeyCountForContainer(nextContainerId));

    reconContainerMetadataManager.storeContainerKeyCount(containerId, 20L);
    assertEquals(20,
        reconContainerMetadataManager.getKeyCountForContainer(containerId));
  }
",non-flaky,5
76910,spring-projects_spring-data-envers,DefaultRevisionMetadataUnitTests.createsLocalDateTimeFromTimestamp,"	@Test // #112
	public void createsLocalDateTimeFromTimestamp() {

		DefaultRevisionEntity entity = new DefaultRevisionEntity();
		entity.setTimestamp(NOW.toEpochMilli());

		DefaultRevisionMetadata metadata = new DefaultRevisionMetadata(entity);

		assertThat(metadata.getRevisionDate()).hasValue(LocalDateTime.ofInstant(NOW, ZoneOffset.systemDefault()));
	}
",non-flaky,5
76911,spring-projects_spring-data-envers,QueryDslRepositoryIntegrationTests.testWithQueryDsl,"	@Test
	public void testWithQueryDsl() {

		Country de = new Country();
		de.code = ""de"";
		de.name = ""Deutschland"";

		countryRepository.save(de);

		Country found = countryRepository.findOne(QCountry.country.name.eq(""Deutschland"")).get();

		assertThat(found).isNotNull();
		assertThat(found.id).isEqualTo(de.id);
	}
",non-flaky,5
76912,spring-projects_spring-data-envers,QueryDslRepositoryIntegrationTests.testWithRevisions,"	@Test
	public void testWithRevisions() {

		Country de = new Country();
		de.code = ""de"";
		de.name = ""Deutschland"";

		countryRepository.save(de);

		de.name = ""Germany"";

		countryRepository.save(de);

		Revisions<Integer, Country> revisions = countryRepository.findRevisions(de.id);

		assertThat(revisions).hasSize(2);

		Iterator<Revision<Integer, Country>> iterator = revisions.iterator();

		Integer firstRevisionNumber = iterator.next().getRevisionNumber().get();
		Integer secondRevisionNumber = iterator.next().getRevisionNumber().get();

		assertThat(countryRepository.findRevision(de.id, firstRevisionNumber).get().getEntity().name)
				.isEqualTo(""Deutschland"");
		assertThat(countryRepository.findRevision(de.id, secondRevisionNumber).get().getEntity().name).isEqualTo(""Germany"");
	}
",non-flaky,5
76913,spring-projects_spring-data-envers,EnversRevisionRepositoryImplUnitTests.findRevisionShortCircuitsOnEmptyRevisionList,"	@Test // #146
	public void findRevisionShortCircuitsOnEmptyRevisionList() {

		failOnEmptyRevisions();

		EnversRevisionRepositoryImplUnderTest<?, Object, ?> repository = new EnversRevisionRepositoryImplUnderTest<>(entityInformation, revisionEntityInformation, entityManager);

		repository.findRevisions(-999, PageRequest.of(0, 5));
	}
",non-flaky,5
76914,spring-projects_spring-data-envers,RepositoryIntegrationTests.testLifeCycle,"	@Test
	public void testLifeCycle() {

		License license = new License();
		license.name = ""Schnitzel"";

		licenseRepository.save(license);

		Country de = new Country();
		de.code = ""de"";
		de.name = ""Deutschland"";

		countryRepository.save(de);

		Country se = new Country();
		se.code = ""se"";
		se.name = ""Schweden"";

		countryRepository.save(se);

		license.laender = new HashSet<Country>();
		license.laender.addAll(Arrays.asList(de, se));

		licenseRepository.save(license);

		de.name = ""Daenemark"";

		countryRepository.save(de);

		Optional<Revision<Integer, License>> revision = licenseRepository.findLastChangeRevision(license.id);

		assertThat(revision).hasValueSatisfying(it -> {

			Page<Revision<Integer, License>> page = licenseRepository.findRevisions(license.id, PageRequest.of(0, 10));
			Revisions<Integer, License> revisions = Revisions.of(page.getContent());
			assertThat(revisions.getLatestRevision()).isEqualTo(it);
		});
	}
",non-flaky,5
76915,spring-projects_spring-data-envers,RepositoryIntegrationTests.returnsEmptyRevisionsForUnrevisionedEntity,"	@Test // #1
	public void returnsEmptyRevisionsForUnrevisionedEntity() {
		assertThat(countryRepository.findRevisions(100L)).isEmpty();
	}
",non-flaky,5
76916,spring-projects_spring-data-envers,RepositoryIntegrationTests.returnsParticularRevisionForAnEntity,"	@Test // #31
	public void returnsParticularRevisionForAnEntity() {

		Country de = new Country();
		de.code = ""de"";
		de.name = ""Deutschland"";

		countryRepository.save(de);

		de.name = ""Germany"";

		countryRepository.save(de);

		Revisions<Integer, Country> revisions = countryRepository.findRevisions(de.id);

		assertThat(revisions).hasSize(2);

		Iterator<Revision<Integer, Country>> iterator = revisions.iterator();
		Revision<Integer, Country> first = iterator.next();
		Revision<Integer, Country> second = iterator.next();

		assertThat(countryRepository.findRevision(de.id, first.getRequiredRevisionNumber())).hasValueSatisfying(it -> {
			assertThat(it.getEntity().name).isEqualTo(""Deutschland"");
		});

		assertThat(countryRepository.findRevision(de.id, second.getRequiredRevisionNumber())).hasValueSatisfying(it -> {
			assertThat(it.getEntity().name).isEqualTo(""Germany"");
		});
	}
",non-flaky,5
76917,spring-projects_spring-data-envers,RepositoryIntegrationTests.considersRevisionNumberSortOrder,"	@Test // #55
	public void considersRevisionNumberSortOrder() {

		Country de = new Country();
		de.code = ""de"";
		de.name = ""Deutschland"";

		countryRepository.save(de);

		de.name = ""Germany"";

		countryRepository.save(de);

		Page<Revision<Integer, Country>> page = countryRepository.findRevisions(de.id,
				PageRequest.of(0, 10, RevisionSort.desc()));

		assertThat(page).hasSize(2);
		assertThat(page.getContent().get(0).getRequiredRevisionNumber())
				.isGreaterThan(page.getContent().get(1).getRequiredRevisionNumber());
	}
",non-flaky,5
76918,spring-projects_spring-data-envers,RepositoryIntegrationTests.findsDeletedRevisions,"	@Test // #21
	public void findsDeletedRevisions() {

		Country de = new Country();
		de.code = ""de"";
		de.name = ""Deutschland"";

		countryRepository.save(de);

		countryRepository.delete(de);

		Revisions<Integer, Country> revisions = countryRepository.findRevisions(de.id);

		assertThat(revisions).hasSize(2);
		assertThat(revisions.getLatestRevision().getEntity()) //
				.isNotNull() //
				.extracting(c -> c.name, c -> c.code) //
				.containsExactly(null, null);
	}
",non-flaky,5
76919,spring-projects_spring-data-envers,RepositoryIntegrationTests.shortCurcuitingWhenOffsetIsToLarge,"	@Test // #146
	public void shortCurcuitingWhenOffsetIsToLarge() {
		Country de = new Country();
		de.code = ""de"";
		de.name = ""Deutschland"";

		countryRepository.save(de);

		countryRepository.delete(de);

		check(de, 0, 1);
		check(de, 1, 1);
		check(de, 2, 0);
	}
",non-flaky,5
77106,networknt_json-schema-validator,Issue406Test.testPreloadingNotHappening,"    @Test
    public void testPreloadingNotHappening() {
        final JsonSchemaFactory factory = JsonSchemaFactory.getInstance(SpecVersion.VersionFlag.V7);
        final JsonSchema schema = factory.getSchema(INVALID_$REF_SCHEMA);
        // not breaking - pass
        Assertions.assertNotNull(schema);
    }
",non-flaky,5
77107,networknt_json-schema-validator,Issue406Test.execute,"    @Test
    public void testPreloadingHappening() {
        final JsonSchemaFactory factory = JsonSchemaFactory.getInstance(SpecVersion.VersionFlag.V7);
        final JsonSchema schema = factory.getSchema(INVALID_$REF_SCHEMA);
        Assertions.assertThrows(JsonSchemaException.class,
                            new Executable() {
                                @Override
                                public void execute() {
                                    schema.initializeValidators();
                                }
",non-flaky,5
77108,networknt_json-schema-validator,Issue406Test.testPreloadingHappeningForCircularDependency,"    @Test
    public void testPreloadingHappeningForCircularDependency() {
        final JsonSchemaFactory factory = JsonSchemaFactory.getInstance(SpecVersion.VersionFlag.V7);
        final JsonSchema schema = factory.getSchema(CIRCULAR_$REF_SCHEMA);
        schema.initializeValidators();
    }
",non-flaky,5
77109,networknt_json-schema-validator,Issue386Test.dataIsValid,"    @ParameterizedTest
    public void dataIsValid(boolean failFast) throws Exception {
        String schemaPath = ""/schema/issue386-v7.json"";
        String dataPath = ""/data/issue386.json"";
        JsonSchema schema = getJsonSchemaFromPathV7(schemaPath, failFast);
        JsonNode node = getJsonNodeFromPath(dataPath).get(""valid"");
        node.forEach(testNode -> {
            Set<ValidationMessage> errors = schema.validate(testNode.get(""data""));
            Assertions.assertEquals(0, errors.size(), ""Expected no errors for "" + testNode.get(""data""));
        });
    }
",non-flaky,5
77110,networknt_json-schema-validator,Issue386Test.dataIsInvalidFailFast,"    @Test
    public void dataIsInvalidFailFast() throws Exception {
        String schemaPath = ""/schema/issue386-v7.json"";
        String dataPath = ""/data/issue386.json"";
        JsonSchema schema = getJsonSchemaFromPathV7(schemaPath, true);
        JsonNode node = getJsonNodeFromPath(dataPath).get(""invalid"");
        node.forEach(testNode -> {
            try {
                schema.validate(testNode.get(""data""));
                Assertions.fail();
            } catch (JsonSchemaException e) {
                Assertions.assertEquals(testNode.get(""expectedErrors"").get(0).asText(), e.getMessage());
            }
        });
    }
",non-flaky,5
77111,networknt_json-schema-validator,Issue386Test.dataIsInvalidFailSlow,"    @Test
    public void dataIsInvalidFailSlow() throws Exception {
        String schemaPath = ""/schema/issue386-v7.json"";
        String dataPath = ""/data/issue386.json"";
        JsonSchema schema = getJsonSchemaFromPathV7(schemaPath, false);
        JsonNode node = getJsonNodeFromPath(dataPath).get(""invalid"");
        node.forEach(testNode -> {
            Set<ValidationMessage> errors = schema.validate(testNode.get(""data""));
            List<String> errorMessages = errors.stream().map(x -> x.getMessage()).collect(Collectors.toList());
            testNode.get(""expectedErrors"").forEach(expectedError -> {
                Assertions.assertTrue(errorMessages.contains(expectedError.asText()));
            });
        });
    }
",non-flaky,5
77112,networknt_json-schema-validator,Issue425Test.testNullableOneOf,"    @Test
    public void testNullableOneOf() throws Exception {
        runTestFile(""data/issue425.json"");
    }
",non-flaky,5
77113,networknt_json-schema-validator,Issue366FailFastTest.setup,"  @BeforeEach
  public void setup() throws IOException {
    setupSchema();
  }
",non-flaky,5
77114,networknt_json-schema-validator,Issue366FailFastTest.firstOneValid,"  @Test
  public void firstOneValid() throws Exception {
    String dataPath = ""/data/issue366.json"";

    InputStream dataInputStream = getClass().getResourceAsStream(dataPath);
    JsonNode node = getJsonNodeFromStreamContent(dataInputStream);
    List<JsonNode> testNodes = node.findValues(""tests"");
    JsonNode testNode = testNodes.get(0).get(0);
    JsonNode dataNode = testNode.get(""data"");
    Set<ValidationMessage> errors = jsonSchema.validate(dataNode);
    assertTrue(errors.isEmpty());
  }
",non-flaky,5
77115,networknt_json-schema-validator,Issue366FailFastTest.secondOneValid,"  @Test
  public void secondOneValid() throws Exception {
    String dataPath = ""/data/issue366.json"";

    InputStream dataInputStream = getClass().getResourceAsStream(dataPath);
    JsonNode node = getJsonNodeFromStreamContent(dataInputStream);
    List<JsonNode> testNodes = node.findValues(""tests"");
    JsonNode testNode = testNodes.get(0).get(1);
    JsonNode dataNode = testNode.get(""data"");
    Set<ValidationMessage> errors = jsonSchema.validate(dataNode);
    assertTrue(errors.isEmpty());
  }
",non-flaky,5
77116,networknt_json-schema-validator,Issue366FailFastTest.bothValid,"  @Test
  public void bothValid() throws Exception {
    String dataPath = ""/data/issue366.json"";

    assertThrows(JsonSchemaException.class, () -> {
        InputStream dataInputStream = getClass().getResourceAsStream(dataPath);
        JsonNode node = getJsonNodeFromStreamContent(dataInputStream);
        List<JsonNode> testNodes = node.findValues(""tests"");
        JsonNode testNode = testNodes.get(0).get(2);
        JsonNode dataNode = testNode.get(""data"");
        jsonSchema.validate(dataNode);
    });
  }
",non-flaky,5
77117,networknt_json-schema-validator,Issue366FailFastTest.neitherValid,"  @Test
  public void neitherValid() throws Exception {
    String dataPath = ""/data/issue366.json"";

    assertThrows(JsonSchemaException.class, () -> {
        InputStream dataInputStream = getClass().getResourceAsStream(dataPath);
        JsonNode node = getJsonNodeFromStreamContent(dataInputStream);
        List<JsonNode> testNodes = node.findValues(""tests"");
        JsonNode testNode = testNodes.get(0).get(3);
        JsonNode dataNode = testNode.get(""data"");
        jsonSchema.validate(dataNode);
    });
  }
",non-flaky,5
77118,networknt_json-schema-validator,TypeFactoryTest.testValidIntegralValuesWithJavaSemantics,"    @Test
    public void testValidIntegralValuesWithJavaSemantics() {
        schemaValidatorsConfig.setJavaSemantics(true);
        for (String validValue : validIntegralValues) {
            assertSame(JsonType.INTEGER,
                    getValueNodeType(DecimalNode.valueOf(new BigDecimal(validValue)), schemaValidatorsConfig),
                    validValue);
        }
    }
",non-flaky,5
77119,networknt_json-schema-validator,TypeFactoryTest.testValidIntegralValuesWithoutJavaSemantics,"    @Test
    public void testValidIntegralValuesWithoutJavaSemantics() {
        schemaValidatorsConfig.setJavaSemantics(false);
        for (String validValue : validIntegralValues) {
            assertSame(JsonType.NUMBER,
                    getValueNodeType(DecimalNode.valueOf(new BigDecimal(validValue)), schemaValidatorsConfig),
                    validValue);
        }
    }
",non-flaky,5
77120,networknt_json-schema-validator,TypeFactoryTest.testWithLosslessNarrowing,"    @Test
    public void testWithLosslessNarrowing() {
        schemaValidatorsConfig.setLosslessNarrowing(true);
        for (String validValue : validIntegralValues) {
            assertSame(JsonType.INTEGER,
                    getValueNodeType(DecimalNode.valueOf(new BigDecimal(""1.0"")), schemaValidatorsConfig),
                    validValue);

            assertSame(JsonType.NUMBER,
                    getValueNodeType(DecimalNode.valueOf(new BigDecimal(""1.5"")), schemaValidatorsConfig),
                    validValue);
        }
    }
",non-flaky,5
77121,networknt_json-schema-validator,TypeFactoryTest.testWithoutLosslessNarrowing,"    @Test
    public void testWithoutLosslessNarrowing() {
        schemaValidatorsConfig.setLosslessNarrowing(false);
        for (String validValue : validIntegralValues) {
            assertSame(JsonType.NUMBER,
                    getValueNodeType(DecimalNode.valueOf(new BigDecimal(""1.0"")), schemaValidatorsConfig),
                    validValue);

            assertSame(JsonType.NUMBER,
                    getValueNodeType(DecimalNode.valueOf(new BigDecimal(""1.5"")), schemaValidatorsConfig),
                    validValue);
        }

    }
",non-flaky,5
77122,networknt_json-schema-validator,Issue461Test.shouldWalkWithValidation,"    @Test
    public void shouldWalkWithValidation() throws URISyntaxException, IOException {
        JsonSchema schema = getJsonSchemaFromStreamContentV7(new URI(""http://json-schema"" +
                "".org/draft-07/schema#""));
        JsonNode data = mapper.readTree(Issue461Test.class.getResource(""/data/issue461-v7.json""));
        ValidationResult result = schema.walk(data, true);
        Assertions.assertTrue(result.getValidationMessages().isEmpty());
    }
",non-flaky,5
77123,networknt_json-schema-validator,TypeValidatorTest.testNumeicValues,"    @Test
    public void testNumeicValues() {
        for (String validValue : validNumericValues) {
            assertTrue(isNumeric(validValue), validValue);
        }
    }
",non-flaky,5
77124,networknt_json-schema-validator,TypeValidatorTest.testNonNumeicValues,"    @Test
    public void testNonNumeicValues() {
        for (String invalidValue : invalidNumericValues) {
            assertFalse(isNumeric(invalidValue), invalidValue);
        }
    }
",non-flaky,5
77125,networknt_json-schema-validator,SpecVersionTest.testGetVersionValue,"    @Test
    public void testGetVersionValue() {
        SpecVersion ds = new SpecVersion();
        Set versionFlags = EnumSet.of(
                SpecVersion.VersionFlag.V4,
                SpecVersion.VersionFlag.V201909);
        Assertions.assertEquals(ds.getVersionValue(versionFlags), 9); // 0001|1000
    }
",non-flaky,5
77126,networknt_json-schema-validator,SpecVersionTest.testGetVersionFlags,"    @Test
    public void testGetVersionFlags() {
        SpecVersion ds = new SpecVersion();

        long numericVersionCode = SpecVersion.VersionFlag.V201909.getVersionFlagValue()
                | SpecVersion.VersionFlag.V6.getVersionFlagValue()
                | SpecVersion.VersionFlag.V7.getVersionFlagValue();  // 14

        Set versionFlags = ds.getVersionFlags(numericVersionCode);

        assert !versionFlags.contains(SpecVersion.VersionFlag.V4);
        assert versionFlags.contains(SpecVersion.VersionFlag.V6);
        assert versionFlags.contains(SpecVersion.VersionFlag.V7);
        assert versionFlags.contains(SpecVersion.VersionFlag.V201909);

    }
",non-flaky,5
77127,networknt_json-schema-validator,SpecVersionTest.testAllVersionValue,"    @Test
    public void testAllVersionValue() {
        long numericVersionCode =
                SpecVersion.VersionFlag.V201909.getVersionFlagValue()
                        | SpecVersion.VersionFlag.V4.getVersionFlagValue()
                        | SpecVersion.VersionFlag.V6.getVersionFlagValue()
                        | SpecVersion.VersionFlag.V7.getVersionFlagValue();  // 15
        Assertions.assertEquals(numericVersionCode, 15);

    }
",non-flaky,5
77128,networknt_json-schema-validator,Issue383Test.nestedOneOfsShouldStillMatchV7,"    @Test
    public void nestedOneOfsShouldStillMatchV7() throws Exception {
        String schemaPath = ""/schema/issue383-v7.json"";
        String dataPath = ""/data/issue383.json"";
        InputStream schemaInputStream = getClass().getResourceAsStream(schemaPath);
        JsonSchema schema = getJsonSchemaFromStreamContentV7(schemaInputStream);
        InputStream dataInputStream = getClass().getResourceAsStream(dataPath);
        JsonNode node = getJsonNodeFromStreamContent(dataInputStream);
        Set<ValidationMessage> errors = schema.validate(node);
        Assertions.assertEquals(0, errors.size());
    }
",non-flaky,5
77129,networknt_json-schema-validator,Issue396Test.testComplexPropertyNamesV7,"    @Test
    public void testComplexPropertyNamesV7() throws Exception {
        String schemaPath = ""/schema/issue396-v7.json"";
        String dataPath = ""/data/issue396.json"";
        InputStream schemaInputStream = getClass().getResourceAsStream(schemaPath);
        JsonSchema schema = getJsonSchemaFromStreamContentV7(schemaInputStream);
        InputStream dataInputStream = getClass().getResourceAsStream(dataPath);
        JsonNode node = getJsonNodeFromStreamContent(dataInputStream);

        final Set<String> invalidPaths = new HashSet<>();
        node.fields().forEachRemaining(entry -> {
            if (!entry.getValue().asBoolean())
                invalidPaths.add(""$."" + entry.getKey());
        });

        Set<ValidationMessage> errors = schema.validate(node);
        final Set<String> failedPaths = errors.stream().map(ValidationMessage::getPath).collect(Collectors.toSet());
        Assertions.assertEquals(failedPaths, invalidPaths);
    }
",non-flaky,5
77130,networknt_json-schema-validator,Issue255Test.shouldFailWhenRequiredPropertiesDoNotExistInReferencedSubSchema,"    @Test
    public void shouldFailWhenRequiredPropertiesDoNotExistInReferencedSubSchema() throws Exception {
        String schemaPath = ""/draft2019-09/issue255.json"";
        String dataPath = ""/data/issue255.json"";
        InputStream schemaInputStream = getClass().getResourceAsStream(schemaPath);
        JsonSchema schema = getJsonSchemaFromStreamContent(schemaInputStream);
        InputStream dataInputStream = getClass().getResourceAsStream(dataPath);
        JsonNode node = getJsonNodeFromStreamContent(dataInputStream);
        Set<ValidationMessage> errors = schema.validate(node);
        Assertions.assertEquals(2, errors.size());
    }
",non-flaky,5
77131,networknt_json-schema-validator,Issue342Test.propertyNameEnumShouldFailV7,"    @Test
    public void propertyNameEnumShouldFailV7() throws Exception {
        String schemaPath = ""/schema/issue342-v7.json"";
        String dataPath = ""/data/issue342.json"";
        InputStream schemaInputStream = getClass().getResourceAsStream(schemaPath);
        JsonSchema schema = getJsonSchemaFromStreamContentV7(schemaInputStream);
        InputStream dataInputStream = getClass().getResourceAsStream(dataPath);
        JsonNode node = getJsonNodeFromStreamContent(dataInputStream);
        Set<ValidationMessage> errors = schema.validate(node);
        Assertions.assertEquals(1, errors.size());
        final ValidationMessage error = errors.iterator().next();
        Assertions.assertEquals(""$.z"", error.getPath());
        Assertions.assertEquals(""Property name $.z is not valid for validation: does not have a value in the enumeration [a, b, c]"", error.getMessage());
    }
",non-flaky,5
77132,networknt_json-schema-validator,Issue285Test.nestedValidation,"    @Test
    public void nestedValidation() throws IOException {
        JsonSchema jsonSchema = schemaFactory.getSchema(schemaStr);
        Set<ValidationMessage> validationMessages = jsonSchema.validate(mapper.readTree(person));

        System.err.println(""\n"" + Arrays.toString(validationMessages.toArray()));

        assertFalse(validationMessages.isEmpty());


    }
",non-flaky,5
77133,networknt_json-schema-validator,Issue285Test.nestedTypeValidation,"    @Test
    public void nestedTypeValidation() throws IOException, URISyntaxException {
        URI uri = new URI(""https://json-schema.org/draft/2019-09/schema"");
        JsonSchema jsonSchema = schemaFactory.getSchema(uri);
        Set<ValidationMessage> validationMessages = jsonSchema.validate(mapper.readTree(invalidNestedSchema));

        System.err.println(""\n"" + Arrays.toString(validationMessages.toArray()));

        assertFalse(validationMessages.isEmpty());
    }
",non-flaky,5
77134,networknt_json-schema-validator,Issue285Test.typeValidation,"    @Test
    public void typeValidation() throws IOException, URISyntaxException {
        URI uri = new URI(""https://json-schema.org/draft/2019-09/schema"");
        JsonSchema jsonSchema = schemaFactory.getSchema(uri);
        Set<ValidationMessage> validationMessages = jsonSchema.validate(mapper.readTree(invalidSchema));

        System.err.println(""\n"" + Arrays.toString(validationMessages.toArray()));

        assertFalse(validationMessages.isEmpty());
    }
",non-flaky,5
77135,networknt_json-schema-validator,Issue426Test.shouldWorkV7,"    @Test
    public void shouldWorkV7() throws Exception {
        String schemaPath = ""/schema/issue426-v7.json"";
        String dataPath = ""/data/issue426.json"";
        InputStream schemaInputStream = getClass().getResourceAsStream(schemaPath);
        JsonSchema schema = getJsonSchemaFromStreamContentV7(schemaInputStream);
        InputStream dataInputStream = getClass().getResourceAsStream(dataPath);
        JsonNode node = getJsonNodeFromStreamContent(dataInputStream);
        Set<ValidationMessage> errors = schema.validate(node);
        Assertions.assertEquals(2, errors.size());
        final JsonNode message = schema.schemaNode.get(""message"");
        for(ValidationMessage error : errors) {
            //validating custom message
            Assertions.assertEquals(message.get(error.getType()).asText(),  error.getMessage());
        }
    }
",non-flaky,5
77136,networknt_json-schema-validator,Issue404Test.expectObjectNotIntegerV7,"    @Test
    public void expectObjectNotIntegerV7() throws Exception {
        String schemaPath = ""/schema/issue404-v7.json"";
        String dataPath = ""/data/issue404.json"";
        InputStream schemaInputStream = getClass().getResourceAsStream(schemaPath);
        JsonSchema schema = getJsonSchemaFromStreamContentV7(schemaInputStream);
        InputStream dataInputStream = getClass().getResourceAsStream(dataPath);
        JsonNode node = getJsonNodeFromStreamContent(dataInputStream);
        Set<ValidationMessage> errors = schema.validate(node);
        Assertions.assertEquals(0, errors.size());
    }
",non-flaky,5
77137,networknt_json-schema-validator,V4JsonSchemaTest.testLoadingWithId,"    @Test(/*expected = java.lang.StackOverflowError.class*/)
    public void testLoadingWithId() throws Exception {
        URL url = new URL(""http://localhost:1234/self_ref/selfRef.json"");
        JsonNode schemaJson = mapper.readTree(url);
        JsonSchemaFactory factory = JsonSchemaFactory.getInstance(SpecVersion.VersionFlag.V4);
        @SuppressWarnings(""unused"")
        JsonSchema schema = factory.getSchema(schemaJson);
    }
",non-flaky,5
77138,networknt_json-schema-validator,V4JsonSchemaTest.testBignumValidator,"    @Test
    public void testBignumValidator() throws Exception {
        runTestFile(""draft4/optional/bignum.json"");
    }
",non-flaky,5
77139,networknt_json-schema-validator,V4JsonSchemaTest.testFormatValidator,"    @Test
    public void testFormatValidator() throws Exception {
        runTestFile(""draft4/optional/format.json"");
    }
",non-flaky,5
77140,networknt_json-schema-validator,V4JsonSchemaTest.testComplexSchema,"    @Test
    public void testComplexSchema() throws Exception {
        runTestFile(""draft4/optional/complex.json"");
    }
",non-flaky,5
77141,networknt_json-schema-validator,V4JsonSchemaTest.testZeroTerminatedFloatsValidator,"    @Test
    public void testZeroTerminatedFloatsValidator() throws Exception {
        runTestFile(""draft4/optional/zeroTerminatedFloats.json"");
    }
",non-flaky,5
77142,networknt_json-schema-validator,V4JsonSchemaTest.testAdditionalItemsValidator,"    @Test
    public void testAdditionalItemsValidator() throws Exception {
        runTestFile(""draft4/additionalItems.json"");
    }
",non-flaky,5
77143,networknt_json-schema-validator,V4JsonSchemaTest.testAdditionalPropertiesValidator,"    @Test
    public void testAdditionalPropertiesValidator() throws Exception {
        runTestFile(""draft4/additionalProperties.json"");
    }
",non-flaky,5
77144,networknt_json-schema-validator,V4JsonSchemaTest.testAllOfValidator,"    @Test
    public void testAllOfValidator() throws Exception {
        runTestFile(""draft4/allOf.json"");
    }
",non-flaky,5
77145,networknt_json-schema-validator,V4JsonSchemaTest.testAnyOFValidator,"    @Test
    public void testAnyOFValidator() throws Exception {
        runTestFile(""draft4/anyOf.json"");
    }
",non-flaky,5
77146,networknt_json-schema-validator,V4JsonSchemaTest.testDefaultValidator,"    @Test
    public void testDefaultValidator() throws Exception {
        runTestFile(""draft4/default.json"");
    }
",non-flaky,5
77147,networknt_json-schema-validator,V4JsonSchemaTest.testDefinitionsValidator,"    @Test
    public void testDefinitionsValidator() throws Exception {
        runTestFile(""draft4/definitions.json"");
    }
",non-flaky,5
77148,networknt_json-schema-validator,V4JsonSchemaTest.testDependenciesValidator,"    @Test
    public void testDependenciesValidator() throws Exception {
        runTestFile(""draft4/dependencies.json"");
    }
",non-flaky,5
77149,networknt_json-schema-validator,V4JsonSchemaTest.testEnumValidator,"    @Test
    public void testEnumValidator() throws Exception {
        runTestFile(""draft4/enum.json"");
    }
",non-flaky,5
77150,networknt_json-schema-validator,V4JsonSchemaTest.testItemsValidator,"    @Test
    public void testItemsValidator() throws Exception {
        runTestFile(""draft4/items.json"");
    }
",non-flaky,5
77151,networknt_json-schema-validator,V4JsonSchemaTest.testMaximumValidator,"    @Test
    public void testMaximumValidator() throws Exception {
        runTestFile(""draft4/maximum.json"");
    }
",non-flaky,5
77152,networknt_json-schema-validator,V4JsonSchemaTest.testMaxItemsValidator,"    @Test
    public void testMaxItemsValidator() throws Exception {
        runTestFile(""draft4/maxItems.json"");
    }
",non-flaky,5
77153,networknt_json-schema-validator,V4JsonSchemaTest.testMaxLengthValidator,"    @Test
    public void testMaxLengthValidator() throws Exception {
        runTestFile(""draft4/maxLength.json"");
    }
",non-flaky,5
77154,networknt_json-schema-validator,V4JsonSchemaTest.testMaxPropertiesValidator,"    @Test
    public void testMaxPropertiesValidator() throws Exception {
        runTestFile(""draft4/maxProperties.json"");
    }
",non-flaky,5
77155,networknt_json-schema-validator,V4JsonSchemaTest.testMinimumValidator,"    @Test
    public void testMinimumValidator() throws Exception {
        runTestFile(""draft4/minimum.json"");
    }
",non-flaky,5
77156,networknt_json-schema-validator,V4JsonSchemaTest.testMinItemsValidator,"    @Test
    public void testMinItemsValidator() throws Exception {
        runTestFile(""draft4/minItems.json"");
    }
",non-flaky,5
77157,networknt_json-schema-validator,V4JsonSchemaTest.testMinLengthValidator,"    @Test
    public void testMinLengthValidator() throws Exception {
        runTestFile(""draft4/minLength.json"");
    }
",non-flaky,5
77158,networknt_json-schema-validator,V4JsonSchemaTest.testMinPropertiesValidator,"    @Test
    public void testMinPropertiesValidator() throws Exception {
        runTestFile(""draft4/minProperties.json"");
    }
",non-flaky,5
77159,networknt_json-schema-validator,V4JsonSchemaTest.testMultipleOfValidator,"    @Test
    public void testMultipleOfValidator() throws Exception {
        runTestFile(""draft4/multipleOf.json"");
    }
",non-flaky,5
77160,networknt_json-schema-validator,V4JsonSchemaTest.testNotValidator,"    @Test
    public void testNotValidator() throws Exception {
        runTestFile(""draft4/not.json"");
    }
",non-flaky,5
77161,networknt_json-schema-validator,V4JsonSchemaTest.testOneOfValidator,"    @Test
    public void testOneOfValidator() throws Exception {
        runTestFile(""draft4/oneOf.json"");
    }
",non-flaky,5
77162,networknt_json-schema-validator,V4JsonSchemaTest.testPatternValidator,"    @Test
    public void testPatternValidator() throws Exception {
        runTestFile(""draft4/pattern.json"");
    }
",non-flaky,5
77163,networknt_json-schema-validator,V4JsonSchemaTest.testPatternPropertiesValidator,"    @Test
    public void testPatternPropertiesValidator() throws Exception {
        runTestFile(""draft4/patternProperties.json"");
    }
",non-flaky,5
77164,networknt_json-schema-validator,V4JsonSchemaTest.testPropertiesValidator,"    @Test
    public void testPropertiesValidator() throws Exception {
        runTestFile(""draft4/properties.json"");
    }
",non-flaky,5
77165,networknt_json-schema-validator,V4JsonSchemaTest.testRefValidator,"    @Test
    public void testRefValidator() throws Exception {
        runTestFile(""draft4/ref.json"");
    }
",non-flaky,5
77166,networknt_json-schema-validator,V4JsonSchemaTest.testRefRemoteValidator,"    @Test
    public void testRefRemoteValidator() throws Exception {
        runTestFile(""draft4/refRemote.json"");
    }
",non-flaky,5
77167,networknt_json-schema-validator,V4JsonSchemaTest.testRefIdReference,"    @Test
    public void testRefIdReference() throws Exception {
        runTestFile(""draft4/idRef.json"");
    }
",non-flaky,5
77168,networknt_json-schema-validator,V4JsonSchemaTest.testRelativeRefRemoteValidator,"    @Test
    public void testRelativeRefRemoteValidator() throws Exception {
        runTestFile(""draft4/relativeRefRemote.json"");
    }
",non-flaky,5
77169,networknt_json-schema-validator,V4JsonSchemaTest.testRequiredValidator,"    @Test
    public void testRequiredValidator() throws Exception {
        runTestFile(""draft4/required.json"");
    }
",non-flaky,5
77170,networknt_json-schema-validator,V4JsonSchemaTest.testTypeValidator,"    @Test
    public void testTypeValidator() throws Exception {
        runTestFile(""draft4/type.json"");
    }
",non-flaky,5
77171,networknt_json-schema-validator,V4JsonSchemaTest.testUnionTypeValidator,"    @Test
    public void testUnionTypeValidator() throws Exception {
        runTestFile(""draft4/union_type.json"");
    }
",non-flaky,5
77172,networknt_json-schema-validator,V4JsonSchemaTest.testUniqueItemsValidator,"    @Test
    public void testUniqueItemsValidator() throws Exception {
        runTestFile(""draft4/uniqueItems.json"");
    }
",non-flaky,5
77173,networknt_json-schema-validator,V4JsonSchemaTest.testEnumObject,"    @Test
    public void testEnumObject() throws Exception {
        runTestFile(""draft4/enumObject.json"");
    }
",non-flaky,5
77174,networknt_json-schema-validator,V4JsonSchemaTest.testIdSchemaWithUrl,"    @Test
    public void testIdSchemaWithUrl() throws Exception {
        runTestFile(""draft4/property.json"");
    }
",non-flaky,5
77175,networknt_json-schema-validator,V4JsonSchemaTest.testSchemaFromClasspath,"    @Test
    public void testSchemaFromClasspath() throws Exception {
        runTestFile(""draft4/classpath/schema.json"");
    }
",non-flaky,5
77176,networknt_json-schema-validator,V4JsonSchemaTest.testUUIDValidator,"    @Test
    public void testUUIDValidator() throws Exception {
        runTestFile(""draft4/uuid.json"");
    }
",non-flaky,5
77177,networknt_json-schema-validator,V4JsonSchemaTest.testFailFast_AllErrors,"    @Test
    public void testFailFast_AllErrors() throws IOException {
        try {
            validateFailingFastSchemaFor(""product.schema.json"", ""product-all-errors-data.json"");
            fail(""Exception must be thrown"");
        } catch (JsonSchemaException e) {
            final Set<ValidationMessage> messages = e.getValidationMessages();
            assertEquals(1, messages.size());
        }
    }
",non-flaky,5
77178,networknt_json-schema-validator,V4JsonSchemaTest.testFailFast_OneErrors,"    @Test
    public void testFailFast_OneErrors() throws IOException {
        try {
            validateFailingFastSchemaFor(""product.schema.json"", ""product-one-error-data.json"");
            fail(""Exception must be thrown"");
        } catch (JsonSchemaException e) {
            final Set<ValidationMessage> messages = e.getValidationMessages();
            assertEquals(1, messages.size());
        }
    }
",non-flaky,5
77179,networknt_json-schema-validator,V4JsonSchemaTest.testFailFast_TwoErrors,"    @Test
    public void testFailFast_TwoErrors() throws IOException {
        try {
            validateFailingFastSchemaFor(""product.schema.json"", ""product-two-errors-data.json"");
            fail(""Exception must be thrown"");
        } catch (JsonSchemaException e) {
            final Set<ValidationMessage> messages = e.getValidationMessages();
            assertEquals(1, messages.size());
        }
    }
",non-flaky,5
77180,networknt_json-schema-validator,V4JsonSchemaTest.testFailFast_NoErrors,"    @Test
    public void testFailFast_NoErrors() throws IOException {
        try {
            final Set<ValidationMessage> messages = validateFailingFastSchemaFor(""product.schema.json"", ""product-no-errors-data.json"");
            assertTrue(messages.isEmpty());
        } catch (JsonSchemaException e) {
            fail(""Must not get an errors"");
        }
    }
",non-flaky,5
77181,networknt_json-schema-validator,Issue451Test.cleanup,"    @AfterEach
    public void cleanup() {
        reset();
    }
",non-flaky,5
77182,networknt_json-schema-validator,Issue451Test.shouldWalkAnyOfProperties,"    @Test
    public void shouldWalkAnyOfProperties() {
        walk(null, false);
    }
",non-flaky,5
77183,networknt_json-schema-validator,Issue451Test.shouldWalkAnyOfPropertiesWithWithPayloadAndValidation,"    @Test
    public void shouldWalkAnyOfPropertiesWithWithPayloadAndValidation() throws Exception {
        JsonNode data = getJsonNodeFromStreamContent(Issue451Test.class.getResourceAsStream(
                ""/data/issue451.json""));
        walk(data,true);
    }
",non-flaky,5
77184,networknt_json-schema-validator,Issue451Test.shouldWalkAnyOfPropertiesWithWithPayload,"    @Test
    public void shouldWalkAnyOfPropertiesWithWithPayload() throws Exception {
        JsonNode data = getJsonNodeFromStreamContent(Issue451Test.class.getResourceAsStream(
                ""/data/issue451.json""));
        walk(data, false);
    }
",non-flaky,5
77185,networknt_json-schema-validator,Issue456Test.shouldWorkT2,"    @Test
    public void shouldWorkT2() throws Exception {
        String schemaPath = ""/schema/issue456-v7.json"";
        String dataPath = ""/data/issue456-T2.json"";
        String dataT3Path = ""/data/issue456-T3.json"";
        InputStream schemaInputStream = getClass().getResourceAsStream(schemaPath);
        JsonSchema schema = getJsonSchemaFromStreamContentV7(schemaInputStream);
        InputStream dataInputStream = getClass().getResourceAsStream(dataPath);
        JsonNode node = getJsonNodeFromStreamContent(dataInputStream);
        Set<ValidationMessage> errors = schema.validate(node);
        Assertions.assertEquals(0, errors.size());
    }
",non-flaky,5
77186,networknt_json-schema-validator,Issue456Test.shouldWorkT3,"    @Test
    public void shouldWorkT3() throws Exception {
        String schemaPath = ""/schema/issue456-v7.json"";
        String dataPath = ""/data/issue456-T3.json"";
        InputStream schemaInputStream = getClass().getResourceAsStream(schemaPath);
        JsonSchema schema = getJsonSchemaFromStreamContentV7(schemaInputStream);
        InputStream dataInputStream = getClass().getResourceAsStream(dataPath);
        JsonNode node = getJsonNodeFromStreamContent(dataInputStream);
        Set<ValidationMessage> errors = schema.validate(node);
        Assertions.assertEquals(0, errors.size());
    }
",non-flaky,5
77187,networknt_json-schema-validator,JsonWalkTest.setup,"    @BeforeEach
    public void setup() {
        setupSchema();
    }
",non-flaky,5
77188,networknt_json-schema-validator,JsonWalkTest.cleanup,"    @AfterEach
    public void cleanup() {
       CollectorContext.getInstance().reset();
    }
",non-flaky,5
77189,networknt_json-schema-validator,JsonWalkTest.testWalk,"    @Test
    public void testWalk() throws IOException {
        ObjectMapper objectMapper = new ObjectMapper();
        ValidationResult result = jsonSchema.walk(
                objectMapper.readTree(getClass().getClassLoader().getResourceAsStream(""data/walk-data.json"")), false);
        JsonNode collectedNode = (JsonNode) result.getCollectorContext().get(SAMPLE_WALK_COLLECTOR_TYPE);
        assertEquals(collectedNode, (objectMapper.readTree(""{"" +
                ""    \""PROPERTY1\"": \""sample1\"",""
                + ""    \""PROPERTY2\"": \""sample2\"",""
                + ""    \""property3\"": {""
                + ""        \""street_address\"":\""test-address\"",""
                + ""        \""phone_number\"": {""
                + ""            \""country-code\"": \""091\"",""
                + ""            \""number\"": \""123456789\""""
                + ""          }""
                + ""     }""
                + ""}"")));
    }
",non-flaky,5
77190,networknt_json-schema-validator,JsonWalkTest.testWalkWithDifferentListeners,"    @Test
    public void testWalkWithDifferentListeners() throws IOException {
        ObjectMapper objectMapper = new ObjectMapper();
        // This instance of schema contains all listeners.
        ValidationResult result = jsonSchema.walk(
                objectMapper.readTree(getClass().getClassLoader().getResourceAsStream(""data/walk-data.json"")), false);
        JsonNode collectedNode = (JsonNode) result.getCollectorContext().get(SAMPLE_WALK_COLLECTOR_TYPE);
        assertEquals(collectedNode, (objectMapper.readTree(""{"" +
                ""    \""PROPERTY1\"": \""sample1\"",""
                + ""    \""PROPERTY2\"": \""sample2\"",""
                + ""    \""property3\"": {""
                + ""        \""street_address\"":\""test-address\"",""
                + ""        \""phone_number\"": {""
                + ""            \""country-code\"": \""091\"",""
                + ""            \""number\"": \""123456789\""""
                + ""          }""
                + ""     }""
                + ""}"")));
        // This instance of schema contains one listener removed.
        CollectorContext collectorContext = result.getCollectorContext();
        collectorContext.reset();
        result = jsonSchema1.walk(
                objectMapper.readTree(getClass().getClassLoader().getResourceAsStream(""data/walk-data.json"")), false);
        collectedNode = (JsonNode) result.getCollectorContext().get(SAMPLE_WALK_COLLECTOR_TYPE);
        assertEquals(collectedNode, (objectMapper.readTree(""{""
                + ""    \""property3\"": {""
                + ""        \""street_address\"":\""test-address\"",""
                + ""        \""phone_number\"": {""
                + ""            \""country-code\"": \""091\"",""
                + ""            \""number\"": \""123456789\""""
                + ""          }""
                + ""     }""
                + ""}"")));
    }
",non-flaky,5
77191,networknt_json-schema-validator,Issue313Test.shouldFailV201909,"    @Test
    public void shouldFailV201909() throws Exception {
        String schemaPath = ""/schema/issue313-2019-09.json"";
        String dataPath = ""/data/issue313.json"";
        InputStream schemaInputStream = getClass().getResourceAsStream(schemaPath);
        JsonSchema schema = getJsonSchemaFromStreamContentV201909(schemaInputStream);
        InputStream dataInputStream = getClass().getResourceAsStream(dataPath);
        JsonNode node = getJsonNodeFromStreamContent(dataInputStream);
        Set<ValidationMessage> errors = schema.validate(node);
        Assertions.assertEquals(2, errors.size());
    }
",non-flaky,5
77192,networknt_json-schema-validator,Issue313Test.shouldFailV7,"    @Test
    public void shouldFailV7() throws Exception {
        String schemaPath = ""/schema/issue313-v7.json"";
        String dataPath = ""/data/issue313.json"";
        InputStream schemaInputStream = getClass().getResourceAsStream(schemaPath);
        JsonSchema schema = getJsonSchemaFromStreamContentV7(schemaInputStream);
        InputStream dataInputStream = getClass().getResourceAsStream(dataPath);
        JsonNode node = getJsonNodeFromStreamContent(dataInputStream);
        Set<ValidationMessage> errors = schema.validate(node);
        Assertions.assertEquals(2, errors.size());
    }
",non-flaky,5
77193,networknt_json-schema-validator,Issue428Test.testNullableOneOf,"    @Test
    public void testNullableOneOf() throws Exception {
        runTestFile(""data/issue428.json"");
    }
",non-flaky,5
77194,networknt_json-schema-validator,MaximumValidatorTest.positiveNumber,"    @Test
    public void positiveNumber() throws IOException {
        String[][] values = augmentWithQuotes(new String[][]{
//            maximum,                       value
                {""1000.1"", ""1000""},
                {""1000"", ""1E3""},
        });

        expectNoMessages(values, NUMBER);

    }
",non-flaky,5
77195,networknt_json-schema-validator,MaximumValidatorTest.negativeNumber,"    @Test
    public void negativeNumber() throws IOException {
        String[][] values = augmentWithQuotes(new String[][]{
//            maximum,                           value
//            These values overflow 64bit IEEE 754
                {""1.7976931348623157e+308"", ""1.7976931348623159e+308""},
                {""1.7976931348623156e+308"", ""1.7976931348623157e+308""},

//            Here, threshold is parsed as integral number, yet payload is 'number'
                {""1000"", ""1000.1""},

//          See a {@link #doubleValueCoarsing() doubleValueCoarsing} test notes below
//            {""1.7976931348623157e+308"",         ""1.7976931348623158e+308""},
        });

        expectSomeMessages(values, NUMBER);

        expectSomeMessages(values, NUMBER, mapper, bigDecimalMapper);

        expectSomeMessages(values, NUMBER, bigDecimalMapper, bigDecimalMapper);
    }
",non-flaky,5
77196,networknt_json-schema-validator,MaximumValidatorTest.positiveInteger,"    @Test
    public void positiveInteger() throws IOException {
        String[][] values = augmentWithQuotes(new String[][]{
//            maximum,                       value
                {""9223372036854775807"", ""9223372036854775807""},
                {""9223372036854775808"", ""9223372036854775808""},

//                testIntegerTypeWithFloatMaxPositive
                {""37.7"", ""37""},

//                testMaximumDoubleValue
                {""1E39"", ""1000""},
        });

        expectNoMessages(values, INTEGER);

        expectNoMessages(values, INTEGER, bigIntegerMapper);
    }
",non-flaky,5
77197,networknt_json-schema-validator,MaximumValidatorTest.negativeInteger,"    @Test
    public void negativeInteger() throws IOException {
        String[][] values = augmentWithQuotes(new String[][]{
//            maximum,                value
                {""9223372036854775800"", ""9223372036854775855""},
                {""9223372036854775807"", ""9223372036854775808""},
                {""9223372036854775807"", new BigDecimal(String.valueOf(Double.MAX_VALUE)).add(BigDecimal.ONE).toString()},
                {""9223372036854775806"", new BigDecimal(String.valueOf(Double.MAX_VALUE)).add(BigDecimal.ONE).toString()},
                {""9223372036854776000"", ""9223372036854776001""},
                {""1000"", ""1E39""},
                {""37.7"", ""38""},
        });

        expectSomeMessages(values, INTEGER);

        expectSomeMessages(values, INTEGER, mapper, bigIntegerMapper);
    }
",non-flaky,5
77198,networknt_json-schema-validator,MaximumValidatorTest.positiveExclusiveInteger,"    @Test
    public void positiveExclusiveInteger() throws IOException {
        String[][] values = augmentWithQuotes(new String[][]{
//            maximum,                       value
                {""9223372036854775000"", ""9223372036854774988""},
                {""20"", ""10""},

//                threshold outside long range
                {""9223372036854775809"", ""9223372036854775806""},

//                both threshold and value are outside long range
                {""9223372036854775809"", ""9223372036854775808""},
        });

        expectNoMessages(values, EXCLUSIVE_INTEGER);

        expectNoMessages(values, EXCLUSIVE_INTEGER, bigIntegerMapper);
    }
",non-flaky,5
77199,networknt_json-schema-validator,MaximumValidatorTest.negativeExclusiveInteger,"    @Test
    public void negativeExclusiveInteger() throws IOException {
        String[][] values = augmentWithQuotes(new String[][]{
//            maximum,                       value
                {""10"", ""20""},

//                value outside long range
                {""9223372036854775806"", ""9223372036854775808""},

//                both threshold and value are outside long range
                {""9223372036854775808"", ""9223372036854775809""},
        });

        expectSomeMessages(values, EXCLUSIVE_INTEGER);

        expectSomeMessages(values, EXCLUSIVE_INTEGER, mapper, bigIntegerMapper);
    }
",non-flaky,5
77200,networknt_json-schema-validator,MaximumValidatorTest.negativeDoubleOverflowTest,"    @Test
    public void negativeDoubleOverflowTest() throws IOException {
        String[][] values = new String[][]{
//            maximum,                           value
//                both of these get parsed into double (with a precision loss) as  1.7976931348623157E+308
                {""1.79769313486231571E+308"", ""1.79769313486231572e+308""},
//                while underflow in not captures in previous case (unquoted number is parsed as double)
//                it is captured if value is passed as string, which is correctly parsed by BidDecimal
//                thus effective comparison is between
//                maximum 1.7976931348623157E+308  and
//                value   1.79769313486231572e+308
//                {""1.79769313486231571E+308"",        ""\""1.79769313486231572e+308\""""},
                {""1.7976931348623157E+309"", ""1.7976931348623157e+309""},
                {""1.7976931348623157E+309"", ""\""1.7976931348623157e+309\""""},
                {""1.000000000000000000000001E+400"", ""1.000000000000000000000001E+401""},
                {""1.000000000000000000000001E+400"", ""\""1.000000000000000000000001E+401\""""},
                {""1.000000000000000000000001E+400"", ""1.000000000000000000000002E+400""},
                {""1.000000000000000000000001E+400"", ""\""1.000000000000000000000002E+400\""""},
                {""1.000000000000000000000001E+400"", ""1.0000000000000000000000011E+400""},
                {""1.000000000000000000000001E+400"", ""\""1.0000000000000000000000011E+400\""""},
        };

        for (String[] aTestCycle : values) {
            String maximum = aTestCycle[0];
            String value = aTestCycle[1];
            String schema = format(NUMBER, maximum);
            SchemaValidatorsConfig config = new SchemaValidatorsConfig();
            config.setTypeLoose(true);
            // Schema and document parsed with just double
            JsonSchema v = factory.getSchema(mapper.readTree(schema), config);
            JsonNode doc = mapper.readTree(value);
            Set<ValidationMessage> messages = v.validate(doc);
            assertTrue(messages.isEmpty(), format(""Maximum %s and value %s are interpreted as Infinity, thus no schema violation should be reported"", maximum, value));

            // document parsed with BigDecimal

            doc = bigDecimalMapper.readTree(value);
            Set<ValidationMessage> messages2 = v.validate(doc);
            if (Double.valueOf(maximum).equals(Double.POSITIVE_INFINITY)) {
                assertTrue(messages2.isEmpty(), format(""Maximum %s and value %s are equal, thus no schema violation should be reported"", maximum, value));
            } else {
                assertFalse(messages2.isEmpty(), format(""Maximum %s is smaller than value %s ,  should be validation error reported"", maximum, value));
            }


            // schema and document parsed with BigDecimal
            v = factory.getSchema(bigDecimalMapper.readTree(schema), config);
            Set<ValidationMessage> messages3 = v.validate(doc);
            //when the schema and value are both using BigDecimal, the value should be parsed in same mechanism.
            if (maximum.toLowerCase().equals(value.toLowerCase()) || Double.valueOf(maximum).equals(Double.POSITIVE_INFINITY)) {
                assertTrue(messages3.isEmpty(), format(""Maximum %s and value %s are equal, thus no schema violation should be reported"", maximum, value));
            } else {
                assertFalse(messages3.isEmpty(), format(""Maximum %s is smaller than value %s ,  should be validation error reported"", maximum, value));
            }
        }
    }
",non-flaky,5
77201,networknt_json-schema-validator,MaximumValidatorTest.doubleValueCoarsing,"    @Test
    public void doubleValueCoarsing() throws IOException {
        String schema = ""{ \""$schema\"":\""http://json-schema.org/draft-04/schema#\"", \""type\"": \""number\"", \""maximum\"": 1.7976931348623157e+308 }"";
        String content = ""1.7976931348623158e+308"";

        JsonNode doc = mapper.readTree(content);
        JsonSchema v = factory.getSchema(mapper.readTree(schema));

        Set<ValidationMessage> messages = v.validate(doc);
        assertTrue(messages.isEmpty(), ""Validation should succeed as by default double values are used by mapper"");

        doc = bigDecimalMapper.readTree(content);
        messages = v.validate(doc);
        // ""1.7976931348623158e+308"" == ""1.7976931348623157e+308"" == Double.MAX_VALUE
        // new BigDecimal(""1.7976931348623158e+308"").compareTo(new BigDecimal(""1.7976931348623157e+308"")) > 0
        assertFalse(messages.isEmpty(), ""Validation should not succeed because content is using bigDecimalMapper, and bigger than the maximum"");

        /*
         * Note: technically this is where 1.7976931348623158e+308 rounding to 1.7976931348623157e+308 could be spotted,
         *       yet it requires a dedicated case of comparison BigDecimal to BigDecimal. Since values above
         *       1.7976931348623158e+308 are parsed as Infinity anyways (jackson uses double as primary type with later
         *       ""upcasting"" to BigDecimal, if property is set) adding a dedicated code block just for this one case
         *       seems infeasible.
         */
        v = factory.getSchema(bigDecimalMapper.readTree(schema));
        messages = v.validate(doc);
        assertFalse(messages.isEmpty(), ""Validation should succeed as by default double values are used by mapper"");
    }
",non-flaky,5
77202,networknt_json-schema-validator,MaximumValidatorTest.doubleValueCoarsingExceedRange,"    @Test
    public void doubleValueCoarsingExceedRange() throws IOException {
        String schema = ""{ \""$schema\"":\""http://json-schema.org/draft-04/schema#\"", \""type\"": \""number\"", \""maximum\"": 1.7976931348623159e+308 }"";
        String content = ""1.7976931348623160e+308"";

        JsonNode doc = mapper.readTree(content);
        JsonSchema v = factory.getSchema(mapper.readTree(schema));

        Set<ValidationMessage> messages = v.validate(doc);
        assertTrue(messages.isEmpty(), ""Validation should succeed as by default double values are used by mapper"");

        doc = bigDecimalMapper.readTree(content);
        messages = v.validate(doc);
        // ""1.7976931348623158e+308"" == ""1.7976931348623157e+308"" == Double.MAX_VALUE
        // new BigDecimal(""1.7976931348623158e+308"").compareTo(new BigDecimal(""1.7976931348623157e+308"")) > 0
        assertTrue(messages.isEmpty(), ""Validation should success because the bug of bigDecimalMapper, it will treat 1.7976931348623159e+308 as INFINITY"");

        /*
         * Note: technically this is where 1.7976931348623158e+308 rounding to 1.7976931348623157e+308 could be spotted,
         *       yet it requires a dedicated case of comparison BigDecimal to BigDecimal. Since values above
         *       1.7976931348623158e+308 are parsed as Infinity anyways (jackson uses double as primary type with later
         *       ""upcasting"" to BigDecimal, if property is set) adding a dedicated code block just for this one case
         *       seems infeasible.
         */
        v = factory.getSchema(bigDecimalMapper.readTree(schema));
        messages = v.validate(doc);
        assertTrue(messages.isEmpty(), ""Validation should success because the bug of bigDecimalMapper, it will treat 1.7976931348623159e+308 as INFINITY"");
    }
",non-flaky,5
77203,networknt_json-schema-validator,MinimumValidatorTest.setUp,"    @BeforeEach
    public void setUp() {
        mapper = new ObjectMapper();
        // due to a jackson bug, a float number which is larger than Double.POSITIVE_INFINITY cannot be convert to BigDecimal correctly
        // https://github.com/FasterXML/jackson-databind/issues/1770
        // https://github.com/FasterXML/jackson-databind/issues/2087
        bigDecimalMapper = new ObjectMapper().enable(DeserializationFeature.USE_BIG_DECIMAL_FOR_FLOATS);
        bigIntegerMapper = new ObjectMapper().enable(DeserializationFeature.USE_BIG_INTEGER_FOR_INTS);

    }
",non-flaky,5
77204,networknt_json-schema-validator,MinimumValidatorTest.positiveNumber,"    @Test
    public void positiveNumber() throws IOException {
        String[][] values = augmentWithQuotes(new String[][]{
//            minimum,                       value
                {""1000"", ""1000.1""},
        });

        expectNoMessages(values, NUMBER, mapper);
    }
",non-flaky,5
77205,networknt_json-schema-validator,MinimumValidatorTest.negativeNumber,"    @Test
    public void negativeNumber() throws IOException {
        String[][] values = augmentWithQuotes(new String[][]{
//            minimum,                           value
                {""-1.7976931348623157e+308"", ""-1.7976931348623159e+308""},
                {""-1.7976931348623156e+308"", ""-1.7976931348623157e+308""},
                {""-1000"", ""-1E309""},
                {""1000.1"", ""1000""},
//          See a {@link #doubleValueCoarsing() doubleValueCoarsing} test notes below
//            {""-1.7976931348623157e+308"",         ""-1.7976931348623158e+308""},
        });

        expectSomeMessages(values, NUMBER, mapper, mapper);

        expectSomeMessages(values, NUMBER, mapper, bigDecimalMapper);

        expectSomeMessages(values, NUMBER, bigDecimalMapper, bigDecimalMapper);
    }
",non-flaky,5
88771,apache_ignite,SharedRDDExampleSelfTest.testSharedRDDExample,"    @Test
    public void testSharedRDDExample() throws Exception {
        SharedRDDExample.main(EMPTY_ARGS);
    }
",non-flaky,5
88772,apache_ignite,JavaIgniteDataFrameSelfTest.testCatalogExample,"    @Test
    public void testCatalogExample() throws Exception {
        JavaIgniteCatalogExample.main(EMPTY_ARGS);
    }
",non-flaky,5
88773,apache_ignite,JavaIgniteDataFrameSelfTest.testDataFrameExample,"    @Test
    public void testDataFrameExample() throws Exception {
        JavaIgniteDataFrameExample.main(EMPTY_ARGS);
    }
",non-flaky,5
88774,apache_ignite,JavaIgniteDataFrameSelfTest.testDataFrameWriteExample,"    @Test
    public void testDataFrameWriteExample() throws Exception {
        JavaIgniteDataFrameWriteExample.main(EMPTY_ARGS);
    }
",non-flaky,5
88775,apache_ignite,IgniteDataFrameSelfTest.testCatalogExample,"    @Test
    public void testCatalogExample() throws Exception {
        IgniteCatalogExample.main(EMPTY_ARGS);
    }
",non-flaky,5
88776,apache_ignite,IgniteDataFrameSelfTest.testDataFrameExample,"    @Test
    public void testDataFrameExample() throws Exception {
        IgniteDataFrameExample.main(EMPTY_ARGS);
    }
",non-flaky,5
88777,apache_ignite,IgniteDataFrameSelfTest.testDataFrameWriteExample,"    @Test
    public void testDataFrameWriteExample() throws Exception {
        IgniteDataFrameWriteExample.main(EMPTY_ARGS);
    }
",non-flaky,5
88778,apache_ignite,IgniteOsgiServiceTest.testServiceExposedAndCallbacksInvoked,"    @Test
    public void testServiceExposedAndCallbacksInvoked() throws Exception {
        assertNotNull(ignite);
        assertEquals(""testGrid"", ignite.name());

        TestOsgiFlags flags = (TestOsgiFlags) bundleCtx.getService(
            bundleCtx.getAllServiceReferences(TestOsgiFlags.class.getName(), null)[0]);

        assertNotNull(flags);
        assertEquals(Boolean.TRUE, flags.getOnBeforeStartInvoked());
        assertEquals(Boolean.TRUE, flags.getOnAfterStartInvoked());

        // The bundle is still not stopped, therefore these callbacks cannot be tested.
        assertNull(flags.getOnBeforeStopInvoked());
        assertNull(flags.getOnAfterStopInvoked());

        // No exceptions.
        assertNull(flags.getOnAfterStartThrowable());
        assertNull(flags.getOnAfterStopThrowable());
    }
",non-flaky,5
88779,apache_ignite,IgniteKarafFeaturesInstallationTest.testAllBundlesActiveAndFeaturesInstalled,"    @Test
    public void testAllBundlesActiveAndFeaturesInstalled() throws Exception {
        // Asssert all bundles except fragments are ACTIVE.
        for (Bundle b : bundleCtx.getBundles()) {
            System.out.println(String.format(""Checking state of bundle [symbolicName=%s, state=%s]"",
                b.getSymbolicName(), b.getState()));

            if (b.getHeaders().get(Constants.FRAGMENT_HOST) == null)
                assertTrue(b.getState() == Bundle.ACTIVE);
        }

        // Check that according to the FeaturesService, all Ignite features except ignite-log4j are installed.
        Feature[] features = featuresSvc.getFeatures(IGNITE_FEATURES_NAME_REGEX);

        assertNotNull(features);
        assertEquals(EXPECTED_FEATURES, features.length);

        for (Feature f : features) {
            if (IGNORED_FEATURES.contains(f.getName()))
                continue;

            boolean installed = featuresSvc.isInstalled(f);

            System.out.println(String.format(""Checking if feature is installed [featureName=%s, installed=%s]"",
                f.getName(), installed));

            assertTrue(installed);
            assertEquals(PROJECT_VERSION.replaceAll(""-"", "".""), f.getVersion().replaceAll(""-"", "".""));
        }
    }
",non-flaky,5
88780,apache_ignite,LongRunningProcessManagerTest.testStart,"    @Test
    public void testStart() {
        UUID nodeId = UUID.randomUUID();
        UUID procId = UUID.randomUUID();

        Ignite ignite = mock(Ignite.class);
        IgniteCluster cluster = mock(IgniteCluster.class);
        ClusterGroup clusterGrp = mock(ClusterGroup.class);
        IgniteCompute igniteCompute = mock(IgniteCompute.class);
        doReturn(cluster).when(ignite).cluster();
        doReturn(igniteCompute).when(ignite).compute(eq(clusterGrp));
        doReturn(clusterGrp).when(cluster).forNodeId(eq(nodeId));
        doReturn(Collections.singletonList(procId)).when(igniteCompute).call(any(IgniteCallable.class));

        List<LongRunningProcess> list = Collections.singletonList(new LongRunningProcess(nodeId, () -> {}));

        LongRunningProcessManager mgr = new LongRunningProcessManager(ignite);
        Map<UUID, List<UUID>> res = mgr.start(list);

        assertEquals(1, res.size());
        assertTrue(res.containsKey(nodeId));
        assertEquals(procId, res.get(nodeId).iterator().next());

        verify(igniteCompute).call(any(LongRunningProcessStartTask.class));
    }
",non-flaky,5
88781,apache_ignite,LongRunningProcessManagerTest.testPing,"    @Test
    public void testPing() {
        UUID nodeId = UUID.randomUUID();
        UUID procId = UUID.randomUUID();

        Ignite ignite = mock(Ignite.class);
        IgniteCluster cluster = mock(IgniteCluster.class);
        ClusterGroup clusterGrp = mock(ClusterGroup.class);
        IgniteCompute igniteCompute = mock(IgniteCompute.class);
        doReturn(cluster).when(ignite).cluster();
        doReturn(igniteCompute).when(ignite).compute(eq(clusterGrp));
        doReturn(clusterGrp).when(cluster).forNodeId(eq(nodeId));
        doReturn(Collections.singletonList(new LongRunningProcessStatus(LongRunningProcessState.RUNNING)))
            .when(igniteCompute).call(any(IgniteCallable.class));

        Map<UUID, List<UUID>> procIds = new HashMap<>();
        procIds.put(nodeId, Collections.singletonList(procId));

        LongRunningProcessManager mgr = new LongRunningProcessManager(ignite);
        Map<UUID, List<LongRunningProcessStatus>> res = mgr.ping(procIds);

        assertEquals(1, res.size());
        assertTrue(res.containsKey(nodeId));
        assertEquals(LongRunningProcessState.RUNNING, res.get(nodeId).iterator().next().getState());

        verify(igniteCompute).call(any(LongRunningProcessPingTask.class));
    }
",non-flaky,5
88782,apache_ignite,LongRunningProcessManagerTest.testStop,"    @Test
    public void testStop() {
        UUID nodeId = UUID.randomUUID();
        UUID procId = UUID.randomUUID();

        Ignite ignite = mock(Ignite.class);
        IgniteCluster cluster = mock(IgniteCluster.class);
        ClusterGroup clusterGrp = mock(ClusterGroup.class);
        IgniteCompute igniteCompute = mock(IgniteCompute.class);
        doReturn(cluster).when(ignite).cluster();
        doReturn(igniteCompute).when(ignite).compute(eq(clusterGrp));
        doReturn(clusterGrp).when(cluster).forNodeId(eq(nodeId));
        doReturn(Collections.singletonList(new LongRunningProcessStatus(LongRunningProcessState.RUNNING)))
            .when(igniteCompute).call(any(IgniteCallable.class));

        Map<UUID, List<UUID>> procIds = new HashMap<>();
        procIds.put(nodeId, Collections.singletonList(procId));

        LongRunningProcessManager mgr = new LongRunningProcessManager(ignite);
        Map<UUID, List<LongRunningProcessStatus>> res = mgr.stop(procIds, true);

        assertEquals(1, res.size());
        assertTrue(res.containsKey(nodeId));
        assertEquals(LongRunningProcessState.RUNNING, res.get(nodeId).iterator().next().getState());

        verify(igniteCompute).call(any(LongRunningProcessStopTask.class));
    }
",non-flaky,5
88783,apache_ignite,LongRunningProcessManagerTest.testClear,"    @Test
    public void testClear() {
        UUID nodeId = UUID.randomUUID();
        UUID procId = UUID.randomUUID();

        Ignite ignite = mock(Ignite.class);
        IgniteCluster cluster = mock(IgniteCluster.class);
        ClusterGroup clusterGrp = mock(ClusterGroup.class);
        IgniteCompute igniteCompute = mock(IgniteCompute.class);
        doReturn(cluster).when(ignite).cluster();
        doReturn(igniteCompute).when(ignite).compute(eq(clusterGrp));
        doReturn(clusterGrp).when(cluster).forNodeId(eq(nodeId));
        doReturn(Collections.singletonList(new LongRunningProcessStatus(LongRunningProcessState.RUNNING)))
            .when(igniteCompute).call(any(IgniteCallable.class));

        Map<UUID, List<UUID>> procIds = new HashMap<>();
        procIds.put(nodeId, Collections.singletonList(procId));

        LongRunningProcessManager mgr = new LongRunningProcessManager(ignite);
        Map<UUID, List<LongRunningProcessStatus>> res = mgr.clear(procIds);

        assertEquals(1, res.size());
        assertTrue(res.containsKey(nodeId));
        assertEquals(LongRunningProcessState.RUNNING, res.get(nodeId).iterator().next().getState());

        verify(igniteCompute).call(any(LongRunningProcessClearTask.class));
    }
",non-flaky,5
88784,apache_ignite,LongRunningProcessClearTaskTest.testCallProcessNotFound,"    @Test
    public void testCallProcessNotFound() {
        LongRunningProcessClearTask clearTask = createTask(UUID.randomUUID());

        List<LongRunningProcessStatus> statuses = clearTask.call();

        assertEquals(1, statuses.size());

        LongRunningProcessStatus status = statuses.get(0);
        assertEquals(LongRunningProcessState.NOT_FOUND, status.getState());
        assertNull(status.getException());

        assertEquals(0, metadataStorage.size());
    }
",non-flaky,5
88785,apache_ignite,LongRunningProcessClearTaskTest.testCallProcessIsRunning,"    @Test(expected = IllegalStateException.class)
    public void testCallProcessIsRunning() {
        UUID procId = UUID.randomUUID();

        Future<?> fut = mock(Future.class);
        doReturn(false).when(fut).isDone();
        metadataStorage.put(procId, fut);

        LongRunningProcessClearTask clearTask = createTask(procId);

        clearTask.call();
    }
",non-flaky,5
88786,apache_ignite,LongRunningProcessClearTaskTest.testCallProcessIsDone,"    @Test
    public void testCallProcessIsDone() {
        UUID procId = UUID.randomUUID();

        Future<?> fut = mock(Future.class);
        doReturn(true).when(fut).isDone();
        metadataStorage.put(procId, fut);

        LongRunningProcessClearTask clearTask = createTask(procId);

        List<LongRunningProcessStatus> statuses = clearTask.call();

        assertEquals(1, statuses.size());

        LongRunningProcessStatus status = statuses.get(0);
        assertEquals(LongRunningProcessState.DONE, status.getState());
        assertNull(status.getException());

        assertEquals(0, metadataStorage.size());
    }
",non-flaky,5
88787,apache_ignite,LongRunningProcessClearTaskTest.testCallProcessIsDoneWithException,"    @Test
    public void testCallProcessIsDoneWithException() throws ExecutionException, InterruptedException {
        UUID procId = UUID.randomUUID();

        Future<?> fut = mock(Future.class);
        doReturn(true).when(fut).isDone();
        doThrow(RuntimeException.class).when(fut).get();
        metadataStorage.put(procId, fut);

        LongRunningProcessClearTask clearTask = createTask(procId);

        List<LongRunningProcessStatus> statuses = clearTask.call();

        assertEquals(1, statuses.size());

        LongRunningProcessStatus status = statuses.get(0);
        assertEquals(LongRunningProcessState.DONE, status.getState());
        assertNotNull(status.getException());
        assertTrue(status.getException() instanceof RuntimeException);

        assertEquals(0, metadataStorage.size());
    }
",non-flaky,5
88788,apache_ignite,LongRunningProcessStopTaskTest.testCallProcessNotFound,"    @Test
    public void testCallProcessNotFound() {
        LongRunningProcessStopTask stopTask = createTask(UUID.randomUUID(), true);

        List<LongRunningProcessStatus> statuses = stopTask.call();

        assertEquals(1, statuses.size());

        LongRunningProcessStatus status = statuses.get(0);
        assertEquals(LongRunningProcessState.NOT_FOUND, status.getState());
        assertNull(status.getException());

        assertEquals(0, metadataStorage.size());
    }
",non-flaky,5
88789,apache_ignite,LongRunningProcessStopTaskTest.testCallProcessIsRunning,"    @Test
    public void testCallProcessIsRunning() {
        UUID procId = UUID.randomUUID();

        Future<?> fut = mock(Future.class);
        doReturn(false).when(fut).isDone();
        metadataStorage.put(procId, fut);

        LongRunningProcessStopTask stopTask = createTask(procId, true);

        List<LongRunningProcessStatus> statuses = stopTask.call();

        assertEquals(1, statuses.size());
        verify(fut).cancel(eq(true));

        LongRunningProcessStatus status = statuses.get(0);
        assertEquals(LongRunningProcessState.DONE, status.getState());
        assertNull(status.getException());

        assertEquals(0, metadataStorage.size());
    }
",non-flaky,5
88790,apache_ignite,LongRunningProcessStopTaskTest.testCallProcessIsDone,"    @Test
    public void testCallProcessIsDone() {
        UUID procId = UUID.randomUUID();

        Future<?> fut = mock(Future.class);
        doReturn(true).when(fut).isDone();
        metadataStorage.put(procId, fut);

        LongRunningProcessStopTask stopTask = createTask(procId, true);

        List<LongRunningProcessStatus> statuses = stopTask.call();

        assertEquals(1, statuses.size());
        verify(fut).cancel(eq(true));

        LongRunningProcessStatus status = statuses.get(0);
        assertEquals(LongRunningProcessState.DONE, status.getState());
        assertNull(status.getException());

        assertEquals(0, metadataStorage.size());
    }
",non-flaky,5
88791,apache_ignite,LongRunningProcessStopTaskTest.testCallProcessIsDoneWithException,"    @Test
    public void testCallProcessIsDoneWithException() throws ExecutionException, InterruptedException {
        UUID procId = UUID.randomUUID();

        Future<?> fut = mock(Future.class);
        doReturn(true).when(fut).isDone();
        doThrow(RuntimeException.class).when(fut).get();
        metadataStorage.put(procId, fut);

        LongRunningProcessStopTask stopTask = createTask(procId, true);

        List<LongRunningProcessStatus> statuses = stopTask.call();

        assertEquals(1, statuses.size());
        verify(fut).cancel(eq(true));

        LongRunningProcessStatus status = statuses.get(0);
        assertEquals(LongRunningProcessState.DONE, status.getState());
        assertNotNull(status.getException());
        assertTrue(status.getException() instanceof RuntimeException);

        assertEquals(0, metadataStorage.size());
    }
",non-flaky,5
88792,apache_ignite,LongRunningProcessStartTaskTest.testCall,"    @Test
    public void testCall() throws ExecutionException, InterruptedException {
        LongRunningProcess proc = new LongRunningProcess(UUID.randomUUID(), () -> {});
        LongRunningProcessStartTask task = createTask(proc);
        List<UUID> procIds = task.call();

        assertEquals(1, procIds.size());

        UUID procId = procIds.get(0);

        assertNotNull(metadataStorage.get(procId));

        Future<?> fut = metadataStorage.get(procId);
        fut.get();

        assertEquals(true, fut.isDone());
    }
",non-flaky,5
88793,apache_ignite,LongRunningProcessStartTaskTest.testCallWithException,"    @Test(expected = ExecutionException.class)
    public void testCallWithException() throws ExecutionException, InterruptedException {
        LongRunningProcess proc = new LongRunningProcess(UUID.randomUUID(), () -> {
            throw new RuntimeException();
        });
        LongRunningProcessStartTask task = createTask(proc);
        List<UUID> procIds = task.call();

        assertEquals(1, procIds.size());

        UUID procId = procIds.get(0);

        assertNotNull(metadataStorage.get(procId));

        Future<?> fut = metadataStorage.get(procId);
        fut.get();
    }
",non-flaky,5
88794,apache_ignite,LongRunningProcessPingTaskTest.testCallProcessNotFound,"    @Test
    public void testCallProcessNotFound() {
        LongRunningProcessPingTask pingTask = createTask(UUID.randomUUID());

        List<LongRunningProcessStatus> statuses = pingTask.call();

        assertEquals(1, statuses.size());

        LongRunningProcessStatus status = statuses.get(0);
        assertEquals(LongRunningProcessState.NOT_FOUND, status.getState());
        assertNull(status.getException());

        assertEquals(0, metadataStorage.size());
    }
",non-flaky,5
88795,apache_ignite,LongRunningProcessPingTaskTest.testCallProcessIsRunning,"    @Test
    public void testCallProcessIsRunning() {
        UUID procId = UUID.randomUUID();

        Future<?> fut = mock(Future.class);
        doReturn(false).when(fut).isDone();
        metadataStorage.put(procId, fut);

        LongRunningProcessPingTask pingTask = createTask(procId);

        List<LongRunningProcessStatus> statuses = pingTask.call();

        assertEquals(1, statuses.size());

        LongRunningProcessStatus status = statuses.get(0);
        assertEquals(LongRunningProcessState.RUNNING, status.getState());
        assertNull(status.getException());

        assertEquals(1, metadataStorage.size());
    }
",non-flaky,5
88796,apache_ignite,LongRunningProcessPingTaskTest.testCallProcessIsDone,"    @Test
    public void testCallProcessIsDone() {
        UUID procId = UUID.randomUUID();

        Future<?> fut = mock(Future.class);
        doReturn(true).when(fut).isDone();
        metadataStorage.put(procId, fut);

        LongRunningProcessPingTask pingTask = createTask(procId);

        List<LongRunningProcessStatus> statuses = pingTask.call();

        assertEquals(1, statuses.size());

        LongRunningProcessStatus status = statuses.get(0);
        assertEquals(LongRunningProcessState.DONE, status.getState());
        assertNull(status.getException());

        assertEquals(1, metadataStorage.size());
    }
",non-flaky,5
88797,apache_ignite,LongRunningProcessPingTaskTest.testCallProcessIsDoneWithException,"    @Test
    public void testCallProcessIsDoneWithException() throws ExecutionException, InterruptedException {
        UUID procId = UUID.randomUUID();

        Future<?> fut = mock(Future.class);
        doReturn(true).when(fut).isDone();
        doThrow(RuntimeException.class).when(fut).get();
        metadataStorage.put(procId, fut);

        LongRunningProcessPingTask pingTask = createTask(procId);

        List<LongRunningProcessStatus> statuses = pingTask.call();

        assertEquals(1, statuses.size());

        LongRunningProcessStatus status = statuses.get(0);
        assertEquals(LongRunningProcessState.DONE, status.getState());
        assertNotNull(status.getException());
        assertTrue(status.getException() instanceof RuntimeException);

        assertEquals(1, metadataStorage.size());
    }
",non-flaky,5
88798,apache_ignite,ProcessManagerWrapperTest.testStart,"    @Test
    public void testStart() {
        wrapper.start(Arrays.asList(1, 2, 3));

        verify(delegate).start(eq(Arrays.asList(""1"", ""2"", ""3"")));
    }
",non-flaky,5
88799,apache_ignite,ProcessManagerWrapperTest.testPing,"    @Test
    public void testPing() {
        Map<UUID, List<UUID>> procIds = Collections.emptyMap();
        wrapper.ping(procIds);

        verify(delegate).ping(eq(procIds));
    }
",non-flaky,5
88800,apache_ignite,ProcessManagerWrapperTest.testStop,"    @Test
    public void testStop() {
        Map<UUID, List<UUID>> procIds = Collections.emptyMap();
        wrapper.stop(procIds, true);

        verify(delegate).stop(eq(procIds), eq(true));
    }
",non-flaky,5
88801,apache_ignite,ProcessManagerWrapperTest.testClear,"    @Test
    public void testClear() {
        Map<UUID, List<UUID>> procIds = Collections.emptyMap();
        wrapper.clear(procIds);

        verify(delegate).clear(eq(procIds));
    }
",non-flaky,5
88802,apache_ignite,LoadTest.testMultithreading,"    @Test
    public void testMultithreading() throws Exception {
        final int THREAD_CNT = 8;
        final int ITERATION_CNT = 20;
        final int BATCH_SIZE = 1000;
        final int PAGE_CNT = 3;

        IgniteConfiguration srvCfg = Config.getServerConfiguration();

        // No peer class loading from thin clients: we need the server to know about this class to deserialize
        // ScanQuery filter.
        srvCfg.setBinaryConfiguration(new BinaryConfiguration().setTypeConfigurations(Arrays.asList(
            new BinaryTypeConfiguration(getClass().getName()),
            new BinaryTypeConfiguration(SerializedLambda.class.getName())
        )));

        try (Ignite ignored = Ignition.start(srvCfg);
             IgniteClient client = Ignition.startClient(new ClientConfiguration().setAddresses(Config.SERVER))
        ) {
            ClientCache<Integer, String> cache = client.createCache(""testMultithreading"");

            AtomicInteger cnt = new AtomicInteger(1);

            AtomicReference<Throwable> error = new AtomicReference<>();

            Runnable assertion = () -> {
                try {
                    int rangeStart = cnt.getAndAdd(BATCH_SIZE);
                    int rangeEnd = rangeStart + BATCH_SIZE;

                    Map<Integer, String> data = IntStream.range(rangeStart, rangeEnd).boxed()
                        .collect(Collectors.toMap(i -> i, i -> String.format(""String %s"", i)));

                    cache.putAll(data);

                    Query<Cache.Entry<Integer, String>> qry = new ScanQuery<Integer, String>()
                        .setPageSize(data.size() / PAGE_CNT)
                        .setFilter((i, s) -> i >= rangeStart && i < rangeEnd);

                    try (QueryCursor<Cache.Entry<Integer, String>> cur = cache.query(qry)) {
                        List<Cache.Entry<Integer, String>> res = cur.getAll();

                        assertEquals(""Unexpected number of entries"", data.size(), res.size());

                        Map<Integer, String> act = res.stream()
                            .collect(Collectors.toMap(Cache.Entry::getKey, Cache.Entry::getValue));

                        assertEquals(""Unexpected entries"", data, act);
                    }
                }
                catch (Throwable ex) {
                    error.set(ex);
                }
            };

            CountDownLatch complete = new CountDownLatch(THREAD_CNT);

            Runnable manyAssertions = () -> {
                for (int i = 0; i < ITERATION_CNT && error.get() == null; i++)
                    assertion.run();

                complete.countDown();
            };

            ExecutorService threadPool = Executors.newFixedThreadPool(THREAD_CNT);

            IntStream.range(0, THREAD_CNT).forEach(t -> threadPool.submit(manyAssertions));

            assertTrue(""Timeout"", complete.await(180, TimeUnit.SECONDS));

            String errMsg = error.get() == null ? """" : error.get().getMessage();

            assertNull(errMsg, error.get());
        }
    }
",non-flaky,5
88803,apache_ignite,ReliabilityTest.testFailover,"    @Test
    public void testFailover() throws Exception {
        final int CLUSTER_SIZE = 3;

        try (LocalIgniteCluster cluster = LocalIgniteCluster.start(CLUSTER_SIZE);
             IgniteClient client = Ignition.startClient(new ClientConfiguration()
                 .setAddresses(cluster.clientAddresses().toArray(new String[CLUSTER_SIZE]))
             )
        ) {
            final Random rnd = new Random();

            final ClientCache<Integer, String> cache = client.getOrCreateCache(
                new ClientCacheConfiguration().setName(""testFailover"").setCacheMode(CacheMode.REPLICATED)
            );

            // Simple operation failover: put/get
            assertOnUnstableCluster(cluster, () -> {
                Integer key = rnd.nextInt();
                String val = key.toString();

                cache.put(key, val);

                String cachedVal = cache.get(key);

                assertEquals(val, cachedVal);
            });

            // Composite operation failover: query
            Map<Integer, String> data = IntStream.rangeClosed(1, 1000).boxed()
                .collect(Collectors.toMap(i -> i, i -> String.format(""String %s"", i)));

            assertOnUnstableCluster(cluster, () -> {
                cache.putAll(data);

                Query<Cache.Entry<Integer, String>> qry =
                    new ScanQuery<Integer, String>().setPageSize(data.size() / 10);

                try (QueryCursor<Cache.Entry<Integer, String>> cur = cache.query(qry)) {
                    List<Cache.Entry<Integer, String>> res = cur.getAll();

                    assertEquals(""Unexpected number of entries"", data.size(), res.size());

                    Map<Integer, String> act = res.stream()
                        .collect(Collectors.toMap(Cache.Entry::getKey, Cache.Entry::getValue));

                    assertEquals(""Unexpected entries"", data, act);
                }
            });

            // Client fails if all nodes go down
            cluster.close();

            boolean igniteUnavailable = false;

            try {
                cache.put(1, ""1"");
            }
            catch (ClientConnectionException ex) {
                igniteUnavailable = true;

                Throwable[] suppressed = ex.getSuppressed();

                assertEquals(suppressed.length, CLUSTER_SIZE - 1);

                assertTrue(Stream.of(suppressed).allMatch(t -> t instanceof ClientConnectionException));
            }

            assertTrue(igniteUnavailable);
        }
    }
",non-flaky,5
88804,apache_ignite,IgniteBinaryTest.testUnmarshalSchemalessIgniteBinaries,"    @Test
    public void testUnmarshalSchemalessIgniteBinaries() throws Exception {
        int key = 1;
        Person val = new Person(key, ""Joe"");

        try (Ignite srv = Ignition.start(Config.getServerConfiguration())) {
            // Add an entry directly to the Ignite server. This stores a schema-less object in the cache and
            // does not register schema in the client's metadata cache.
            srv.cache(Config.DEFAULT_CACHE_NAME).put(key, val);

            try (IgniteClient client = Ignition.startClient(new ClientConfiguration().setAddresses(Config.SERVER))) {
                ClientCache<Integer, Person> cache = client.cache(Config.DEFAULT_CACHE_NAME);

                Person cachedVal = cache.get(key);

                assertEquals(val, cachedVal);
            }
        }
    }
",non-flaky,5
88805,apache_ignite,IgniteBinaryTest.testReadingSchemalessIgniteBinaries,"    @Test
    public void testReadingSchemalessIgniteBinaries() throws Exception {
        int key = 1;
        Person val = new Person(key, ""Joe"");

        try (Ignite srv = Ignition.start(Config.getServerConfiguration())) {
            // Add an entry directly to the Ignite server. This stores a schema-less object in the cache and
            // does not register schema in the client's metadata cache.
            srv.cache(Config.DEFAULT_CACHE_NAME).put(key, val);

            try (IgniteClient client = Ignition.startClient(new ClientConfiguration().setAddresses(Config.SERVER))) {
                ClientCache<Integer, BinaryObject> cache = client.cache(Config.DEFAULT_CACHE_NAME).withKeepBinary();

                BinaryObject cachedVal = cache.get(key);

                assertEquals(val.getId(), cachedVal.field(""id""));
                assertEquals(val.getName(), cachedVal.field(""name""));
            }
        }
    }
",non-flaky,5
88806,apache_ignite,IgniteBinaryTest.testBinaryObjectPutGet,"    @Test
    public void testBinaryObjectPutGet() throws Exception {
        int key = 1;

        try (Ignite ignored = Ignition.start(Config.getServerConfiguration())) {
            try (IgniteClient client =
                     Ignition.startClient(new ClientConfiguration().setAddresses(Config.SERVER))
            ) {
                IgniteBinary binary = client.binary();

                BinaryObject val = binary.builder(""Person"")
                    .setField(""id"", 1, int.class)
                    .setField(""name"", ""Joe"", String.class)
                    .build();

                ClientCache<Integer, BinaryObject> cache = client.cache(Config.DEFAULT_CACHE_NAME).withKeepBinary();

                cache.put(key, val);

                BinaryObject cachedVal =
                    client.cache(Config.DEFAULT_CACHE_NAME).<Integer, BinaryObject>withKeepBinary().get(key);

                assertBinaryObjectsEqual(val, cachedVal);
            }
        }
    }
",non-flaky,5
88807,apache_ignite,IgniteBinaryTest.testBinaryObjectApi,"    @Test
    public void testBinaryObjectApi() throws Exception {
        try (Ignite srv = Ignition.start(Config.getServerConfiguration())) {
            try (IgniteClient client = Ignition.startClient(new ClientConfiguration().setAddresses(Config.SERVER))) {
                // Use ""server-side"" IgniteBinary as a reference to test the thin client IgniteBinary against
                IgniteBinary refBinary = srv.binary();

                IgniteBinary binary = client.binary();

                Person obj = new Person(1, ""Joe"");

                int refTypeId = refBinary.typeId(Person.class.getName());
                int typeId = binary.typeId(Person.class.getName());

                assertEquals(refTypeId, typeId);

                BinaryObject refBinObj = refBinary.toBinary(obj);
                BinaryObject binObj = binary.toBinary(obj);

                assertBinaryObjectsEqual(refBinObj, binObj);

                assertBinaryTypesEqual(refBinary.type(typeId), binary.type(typeId));

                assertBinaryTypesEqual(refBinary.type(Person.class), binary.type(Person.class));

                assertBinaryTypesEqual(refBinary.type(Person.class.getName()), binary.type(Person.class.getName()));

                Collection<BinaryType> refTypes = refBinary.types();
                Collection<BinaryType> types = binary.types();

                assertEquals(refTypes.size(), types.size());

                BinaryObject refEnm = refBinary.buildEnum(Enum.class.getName(), Enum.DEFAULT.ordinal());
                BinaryObject enm = binary.buildEnum(Enum.class.getName(), Enum.DEFAULT.ordinal());

                assertBinaryObjectsEqual(refEnm, enm);

                Map<String, Integer> enumMap = Arrays.stream(Enum.values())
                    .collect(Collectors.toMap(java.lang.Enum::name, java.lang.Enum::ordinal));

                BinaryType refEnumType = refBinary.registerEnum(Enum.class.getName(), enumMap);
                BinaryType enumType = binary.registerEnum(Enum.class.getName(), enumMap);

                assertBinaryTypesEqual(refEnumType, enumType);

                refEnm = refBinary.buildEnum(Enum.class.getName(), Enum.DEFAULT.name());
                enm = binary.buildEnum(Enum.class.getName(), Enum.DEFAULT.name());

                assertBinaryObjectsEqual(refEnm, enm);
            }
        }
    }
",non-flaky,5
88808,apache_ignite,FunctionalTest.testCacheManagement,"    @Test
    public void testCacheManagement() throws Exception {
        try (LocalIgniteCluster ignored = LocalIgniteCluster.start(2);
             IgniteClient client = Ignition.startClient(getClientConfiguration())
        ) {
            final String CACHE_NAME = ""testCacheManagement"";

            ClientCacheConfiguration cacheCfg = new ClientCacheConfiguration().setName(CACHE_NAME)
                .setCacheMode(CacheMode.REPLICATED)
                .setWriteSynchronizationMode(CacheWriteSynchronizationMode.FULL_SYNC);

            int key = 1;
            Person val = new Person(key, Integer.toString(key));

            ClientCache<Integer, Person> cache = client.getOrCreateCache(cacheCfg);

            cache.put(key, val);

            assertEquals(1, cache.size());
            assertEquals(2, cache.size(CachePeekMode.ALL));

            cache = client.cache(CACHE_NAME);

            Person cachedVal = cache.get(key);

            assertEquals(val, cachedVal);

            Object[] cacheNames = new TreeSet<>(client.cacheNames()).toArray();

            assertArrayEquals(new TreeSet<>(Arrays.asList(Config.DEFAULT_CACHE_NAME, CACHE_NAME)).toArray(), cacheNames);

            client.destroyCache(CACHE_NAME);

            cacheNames = client.cacheNames().toArray();

            assertArrayEquals(new Object[] {Config.DEFAULT_CACHE_NAME}, cacheNames);

            cache = client.createCache(CACHE_NAME);

            assertFalse(cache.containsKey(key));

            cacheNames = client.cacheNames().toArray();

            assertArrayEquals(new TreeSet<>(Arrays.asList(Config.DEFAULT_CACHE_NAME, CACHE_NAME)).toArray(), cacheNames);

            client.destroyCache(CACHE_NAME);

            cache = client.createCache(cacheCfg);

            assertFalse(cache.containsKey(key));

            assertArrayEquals(new TreeSet<>(Arrays.asList(Config.DEFAULT_CACHE_NAME, CACHE_NAME)).toArray(), cacheNames);
        }
    }
",non-flaky,5
88809,apache_ignite,FunctionalTest.testCacheConfiguration,"    @Test
    public void testCacheConfiguration() throws Exception {
        try (Ignite ignored = Ignition.start(Config.getServerConfiguration());
             IgniteClient client = Ignition.startClient(getClientConfiguration())
        ) {
            final String CACHE_NAME = ""testCacheConfiguration"";

            ClientCacheConfiguration cacheCfg = new ClientCacheConfiguration().setName(CACHE_NAME)
                .setAtomicityMode(CacheAtomicityMode.TRANSACTIONAL)
                .setBackups(3)
                .setCacheMode(CacheMode.PARTITIONED)
                .setWriteSynchronizationMode(CacheWriteSynchronizationMode.FULL_SYNC)
                .setEagerTtl(false)
                .setGroupName(""FunctionalTest"")
                .setDefaultLockTimeout(12345)
                .setPartitionLossPolicy(PartitionLossPolicy.READ_WRITE_ALL)
                .setReadFromBackup(true)
                .setRebalanceBatchSize(67890)
                .setRebalanceBatchesPrefetchCount(102938)
                .setRebalanceDelay(54321)
                .setRebalanceMode(CacheRebalanceMode.SYNC)
                .setRebalanceOrder(2)
                .setRebalanceThrottle(564738)
                .setRebalanceTimeout(142536)
                .setKeyConfiguration(new CacheKeyConfiguration(""Employee"", ""orgId""))
                .setQueryEntities(new QueryEntity(int.class.getName(), ""Employee"")
                    .setTableName(""EMPLOYEE"")
                    .setFields(
                        Stream.of(
                            new SimpleEntry<>(""id"", Integer.class.getName()),
                            new SimpleEntry<>(""orgId"", Integer.class.getName())
                        ).collect(Collectors.toMap(
                            SimpleEntry::getKey, SimpleEntry::getValue, (a, b) -> a, LinkedHashMap::new
                        ))
                    )
                    .setKeyFields(Collections.singleton(""id""))
                    .setNotNullFields(Collections.singleton(""id""))
                    .setDefaultFieldValues(Collections.singletonMap(""id"", 0))
                    .setIndexes(Collections.singletonList(new QueryIndex(""id"", true, ""IDX_EMPLOYEE_ID"")))
                    .setAliases(Stream.of(""id"", ""orgId"").collect(Collectors.toMap(f -> f, String::toUpperCase)))
                );

            ClientCache cache = client.createCache(cacheCfg);

            assertEquals(CACHE_NAME, cache.getName());

            assertTrue(Comparers.equal(cacheCfg, cache.getConfiguration()));
        }
    }
",non-flaky,5
88810,apache_ignite,FunctionalTest.testPutGet,"    @Test
    public void testPutGet() throws Exception {
        // Existing cache, primitive key and object value
        try (Ignite ignored = Ignition.start(Config.getServerConfiguration());
             IgniteClient client = Ignition.startClient(getClientConfiguration())
        ) {
            ClientCache<Integer, Person> cache = client.getOrCreateCache(Config.DEFAULT_CACHE_NAME);

            Integer key = 1;
            Person val = new Person(key, ""Joe"");

            cache.put(key, val);

            assertTrue(cache.containsKey(key));

            Person cachedVal = cache.get(key);

            assertEquals(val, cachedVal);
        }

        // Non-existing cache, object key and primitive value
        try (Ignite ignored = Ignition.start(Config.getServerConfiguration());
             IgniteClient client = Ignition.startClient(getClientConfiguration())
        ) {
            ClientCache<Person, Integer> cache = client.getOrCreateCache(""testPutGet"");

            Integer val = 1;

            Person key = new Person(val, ""Joe"");

            cache.put(key, val);

            Integer cachedVal = cache.get(key);

            assertEquals(val, cachedVal);
        }

        // Object key and Object value
        try (Ignite ignored = Ignition.start(Config.getServerConfiguration());
             IgniteClient client = Ignition.startClient(getClientConfiguration())
        ) {
            ClientCache<Person, Person> cache = client.getOrCreateCache(""testPutGet"");

            Person key = new Person(1, ""Joe Key"");

            Person val = new Person(1, ""Joe Value"");

            cache.put(key, val);

            Person cachedVal = cache.get(key);

            assertEquals(val, cachedVal);
        }
    }
",non-flaky,5
88811,apache_ignite,FunctionalTest.testBatchPutGet,"    @Test
    public void testBatchPutGet() throws Exception {
        // Existing cache, primitive key and object value
        try (Ignite ignored = Ignition.start(Config.getServerConfiguration());
             IgniteClient client = Ignition.startClient(getClientConfiguration())
        ) {
            ClientCache<Integer, Person> cache = client.cache(Config.DEFAULT_CACHE_NAME);

            Map<Integer, Person> data = IntStream
                .rangeClosed(1, 1000).boxed()
                .collect(Collectors.toMap(i -> i, i -> new Person(i, String.format(""Person %s"", i))));

            cache.putAll(data);

            Map<Integer, Person> cachedData = cache.getAll(data.keySet());

            assertEquals(data, cachedData);
        }

        // Non-existing cache, object key and primitive value
        try (Ignite ignored = Ignition.start(Config.getServerConfiguration());
             IgniteClient client = Ignition.startClient(getClientConfiguration())
        ) {
            ClientCache<Person, Integer> cache = client.createCache(""testBatchPutGet"");

            Map<Person, Integer> data = IntStream
                .rangeClosed(1, 1000).boxed()
                .collect(Collectors.toMap(i -> new Person(i, String.format(""Person %s"", i)), i -> i));

            cache.putAll(data);

            Map<Person, Integer> cachedData = cache.getAll(data.keySet());

            assertEquals(data, cachedData);

            cache.clear();

            assertEquals(0, cache.size(CachePeekMode.ALL));
        }
    }
",non-flaky,5
88812,apache_ignite,FunctionalTest.testAtomicPutGet,"    @Test
    public void testAtomicPutGet() throws Exception {
        try (Ignite ignored = Ignition.start(Config.getServerConfiguration());
             IgniteClient client = Ignition.startClient(getClientConfiguration())
        ) {
            ClientCache<Integer, String> cache = client.createCache(""testRemoveReplace"");

            assertNull(cache.getAndPut(1, ""1""));
            assertEquals(""1"", cache.getAndPut(1, ""1.1""));

            assertEquals(""1.1"", cache.getAndRemove(1));
            assertNull(cache.getAndRemove(1));

            assertTrue(cache.putIfAbsent(1, ""1""));
            assertFalse(cache.putIfAbsent(1, ""1.1""));

            assertEquals(""1"", cache.getAndReplace(1, ""1.1""));
            assertEquals(""1.1"", cache.getAndReplace(1, ""1""));
            assertNull(cache.getAndReplace(2, ""2""));
        }
    }
",non-flaky,5
88813,apache_ignite,FunctionalTest.testRemoveReplace,"    @Test
    public void testRemoveReplace() throws Exception {
        try (Ignite ignored = Ignition.start(Config.getServerConfiguration());
             IgniteClient client = Ignition.startClient(getClientConfiguration())
        ) {
            ClientCache<Integer, String> cache = client.createCache(""testRemoveReplace"");

            Map<Integer, String> data = IntStream.rangeClosed(1, 100).boxed()
                .collect(Collectors.toMap(i -> i, Object::toString));

            cache.putAll(data);

            assertFalse(cache.replace(1, ""2"", ""3""));
            assertEquals(""1"", cache.get(1));
            assertTrue(cache.replace(1, ""1"", ""3""));
            assertEquals(""3"", cache.get(1));

            assertFalse(cache.replace(101, ""101""));
            assertNull(cache.get(101));
            assertTrue(cache.replace(100, ""101""));
            assertEquals(""101"", cache.get(100));

            assertFalse(cache.remove(101));
            assertTrue(cache.remove(100));
            assertNull(cache.get(100));

            assertFalse(cache.remove(99, ""100""));
            assertEquals(""99"", cache.get(99));
            assertTrue(cache.remove(99, ""99""));
            assertNull(cache.get(99));

            cache.put(101, ""101"");

            cache.removeAll(data.keySet());
            assertEquals(1, cache.size());
            assertEquals(""101"", cache.get(101));

            cache.removeAll();
            assertEquals(0, cache.size());
        }
    }
",non-flaky,5
88814,apache_ignite,FunctionalTest.testClientFailsOnStart,"    @Test
    public void testClientFailsOnStart() {
        ClientConnectionException expEx = null;

        try (IgniteClient ignored = Ignition.startClient(getClientConfiguration())) {
            // No-op.
        }
        catch (ClientConnectionException connEx) {
            expEx = connEx;
        }
        catch (Exception ex) {
            fail(String.format(
                ""%s expected but %s was received: %s"",
                ClientConnectionException.class.getName(),
                ex.getClass().getName(),
                ex
            ));
        }

        assertNotNull(
            String.format(""%s expected but no exception was received"", ClientConnectionException.class.getName()),
            expEx
        );
    }
",non-flaky,5
88815,apache_ignite,ClientCacheConfigurationTest.testSerialization,"    @Test
    public void testSerialization() throws IOException, ClassNotFoundException {
        ClientCacheConfiguration target = new ClientCacheConfiguration().setName(""Person"")
            .setAtomicityMode(CacheAtomicityMode.TRANSACTIONAL)
            .setBackups(3)
            .setCacheMode(CacheMode.PARTITIONED)
            .setWriteSynchronizationMode(CacheWriteSynchronizationMode.FULL_SYNC)
            .setEagerTtl(false)
            .setGroupName(""FunctionalTest"")
            .setDefaultLockTimeout(12345)
            .setPartitionLossPolicy(PartitionLossPolicy.READ_WRITE_ALL)
            .setReadFromBackup(true)
            .setRebalanceBatchSize(67890)
            .setRebalanceBatchesPrefetchCount(102938)
            .setRebalanceDelay(54321)
            .setRebalanceMode(CacheRebalanceMode.SYNC)
            .setRebalanceOrder(2)
            .setRebalanceThrottle(564738)
            .setRebalanceTimeout(142536)
            .setKeyConfiguration(new CacheKeyConfiguration(""Employee"", ""orgId""))
            .setQueryEntities(new QueryEntity(int.class.getName(), ""Employee"")
                .setTableName(""EMPLOYEE"")
                .setFields(
                    Stream.of(
                        new SimpleEntry<>(""id"", Integer.class.getName()),
                        new SimpleEntry<>(""orgId"", Integer.class.getName())
                    ).collect(Collectors.toMap(
                        SimpleEntry::getKey, SimpleEntry::getValue, (a, b) -> a, LinkedHashMap::new
                    ))
                )
                .setKeyFields(Collections.singleton(""id""))
                .setNotNullFields(Collections.singleton(""id""))
                .setDefaultFieldValues(Collections.singletonMap(""id"", 0))
                .setIndexes(Collections.singletonList(new QueryIndex(""id"", true, ""IDX_EMPLOYEE_ID"")))
                .setAliases(Stream.of(""id"", ""orgId"").collect(Collectors.toMap(f -> f, String::toUpperCase)))
            );

        ByteArrayOutputStream outBytes = new ByteArrayOutputStream();

        ObjectOutput out = new ObjectOutputStream(outBytes);

        out.writeObject(target);
        out.flush();

        ObjectInput in = new ObjectInputStream(new ByteArrayInputStream(outBytes.toByteArray()));

        Object desTarget = in.readObject();

        assertTrue(Comparers.equal(target, desTarget));
    }
",non-flaky,5
88816,apache_ignite,ClientConfigurationTest.testSerialization,"    @Test
    public void testSerialization() throws IOException, ClassNotFoundException {
        ClientConfiguration target = new ClientConfiguration()
            .setAddresses(""127.0.0.1:10800"", ""127.0.0.1:10801"")
            .setTimeout(123)
            .setBinaryConfiguration(new BinaryConfiguration()
                .setClassNames(Collections.singleton(""Person""))
            )
            .setSslMode(SslMode.REQUIRED)
            .setSslClientCertificateKeyStorePath(""client.jks"")
            .setSslClientCertificateKeyStoreType(""JKS"")
            .setSslClientCertificateKeyStorePassword(""123456"")
            .setSslTrustCertificateKeyStorePath(""trust.jks"")
            .setSslTrustCertificateKeyStoreType(""JKS"")
            .setSslTrustCertificateKeyStorePassword(""123456"")
            .setSslKeyAlgorithm(""SunX509"");

        ByteArrayOutputStream outBytes = new ByteArrayOutputStream();

        ObjectOutput out = new ObjectOutputStream(outBytes);

        out.writeObject(target);
        out.flush();

        ObjectInput in = new ObjectInputStream(new ByteArrayInputStream(outBytes.toByteArray()));

        Object desTarget = in.readObject();

        assertTrue(Comparers.equal(target, desTarget));
    }
",non-flaky,5
88817,apache_ignite,FullyConnectedComponentSearcherTest.testFind,"    @Test
    public void testFind() {
        BitSet[] matrix = provider.provide();

        int nodes = matrix.length;

        BitSet all = new BitSet(nodes);
        for (int i = 0; i < nodes; i++)
            all.set(i);

        FullyConnectedComponentSearcher searcher = new FullyConnectedComponentSearcher(matrix);

        BitSet res = searcher.findLargest(all);
        int size = res.cardinality();

        Assert.assertTrue(""Actual = "" + size + "", Expected = "" + minAcceptableRes,
            size >= minAcceptableRes);
    }
",non-flaky,5
88818,apache_ignite,IgnitePageMemReplaceDelayedWriteUnitTest.testReplacementWithDelayCausesLockForRead,"    @Test
    public void testReplacementWithDelayCausesLockForRead() throws IgniteCheckedException {
        IgniteConfiguration cfg = getConfiguration(16 * MB);

        AtomicInteger totalEvicted = new AtomicInteger();

        ReplacedPageWriter pageWriter = (FullPageId fullPageId, ByteBuffer byteBuf, int tag) -> {
            log.info(""Evicting "" + fullPageId);

            assert getLockedPages(fullPageId).contains(fullPageId);

            assert !getSegment(fullPageId).writeLock().isHeldByCurrentThread();

            totalEvicted.incrementAndGet();
        };

        int pageSize = 4096;
        PageMemoryImpl memory = createPageMemory(cfg, pageWriter, pageSize);

        this.pageMemory = memory;

        long pagesTotal = cfg.getDataStorageConfiguration().getDefaultDataRegionConfiguration().getMaxSize() / pageSize;
        long markDirty = pagesTotal * 2 / 3;
        for (int i = 0; i < markDirty; i++) {
            long pageId = memory.allocatePage(1, 1, PageIdAllocator.FLAG_DATA);
            long ptr = memory.acquirePage(1, pageId);

            memory.releasePage(1, pageId, ptr);
        }

        GridMultiCollectionWrapper<FullPageId> ids = memory.beginCheckpoint();
        int cpPages = ids.size();
        log.info(""Started CP with ["" + cpPages + ""] pages in it, created ["" + markDirty + ""] pages"");

        for (int i = 0; i < cpPages; i++) {
            long pageId = memory.allocatePage(1, 1, PageIdAllocator.FLAG_DATA);
            long ptr = memory.acquirePage(1, pageId);
            memory.releasePage(1, pageId, ptr);
        }

        List<Collection<FullPageId>> stripes = getAllLockedPages();

        assert !stripes.isEmpty();

        for (Collection<FullPageId> pageIds : stripes) {
            assert pageIds.isEmpty();
        }

        assert totalEvicted.get() > 0;

        memory.stop(true);
    }
",non-flaky,5
88819,apache_ignite,IgnitePageMemReplaceDelayedWriteUnitTest.testBackwardCompatibilityMode,"    @Test
    public void testBackwardCompatibilityMode() throws IgniteCheckedException {
        IgniteConfiguration cfg = getConfiguration(16 * MB);

        AtomicInteger totalEvicted = new AtomicInteger();

        ReplacedPageWriter pageWriter = (FullPageId fullPageId, ByteBuffer byteBuf, int tag) -> {
            log.info(""Evicting "" + fullPageId);

            assert getSegment(fullPageId).writeLock().isHeldByCurrentThread();

            totalEvicted.incrementAndGet();
        };

        System.setProperty(IgniteSystemProperties.IGNITE_DELAYED_REPLACED_PAGE_WRITE, ""false"");
        int pageSize = 4096;
        PageMemoryImpl memory;

        try {
            memory = createPageMemory(cfg, pageWriter, pageSize);
        }
        finally {
            System.clearProperty(IgniteSystemProperties.IGNITE_DELAYED_REPLACED_PAGE_WRITE);
        }

        this.pageMemory = memory;

        long pagesTotal = cfg.getDataStorageConfiguration().getDefaultDataRegionConfiguration().getMaxSize() / pageSize;
        long markDirty = pagesTotal * 2 / 3;
        for (int i = 0; i < markDirty; i++) {
            long pageId = memory.allocatePage(1, 1, PageIdAllocator.FLAG_DATA);
            long ptr = memory.acquirePage(1, pageId);

            memory.releasePage(1, pageId, ptr);
        }

        GridMultiCollectionWrapper<FullPageId> ids = memory.beginCheckpoint();
        int cpPages = ids.size();
        log.info(""Started CP with ["" + cpPages + ""] pages in it, created ["" + markDirty + ""] pages"");

        for (int i = 0; i < cpPages; i++) {
            long pageId = memory.allocatePage(1, 1, PageIdAllocator.FLAG_DATA);
            long ptr = memory.acquirePage(1, pageId);
            memory.releasePage(1, pageId, ptr);
        }

        assert totalEvicted.get() > 0;

        memory.stop(true);
    }
",non-flaky,5
88820,apache_ignite,IgniteThrottlingUnitTest.breakInCaseTooFast,"    @Test
    public void breakInCaseTooFast() {
        PagesWriteSpeedBasedThrottle throttle = new PagesWriteSpeedBasedThrottle(pageMemory2g, null, stateChecker, log);

        long time = throttle.getParkTime(0.67,
            (362584 + 67064) / 2,
            328787,
            1,
            60184,
            23103);

        assertTrue(time > 0);
    }
",non-flaky,5
88821,apache_ignite,IgniteThrottlingUnitTest.noBreakIfNotFastWrite,"    @Test
    public void noBreakIfNotFastWrite() {
        PagesWriteSpeedBasedThrottle throttle = new PagesWriteSpeedBasedThrottle(pageMemory2g, null, stateChecker, log);

        long time = throttle.getParkTime(0.47,
            ((362584 + 67064) / 2),
            328787,
            1,
            20103,
            23103);

        assertTrue(time == 0);
    }
",non-flaky,5
88822,apache_ignite,IgniteThrottlingUnitTest.averageCalculation,"    @Test
    public void averageCalculation() throws InterruptedException {
        IntervalBasedMeasurement measurement = new IntervalBasedMeasurement(100, 1);

        for (int i = 0; i < 1000; i++)
            measurement.addMeasurementForAverageCalculation(100);

        assertEquals(100, measurement.getAverage());

        Thread.sleep(220);

        assertEquals(0, measurement.getAverage());

        assertEquals(0, measurement.getSpeedOpsPerSec(System.nanoTime()));
    }
",non-flaky,5
88823,apache_ignite,IgniteThrottlingUnitTest.speedCalculation,"    @Test
    public void speedCalculation() throws InterruptedException {
        IntervalBasedMeasurement measurement = new IntervalBasedMeasurement(100, 1);

        for (int i = 0; i < 1000; i++)
            measurement.setCounter(i, System.nanoTime());

        long speed = measurement.getSpeedOpsPerSec(System.nanoTime());
        System.out.println(""speed measured "" + speed);
        assertTrue(speed > 1000);

        Thread.sleep(230);

        assertEquals(0, measurement.getSpeedOpsPerSec(System.nanoTime()));
    }
",non-flaky,5
88824,apache_ignite,IgniteThrottlingUnitTest.speedWithDelayCalculation,"    @Test
    public void speedWithDelayCalculation() throws InterruptedException {
        IntervalBasedMeasurement measurement = new IntervalBasedMeasurement(100, 1);

        int runs = 10;
        int nanosPark = 100;
        int multiplier = 100000;
        for (int i = 0; i < runs; i++) {
            measurement.setCounter(i * multiplier, System.nanoTime());

            LockSupport.parkNanos(nanosPark);
        }

        long speed = measurement.getSpeedOpsPerSec(System.nanoTime());

        assertTrue(speed > 0);
        long maxSpeed = (TimeUnit.SECONDS.toNanos(1) * multiplier * runs) / ((long)(runs * nanosPark));
        assertTrue(speed < maxSpeed);

        Thread.sleep(200);

        assertEquals(0, measurement.getSpeedOpsPerSec(System.nanoTime()));
    }
",non-flaky,5
88825,apache_ignite,IgniteThrottlingUnitTest.beginOfCp,"    @Test
    public void beginOfCp() {
        PagesWriteSpeedBasedThrottle throttle = new PagesWriteSpeedBasedThrottle(pageMemory2g, null, stateChecker, log);

        assertTrue(throttle.getParkTime(0.01, 100,400000,
            1,
            20103,
            23103) == 0);

        //mark speed 22413 for mark all remaining as dirty
        long time = throttle.getParkTime(0.024, 100, 400000,
            1,
            24000,
            23103);
        assertTrue(time > 0);

        assertTrue(throttle.getParkTime(0.01,
            100,
            400000,
            1,
            22412,
            23103) == 0);
    }
",non-flaky,5
88826,apache_ignite,IgniteThrottlingUnitTest.enforceThrottleAtTheEndOfCp,"    @Test
    public void enforceThrottleAtTheEndOfCp() {
        PagesWriteSpeedBasedThrottle throttle = new PagesWriteSpeedBasedThrottle(pageMemory2g, null, stateChecker, log);

        long time1 = throttle.getParkTime(0.70, 300000, 400000,
            1, 20200, 23000);
        long time2 = throttle.getParkTime(0.71, 300000, 400000,
            1, 20200, 23000);

        assertTrue(time2 >= time1 * 2); // extra slowdown should be applied.

        long time3 = throttle.getParkTime(0.73, 300000, 400000,
            1, 20200, 23000);
        long time4 = throttle.getParkTime(0.74, 300000, 400000,
            1, 20200, 23000);

        assertTrue(time3 > time2);
        assertTrue(time4 > time3);
    }
",non-flaky,5
88827,apache_ignite,IgniteThrottlingUnitTest.tooMuchPagesMarkedDirty,"    @Test
    public void tooMuchPagesMarkedDirty() {
        PagesWriteSpeedBasedThrottle throttle = new PagesWriteSpeedBasedThrottle(pageMemory2g, null, stateChecker, log);

       // 363308	350004	348976	10604
        long time = throttle.getParkTime(0.75,
            ((350004 + 348976) / 2),
            350004-10604,
            4,
            279,
            23933);

        System.err.println(time);

        assertTrue(time == 0);
    }
",non-flaky,5
88828,apache_ignite,IgniteThrottlingUnitTest.warningInCaseTooMuchThrottling,"    @Test
    public void warningInCaseTooMuchThrottling() {
        AtomicInteger warnings = new AtomicInteger(0);
        IgniteLogger log = mock(IgniteLogger.class);

        doAnswer(invocation -> {
            Object[] args = invocation.getArguments();

            System.out.println(""log.info() called with arguments: "" + Arrays.toString(args));

            warnings.incrementAndGet();

            return null;
        }).when(log).info(anyString());

        AtomicInteger written = new AtomicInteger();
        CheckpointWriteProgressSupplier cpProgress = mock(CheckpointWriteProgressSupplier.class);
        when(cpProgress.writtenPagesCounter()).thenReturn(written);

        PagesWriteSpeedBasedThrottle throttle = new PagesWriteSpeedBasedThrottle(pageMemory2g, cpProgress, stateChecker, log) {
            @Override protected void doPark(long throttleParkTimeNs) {
                //do nothing
            }
        };
        throttle.onBeginCheckpoint();
        written.set(200); //emulating some pages written

        for (int i = 0; i < 100000; i++) {
            //emulating high load on marking
            throttle.onMarkDirty(false);

            if (throttle.throttleWeight() > PagesWriteSpeedBasedThrottle.WARN_THRESHOLD)
                break;
        }

        for (int i = 0; i < 1000; i++) {
            //emulating additional page writes to be sure log message is generated

            throttle.onMarkDirty(false);

            if(warnings.get()>0)
                break;
        }

        System.out.println(throttle.throttleWeight());

        assertTrue(warnings.get() > 0);
    }
",non-flaky,5
88829,apache_ignite,RobinHoodBackwardShiftHashMapTest.testShortSize,"    @Test
    public void testShortSize() throws Exception {
        withMap(map -> {
            map.put(1, 1, 0, 0);
            map.put(2, 0, 1, 1);
            map.remove(1, 1);
        }, 2);
    }
",non-flaky,5
88830,apache_ignite,RobinHoodBackwardShiftHashMapTest.testSimplestPutGet,"    @Test
    public void testSimplestPutGet() throws Exception {
        int cnt = 100;
        withMap(map -> {
                for (int i = 0; i < cnt; i++) {
                    int grpId = i + 1;
                    int val = grpId * grpId;

                    assertSizeChanged(""Unique put should be successful "" + grpId,
                        map, () -> map.put(grpId, 1, val, 1));
                    assertEquals(val, map.get(grpId, 1, 0, -1, -2));

                    assertSizeNotChanged(""Duplicate put for "" + grpId,
                        map, () -> map.put(grpId, 1, 1, 1));
                    assertEquals(1, map.get(grpId, 1, 0, -1, -2));
                }

                assertEquals(cnt, map.size());
            }
            , cnt);
    }
",non-flaky,5
88831,apache_ignite,RobinHoodBackwardShiftHashMapTest.testSimplestOverflow,"    @Test(expected = IgniteOutOfMemoryException.class)
    public void testSimplestOverflow() throws Exception {
        withMap(map -> {
                for (int i = 0; i < 10; i++) {
                    int grpId = i + 1;
                    int val = grpId * grpId;
                    assertSizeChanged(""Unique put should be successful ["" + grpId + ""]"", map, () -> map.put(grpId, 1, val, 1));

                    assertEquals(val, map.get(grpId, 1, 0, -1, -2));

                    assertSizeNotChanged(""Duplicate put for "" + grpId, map, () -> map.put(grpId, 1, 1, 1));
                    assertEquals(1, map.get(grpId, 1, 0, -1, -2));
                }

                map.put(11, 1, 11, 1);
            }
            , 10);
    }
",non-flaky,5
88832,apache_ignite,RobinHoodBackwardShiftHashMapTest.testPutRemoveOnSamePlaces,"    @Test
    public void testPutRemoveOnSamePlaces() throws Exception {
        withMap(map -> {
                doAddRemove(map);

                //fill with 1 space left;
                for (int i = 0; i < 99; i++) {
                    int grpId = i + 1;
                    int val = grpId * grpId;
                    assertSizeChanged(""Unique put should be successful "" + grpId, map,
                        () -> map.put(grpId, 1, val, 1));
                }

                doAddRemove(map);
            }
            , 100);
    }
",non-flaky,5
88833,apache_ignite,RobinHoodBackwardShiftHashMapTest.testCollisionOnRemove,"    @Test
    public void testCollisionOnRemove() {
        Map<FullPageId, Long> ctrl = new LinkedHashMap<>();
        int cap = 10;
        FullPageId baseId = new FullPageId(0, 1);

        withMap(map -> {
            for (int i = 0; i < cap; i++) {
                int grpId = i + 1;
                int pageId = findPageIdForCollision(grpId, baseId, cap);
                ctrl.put(new FullPageId(pageId, grpId), (long)grpId);
                map.put(grpId, pageId, (long)grpId, 1);
            }
            for (FullPageId next : ctrl.keySet()) {
                assertTrue(map.remove(next.groupId(), next.pageId()));
            }
        }, cap);
    }
",non-flaky,5
88834,apache_ignite,RobinHoodBackwardShiftHashMapTest.testRandomOpsPutRemove,"    @Test
    public void testRandomOpsPutRemove() {
        doPutRemoveTest(System.currentTimeMillis());
    }
",non-flaky,5
88835,apache_ignite,RobinHoodBackwardShiftHashMapTest.testPutAndCantGetOutdatedValue,"    @Test
    public void testPutAndCantGetOutdatedValue() throws Exception {
        withMap(map -> {
            //fill with 1 space left;
            for (int i = 0; i < 99; i++) {
                int ver = i;
                int grpId = ver + 1;
                int val = grpId * grpId;
                map.put(grpId, 1, val, ver);

                assertEquals(val, map.get(grpId, 1, ver, -1, -2));

                assertEquals(-2, map.get(grpId, 1, ver + 1, -1, -2));
            }
        }, 100);
    }
",non-flaky,5
88836,apache_ignite,RobinHoodBackwardShiftHashMapTest.testPutAndRefreshValue,"    @Test
    public void testPutAndRefreshValue() throws Exception {
        withMap(map -> {
            //fill with 1 space left;
            for (int i = 0; i < 99; i++) {
                int ver = i;
                int grpId = ver + 1;
                int val = grpId * grpId;
                int pageId = 1;
                map.put(grpId, pageId, val, ver);

                map.refresh(grpId, pageId, ver + 1);

                assertEquals(val, map.get(grpId, pageId, ver + 1, -1, -2));

            }

            doAddRemove(map);
        }, 100);
    }
",non-flaky,5
88837,apache_ignite,RobinHoodBackwardShiftHashMapTest.testClearAtWithControlMap3,"    @Test
    public void testClearAtWithControlMap3() throws Exception {
        int cap = 100;

        doRemovalTests(cap, (grpId, pageId) -> {
            int hc = Integer.hashCode(grpId) + 31 * Long.hashCode(pageId);

            return hc % 3 == 0;
        });
    }
",non-flaky,5
88838,apache_ignite,RobinHoodBackwardShiftHashMapTest.testClearAtWithControlMap7,"    @Test
    public void testClearAtWithControlMap7() throws Exception {
        int cap = 100;

        doRemovalTests(cap, (grpId, pageId) -> {
            int hc = Integer.hashCode(grpId) + 31 * Long.hashCode(pageId);

            return hc % 7 == 0;
        });
    }
",non-flaky,5
88839,apache_ignite,RobinHoodBackwardShiftHashMapTest.testClearAllWithControlMap,"    @Test
    public void testClearAllWithControlMap() throws Exception {
        int cap = 100;

        doRemovalTests(cap, (grpId, pageId) -> true);
    }
",non-flaky,5
88840,apache_ignite,FullPageIdTableTest.testRandomOperations,"    @Test
    public void testRandomOperations() throws Exception {
        int cnt = CACHE_ID_RANGE * PAGE_ID_RANGE;

        long mem = FullPageIdTable.requiredMemory(cnt);

        UnsafeMemoryProvider prov = new UnsafeMemoryProvider(log);

        prov.initialize(new long[] {mem});

        DirectMemoryRegion region = prov.nextRegion();

        try {
            long seed = U.currentTimeMillis();

            info(""Seed: "" + seed + ""L; //"");

            Random rnd = new Random(seed);

            LoadedPagesMap tbl = new FullPageIdTable(region.address(), region.size(), true);

            Map<FullPageId, Long> check = new HashMap<>();

            for (int i = 0; i < 10_000; i++) {
                int cacheId = rnd.nextInt(CACHE_ID_RANGE) + 1;
                int pageId = rnd.nextInt(PAGE_ID_RANGE);

                FullPageId fullId = new FullPageId(pageId, cacheId);

                boolean put = rnd.nextInt(3) != -1;

                if (put) {
                    long val = rnd.nextLong();

                    tbl.put(cacheId, pageId, val, 0);
                    check.put(fullId, val);
                }
                else {
                    tbl.remove(cacheId, pageId);
                    check.remove(fullId);
                }

                verifyLinear(tbl, check);

                if (i > 0 && i % 1000 == 0)
                    info(""Done: "" + i);
            }
        }
        finally {
            prov.shutdown(true);
        }
    }
",non-flaky,5
88841,apache_ignite,FullPageIdTableTest.putRemoveScenario,"    @Test
    public void putRemoveScenario() throws Exception {
        long seed = U.currentTimeMillis();

        doPutRemoveTest(seed, false, 1_000_000);
    }
",non-flaky,5
88842,apache_ignite,FullPageIdTableTest.putRemoveScenarioNewMap,"    @Test
    public void putRemoveScenarioNewMap() throws Exception {
        long seed = U.currentTimeMillis();
        doPutRemoveTest(seed, true, 30_000_000);
    }
",non-flaky,5
88843,apache_ignite,ObjectHistogramTest.testBuckets,"    @Test
    public void testBuckets() {
        testBuckets(hist1, new int[] {0, 1, 2, 3, 4, 5}, new int[] {4, 3, 2, 1, 1, 1});
        testBuckets(hist2, new int[] {0, 1, 5, 6}, new int[] {6, 5, 1, 1});
    }
",non-flaky,5
88844,apache_ignite,ObjectHistogramTest.testAdd,"    @Test
    public void testAdd() {
        double val = 100.0;
        hist1.addElement(val);
        Optional<Double> cntr = hist1.getValue(computeBucket(val));

        assertTrue(cntr.isPresent());
        assertEquals(1, cntr.get().intValue());
    }
",non-flaky,5
88845,apache_ignite,ObjectHistogramTest.testAddHist,"    @Test
    public void testAddHist() {
        ObjectHistogram<Double> res = hist1.plus(hist2);
        testBuckets(res, new int[] {0, 1, 2, 3, 4, 5, 6}, new int[] {10, 8, 2, 1, 1, 2, 1});
    }
",non-flaky,5
88846,apache_ignite,ObjectHistogramTest.testDistributionFunction,"    @Test
    public void testDistributionFunction() {
        TreeMap<Integer, Double> distribution = hist1.computeDistributionFunction();

        int[] buckets = new int[distribution.size()];
        double[] sums = new double[distribution.size()];

        int ptr = 0;
        for(int bucket : distribution.keySet()) {
            sums[ptr] = distribution.get(bucket);
            buckets[ptr++] = bucket;
        }

        assertArrayEquals(new int[] {0, 1, 2, 3, 4, 5}, buckets);
        assertArrayEquals(new double[] {4., 7., 9., 10., 11., 12.}, sums, 0.01);
    }
",non-flaky,5
88847,apache_ignite,ObjectHistogramTest.testOfSum,"    @Test
    public void testOfSum() {
        IgniteFunction<Double, Integer> bucketMap = x -> (int) (Math.ceil(x * 100) % 100);
        IgniteFunction<Double, Double> cntrMap = x -> Math.pow(x, 2);

        ObjectHistogram<Double> forAllHistogram = new ObjectHistogram<>(bucketMap, cntrMap);
        Random rnd = new Random();
        List<ObjectHistogram<Double>> partitions = new ArrayList<>();
        int cntOfPartitions = rnd.nextInt(100);
        int sizeOfDataset = rnd.nextInt(10000);
        for(int i = 0; i < cntOfPartitions; i++)
            partitions.add(new ObjectHistogram<>(bucketMap, cntrMap));

        for(int i = 0; i < sizeOfDataset; i++) {
            double objVal = rnd.nextDouble();
            forAllHistogram.addElement(objVal);
            partitions.get(rnd.nextInt(partitions.size())).addElement(objVal);
        }

        Optional<ObjectHistogram<Double>> leftSum = partitions.stream().reduce(ObjectHistogram::plus);
        Optional<ObjectHistogram<Double>> rightSum = partitions.stream().reduce((x,y) -> y.plus(x));
        assertTrue(leftSum.isPresent());
        assertTrue(rightSum.isPresent());
        assertTrue(forAllHistogram.isEqualTo(leftSum.get()));
        assertTrue(forAllHistogram.isEqualTo(rightSum.get()));
        assertTrue(leftSum.get().isEqualTo(rightSum.get()));
    }
",non-flaky,5
88848,apache_ignite,SimpleDatasetTest.basicTest,"    @Test
    public void basicTest() throws Exception {
        Map<Integer, DataPoint> dataPoints = new HashMap<Integer, DataPoint>() {{
            put(1, new DataPoint(42, 10000));
            put(2, new DataPoint(32, 64000));
            put(3, new DataPoint(53, 120000));
            put(4, new DataPoint(24, 70000));
        }};

        // Creates a local simple dataset containing features and providing standard dataset API.
        try (SimpleDataset<?> dataset = DatasetFactory.createSimpleDataset(
            dataPoints,
            2,
            (k, v) -> VectorUtils.of(v.getAge(), v.getSalary())
        )) {
            assertArrayEquals(""Mean values."", new double[] {37.75, 66000.0}, dataset.mean(), 0);

            assertArrayEquals(""Standard deviation values."",
                new double[] {10.871407452579449, 38961.519477556314}, dataset.std(), 0);

            double[][] covExp = new double[][] {
                new double[] {118.1875, 135500.0},
                new double[] {135500.0, 1.518E9}
            };
            double[][] cov = dataset.cov();
            int rowCov = 0;
            for (double[] row : cov)
                assertArrayEquals(""Covariance matrix row "" + rowCov,
                    covExp[rowCov++], row, 0);


            double[][] corrExp = new double[][] {
                new double[] {1.0000000000000002, 0.31990250167874007},
                new double[] {0.31990250167874007, 1.0}
            };
            double[][] corr = dataset.corr();
            int rowCorr = 0;
            for (double[] row : corr)
                assertArrayEquals(""Correlation matrix row "" + rowCorr,
                    corrExp[rowCorr++], row, 0);
        }
    }
",non-flaky,5
88849,apache_ignite,SimpleLabeledDatasetTest.basicTest,"    @Test
    public void basicTest() throws Exception {
        Map<Integer, DataPoint> dataPoints = new HashMap<Integer, DataPoint>() {{
            put(5, new DataPoint(42, 10000));
            put(6, new DataPoint(32, 64000));
            put(7, new DataPoint(53, 120000));
            put(8, new DataPoint(24, 70000));
        }};

        double[][] actualFeatures = new double[2][];
        double[][] actualLabels = new double[2][];
        int[] actualRows = new int[2];

        // Creates a local simple dataset containing features and providing standard dataset API.
        try (SimpleLabeledDataset<?> dataset = DatasetFactory.createSimpleLabeledDataset(
            dataPoints,
            2,
            (k, v) -> VectorUtils.of(v.getAge(), v.getSalary()),
            (k, v) -> new double[] {k, v.getAge(), v.getSalary()}
        )) {
            assertNull(dataset.compute((data, partIdx) -> {
                actualFeatures[partIdx] = data.getFeatures();
                actualLabels[partIdx] = data.getLabels();
                actualRows[partIdx] = data.getRows();
                return null;
            }, (k, v) -> null));
        }

        double[][] expFeatures = new double[][] {
            new double[] {42.0, 32.0, 10000.0, 64000.0},
            new double[] {53.0, 24.0, 120000.0, 70000.0}
        };
        int rowFeat = 0;
        for (double[] row : actualFeatures)
            assertArrayEquals(""Features partition index "" + rowFeat,
                expFeatures[rowFeat++], row, 0);

        double[][] expLabels = new double[][] {
            new double[] {5.0, 6.0, 42.0, 32.0, 10000.0, 64000.0},
            new double[] {7.0, 8.0, 53.0, 24.0, 120000.0, 70000.0}
        };
        int rowLbl = 0;
        for (double[] row : actualLabels)
            assertArrayEquals(""Labels partition index "" + rowLbl,
                expLabels[rowLbl++], row, 0);

        assertArrayEquals(""Rows per partitions"", new int[] {2, 2}, actualRows);
    }
",non-flaky,5
88850,apache_ignite,DatasetWrapperTest.testComputeWithCtx,"    @Test
    public void testComputeWithCtx() {
        doReturn(42).when(dataset).computeWithCtx(any(IgniteTriFunction.class), any(), any());

        Integer res = (Integer) wrapper.computeWithCtx(mock(IgniteTriFunction.class), mock(IgniteBinaryOperator.class),
            null);

        assertEquals(42, res.intValue());

        verify(dataset, times(1)).computeWithCtx(any(IgniteTriFunction.class), any(), any());
    }
",non-flaky,5
88851,apache_ignite,DatasetWrapperTest.testComputeWithCtx2,"    @Test
    public void testComputeWithCtx2() {
        doReturn(42).when(dataset).computeWithCtx(any(IgniteTriFunction.class), any(), any());

        Integer res = (Integer) wrapper.computeWithCtx(mock(IgniteBiFunction.class), mock(IgniteBinaryOperator.class),
            null);

        assertEquals(42, res.intValue());

        verify(dataset, times(1)).computeWithCtx(any(IgniteTriFunction.class), any(), any());
    }
",non-flaky,5
88852,apache_ignite,DatasetWrapperTest.testComputeWithCtx3,"    @Test
    public void testComputeWithCtx3() {
        wrapper.computeWithCtx((ctx, data) -> {
            assertNotNull(ctx);
            assertNotNull(data);
        });

        verify(dataset, times(1)).computeWithCtx(any(IgniteTriFunction.class),
            any(IgniteBinaryOperator.class), any());
    }
",non-flaky,5
88853,apache_ignite,DatasetWrapperTest.testCompute,"    @Test
    public void testCompute() {
        doReturn(42).when(dataset).compute(any(IgniteBiFunction.class), any(), any());

        Integer res = (Integer) wrapper.compute(mock(IgniteBiFunction.class), mock(IgniteBinaryOperator.class),
            null);

        assertEquals(42, res.intValue());

        verify(dataset, times(1)).compute(any(IgniteBiFunction.class), any(), any());
    }
",non-flaky,5
88854,apache_ignite,DatasetWrapperTest.testCompute2,"    @Test
    public void testCompute2() {
        doReturn(42).when(dataset).compute(any(IgniteBiFunction.class), any(IgniteBinaryOperator.class), any());

        Integer res = (Integer) wrapper.compute(mock(IgniteFunction.class), mock(IgniteBinaryOperator.class),
            null);

        assertEquals(42, res.intValue());

        verify(dataset, times(1)).compute(any(IgniteBiFunction.class), any(IgniteBinaryOperator.class), any());
    }
",non-flaky,5
88855,apache_ignite,DatasetWrapperTest.testClose,"    @Test
    public void testClose() throws Exception {
        wrapper.close();

        verify(dataset, times(1)).close();
    }
",non-flaky,5
88856,apache_ignite,LocalDatasetBuilderTest.testBuild,"    @Test
    public void testBuild() {
        Map<Integer, Integer> data = new HashMap<>();
        for (int i = 0; i < 100; i++)
            data.put(i, i);

        LocalDatasetBuilder<Integer, Integer> builder = new LocalDatasetBuilder<>(data, 10);

        LocalDataset<Serializable, TestPartitionData> dataset = buildDataset(builder);

        assertEquals(10, dataset.getCtx().size());
        assertEquals(10, dataset.getData().size());

        AtomicLong cnt = new AtomicLong();

        dataset.compute((partData, partIdx) -> {
           cnt.incrementAndGet();

           int[] arr = partData.data;

           assertEquals(10, arr.length);

           for (int i = 0; i < 10; i++)
               assertEquals(partIdx * 10 + i, arr[i]);
        });

        assertEquals(10, cnt.intValue());
    }
",non-flaky,5
88857,apache_ignite,LocalDatasetBuilderTest.testBuildWithPredicate,"    @Test
    public void testBuildWithPredicate() {
        Map<Integer, Integer> data = new HashMap<>();
        for (int i = 0; i < 100; i++)
            data.put(i, i);

        LocalDatasetBuilder<Integer, Integer> builder = new LocalDatasetBuilder<>(data, (k, v) -> k % 2 == 0,10);

        LocalDataset<Serializable, TestPartitionData> dataset = buildDataset(builder);

        AtomicLong cnt = new AtomicLong();

        dataset.compute((partData, partIdx) -> {
            cnt.incrementAndGet();

            int[] arr = partData.data;

            assertEquals(5, arr.length);

            for (int i = 0; i < 5; i++)
                assertEquals((partIdx * 5 + i) * 2, arr[i]);
        });

        assertEquals(10, cnt.intValue());
    }
",non-flaky,5
88858,apache_ignite,IteratorWithConcurrentModificationCheckerTest.testNextWhenIteratorHasLessElementsThanExpected,"    @Test(expected = ConcurrentModificationException.class)
    public void testNextWhenIteratorHasLessElementsThanExpected() {
        List<Integer> list = Arrays.asList(1, 2, 3);

        Iterator<Integer> iter = new IteratorWithConcurrentModificationChecker<>(list.iterator(), 4, ""Exception"");

        assertEquals(Integer.valueOf(1), iter.next());
        assertEquals(Integer.valueOf(2), iter.next());
        assertEquals(Integer.valueOf(3), iter.next());

        iter.next(); // Should throw an exception.
    }
",non-flaky,5
88859,apache_ignite,IteratorWithConcurrentModificationCheckerTest.testNextWhenIteratorHasMoreElementsThanExpected,"    @Test(expected = ConcurrentModificationException.class)
    public void testNextWhenIteratorHasMoreElementsThanExpected() {
        List<Integer> list = Arrays.asList(1, 2, 3);

        Iterator<Integer> iter = new IteratorWithConcurrentModificationChecker<>(list.iterator(), 2, ""Exception"");

        assertEquals(Integer.valueOf(1), iter.next());
        assertEquals(Integer.valueOf(2), iter.next());

        iter.next(); // Should throw an exception.
    }
",non-flaky,5
88860,apache_ignite,IteratorWithConcurrentModificationCheckerTest.testHasNextWhenIteratorHasLessElementsThanExpected,"    @Test(expected = ConcurrentModificationException.class)
    public void testHasNextWhenIteratorHasLessElementsThanExpected() {
        List<Integer> list = Arrays.asList(1, 2, 3);

        Iterator<Integer> iter = new IteratorWithConcurrentModificationChecker<>(list.iterator(), 4, ""Exception"");

        assertTrue(iter.hasNext());
        iter.next();
        assertTrue(iter.hasNext());
        iter.next();
        assertTrue(iter.hasNext());
        iter.next();

        iter.hasNext(); // Should throw an exception.
    }
",non-flaky,5
88861,apache_ignite,IteratorWithConcurrentModificationCheckerTest.testHasNextWhenIteratorHasMoreElementsThanExpected,"    @Test(expected = ConcurrentModificationException.class)
    public void testHasNextWhenIteratorHasMoreElementsThanExpected() {
        List<Integer> list = Arrays.asList(1, 2, 3);

        Iterator<Integer> iter = new IteratorWithConcurrentModificationChecker<>(list.iterator(), 2, ""Exception"");

        assertTrue(iter.hasNext());
        iter.next();
        assertTrue(iter.hasNext());
        iter.next();

        iter.hasNext(); // Should throw an exception.
    }
",non-flaky,5
88862,apache_ignite,DatasetAffinityFunctionWrapperTest.testReset,"    @Test
    public void testReset() {
        wrapper.reset();

        verify(affinityFunction, times(1)).reset();
    }
",non-flaky,5
88863,apache_ignite,DatasetAffinityFunctionWrapperTest.testPartitions,"    @Test
    public void testPartitions() {
        doReturn(42).when(affinityFunction).partitions();

        int partitions = wrapper.partitions();

        assertEquals(42, partitions);
        verify(affinityFunction, times(1)).partitions();
    }
",non-flaky,5
88864,apache_ignite,DatasetAffinityFunctionWrapperTest.testPartition,"    @Test
    public void testPartition() {
        doReturn(0).when(affinityFunction).partition(eq(42));

        int part = wrapper.partition(42);

        assertEquals(42, part);
        verify(affinityFunction, times(0)).partition(any());
    }
",non-flaky,5
88865,apache_ignite,DatasetAffinityFunctionWrapperTest.testAssignPartitions,"    @Test
    public void testAssignPartitions() {
        List<List<ClusterNode>> nodes = Collections.singletonList(Collections.singletonList(mock(ClusterNode.class)));

        doReturn(nodes).when(affinityFunction).assignPartitions(any());

        List<List<ClusterNode>> resNodes = wrapper.assignPartitions(mock(AffinityFunctionContext.class));

        assertEquals(nodes, resNodes);
        verify(affinityFunction, times(1)).assignPartitions(any());
    }
",non-flaky,5
88866,apache_ignite,DatasetAffinityFunctionWrapperTest.testRemoveNode,"    @Test
    public void testRemoveNode() {
        UUID nodeId = UUID.randomUUID();

        wrapper.removeNode(nodeId);

        verify(affinityFunction, times(1)).removeNode(eq(nodeId));
    }
",non-flaky,5
88867,apache_ignite,PartitionDataStorageTest.testComputeDataIfAbsent,"    @Test
    public void testComputeDataIfAbsent() {
        AtomicLong cnt = new AtomicLong();

        for (int i = 0; i < 10; i++) {
            Integer res = (Integer) dataStorage.computeDataIfAbsent(0, () -> {
                cnt.incrementAndGet();

                return 42;
            });

            assertEquals(42, res.intValue());
        }

        assertEquals(1, cnt.intValue());
    }
",non-flaky,5
88868,apache_ignite,PipelineMdlTest.testPredict,"    @Test
    public void testPredict() {
        Vector weights = new DenseVector(new double[] {2.0, 3.0});

        verifyPredict(getMdl(new LogisticRegressionModel(weights, 1.0).withRawLabels(true)));
    }
",non-flaky,5
88869,apache_ignite,PipelineTest.testTrainWithTheLinearlySeparableCase,"    @Test
    public void testTrainWithTheLinearlySeparableCase() {
        Map<Integer, Double[]> cacheMock = new HashMap<>();

        for (int i = 0; i < twoLinearlySeparableClasses.length; i++) {
            double[] row = twoLinearlySeparableClasses[i];
            Double[] convertedRow = new Double[row.length];
            for (int j = 0; j < row.length; j++)
                convertedRow[j] = row[j];
            cacheMock.put(i, convertedRow);
        }

        LogisticRegressionSGDTrainer<?> trainer = new LogisticRegressionSGDTrainer<>()
            .withUpdatesStgy(new UpdatesStrategy<>(new SimpleGDUpdateCalculator(0.2),
                SimpleGDParameterUpdate::sumLocal, SimpleGDParameterUpdate::avg))
            .withMaxIterations(100000)
            .withLocIterations(100)
            .withBatchSize(10)
            .withSeed(123L);

        PipelineMdl<Integer, Double[]> mdl = new Pipeline<Integer, Double[], Vector>()
            .addFeatureExtractor((k, v) -> VectorUtils.of(Arrays.copyOfRange(v, 1, v.length)))
            .addLabelExtractor((k, v) -> v[0])
            .addPreprocessor(new MinMaxScalerTrainer<Integer, Object[]>())
            .addPreprocessor(new NormalizationTrainer<Integer, Object[]>()
                .withP(1))
            .addTrainer(trainer)
            .fit(
                cacheMock,
                parts
            );

        TestUtils.assertEquals(0, mdl.apply(VectorUtils.of(100, 10)), PRECISION);
        TestUtils.assertEquals(1, mdl.apply(VectorUtils.of(10, 100)), PRECISION);
    }
",non-flaky,5
88870,apache_ignite,PipelineTest.testTrainWithMissedFinalStage,"    @Test(expected = IllegalStateException.class)
    public void testTrainWithMissedFinalStage() {
        Map<Integer, Double[]> cacheMock = new HashMap<>();

        for (int i = 0; i < twoLinearlySeparableClasses.length; i++) {
            double[] row = twoLinearlySeparableClasses[i];
            Double[] convertedRow = new Double[row.length];
            for (int j = 0; j < row.length; j++)
                convertedRow[j] = row[j];
            cacheMock.put(i, convertedRow);
        }

        PipelineMdl<Integer, Double[]> mdl = new Pipeline<Integer, Double[], Vector>()
            .addFeatureExtractor((k, v) -> VectorUtils.of(Arrays.copyOfRange(v, 1, v.length)))
            .addLabelExtractor((k, v) -> v[0])
            .addPreprocessor(new MinMaxScalerTrainer<Integer, Object[]>())
            .addPreprocessor(new NormalizationTrainer<Integer, Object[]>()
                .withP(1))
            .fit(
                cacheMock,
                parts
            );

        TestUtils.assertEquals(0, mdl.apply(VectorUtils.of(100, 10)), PRECISION);
        TestUtils.assertEquals(1, mdl.apply(VectorUtils.of(10, 100)), PRECISION);
    }
",non-flaky,5
91494,apache_kylin,DriverTest.testVersion,"    @Test
    public void testVersion() {
        Driver driver = new DummyDriver();
        DriverVersion version = driver.getDriverVersion();
        Assert.assertNotEquals(""unknown version"", version.productVersion);
    }
",non-flaky,5
91495,apache_kylin,DriverTest.testStatementWithMockData,"    @Test
    public void testStatementWithMockData() throws SQLException {
        Driver driver = new DummyDriver();

        Connection conn = driver.connect(""jdbc:kylin://test_url/test_db"", null);

        ResultSet tables = conn.getMetaData().getTables(null, null, null, null);
        while (tables.next()) {
            for (int i = 0; i < 4; i++) {
                assertEquals(""dummy"", tables.getString(i + 1));
            }
            for (int i = 4; i < 10; i++) {
                assertEquals(null, tables.getString(i + 1));
            }
        }

        Statement state = conn.createStatement();
        ResultSet resultSet = state.executeQuery(""select * from test_table"");

        ResultSetMetaData metadata = resultSet.getMetaData();
        assertEquals(12, metadata.getColumnType(1));
        assertEquals(""varchar"", metadata.getColumnTypeName(1));
        assertEquals(1, metadata.isNullable(1));

        while (resultSet.next()) {
            assertEquals(""foo"", resultSet.getString(1));
            assertEquals(""bar"", resultSet.getString(2));
            assertEquals(""tool"", resultSet.getString(3));
        }

        resultSet.close();
        state.close();
        conn.close();
    }
",non-flaky,5
91496,apache_kylin,DriverTest.testStatementWithQuestionMask,"    @Test
    public void testStatementWithQuestionMask() throws SQLException {
        Driver driver = new DummyDriver();

        Connection conn = driver.connect(""jdbc:kylin://test_url/test_db"", null);
        Statement state = conn.createStatement();
        ResultSet resultSet = state.executeQuery(""select * from test_table where url not in ('http://a.b.com/?a=b')"");
        ResultSetMetaData metadata = resultSet.getMetaData();
        assertEquals(12, metadata.getColumnType(1));
        assertEquals(""varchar"", metadata.getColumnTypeName(1));
        assertEquals(1, metadata.isNullable(1));

        while (resultSet.next()) {
            assertEquals(""foo"", resultSet.getString(1));
            assertEquals(""bar"", resultSet.getString(2));
            assertEquals(""tool"", resultSet.getString(3));
        }

        resultSet.close();
        state.close();
        conn.close();
    }
",non-flaky,5
91497,apache_kylin,DriverTest.testDateAndTimeStampWithMockData,"    @Test
    public void testDateAndTimeStampWithMockData() throws SQLException {
        Driver driver = new DummyDriver();

        Connection conn = driver.connect(""jdbc:kylin://test_url/test_db"", null);
        PreparedStatement state = conn.prepareStatement(""select * from test_table where id=?"");
        state.setInt(1, 10);
        ResultSet resultSet = state.executeQuery();

        ResultSetMetaData metadata = resultSet.getMetaData();
        assertEquals(""date"", metadata.getColumnTypeName(4));
        assertEquals(""timestamp"", metadata.getColumnTypeName(5));

        while (resultSet.next()) {
            assertEquals(""2019-04-27"", resultSet.getString(4));
            assertEquals(""2019-04-27 17:30:03"", resultSet.getString(5));
        }

        resultSet.close();
        state.close();
        conn.close();
    }
",non-flaky,5
91498,apache_kylin,DriverTest.testMultipathOfDomainForConnection,"    @Test
    public void testMultipathOfDomainForConnection() throws SQLException {
        Driver driver = new DummyDriver();

        Connection conn = driver.connect(""jdbc:kylin://test_url/kylin/test_db/"", null);
        Statement state = conn.createStatement();
        ResultSet resultSet = state.executeQuery(""select * from test_table where url not in ('http://a.b.com/?a=b') limit 1"");
        ResultSetMetaData metadata = resultSet.getMetaData();
        assertEquals(12, metadata.getColumnType(1));
        assertEquals(""varchar"", metadata.getColumnTypeName(1));
        assertEquals(1, metadata.isNullable(1));

        while (resultSet.next()) {
            assertEquals(""foo"", resultSet.getString(1));
            assertEquals(""bar"", resultSet.getString(2));
            assertEquals(""tool"", resultSet.getString(3));
        }

        resultSet.close();
        state.close();
        conn.close();
    }
",non-flaky,5
91499,apache_kylin,DriverTest.testPreparedStatementWithMockData,"    @Test
    public void testPreparedStatementWithMockData() throws SQLException {
        Driver driver = new DummyDriver();

        Connection conn = driver.connect(""jdbc:kylin://test_url/test_db"", null);
        PreparedStatement state = conn.prepareStatement(""select * from test_table where id=?"");
        state.setInt(1, 10);
        ResultSet resultSet = state.executeQuery();

        ResultSetMetaData metadata = resultSet.getMetaData();
        assertEquals(12, metadata.getColumnType(1));
        assertEquals(""varchar"", metadata.getColumnTypeName(1));
        assertEquals(1, metadata.isNullable(1));

        while (resultSet.next()) {
            assertEquals(""foo"", resultSet.getString(1));
            assertEquals(""bar"", resultSet.getString(2));
            assertEquals(""tool"", resultSet.getString(3));
        }

        resultSet.close();
        state.close();
        conn.close();
    }
",non-flaky,5
91500,apache_kylin,DriverTest.testWithCubeData,"    @Test
    public void testWithCubeData() throws Exception {
        Driver driver = new Driver();
        Properties info = new Properties();
        info.put(""user"", ""ADMIN"");
        info.put(""password"", ""KYLIN"");
        Connection conn = driver.connect(""jdbc:kylin://localhost:7070/default"", info);

        ResultSet catalogs = conn.getMetaData().getCatalogs();
        System.out.println(""CATALOGS"");
        printResultSetMetaData(catalogs);
        printResultSet(catalogs);

        ResultSet schemas = conn.getMetaData().getSchemas();
        System.out.println(""SCHEMAS"");
        printResultSetMetaData(schemas);
        printResultSet(schemas);

        ResultSet tables = conn.getMetaData().getTables(null, null, null, null);
        System.out.println(""TABLES"");
        printResultSetMetaData(tables);
        printResultSet(tables);

        for (int j = 0; j < 3; j++) {
            Statement state = conn.createStatement();
            ResultSet resultSet = state.executeQuery(""select * from test_kylin_fact"");

            printResultSetMetaData(resultSet);
            printResultSet(resultSet);

            resultSet.close();
        }

        catalogs.close();
        schemas.close();
        tables.close();
        conn.close();
    }
",non-flaky,5
91501,apache_kylin,DriverTest.testPreparedStatementWithCubeData,"    @Test
    public void testPreparedStatementWithCubeData() throws SQLException {
        Driver driver = new Driver();
        Properties info = new Properties();
        info.put(""user"", ""ADMIN"");
        info.put(""password"", ""KYLIN"");
        Connection conn = driver.connect(""jdbc:kylin://localhost:7070/default"", info);

        PreparedStatement state = conn
                .prepareStatement(""select cal_dt, count(*) from test_kylin_fact where seller_id=? group by cal_dt"");
        state.setLong(1, 10000001);
        ResultSet resultSet = state.executeQuery();

        printResultSetMetaData(resultSet);
        printResultSet(resultSet);

        resultSet.close();
        state.close();
        conn.close();
    }
",non-flaky,5
91502,apache_kylin,DriverTest.testSSLFromURL,"    @Test
    public void testSSLFromURL() throws SQLException {
        Driver driver = new DummyDriver();
        Connection conn = driver.connect(""jdbc:kylin:ssl=True;//test_url/test_db"", null);
        assertEquals(""test_url"", ((KylinConnection) conn).getBaseUrl());
        assertEquals(""test_db"", ((KylinConnection) conn).getProject());
        assertTrue(Boolean.parseBoolean((String) ((KylinConnection) conn).getConnectionProperties().get(""ssl"")));
        conn.close();
    }
",non-flaky,5
91503,apache_kylin,DriverTest.testCalciteProps,"    @Test
    public void testCalciteProps() throws SQLException {
        Driver driver = new DummyDriver();
        Properties props = new Properties();
        props.setProperty(""kylin.query.calcite.extras-props.caseSensitive"", ""true"");
        props.setProperty(""kylin.query.calcite.extras-props.unquotedCasing"", ""TO_LOWER"");
        props.setProperty(""kylin.query.calcite.extras-props.quoting"", ""BRACKET"");
        KylinConnection conn = (KylinConnection) driver.connect(""jdbc:kylin:test_url/test_db"", props);
        Properties connProps = conn.getConnectionProperties();
        assertEquals(""true"", connProps.getProperty(""kylin.query.calcite.extras-props.caseSensitive""));
        assertEquals(""TO_LOWER"", connProps.getProperty(""kylin.query.calcite.extras-props.unquotedCasing""));
        assertEquals(""BRACKET"", connProps.getProperty(""kylin.query.calcite.extras-props.quoting""));

        // parameters in url is prior to props parameter
        KylinConnection conn2 = (KylinConnection) driver.connect(""jdbc:kylin:kylin.query.calcite.extras-props.caseSensitive=false;"" +
                ""kylin.query.calcite.extras-props.unquotedCasing=UNCHANGED;"" +
                ""kylin.query.calcite.extras-props.quoting=BACK_TICK;"" +
                ""test_url/test_db"", props);
        Properties connProps2 = conn2.getConnectionProperties();
        assertEquals(""false"", connProps2.getProperty(""kylin.query.calcite.extras-props.caseSensitive""));
        assertEquals(""UNCHANGED"", connProps2.getProperty(""kylin.query.calcite.extras-props.unquotedCasing""));
        assertEquals(""BACK_TICK"", connProps2.getProperty(""kylin.query.calcite.extras-props.quoting""));
        conn.close();
        conn2.close();
    }
",non-flaky,5
91504,apache_kylin,KylinClientTest.connect,"    @Test
    public void connect() throws IOException {
        HttpResponse response = mock(HttpResponse.class);
        when(httpClient.execute(any(HttpUriRequest.class))).thenReturn(response);
        when(response.getStatusLine()).thenReturn(new BasicStatusLine(HTTP_1_1, 200, ""OK""));
        client.connect();
    }
",non-flaky,5
91505,apache_kylin,KylinClientTest.retrieveMetaData,"    @Test
    public void retrieveMetaData() throws IOException {
        HttpResponse response = TestUtil.mockHttpResponseWithFile(200, ""OK"", ""tables_and_columns.json"");
        when(httpClient.execute(any(HttpUriRequest.class))).thenReturn(response);

        KylinMeta.KMetaProject metaData = client.retrieveMetaData(connInfo.getProject());

        assertEquals(connInfo.getProject(), metaData.projectName);
        assertTrue(!metaData.catalogs.isEmpty());
        KylinMeta.KMetaCatalog catalog = metaData.catalogs.get(0);
        assertEquals(""defaultCatalog"", catalog.getName());
        assertEquals(1, catalog.schemas.size());
        KylinMeta.KMetaSchema schema = catalog.schemas.get(0);
        assertEquals(""DEFAULT"", schema.getName());
        assertEquals(5, schema.tables.size());
    }
",non-flaky,5
91506,apache_kylin,KylinClientTest.retrieveMetaDataWithWrongProject,"    @Test(expected = AssertionError.class)
    public void retrieveMetaDataWithWrongProject() throws IOException {
        client.retrieveMetaData(""defualt2"");
    }
",non-flaky,5
91507,apache_kylin,KylinClientTest.executeQuery,"    @Test
    public void executeQuery() throws IOException {
        HttpResponse response = TestUtil.mockHttpResponseWithFile(200, ""OK"", ""query.json"");
        when(httpClient.execute(any(HttpUriRequest.class))).thenReturn(response);
        IRemoteClient.QueryResult queryResult = client.executeQuery(""SELECT 1 as val"", Collections.emptyList(), new HashMap<String, String>());
        assertEquals(1, queryResult.columnMeta.size());
        Iterable<Object> iterable = queryResult.iterable;
        ArrayList<Object> list = Lists.newArrayList(iterable);
        assertEquals(1, list.size());
    }
",non-flaky,5
91508,apache_kylin,KylinClientTest.testWrapObjectThrowsIllegalArgumentExceptionUsingDateType,"  @Test(expected = IllegalArgumentException.class)
  public void testWrapObjectThrowsIllegalArgumentExceptionUsingDateType() {
      KylinClient.wrapObject(""OQ? PYC6BWm`kOE"", Types.DATE);
  }
",non-flaky,5
91509,apache_kylin,KylinClientTest.testWrapObjectUsingNull,"  @Test
  public void testWrapObjectUsingNull() {
      assertNull(KylinClient.wrapObject(null, 1));
  }
",non-flaky,5
91510,apache_kylin,KylinClientTest.testConvertBooleanType,"  @Test
  public void testConvertBooleanType() {
      assertEquals(""java.lang.Boolean"", KylinClient.convertType(Types.BOOLEAN).getName());
  }
",non-flaky,5
91511,apache_kylin,SQLResonseStubTest.testReadValuePartRecognizedField,"    @Test
    public void testReadValuePartRecognizedField() throws IOException {
        final String payload = ""{ \""columnMetas\"":[ { \""isNullable\"":1, \""displaySize\"":0, \""schemaName\"":null, \""catelogName\"":null, \""tableName\"":null, \""precision\"":0, \""scale\"":0, \""columnType\"":91, \""columnTypeName\"":\""DATE\"", \""readOnly\"":true, \""writable\"":false, \""caseSensitive\"":true, \""searchable\"":false, \""currency\"":false, \""signed\"":true, \""autoIncrement\"":false, \""definitelyWritable\"":false },"" + ""{ \""isNullable\"":1, \""displaySize\"":10, \""label\"":\""LEAF_CATEG_ID\"", \""name\"":\""LEAF_CATEG_ID\"", ""
                + ""\""schemaName\"":null, \""catelogName\"":null, \""tableName\"":null, \""precision\"":10, \""scale\"":0, \""columnType\"":4, \""columnTypeName\"":\""INTEGER\"", \""readOnly\"":true, \""writable\"":false, \""caseSensitive\"":true, \""searchable\"":false, \""currency\"":false, \""signed\"":true, \""autoIncrement\"":false, \""definitelyWritable\"":false } ], \""results\"":[ [ \""2013-08-07\"", \""32996\"", \""15\"", \""15\"", \""Auction\"", \""10000000\"", \""49.048952730908745\"", \""49.048952730908745\"", \""49.048952730908745\"", \""1\"" ], [ \""2013-08-07\"", \""43398\"", \""0\"", \""14\"", \""ABIN\"", \""10000633\"", \""85.78317064220418\"", \""85.78317064220418\"", \""85.78317064220418\"", \""1\"" ] ], \""cube\"":\""test_kylin_cube_with_slr_desc\"", \""affectedRowCount\"":0, \""isException\"":false, \""exceptionMessage\"":null, \""duration\"":3451, \""partial\"":false }"";
        final SQLResponseStub stub = new ObjectMapper().readValue(payload, SQLResponseStub.class);
        assertEquals(""test_kylin_cube_with_slr_desc"", stub.getCube());
        assertEquals(3451, stub.getDuration());
        assertFalse(stub.getColumnMetas().isEmpty());
        assertEquals(91, stub.getColumnMetas().get(0).getColumnType());
        assertNull(stub.getColumnMetas().get(0).getLabel());
        assertFalse(stub.getResults().isEmpty());
        assertNull(stub.getExceptionMessage());
    }
",non-flaky,5
91512,apache_kylin,SQLResonseStubTest.testReadValueWithUnrecognizedField,"    @Test
    public void testReadValueWithUnrecognizedField() throws IOException {
        final String payload = ""{ \""columnMetas\"":[ { \""Unrecognized\"":0, \""isNullable\"":1, \""displaySize\"":0, "" + ""\""label\"":\""CAL_DT\"", \""name\"":\""CAL_DT\"", \""schemaName\"":null, \""catelogName\"":null, "" + ""\""tableName\"":null, \""precision\"":0, \""scale\"":0, \""columnType\"":91, \""columnTypeName\"":\""DATE\"", "" + ""\""readOnly\"":true, \""writable\"":false, \""caseSensitive\"":true, \""searchable\"":false, \""currency\"":false, \""signed\"":true, \""autoIncrement\"":false, \""definitelyWritable\"":false },""
                + "" { \""isNullable\"":1, \""displaySize\"":10, \""label\"":\""LEAF_CATEG_ID\"", \""name\"":\""LEAF_CATEG_ID\"", \""schemaName\"":null, \""catelogName\"":null, \""tableName\"":null, \""precision\"":10, \""scale\"":0, \""columnType\"":4, \""columnTypeName\"":\""INTEGER\"", \""readOnly\"":true, \""writable\"":false, \""caseSensitive\"":true, \""searchable\"":false, \""currency\"":false, \""signed\"":true, \""autoIncrement\"":false, \""definitelyWritable\"":false } ], \""results\"":[ [ \""2013-08-07\"", \""32996\"", \""15\"", \""15\"", \""Auction\"", \""10000000\"", \""49.048952730908745\"", \""49.048952730908745\"", \""49.048952730908745\"", \""1\"" ], [ \""2013-08-07\"", \""43398\"", \""0\"", \""14\"", \""ABIN\"", \""10000633\"", \""85.78317064220418\"", \""85.78317064220418\"", \""85.78317064220418\"", \""1\"" ] ], \""cube\"":\""test_kylin_cube_with_slr_desc\"", \""affectedRowCount\"":0, \""isException\"":false, \""exceptionMessage\"":null, \""duration\"":3451, \""partial\"":false, \""hitCache\"":false }"";
        final SQLResponseStub stub = new ObjectMapper().readValue(payload, SQLResponseStub.class);
        assertEquals(""test_kylin_cube_with_slr_desc"", stub.getCube());
        assertEquals(3451, stub.getDuration());
        assertFalse(stub.getColumnMetas().isEmpty());
        assertEquals(91, stub.getColumnMetas().get(0).getColumnType());
        assertEquals(""CAL_DT"", stub.getColumnMetas().get(0).getLabel());
        assertFalse(stub.getResults().isEmpty());
        assertNull(stub.getExceptionMessage());
    }
",non-flaky,5
91513,apache_kylin,KylinConnectionTest.testPrepareStatementWithMockKylinClient,"    @Test
    public void testPrepareStatementWithMockKylinClient() throws SQLException, IOException {
        String sql = ""select 1 as val"";
        // mock client
        when(client.executeQuery(anyString(), Mockito.<List<Object>>any(), Mockito.<Map<String, String>>any())).thenReturn(getMockResult());

        try (KylinConnection conn = getConnectionWithMockClient()) {
            PreparedStatement preparedStatement = conn.prepareStatement(sql);
            try (ResultSet resultSet = preparedStatement.executeQuery()) {
                verify(client).executeQuery(eq(sql), Mockito.<List<Object>>any(), Mockito.<Map<String, String>>any());

                assertTrue(resultSet.next());
                ResultSetMetaData metaData = resultSet.getMetaData();
                assertEquals(""VAL"", metaData.getColumnName(1));
                assertEquals(1, resultSet.getInt(""VAL""));
            }
        }
    }
",non-flaky,5
91514,apache_kylin,KylinConnectionTest.testPrepareStatementWithMockHttp,"    @Test
    public void testPrepareStatementWithMockHttp() throws IOException, SQLException {
        String sql = ""select 1 as val"";
        try (KylinConnection connection = getConnectionWithMockHttp()) {

            // mock http
            HttpResponse response = TestUtil.mockHttpResponseWithFile(200, ""OK"", ""query.json"");
            when(httpClient.execute(any(HttpUriRequest.class))).thenReturn(response);

            try (ResultSet resultSet = connection.prepareStatement(sql).executeQuery()) {
                assertTrue(resultSet.next());
                ResultSetMetaData metaData = resultSet.getMetaData();
                assertEquals(""VAL"", metaData.getColumnName(1));
                assertEquals(1, resultSet.getInt(""VAL""));
            }
        }
    }
",non-flaky,5
91515,apache_kylin,KylinConnectionTest.matches,"    @Test
    public void testJdbcClientCalcitePropsInUrl() throws Exception {
        String sql = ""select 1 as val"";

        // mock client
        when(client.executeQuery(anyString(), Mockito.<List<Object>>any(), Mockito.<Map<String, String>>any())).thenReturn(getMockResult());
        Map<String, String> toggles = new HashMap<>();
        Properties info = new Properties();
        info.setProperty(""caseSensitive"", ""false"");
        info.setProperty(""unquotedCasing"", ""UNCHANGED"");
        try (KylinConnection conn = getConnectionWithMockClient(""jdbc:kylin:test_url/test_db"", info)) {
            PreparedStatement preparedStatement = conn.prepareStatement(sql);
            try (ResultSet resultSet = preparedStatement.executeQuery()) {
                verify(client).executeQuery(eq(sql), Mockito.<List<Object>>any(), argThat(new ArgumentMatcher<Map<String, String>>() {
                    @Override
                    public boolean matches(Map<String, String> argument) {
                        String propsStr = argument.get(""JDBC_CLIENT_CALCITE_PROPS"");
                        assertNotNull(propsStr);
                        Properties props = new Properties();
                        try {
                            props.load(new StringReader(propsStr));
                        } catch (IOException e) {
                            throw new RuntimeException(e);
                        }
                        assertEquals(""false"", props.getProperty(""caseSensitive""));
                        assertEquals(""UNCHANGED"", props.getProperty(""unquotedCasing""));
                        return true;
                    }
",non-flaky,5
91516,apache_kylin,KafkaInputRecordReaderTest.testNextKeyValue,"    @Test
    public void testNextKeyValue()  throws Throwable  {
        KafkaInputRecordReader kafkaInputRecordReader = new KafkaInputRecordReader();
        assertFalse(kafkaInputRecordReader.nextKeyValue());
        assertFalse(kafkaInputRecordReader.nextKeyValue());
    }
",non-flaky,5
91517,apache_kylin,KafkaInputRecordReaderTest.testGetProgress,"    @Test
    public void testGetProgress()  throws Throwable  {
        KafkaInputRecordReader kafkaInputRecordReader = new KafkaInputRecordReader();
        assertEquals(1.0F, kafkaInputRecordReader.getProgress(), 0.01F);
    }
",non-flaky,5
91518,apache_kylin,KafkaInputRecordReaderTest.testGetCurrentKey,"    @Test
    public void testGetCurrentKey()  throws Throwable  {
        KafkaInputRecordReader kafkaInputRecordReader = new KafkaInputRecordReader();
        kafkaInputRecordReader.nextKeyValue();
        assertEquals(0L, kafkaInputRecordReader.getCurrentKey().get());
    }
",non-flaky,5
91519,apache_kylin,KafkaInputRecordReaderTest.testGetCurrentValue,"    @Test
    public void testGetCurrentValue()  throws Throwable  {
        KafkaInputRecordReader kafkaInputRecordReader = new KafkaInputRecordReader();
        kafkaInputRecordReader.nextKeyValue();
        assertEquals(0, kafkaInputRecordReader.getCurrentValue().getBytes().length);
    }
",non-flaky,5
91520,apache_kylin,TimedJsonStreamParserTest.testNormalValue,"    @Test
    public void testNormalValue() throws Exception {
        userNeedColNames = new String[] { ""createdAt"", ""id"", ""isTruncated"", ""text"" };
        List<TblColRef> allCol = mockupTblColRefList();
        TimedJsonStreamParser parser = new TimedJsonStreamParser(allCol, null);
        Object msg = mapper.readValue(new File(jsonFilePath), mapType);
        ByteBuffer buffer = getJsonByteBuffer(msg);
        List<StreamingMessageRow> msgList = parser.parse(buffer);
        List<String> result = msgList.get(0).getData();
        assertEquals(""Jul 20, 2016 9:59:17 AM"", result.get(0));
        assertEquals(""755703618762862600"", result.get(1));
        assertEquals(""false"", result.get(2));
        assertEquals(""dejamos"", result.get(3));
    }
",non-flaky,5
91521,apache_kylin,TimedJsonStreamParserTest.testEmbeddedValue,"    @Test
    public void testEmbeddedValue() throws Exception {
        userNeedColNames = new String[] { ""user_id"", ""user_description"", ""user_isProtected"",
                ""user_is_Default_Profile_Image"" };
        userNeedColNamesComment = new String[] { """", """", """",
                ""user"" + TimedJsonStreamParser.EMBEDDED_PROPERTY_SEPARATOR + ""is_Default_Profile_Image"" };
        List<TblColRef> allCol = mockupTblColRefListWithComment(userNeedColNamesComment);
        TimedJsonStreamParser parser = new TimedJsonStreamParser(allCol, null);
        Object msg = mapper.readValue(new File(jsonFilePath), mapType);
        ByteBuffer buffer = getJsonByteBuffer(msg);
        List<StreamingMessageRow> msgList = parser.parse(buffer);
        List<String> result = msgList.get(0).getData();
        assertEquals(""4853763947"", result.get(0));
        assertEquals(""Noticias"", result.get(1));
        assertEquals(""false"", result.get(2));
        assertEquals(""false"", result.get(3));
    }
",non-flaky,5
91522,apache_kylin,TimedJsonStreamParserTest.testEmbeddedValueFaultTolerant,"    @Test
    public void testEmbeddedValueFaultTolerant() throws Exception {
        userNeedColNames = new String[] { ""user_id"", ""nonexisted_description"" };
        userNeedColNamesComment = new String[] { """", """" };
        List<TblColRef> allCol = mockupTblColRefList();
        TimedJsonStreamParser parser = new TimedJsonStreamParser(allCol, null);
        Object msg = mapper.readValue(new File(jsonFilePath), mapType);
        ByteBuffer buffer = getJsonByteBuffer(msg);
        List<StreamingMessageRow> msgList = parser.parse(buffer);
        List<String> result = msgList.get(0).getData();
        assertEquals(""4853763947"", result.get(0));
        assertEquals(StringUtils.EMPTY, result.get(1));
    }
",non-flaky,5
91523,apache_kylin,TimedJsonStreamParserTest.testArrayValue,"    @Test
    public void testArrayValue() throws Exception {
        userNeedColNames = new String[] { ""userMentionEntities"", ""mediaEntities"" };
        List<TblColRef> allCol = mockupTblColRefList();
        TimedJsonStreamParser parser = new TimedJsonStreamParser(allCol, null);
        Object msg = mapper.readValue(new File(jsonFilePath), mapType);
        HashMap<String, Object> map = (HashMap<String, Object>) msg;
        Object array = map.get(""mediaEntities"");
        ByteBuffer buffer = getJsonByteBuffer(msg);
        List<StreamingMessageRow> msgList = parser.parse(buffer);
        List<String> result = msgList.get(0).getData();
        System.out.println(result);

    }
",non-flaky,5
91524,apache_kylin,TimedJsonStreamParserTest.testMapValue,"    @Test
    public void testMapValue() throws Exception {
        userNeedColNames = new String[] { ""user"" };
        List<TblColRef> allCol = mockupTblColRefList();
        TimedJsonStreamParser parser = new TimedJsonStreamParser(allCol, null);
        Object msg = mapper.readValue(new File(jsonFilePath), mapType);
        ByteBuffer buffer = getJsonByteBuffer(msg);
        List<StreamingMessageRow> msgList = parser.parse(buffer);
        List<String> result = msgList.get(0).getData();

    }
",non-flaky,5
91525,apache_kylin,TimedJsonStreamParserTest.testNullKey,"    @Test
    public void testNullKey() throws Exception {
        userNeedColNames = new String[] { ""null"", """" };
        List<TblColRef> allCol = mockupTblColRefList();
        TimedJsonStreamParser parser = new TimedJsonStreamParser(allCol, null);
        Object msg = mapper.readValue(new File(jsonFilePath), mapType);
        ByteBuffer buffer = getJsonByteBuffer(msg);
        List<StreamingMessageRow> msgList = parser.parse(buffer);
        List<String> result = msgList.get(0).getData();
        assertEquals(StringUtils.EMPTY, result.get(0));
        assertEquals(StringUtils.EMPTY, result.get(1));
    }
",non-flaky,5
91526,apache_kylin,KafkaConsumerPropertiesTest.testLoadKafkaProperties,"    @Test
    public void testLoadKafkaProperties() {
        KafkaConsumerProperties kafkaConsumerProperties = KafkaConsumerProperties.getInstanceFromEnv();
        assertFalse(kafkaConsumerProperties.extractKafkaConfigToProperties().containsKey(""acks""));
        assertTrue(kafkaConsumerProperties.extractKafkaConfigToProperties().containsKey(""session.timeout.ms""));
        assertEquals(""10000"", kafkaConsumerProperties.extractKafkaConfigToProperties().getProperty(""session.timeout.ms""));
        assertTrue(kafkaConsumerProperties.extractKafkaConfigToProperties().containsKey(""client.id""));
        assertEquals(""kylin"", kafkaConsumerProperties.extractKafkaConfigToProperties().getProperty(""client.id""));
    }
",non-flaky,5
91527,apache_kylin,KafkaConsumerPropertiesTest.testLoadKafkaPropertiesAsHadoopJobConf,"    @Test
    public void testLoadKafkaPropertiesAsHadoopJobConf() throws IOException, ParserConfigurationException, SAXException {
        KafkaConsumerProperties kafkaConsumerProperties = KafkaConsumerProperties.getInstanceFromEnv();
        Configuration conf = new Configuration(false);
        conf.addResource(new FileInputStream(new File(kafkaConsumerProperties.getKafkaConsumerHadoopJobConf())), KafkaConsumerProperties.KAFKA_CONSUMER_FILE);
        assertEquals(""10000"", conf.get(""session.timeout.ms""));

        Properties prop = KafkaConsumerProperties.extractKafkaConfigToProperties(conf);
        assertEquals(""10000"", prop.getProperty(""session.timeout.ms""));
    }
",non-flaky,5
91528,apache_kylin,SqlUtilTest.testJdbcTypetoKylinDataType,"    @Test
    public void testJdbcTypetoKylinDataType() {
        this.getClass().getClassLoader().toString();
        assertEquals(""double"", SqlUtil.jdbcTypeToKylinDataType(Types.FLOAT));
        assertEquals(""varchar"", SqlUtil.jdbcTypeToKylinDataType(Types.NVARCHAR));
        assertEquals(""any"", SqlUtil.jdbcTypeToKylinDataType(Types.ARRAY));
        assertEquals(""integer"", SqlUtil.jdbcTypeToKylinDataType((4)));
        assertEquals(""smallint"", SqlUtil.jdbcTypeToKylinDataType((5)));
        assertEquals(""tinyint"", SqlUtil.jdbcTypeToKylinDataType((-6)));
        assertEquals(""char"", SqlUtil.jdbcTypeToKylinDataType((1)));
        assertEquals(""decimal"", SqlUtil.jdbcTypeToKylinDataType((2)));
        assertEquals(""varchar"", SqlUtil.jdbcTypeToKylinDataType((-1)));
        assertEquals(""byte"", SqlUtil.jdbcTypeToKylinDataType((-2)));
        assertEquals(""any"", SqlUtil.jdbcTypeToKylinDataType((-1720774701)));
        assertEquals(""boolean"", SqlUtil.jdbcTypeToKylinDataType((-7)));
        assertEquals(""timestamp"", SqlUtil.jdbcTypeToKylinDataType((93)));
        assertEquals(""time"", SqlUtil.jdbcTypeToKylinDataType((92)));
        assertEquals(""date"", SqlUtil.jdbcTypeToKylinDataType((91)));
        assertEquals(""bigint"", SqlUtil.jdbcTypeToKylinDataType((-5)));
    }
",non-flaky,5
91529,apache_kylin,SqlUtilTest.testIsPrecisionApplicable,"    @Test
    public void testIsPrecisionApplicable() {
        assertFalse(SqlUtil.isPrecisionApplicable(""boolean""));
        assertTrue(SqlUtil.isPrecisionApplicable(""varchar""));
    }
",non-flaky,5
91530,apache_kylin,SqlUtilTest.testIsScaleApplicable,"    @Test
    public void testIsScaleApplicable() {
        assertFalse(SqlUtil.isScaleApplicable(""varchar""));
        assertTrue(SqlUtil.isScaleApplicable(""decimal""));
    }
",non-flaky,5
91531,apache_kylin,DefaultJdbcMetadataTest.testListDatabases,"    @Test
    public void testListDatabases() throws SQLException {
        ResultSet rs = mock(ResultSet.class);
        when(rs.next()).thenReturn(true).thenReturn(true).thenReturn(false);
        when(rs.getString(""TABLE_SCHEM"")).thenReturn(""schema1"").thenReturn(""schema2"");
        when(rs.getString(""TABLE_CATALOG"")).thenReturn(""catalog1"").thenReturn(""catalog2"");

        when(connection.getMetaData()).thenReturn(dbmd);
        when(dbmd.getSchemas()).thenReturn(rs);

        List<String> dbs = jdbcMetadata.listDatabases();

        Assert.assertEquals(2, dbs.size());
        Assert.assertEquals(""schema1"", dbs.get(0));
    }
",non-flaky,5
91532,apache_kylin,DefaultJdbcMetadataTest.testListTables,"    @Test
    public void testListTables() throws SQLException {
        ResultSet rs = mock(ResultSet.class);
        when(rs.next()).thenReturn(true).thenReturn(true).thenReturn(true).thenReturn(false);
        when(rs.getString(""TABLE_NAME"")).thenReturn(""KYLIN_SALES"").thenReturn(""CAT_DT"").thenReturn(""KYLIN_CAT"");

        String schema = ""testschema"";
        when(connection.getMetaData()).thenReturn(dbmd);
        when(dbmd.getTables(null, schema, null, null)).thenReturn(rs);

        List<String> tables = jdbcMetadata.listTables(schema);

        Assert.assertEquals(3, tables.size());
        Assert.assertEquals(""CAT_DT"", tables.get(1));
    }
",non-flaky,5
91533,apache_kylin,DefaultJdbcMetadataTest.testGetTable,"    @Test
    public void testGetTable() throws SQLException {
        String schema = ""testSchema"";
        String table = ""testTable"";
        ResultSet rs = mock(ResultSet.class);
        when(dbmd.getTables(null, schema, table, null)).thenReturn(rs);

        ResultSet result = jdbcMetadata.getTable(dbmd, schema, table);

        verify(dbmd, times(1)).getTables(null, schema, table, null);
        Assert.assertEquals(rs, result);
    }
",non-flaky,5
91534,apache_kylin,DefaultJdbcMetadataTest.testListColumns,"    @Test
    public void testListColumns() throws SQLException {
        String schema = ""testSchema"";
        String table = ""testTable"";
        ResultSet rs = mock(ResultSet.class);
        when(dbmd.getColumns(null, schema, table, null)).thenReturn(rs);

        ResultSet result = jdbcMetadata.listColumns(dbmd, schema, table);

        verify(dbmd, times(1)).getColumns(null, schema, table, null);
        Assert.assertEquals(rs, result);
    }
",non-flaky,5
91535,apache_kylin,SQLServerJdbcMetadataTest.testListDatabases,"    @Test
    public void testListDatabases() throws SQLException {
        ResultSet rs = mock(ResultSet.class);
        when(rs.next()).thenReturn(true).thenReturn(true).thenReturn(false);
        when(rs.getString(""TABLE_SCHEM"")).thenReturn(""schema1"").thenReturn(""schema2"");
        when(rs.getString(""TABLE_CAT"")).thenReturn(""catalog1"").thenReturn(""testdb"");

        when(connection.getCatalog()).thenReturn(""testdb"");
        when(connection.getMetaData()).thenReturn(dbmd);
        when(dbmd.getTables(""testdb"", null, null, null)).thenReturn(rs);

        List<String> dbs = jdbcMetadata.listDatabases();

        Assert.assertEquals(1, dbs.size());
        Assert.assertEquals(""schema2"", dbs.get(0));
    }
",non-flaky,5
91536,apache_kylin,SQLServerJdbcMetadataTest.testListDatabasesWithoutSpecificDB,"    @Test(expected = IllegalArgumentException.class)
    public void testListDatabasesWithoutSpecificDB() throws SQLException {
        when(connection.getCatalog()).thenReturn("""");
        jdbcMetadata.listDatabases();
    }
",non-flaky,5
91537,apache_kylin,MySQLJdbcMetadataTest.testListDatabases,"    @Test
    public void testListDatabases() throws SQLException {
        when(connection.getCatalog()).thenReturn(""catalog1"");

        List<String> dbs = jdbcMetadata.listDatabases();

        Assert.assertEquals(1, dbs.size());
        Assert.assertEquals(""catalog1"", dbs.get(0));
    }
",non-flaky,5
91538,apache_kylin,MySQLJdbcMetadataTest.testListTables,"    @Test
    public void testListTables() throws SQLException {
        ResultSet rs = mock(ResultSet.class);
        when(rs.next()).thenReturn(true).thenReturn(true).thenReturn(true).thenReturn(false);
        when(rs.getString(""TABLE_NAME"")).thenReturn(""KYLIN_SALES"").thenReturn(""CAT_DT"").thenReturn(""KYLIN_CAT"");

        String catalog = ""testCatalog"";
        when(connection.getMetaData()).thenReturn(dbmd);
        when(dbmd.getTables(catalog, null, null, null)).thenReturn(rs);

        List<String> tables = jdbcMetadata.listTables(catalog);

        Assert.assertEquals(3, tables.size());
        Assert.assertEquals(""CAT_DT"", tables.get(1));
    }
",non-flaky,5
91539,apache_kylin,MySQLJdbcMetadataTest.testGetTable,"    @Test
    public void testGetTable() throws SQLException {
        String catalog = ""testSchema"";
        String table = ""testTable"";
        ResultSet rs = mock(ResultSet.class);
        when(dbmd.getTables(catalog, null, table, null)).thenReturn(rs);

        ResultSet result = jdbcMetadata.getTable(dbmd, catalog, table);

        verify(dbmd, times(1)).getTables(catalog, null, table, null);
        Assert.assertEquals(rs, result);
    }
",non-flaky,5
91540,apache_kylin,MySQLJdbcMetadataTest.testListColumns,"    @Test
    public void testListColumns() throws SQLException {
        String catalog = ""testSchema"";
        String table = ""testTable"";
        ResultSet rs = mock(ResultSet.class);
        when(dbmd.getColumns(catalog, null, table, null)).thenReturn(rs);

        ResultSet result = jdbcMetadata.listColumns(dbmd, catalog, table);

        verify(dbmd, times(1)).getColumns(catalog, null, table, null);
        Assert.assertEquals(rs, result);
    }
",non-flaky,5
91541,apache_kylin,JdbcExplorerTest.testListDatabases,"    @Test
    public void testListDatabases() throws SQLException {
        List<String> databases = new ArrayList<>();
        databases.add(""DB1"");
        databases.add(""DB2"");
        when(jdbcMetadata.listDatabases()).thenReturn(databases);

        List<String> result = jdbcExplorer.listDatabases();

        verify(jdbcMetadata, times(1)).listDatabases();
        Assert.assertEquals(databases, result);
    }
",non-flaky,5
91542,apache_kylin,JdbcExplorerTest.testListTables,"    @Test
    public void testListTables() throws SQLException {
        List<String> tables = new ArrayList<>();
        tables.add(""T1"");
        tables.add(""T2"");
        String databaseName = ""testDb"";
        when(jdbcMetadata.listTables(databaseName)).thenReturn(tables);

        List<String> result = jdbcExplorer.listTables(databaseName);
        verify(jdbcMetadata, times(1)).listTables(databaseName);
        Assert.assertEquals(tables, result);
    }
",non-flaky,5
91543,apache_kylin,JdbcExplorerTest.testLoadTableMetadata,"    @Test
    public void testLoadTableMetadata() throws SQLException {
        String tableName = ""tb1"";
        String databaseName = ""testdb"";
        ResultSet rs1 = mock(ResultSet.class);
        when(rs1.next()).thenReturn(true).thenReturn(false);
        when(rs1.getString(""TABLE_TYPE"")).thenReturn(""TABLE"");

        ResultSet rs2 = mock(ResultSet.class);
        when(rs2.next()).thenReturn(true).thenReturn(true).thenReturn(true).thenReturn(false);
        when(rs2.getString(""COLUMN_NAME"")).thenReturn(""COL1"").thenReturn(""COL2"").thenReturn(""COL3"");
        when(rs2.getInt(""DATA_TYPE"")).thenReturn(Types.VARCHAR).thenReturn(Types.INTEGER).thenReturn(Types.DECIMAL);
        when(rs2.getInt(""COLUMN_SIZE"")).thenReturn(128).thenReturn(10).thenReturn(19);
        when(rs2.getInt(""DECIMAL_DIGITS"")).thenReturn(0).thenReturn(0).thenReturn(4);
        when(rs2.getInt(""ORDINAL_POSITION"")).thenReturn(1).thenReturn(3).thenReturn(2);
        when(rs2.getString(""REMARKS"")).thenReturn(""comment1"").thenReturn(""comment2"").thenReturn(""comment3"");

        when(jdbcMetadata.getTable(dbmd, databaseName, tableName)).thenReturn(rs1);
        when(jdbcMetadata.listColumns(dbmd, databaseName, tableName)).thenReturn(rs2);

        Pair<TableDesc, TableExtDesc> result = jdbcExplorer.loadTableMetadata(databaseName, tableName, ""proj"");
        TableDesc tableDesc = result.getFirst();
        ColumnDesc columnDesc = tableDesc.getColumns()[1];

        Assert.assertEquals(databaseName.toUpperCase(Locale.ROOT), tableDesc.getDatabase());
        Assert.assertEquals(3, tableDesc.getColumnCount());
        Assert.assertEquals(""TABLE"", tableDesc.getTableType());
        Assert.assertEquals(""COL2"", columnDesc.getName());
        Assert.assertEquals(""integer"", columnDesc.getTypeName());
        Assert.assertEquals(""comment2"", columnDesc.getComment());
        Assert.assertEquals(databaseName.toUpperCase(Locale.ROOT) + ""."" + tableName.toUpperCase(Locale.ROOT),
                result.getSecond().getIdentity());
    }
",non-flaky,5
91544,apache_kylin,JdbcExplorerTest.testListDatabases,"    @Test
    public void testListDatabases() throws Exception {
        List<String> dbList = explorer.listDatabases();
        Assert.assertTrue(dbList.size() >= 3);
        Assert.assertTrue(dbList.contains(""EDW""));
        Assert.assertTrue(dbList.contains(""DEFAULT""));
    }
",non-flaky,5
91545,apache_kylin,JdbcExplorerTest.testListTables,"    @Test
    public void testListTables() throws Exception {
        List<String> tblList = explorer.listTables(""DEFAULT"");
        Assert.assertTrue(tblList.size() >= 3);
        Assert.assertTrue(tblList.contains(""TEST_KYLIN_FACT""));
        Assert.assertTrue(tblList.contains(""TEST_ACCOUNT""));
        Assert.assertTrue(tblList.contains(""TEST_COUNTRY""));
    }
",non-flaky,5
91546,apache_kylin,JdbcExplorerTest.testValidateSql,"    @Test
    public void testValidateSql() throws Exception {
        explorer.validateSQL(""select 1"");
        validateSQLInvalidEx.expect(Exception.class);
        explorer.validateSQL(""select"");
    }
",non-flaky,5
91547,apache_kylin,JdbcExplorerTest.testGetRelatedKylinResources,"    @Test
    public void testGetRelatedKylinResources() {
        Assert.assertTrue(explorer.getRelatedKylinResources(null).isEmpty());
    }
",non-flaky,5
91548,apache_kylin,JdbcExplorerTest.testLoadTableMetadata,"    @Test
    public void testLoadTableMetadata() throws Exception {
        Pair<TableDesc, TableExtDesc> pair = explorer.loadTableMetadata(""DEFAULT"", ""TEST_KYLIN_FACT"", ""DEFAULT"");
        Assert.assertNotNull(pair.getFirst());
        Assert.assertNotNull(pair.getSecond());

        TableDesc tblDesc = pair.getFirst();
        TableExtDesc tblExtDesc = pair.getSecond();
        Assert.assertEquals(""TEST_KYLIN_FACT"", tblDesc.getName());
        Assert.assertEquals(""TABLE"", tblDesc.getTableType());
        Assert.assertEquals(""DEFAULT.TEST_KYLIN_FACT"", tblDesc.getIdentity());
        Assert.assertEquals(""DEFAULT"", tblDesc.getDatabase());
        Assert.assertEquals(""DEFAULT"", tblDesc.getProject());
        Assert.assertEquals(tblDesc.getIdentity(), tblExtDesc.getIdentity());
        Assert.assertEquals(tblDesc.getProject(), tblExtDesc.getProject());

        ColumnDesc[] columnDescs = tblDesc.getColumns();
        Assert.assertEquals(tblDesc.getColumnCount(), columnDescs.length);
        Assert.assertNotNull(columnDescs[0].getName());
        Assert.assertNotNull(columnDescs[0].getDatatype());
        Assert.assertNotNull(columnDescs[0].getType());
        Assert.assertNotNull(columnDescs[0].getId());
    }
",non-flaky,5
91549,apache_kylin,JdbcExplorerTest.testEvalQueryMetadata,"    @Test
    public void testEvalQueryMetadata() {
        ColumnDesc[] columnDescs = explorer
                .evalQueryMetadata(""select cal_dt, count(*) as cnt from DEFAULT.test_kylin_fact group by cal_dt"");
        Assert.assertNotNull(columnDescs);
        Assert.assertEquals(2, columnDescs.length);
        Assert.assertEquals(""date"", columnDescs[0].getDatatype());
        Assert.assertEquals(""CAL_DT"", columnDescs[0].getName());
        Assert.assertEquals(""bigint"", columnDescs[1].getDatatype());
        Assert.assertEquals(""CNT"", columnDescs[1].getName());
    }
",non-flaky,5
91550,apache_kylin,JdbcTableTest.testBasics,"    @Test
    public void testBasics() throws Exception {
        TableMetadataManager tblManager = TableMetadataManager.getInstance(getTestConfig());
        TableDesc tblDesc = tblManager.getTableDesc(""test_kylin_fact"", ""default"");
        IReadableTable table = SourceManager.getSource(new JdbcSourceTest.JdbcSourceAware())
                .createReadableTable(tblDesc, null);

        // test TableReader
        try (IReadableTable.TableReader reader = table.getReader()) {
            Assert.assertTrue(reader instanceof JdbcTableReader);
            Assert.assertTrue(table instanceof JdbcTable);

            Assert.assertTrue(reader.next());
            String[] row = reader.getRow();
            Assert.assertNotNull(row);
            Assert.assertEquals(tblDesc.getColumnCount(), row.length);
        }

        // test basics
        Assert.assertTrue(table.exists());

        IReadableTable.TableSignature sign = table.getSignature();
        Assert.assertNotNull(sign);
        Assert.assertEquals(String.format(Locale.ROOT, ""%s.%s"", tblDesc.getDatabase(), tblDesc.getName()), sign.getPath());
        Assert.assertTrue(sign.getLastModifiedTime() > 0);
    }
",non-flaky,5
91551,apache_kylin,JdbcHiveMRInputTest.testGenSqoopCmd_Partition,"    @Test
    public void testGenSqoopCmd_Partition() throws IOException {
        ISource source = SourceManager.getSource(new JdbcSourceAware());
        IMRInput input = source.adaptToBuildEngine(IMRInput.class);
        Assert.assertNotNull(input);

        CubeManager cubeManager = CubeManager.getInstance(getTestConfig());
        CubeDesc cubeDesc = CubeDescManager.getInstance(getTestConfig()).getCubeDesc(""ci_inner_join_cube"");
        CubeSegment seg = cubeManager.appendSegment(cubeManager.getCube(cubeDesc.getName()),
                new SegmentRange.TSRange(System.currentTimeMillis() - 100L, System.currentTimeMillis() + 100L));
        CubeJoinedFlatTableDesc flatDesc = new CubeJoinedFlatTableDesc(seg);
        JdbcHiveMRInput.JdbcMRBatchCubingInputSide inputSide = (JdbcHiveMRInput.JdbcMRBatchCubingInputSide) input
                .getBatchCubingInputSide(flatDesc);

        AbstractExecutable executable = new MockInputSide(flatDesc, inputSide).createSqoopToFlatHiveStep(""/tmp"",
                cubeDesc.getName());
        Assert.assertNotNull(executable);

        String cmd = executable.getParam(""cmd"");
        Assert.assertTrue(cmd.contains(""org.h2.Driver""));
        Assert.assertTrue(cmd.contains(
                ""--boundary-query \""SELECT MIN(\\\""TEST_KYLIN_FACT\\\"".\\\""LEAF_CATEG_ID\\\""), MAX(\\\""TEST_KYLIN_FACT\\\"".\\\""LEAF_CATEG_ID\\\"")"" + System.lineSeparator()
                        + ""FROM \\\""DEFAULT\\\"".\\\""TEST_KYLIN_FACT\\\"" AS \\\""TEST_KYLIN_FACT\\\""""));
        source.close();
    }
",non-flaky,5
91552,apache_kylin,JdbcHiveMRInputTest.testGenSqoopCmd_NoPartition,"    @Test
    public void testGenSqoopCmd_NoPartition() throws IOException {
        ISource source = SourceManager.getSource(new JdbcSourceAware());
        IMRInput input = source.adaptToBuildEngine(IMRInput.class);
        Assert.assertNotNull(input);

        CubeManager cubeManager = CubeManager.getInstance(getTestConfig());
        CubeDesc cubeDesc = CubeDescManager.getInstance(getTestConfig()).getCubeDesc(""ci_left_join_cube"");
        CubeSegment seg = cubeManager.appendSegment(cubeManager.getCube(cubeDesc.getName()),
                new SegmentRange.TSRange(0L, Long.MAX_VALUE));
        CubeJoinedFlatTableDesc flatDesc = new CubeJoinedFlatTableDesc(seg);
        JdbcHiveMRInput.JdbcMRBatchCubingInputSide inputSide = (JdbcHiveMRInput.JdbcMRBatchCubingInputSide) input
                .getBatchCubingInputSide(flatDesc);

        AbstractExecutable executable = new MockInputSide(flatDesc, inputSide).createSqoopToFlatHiveStep(""/tmp"",
                cubeDesc.getName());
        Assert.assertNotNull(executable);
        String cmd = executable.getParam(""cmd"");
        Assert.assertTrue(cmd.contains(""org.h2.Driver""));
        Assert.assertTrue(
                cmd.contains(""--boundary-query \""SELECT MIN(\\\""TEST_KYLIN_FACT\\\"".\\\""CAL_DT\\\""), MAX(\\\""TEST_KYLIN_FACT\\\"".\\\""CAL_DT\\\"")"" + System.lineSeparator()
                        + ""FROM \\\""DEFAULT\\\"".\\\""TEST_KYLIN_FACT\\\"" AS \\\""TEST_KYLIN_FACT\\\""\""""));
        source.close();
    }
",non-flaky,5
91553,apache_kylin,JdbcHiveMRInputTest.testGenSqoopCmd_WithLookupShardBy,"    @Test
    public void testGenSqoopCmd_WithLookupShardBy() throws IOException {
        ISource source = SourceManager.getSource(new JdbcSourceAware());
        IMRInput input = source.adaptToBuildEngine(IMRInput.class);
        Assert.assertNotNull(input);

        CubeManager cubeManager = CubeManager.getInstance(getTestConfig());
        CubeDesc cubeDesc = CubeDescManager.getInstance(getTestConfig()).getCubeDesc(""ut_jdbc_shard"");
        CubeSegment seg = cubeManager.appendSegment(cubeManager.getCube(cubeDesc.getName()),
                new SegmentRange.TSRange(System.currentTimeMillis() - 100L, System.currentTimeMillis() + 100L));
        CubeJoinedFlatTableDesc flatDesc = new CubeJoinedFlatTableDesc(seg);
        JdbcHiveMRInput.JdbcMRBatchCubingInputSide inputSide = (JdbcHiveMRInput.JdbcMRBatchCubingInputSide) input
                .getBatchCubingInputSide(flatDesc);

        AbstractExecutable executable = new MockInputSide(flatDesc, inputSide).createSqoopToFlatHiveStep(""/tmp"",
                cubeDesc.getName());
        Assert.assertNotNull(executable);

        String cmd = executable.getParam(""cmd"");
        Assert.assertTrue(cmd.contains(""org.h2.Driver""));
        Assert.assertTrue(cmd.contains(
                ""--boundary-query \""SELECT MIN(\\\""TEST_CATEGORY_GROUPINGS\\\"".\\\""META_CATEG_NAME\\\""), MAX(\\\""TEST_CATEGORY_GROUPINGS\\\"".\\\""META_CATEG_NAME\\\"")"" + System.lineSeparator()
                        + ""FROM \\\""DEFAULT\\\"".\\\""TEST_CATEGORY_GROUPINGS\\\"" AS \\\""TEST_CATEGORY_GROUPINGS\\\""\""""));

        source.close();
    }
",non-flaky,5
91554,apache_kylin,JdbcSourceTest.testBasics,"    @Test
    public void testBasics() throws IOException {
        ISource source = SourceManager.getSource(new JdbcSourceAware());
        ISourceMetadataExplorer explorer = source.getSourceMetadataExplorer();
        ISampleDataDeployer deployer = source.getSampleDataDeployer();

        Assert.assertTrue(source instanceof JdbcSource);
        Assert.assertTrue(explorer instanceof JdbcExplorer);
        Assert.assertTrue(deployer instanceof JdbcExplorer);

        IMRInput input = source.adaptToBuildEngine(IMRInput.class);
        Assert.assertNotNull(input);

        Class adaptTo = Object.class;
        expectedCannotAdaptEx.expect(RuntimeException.class);
        expectedCannotAdaptEx.expectMessage(""Cannot adapt to "" + adaptTo);
        source.adaptToBuildEngine(adaptTo);

        TableMetadataManager tblManager = TableMetadataManager.getInstance(getTestConfig());
        IReadableTable table = source.createReadableTable(tblManager.getTableDesc(""test_kylin_fact"", ""default""), null);
        Assert.assertTrue(table instanceof JdbcTable);

        source.close();
    }
",non-flaky,5
91555,apache_kylin,JdbcHiveInputBaseTest.testFetchValue,"    @Test
    public void testFetchValue() {
        Map<String, String> map = new HashMap<>();
        String guess = JdbcHiveInputBase.fetchValue(""DB_1"", ""TB_2"", ""COL_3"", map);

        // not found, return input value
        assertEquals(""DB_1.TB_2.COL_3"", guess);
        map.put(""DB_1.TB_2.COL_3"", ""Db_1.Tb_2.Col_3"");

        guess = JdbcHiveInputBase.fetchValue(""DB_1"", ""TB_2"", ""COL_3"", map);
        // found, return cached value
        assertEquals(""Db_1.Tb_2.Col_3"", guess);
    }
",non-flaky,5
91556,apache_kylin,JdbcHiveInputBaseTest.testQuoteIdentifier,"    @Test
    public void testQuoteIdentifier() {
        String guess = JdbcHiveInputBase.quoteIdentifier(""Tbl1.Col1"", SourceDialect.MYSQL);
        assertEquals(""`Tbl1`.`Col1`"", guess);
        guess = JdbcHiveInputBase.quoteIdentifier(""Tbl1.Col1"", SourceDialect.SQL_SERVER);
        assertEquals(""[Tbl1].[Col1]"", guess);
    }
",non-flaky,5
91557,apache_kylin,ResourceToolTest.testCopy,"    @Test
    public void testCopy() throws IOException {
        KylinConfig dstConfig = KylinConfig.createInstanceFromUri(dstPath);
        ResourceStore srcStore = ResourceStore.getStore(KylinConfig.getInstanceFromEnv());
        ResourceStore dstStore = ResourceStore.getStore(dstConfig);

        //metadata under source path and destination path are not equal before copy
        Assert.assertNotEquals(srcStore.listResources(""/""), dstStore.listResources(""/""));

        new ResourceTool().copy(KylinConfig.getInstanceFromEnv(), dstConfig, ""/"");

        //After copy, two paths have same metadata
        NavigableSet<String> dstFiles = dstStore.listResourcesRecursively(""/"");
        NavigableSet<String> srcFiles = srcStore.listResourcesRecursively(""/"");
        Assert.assertTrue(srcFiles.containsAll(EXEC_FILES));
        Assert.assertFalse(dstFiles.containsAll(EXEC_FILES));
        srcFiles.removeAll(EXEC_FILES);
        Assert.assertEquals(srcFiles, dstFiles);
    }
",non-flaky,5
91558,apache_kylin,HDFSResourceStoreTest.testListResourcesImpl,"    @Test
    public void testListResourcesImpl() throws Exception {
        String path = ""../examples/test_metadata/"";
        String cp = new File(path).getCanonicalFile().getPath();
        FileSystem fs = HadoopUtil.getFileSystem(cp);
        HDFSResourceStore store = new HDFSResourceStore(KylinConfig.getInstanceFromEnv(),
                StorageURL.valueOf(""hdfs@hdfs""));
        Field field = store.getClass().getDeclaredField(""fs"");
        field.setAccessible(true);
        field.set(store, fs);

        File f1 = new File(cp + ""/resource/resource/e1.json"");
        File f2 = new File(cp + ""/resource/resource/e2.json"");
        if (!f1.getParentFile().exists()) {
            if (!f1.getParentFile().mkdirs()) {
                throw new RuntimeException(""Can not create dir."");
            }
        }
        if (!(f1.createNewFile() && f2.createNewFile())) {
            throw new RuntimeException(""Can not create file."");
        }

        Path p = new Path(cp);
        TreeSet<String> resources = store.getAllFilePath(new Path(p, ""resource""), ""/resource/"");
        TreeSet<String> expected = new TreeSet<>();
        expected.add(""/resource/resource/e1.json"");
        expected.add(""/resource/resource/e2.json"");
        Assert.assertEquals(expected, resources);
    }
",non-flaky,5
91559,apache_kylin,AutoDeleteDirectoryTest.testBasic,"    @Test
    public void testBasic() throws IOException {
        File tempFile = null;
        try (AutoDeleteDirectory autoTempFile = new AutoDeleteDirectory(""test"", """")) {
            Assert.assertTrue(autoTempFile.getFile().isDirectory());
            Assert.assertEquals(0, autoTempFile.getFile().listFiles().length);
            tempFile = autoTempFile.getFile();
        }
        Assert.assertTrue(!tempFile.exists());
    }
",non-flaky,5
91560,apache_kylin,LocalFileResourceStoreTest.testFileStore,"    @Test
    public void testFileStore() throws Exception {
        KylinConfig config = KylinConfig.getInstanceFromEnv();
        ResourceStoreTest.testAStore(config.getMetadataUrl().toString(), config);
    }
",non-flaky,5
91561,apache_kylin,LocalFileResourceStoreTest.testRollback,"    @Test
    public void testRollback() throws Exception {
        ResourceStore store = ResourceStore.getStore(KylinConfig.getInstanceFromEnv());
        byte[] bytes = new byte[] { 0, 1, 2 };
        RawResource raw;
        Checkpoint cp;

        cp = store.checkpoint();
        try {
            store.putResource(""/res1"", new StringEntity(""data1""), 1000, StringEntity.serializer);
        } finally {
            cp.close();
        }
        StringEntity str = store.getResource(""/res1"", StringEntity.serializer);
        assertEquals(""data1"", str.toString());

        cp = store.checkpoint();
        try {
            ByteArrayInputStream is = new ByteArrayInputStream(bytes);
            store.putResource(""/res2"", is, 2000);
            is.close();
            
            store.putResource(""/res1"", str, 2000, StringEntity.serializer);
            store.deleteResource(""/res1"");

            assertEquals(null, store.getResource(""/res1""));
            assertEquals(2000, (raw = store.getResource(""/res2"")).lastModified());
            raw.content().close();
            
            cp.rollback();
            
            assertEquals(null, store.getResource(""/res2""));
            assertEquals(1000, (raw = store.getResource(""/res1"")).lastModified());
            raw.content().close();
        } finally {
            cp.close();
        }
    }
",non-flaky,5
91562,apache_kylin,KylinVersionTest.testNormal,"    @Test
    public void testNormal() {
        KylinVersion ver1 = new KylinVersion(""2.1.0"");
        Assert.assertEquals(2, ver1.major);
        Assert.assertEquals(1, ver1.minor);
        Assert.assertEquals(0, ver1.revision);
    }
",non-flaky,5
91563,apache_kylin,KylinVersionTest.testNoRevision,"    @Test
    public void testNoRevision() {
        KylinVersion ver1 = new KylinVersion(""2.1"");
        Assert.assertEquals(2, ver1.major);
        Assert.assertEquals(1, ver1.minor);
        Assert.assertEquals(0, ver1.revision);
    }
",non-flaky,5
91564,apache_kylin,KylinVersionTest.testToString,"    @Test
    public void testToString() {
        KylinVersion ver1 = new KylinVersion(""2.1.7.321"");
        Assert.assertEquals(2, ver1.major);
        Assert.assertEquals(1, ver1.minor);
        Assert.assertEquals(7, ver1.revision);
        Assert.assertEquals(321, ver1.internal);
        Assert.assertEquals(""2.1.7.321"", ver1.toString());
    }
",non-flaky,5
91565,apache_kylin,KylinVersionTest.testCompare,"    @Test
    public void testCompare() {
        Assert.assertEquals(true, KylinVersion.isBefore200(""1.9.9""));
        Assert.assertEquals(false, KylinVersion.isBefore200(""2.0.0""));
        Assert.assertEquals(true, new KylinVersion(""2.1.0"").compareTo(new KylinVersion(""2.1.0.123"")) < 0);
    }
",non-flaky,5
91566,apache_kylin,RestClientTest.basicTests,"    @Test
    public void basicTests() throws IOException {
        RestClient a = new RestClient(""prod01:80"");
        //a.wipeCache(""metadata"", ""a"", ""a"");
        //String aa = a.getKylinProperties();
        //System.out.println(aa);
        RestClient b = new RestClient(""sandbox.hortonworks.com:7070"");
        //b.wipeCache(""metadata"", ""a"", ""a"");
        //String bb = b.getKylinProperties();
        //System.out.println(bb);

    }
",non-flaky,5
91567,apache_kylin,RandomUtilTest.testRandomUUID,"    @Test
    public void testRandomUUID() {
        Assert.assertEquals(RandomUtil.randomUUID().toString().length(), UUID.randomUUID().toString().length());
        Assert.assertNotEquals(RandomUtil.randomUUID().toString(), RandomUtil.randomUUID().toString());
    }
",non-flaky,5
91568,apache_kylin,MailServiceTest.testSendEmail,"    @Test
    public void testSendEmail() throws IOException {

        KylinConfig config = KylinConfig.getInstanceFromEnv();

        MailService mailservice = new MailService(config);
        boolean sent = sendTestEmail(mailservice);
        assert sent;

        System.setProperty(""kylin.job.notification-enabled"", ""false"");
        // set kylin.job.notification-enabled=false, and run again, this time should be no mail delivered
        mailservice = new MailService(config);
        sent = sendTestEmail(mailservice);
        assert !sent;

    }
",non-flaky,5
91569,apache_kylin,IdentityUtilTest.basicTest,"    @Test
    public void basicTest() {
        String s1 = new String(""hi"");
        String s2 = new String(""hi"");

        List<String> c1 = Lists.newArrayList(s1);
        List<String> c2 = Lists.newArrayList(s2);
        List<String> c3 = Lists.newArrayList(s2);

        Assert.assertFalse(IdentityUtils.collectionReferenceEquals(c1, c2));
        Assert.assertTrue(IdentityUtils.collectionReferenceEquals(c3, c2));
    }
",non-flaky,5
91570,apache_kylin,DateFormatTest.testIsSupportedDateFormat,"    @Test
    public void testIsSupportedDateFormat() {
        assertTrue(DateFormat.isSupportedDateFormat(""2010-01-01""));
        assertTrue(DateFormat.isSupportedDateFormat(""20100101""));
        assertTrue(DateFormat.isSupportedDateFormat(""2010-01-01 01:01:01""));
        assertTrue(DateFormat.isSupportedDateFormat(""2010-01-01 01:00:00.000""));

        assertFalse(DateFormat.isSupportedDateFormat(""2010-01""));
        assertFalse(DateFormat.isSupportedDateFormat(""2010/01/01""));
        assertFalse(DateFormat.isSupportedDateFormat(""2010-1-1""));
        assertFalse(DateFormat.isSupportedDateFormat(""abc""));
    }
",non-flaky,5
91571,apache_kylin,StringSplitterTest.testSplitReturningNonEmptyArray,"  @Test
  public void testSplitReturningNonEmptyArray() {
      String[] stringArray = StringSplitter.split(""Fc8!v~f?aQL"", ""Fc8!v~f?aQL"");

      assertEquals(2, stringArray.length);
      assertEquals("""", stringArray[0]);
      assertEquals("""", stringArray[1]);
  }
",non-flaky,5
91572,apache_kylin,StringSplitterTest.testSplitWithNonEmptyString,"  @Test
  public void testSplitWithNonEmptyString() {
      String[] stringArray = StringSplitter.split(""]sZ}gR\""cws,8p#|m"", ""Fc8!v~f?aQL"");
",non-flaky,5
91573,apache_kylin,ClassUtilTest.testFindContainingJar,"    @Test
    public void testFindContainingJar() throws ClassNotFoundException {
        Assert.assertTrue(ClassUtil.findContainingJar(Class.forName(""org.apache.commons.beanutils.BeanUtils"")).contains(""commons-beanutils""));
        Assert.assertTrue(ClassUtil.findContainingJar(Class.forName(""org.apache.commons.beanutils.BeanUtils""), ""core"").contains(""commons-beanutils-core""));
    }
",non-flaky,5
91574,apache_kylin,EncryptUtilTest.testAESEncrypt,"    @Test
    public void testAESEncrypt(){
        String input = ""hello world"";
        String result = EncryptUtil.encrypt(input);
        Assert.assertEquals(""4stv/RRleOtvie/8SLHmXA=="", result);
    }
",non-flaky,5
91575,apache_kylin,CaseInsensitiveStringCollectionTest.testCaseInsensitiveMap,"    @Test
    public void testCaseInsensitiveMap() {
        CaseInsensitiveStringMap<String> m1 = new CaseInsensitiveStringMap<>();
        m1.put(""a"", ""a"");
        Map<String, String> m2 = new HashMap<>();
        m2.put(""a"", ""a"");
        Assert.assertEquals(m2, m1);
        Assert.assertTrue(m1.containsKey(""A""));
        Assert.assertFalse(m1.containsValue(""A""));
    }
",non-flaky,5
91576,apache_kylin,CaseInsensitiveStringCollectionTest.testCaseInsensitiveSet,"    @Test
    public void testCaseInsensitiveSet() {
        CaseInsensitiveStringSet s1 = new CaseInsensitiveStringSet();
        s1.add(""a"");
        Set<String> s2 = new HashSet<>();
        s2.add(""a"");
        Assert.assertEquals(s2, s1);
        Assert.assertTrue(s1.contains(""A""));
    }
",non-flaky,5
91577,apache_kylin,JacksonTest.foo,"    @Test
    public void foo() throws IOException {
        HashMap a = new HashMap<String, String>();
        a.put(""1"", ""1"");
        a.put(""3"", ""3"");
        a.put(""2"", ""2"");


        JacksonBean bean = new JacksonBean();
        bean.setA(""valuea"");
        bean.setConfiguration(a);

        String s = JsonUtil.writeValueAsString(bean);
        System.out.println(s);

        JacksonBean desBean = (JacksonBean) JsonUtil.readValue(""{\""a\"":\""valuea\"",\""b\"":0,\""configuration\"":{\""2\"":\""2\"",\""3\"":\""3\"",\""1\"":\""1\""}}"", JacksonBean.class);
        
        String x2 = JsonUtil.writeValueAsString(desBean);
        System.out.println(x2);
        
        System.out.println(desBean);
    }
",non-flaky,5
91578,apache_kylin,BasicTest.testxx,"    @Test
    public void testxx() throws InterruptedException {
        System.out.println(
                ""((?<![\\p{L}_0-9\\.\\\""])(\\\""[\\p{L}_0-9]+\\\""\\.)?(\\\""[\\p{L}_0-9]+\\\"")(?![\\p{L}_0-9\\.\\\""]))"");
        System.out.println(0x8fL);
        byte[] space = new byte[100];
        ByteBuffer buffer = ByteBuffer.wrap(space, 10, 20);
        buffer.put((byte) 1);
    }
",non-flaky,5
91579,apache_kylin,BasicTest.testyy,"    @Test
    public void testyy() throws InterruptedException {
        long wallClock = System.currentTimeMillis();

        HashMap<Integer, byte[]> map = Maps.newHashMap();
        for (int i = 0; i < 10000000; i++) {
            byte[] a = new byte[100];
            map.put(i, a);
        }

        System.out.println(""Time Consumed: "" + (System.currentTimeMillis() - wallClock));
    }
",non-flaky,5
91580,apache_kylin,BasicTest.run,"    @Test
    public void test0() throws Exception {

        ExecutorService executorService = Executors.newCachedThreadPool();
        List<Future<?>> futures = Lists.newArrayList();

        futures.add(executorService.submit(new Runnable() {
            @Override
            public void run() {
                throw new RuntimeException(""hi"");
            }
",non-flaky,5
91581,apache_kylin,BasicTest.test1,"    @Test
    public void test1() throws Exception {

        System.out.println(org.apache.kylin.common.util.DateFormat.formatToTimeStr(1433833611000L));
        System.out.println(org.apache.kylin.common.util.DateFormat.formatToTimeStr(1433250517000L));
        System.out.println(org.apache.kylin.common.util.DateFormat.stringToMillis(""2015-06-01 00:00:00""));
        System.out.println(org.apache.kylin.common.util.DateFormat.stringToMillis(""2015-05-15 17:00:00""));
        Assert.assertEquals(1568960682251L,
                org.apache.kylin.common.util.DateFormat.stringToMillis(""2019-09-20T14:24:42.251+08:00""));

        String bb = ""\\x00\\x00\\x00\\x00\\x01\\x3F\\xD0\\x2D\\58\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00"";//2013/07/12 07:59:37
        String cc = ""\\x00\\x00\\x00\\x00\\x01\\x41\\xBE\\x8F\\xD8\\x00\\x00\\x00\\x00\\x00\\x00\\x00"";//2013/10/16 08:00:00
        String dd = ""\\x00\\x00\\x00\\x00\\x01\\x41\\xBE\\x8F\\xD8\\x07\\x00\\x18\\x00\\x00\\x00"";

        byte[] bytes = BytesUtil.fromReadableText(dd);
        long ttt = BytesUtil.readLong(bytes, 2, 8);
        System.out.println(time(ttt));

        System.out.println(""\\"");
        System.out.println(""n"");

        System.out.println(""The start key is set to "" + null);
        long current = System.currentTimeMillis();
        System.out.println(time(current));

        Calendar a = Calendar.getInstance(TimeZone.getDefault(), Locale.ROOT);
        Calendar b = Calendar.getInstance(TimeZone.getDefault(), Locale.ROOT);
        Calendar c = Calendar.getInstance(TimeZone.getDefault(), Locale.ROOT);
        b.clear();
        c.clear();

        System.out.println(time(b.getTimeInMillis()));
        System.out.println(time(c.getTimeInMillis()));

        a.setTimeInMillis(current);
        b.set(a.get(Calendar.YEAR), a.get(Calendar.MONTH), a.get(Calendar.DAY_OF_MONTH), a.get(Calendar.HOUR_OF_DAY),
                a.get(Calendar.MINUTE));
        c.set(a.get(Calendar.YEAR), a.get(Calendar.MONTH), a.get(Calendar.DAY_OF_MONTH), a.get(Calendar.HOUR_OF_DAY),
                0);

        System.out.println(time(b.getTimeInMillis()));
        System.out.println(time(c.getTimeInMillis()));

    }
",non-flaky,5
91582,apache_kylin,BasicTest.test3,"    @Test
    public void test3() throws Exception {
        FastDateFormat formatter = org.apache.kylin.common.util.DateFormat.getDateFormat(""MM dd, yyyy hh:mm:ss a"");

        String timeStr = ""07 20, 2016 09:59:17 AM"";

        System.out.println(formatter.parse(timeStr).getTime());
    }
",non-flaky,5
91583,apache_kylin,BasicTest.testStringSplit,"    @Test
    public void testStringSplit() throws Exception {

        String[] origin = new String[] { ""ab,c"", ""cd|e"" };

        // test with sequence file default delimiter
        String delimiter = ""\01""; //""\u001F""; ""\t"";
        String concated = StringUtils.join(Arrays.asList(origin), delimiter);
        System.out.println(concated);

        String[] newValues = concated.split(delimiter);
        Assert.assertEquals(origin, newValues);

        newValues = concated.split(""\\"" + delimiter);
        Assert.assertEquals(origin, newValues);
    }
",non-flaky,5
91584,apache_kylin,RandomSamplerTest.test,"    @Test
    public void test() {
        RandomSampler<String> s = new RandomSampler<String>();
        List<String> data = new ArrayList<String>();
        for (int i = 0; i < 1000; i++) {
            data.add(String.valueOf(i));
        }

        List<String> result = s.sample(data, 50);
        System.out.println(result);
        assertEquals(50, result.size());
    }
",non-flaky,5
91585,apache_kylin,BytesUtilTest.test,"    @Test
    public void test() {
        ByteBuffer buffer = ByteBuffer.allocate(10000);
        int[] x = new int[] { 1, 2, 3 };
        BytesUtil.writeIntArray(x, buffer);
        buffer.flip();

        byte[] buf = new byte[buffer.limit()];
        System.arraycopy(buffer.array(), 0, buf, 0, buffer.limit());

        ByteBuffer newBuffer = ByteBuffer.wrap(buf);
        int[] y = BytesUtil.readIntArray(newBuffer);
        assertEquals(y[2], 3);
    }
",non-flaky,5
91586,apache_kylin,BytesUtilTest.testBooleanArray,"    @Test
    public void testBooleanArray() {
        ByteBuffer buffer = ByteBuffer.allocate(10000);
        boolean[] x = new boolean[] { true, false, true };
        BytesUtil.writeBooleanArray(x, buffer);
        buffer.flip();
        boolean[] y = BytesUtil.readBooleanArray(buffer);
        assertEquals(y[2], true);
        assertEquals(y[1], false);
    }
",non-flaky,5
91587,apache_kylin,BytesUtilTest.testWriteReadUnsignedInt,"    @Test
    public void testWriteReadUnsignedInt() {
        testWriteReadUnsignedInt(735033, 3);
        testWriteReadUnsignedInt(73503300, 4);
    }
",non-flaky,5
91588,apache_kylin,BytesUtilTest.testReadable,"    @Test
    public void testReadable() {
        String x = ""\\x00\\x00\\x00\\x00\\x00\\x01\\xFC\\xA8"";
        byte[] bytes = BytesUtil.fromReadableText(x);
        String y = BytesUtil.toHex(bytes);
        assertEquals(x, y);
    }
",non-flaky,5
91589,apache_kylin,PartialSorterTest.compare,"    @Test
    public void basicTest() {
        List<Integer> a = Lists.newArrayList();
        a.add(100);
        a.add(2);
        a.add(92);
        a.add(1);
        a.add(0);
        PartialSorter.partialSort(a, Lists.newArrayList(1, 3, 4), new Comparator<Integer>() {
            @Override
            public int compare(Integer o1, Integer o2) {
                return o1.compareTo(o2);
            }
",non-flaky,5
91590,apache_kylin,SourceConfigurationUtilTest.testHiveConf,"    @Test
    public void testHiveConf() {
        Properties properties = SourceConfigurationUtil.loadHiveJDBCProperties();
        assertTrue(properties.containsKey(""hiveconf:hive.auto.convert.join.noconditionaltask.size""));
    }
",non-flaky,5
91591,apache_kylin,SourceConfigurationUtilTest.testSqoopConf,"    @Test
    public void testSqoopConf() {
        Map<String, String> configMap = SourceConfigurationUtil.loadSqoopConfiguration();
        assertFalse(configMap.isEmpty());
        assertEquals(""1"", configMap.get(""dfs.replication""));
    }
",non-flaky,5
91592,apache_kylin,HiveCmdBuilderTest.testHiveCLI,"    @Test
    public void testHiveCLI() {
        System.setProperty(""kylin.source.hive.client"", ""cli"");

        Map<String, String> hiveProps = new HashMap<>();
        hiveProps.put(""hive.execution.engine"", ""mr"");
        Map<String, String> hivePropsOverwrite = new HashMap<>();
        hivePropsOverwrite.put(""hive.execution.engine"", ""tez"");
        HiveCmdBuilder hiveCmdBuilder = new HiveCmdBuilder(""test HiveCLI"");
        hiveCmdBuilder.addStatement(""USE default;"");
        hiveCmdBuilder.addStatement(""DROP TABLE `test`;"");
        hiveCmdBuilder.addStatement(""SHOW\n TABLES;"");
        hiveCmdBuilder.setHiveConfProps(hiveProps);
        hiveCmdBuilder.overwriteHiveProps(hivePropsOverwrite);
        assertEquals(
                ""hive -e \""set mapred.job.name='test HiveCLI';\nUSE default;\nDROP TABLE \\`test\\`;\nSHOW\n TABLES;\n\"" --hiveconf hive.execution.engine=tez"",
                hiveCmdBuilder.build());
    }
",non-flaky,5
91593,apache_kylin,HiveCmdBuilderTest.testBeeline,"    @Test
    public void testBeeline() throws IOException {
        String lineSeparator = java.security.AccessController
                .doPrivileged(new sun.security.action.GetPropertyAction(""line.separator""));
        System.setProperty(""kylin.source.hive.client"", ""beeline"");
        System.setProperty(""kylin.source.hive.beeline-shell"", ""/spark-client/bin/beeline"");
        System.setProperty(""kylin.source.hive.beeline-params"", ""-u jdbc_url"");

        HiveCmdBuilder hiveCmdBuilder = new HiveCmdBuilder();
        hiveCmdBuilder.addStatement(""USE default;"");
        hiveCmdBuilder.addStatement(""DROP TABLE `test`;"");
        hiveCmdBuilder.addStatement(""SHOW TABLES;"");

        String cmd = hiveCmdBuilder.build();
        String hqlFile = cmd.substring(cmd.lastIndexOf(""-f "") + 3).trim();
        hqlFile = hqlFile.substring(0, hqlFile.length() - "";exit $ret_code"".length());
        String createFileCmd = cmd.substring(0, cmd.indexOf(""EOL\n"", cmd.indexOf(""EOL\n"") + 1) + 3);
        CliCommandExecutor cliCommandExecutor = new CliCommandExecutor();
        Pair<Integer, String> execute = cliCommandExecutor.execute(createFileCmd);
        String hqlStatement = FileUtils.readFileToString(new File(hqlFile), Charset.defaultCharset());
        assertEquals(
                ""USE default;"" + lineSeparator + ""DROP TABLE `test`;"" + lineSeparator + ""SHOW TABLES;"" + lineSeparator,
                hqlStatement);
        assertBeelineCmd(cmd);
        FileUtils.forceDelete(new File(hqlFile));
    }
",non-flaky,5
92624,apache_dubbo,GenericServiceTest.testGeneric,"    @Test
    public void testGeneric() {
        DemoService server = new DemoServiceImpl();
        ProxyFactory proxyFactory = ExtensionLoader.getExtensionLoader(ProxyFactory.class).getAdaptiveExtension();
        Protocol protocol = ExtensionLoader.getExtensionLoader(Protocol.class).getAdaptiveExtension();
        URL url = URL.valueOf(""dubbo://127.0.0.1:5342/"" + DemoService.class.getName() + ""?version=1.0.0"");
        Exporter<DemoService> exporter = protocol.export(proxyFactory.getInvoker(server, DemoService.class, url));
        Invoker<DemoService> invoker = protocol.refer(DemoService.class, url);

        GenericService client = (GenericService) proxyFactory.getProxy(invoker, true);
        Object result = client.$invoke(""sayHello"", new String[]{""java.lang.String""}, new Object[]{""haha""});
        Assert.assertEquals(""hello haha"", result);

        org.apache.dubbo.rpc.service.GenericService newClient = (org.apache.dubbo.rpc.service.GenericService) proxyFactory.getProxy(invoker, true);
        Object res = newClient.$invoke(""sayHello"", new String[]{""java.lang.String""}, new Object[]{""hehe""});
        Assert.assertEquals(""hello hehe"", res);
        invoker.destroy();
        exporter.unexport();
    }
",non-flaky,5
92625,apache_dubbo,GenericServiceTest.testGeneric2,"    @Test
    public void testGeneric2() {
        DemoService server = new DemoServiceImpl();
        ProxyFactory proxyFactory = ExtensionLoader.getExtensionLoader(ProxyFactory.class).getAdaptiveExtension();
        Protocol protocol = ExtensionLoader.getExtensionLoader(Protocol.class).getAdaptiveExtension();
        URL url = URL.valueOf(""dubbo://127.0.0.1:5342/"" + DemoService.class.getName() + ""?version=1.0.0&generic=true"");
        Exporter<DemoService> exporter = protocol.export(proxyFactory.getInvoker(server, DemoService.class, url));
        Invoker<GenericService> invoker = protocol.refer(GenericService.class, url);

        GenericService client = proxyFactory.getProxy(invoker, true);
        Object result = client.$invoke(""sayHello"", new String[]{""java.lang.String""}, new Object[]{""haha""});
        Assert.assertEquals(""hello haha"", result);

        Invoker<DemoService> invoker2 = protocol.refer(DemoService.class, url);

        GenericService client2 = (GenericService) proxyFactory.getProxy(invoker2, true);
        Object result2 = client2.$invoke(""sayHello"", new String[]{""java.lang.String""}, new Object[]{""haha""});
        Assert.assertEquals(""hello haha"", result2);

        invoker.destroy();
        exporter.unexport();
    }
",non-flaky,5
92626,apache_dubbo,ApplicationConfigTest.testName,"    @Test
    public void testName() throws Exception {
        ApplicationConfig application = new ApplicationConfig();
        application.setName(""app"");
        assertThat(application.getName(), equalTo(""app""));
        application = new ApplicationConfig(""app2"");
        assertThat(application.getName(), equalTo(""app2""));
        Map<String, String> parameters = new HashMap<String, String>();
        ApplicationConfig.appendParameters(parameters, application);
        assertThat(parameters, hasEntry(Constants.APPLICATION_KEY, ""app2""));
    }
",non-flaky,5
92627,apache_dubbo,ApplicationConfigTest.testVersion,"    @Test
    public void testVersion() throws Exception {
        ApplicationConfig application = new ApplicationConfig(""app"");
        application.setVersion(""1.0.0"");
        assertThat(application.getVersion(), equalTo(""1.0.0""));
        Map<String, String> parameters = new HashMap<String, String>();
        ApplicationConfig.appendParameters(parameters, application);
        assertThat(parameters, hasEntry(""application.version"", ""1.0.0""));
    }
",non-flaky,5
92628,apache_dubbo,ApplicationConfigTest.testOwner,"    @Test
    public void testOwner() throws Exception {
        ApplicationConfig application = new ApplicationConfig(""app"");
        application.setOwner(""owner"");
        assertThat(application.getOwner(), equalTo(""owner""));
    }
",non-flaky,5
92629,apache_dubbo,ApplicationConfigTest.testOrganization,"    @Test
    public void testOrganization() throws Exception {
        ApplicationConfig application = new ApplicationConfig(""app"");
        application.setOrganization(""org"");
        assertThat(application.getOrganization(), equalTo(""org""));
    }
",non-flaky,5
92630,apache_dubbo,ApplicationConfigTest.testArchitecture,"    @Test
    public void testArchitecture() throws Exception {
        ApplicationConfig application = new ApplicationConfig(""app"");
        application.setArchitecture(""arch"");
        assertThat(application.getArchitecture(), equalTo(""arch""));
    }
",non-flaky,5
92631,apache_dubbo,ApplicationConfigTest.testEnvironment1,"    @Test
    public void testEnvironment1() throws Exception {
        ApplicationConfig application = new ApplicationConfig(""app"");
        application.setEnvironment(""develop"");
        assertThat(application.getEnvironment(), equalTo(""develop""));
        application.setEnvironment(""test"");
        assertThat(application.getEnvironment(), equalTo(""test""));
        application.setEnvironment(""product"");
        assertThat(application.getEnvironment(), equalTo(""product""));
    }
",non-flaky,5
92632,apache_dubbo,ApplicationConfigTest.testEnvironment2,"    @Test(expected = IllegalStateException.class)
    public void testEnvironment2() throws Exception {
        ApplicationConfig application = new ApplicationConfig(""app"");
        application.setEnvironment(""illegal-env"");
    }
",non-flaky,5
92633,apache_dubbo,ApplicationConfigTest.testRegistry,"    @Test
    public void testRegistry() throws Exception {
        ApplicationConfig application = new ApplicationConfig(""app"");
        RegistryConfig registry = new RegistryConfig();
        application.setRegistry(registry);
        assertThat(application.getRegistry(), sameInstance(registry));
        application.setRegistries(Collections.singletonList(registry));
        assertThat(application.getRegistries(), contains(registry));
        assertThat(application.getRegistries(), hasSize(1));
    }
",non-flaky,5
92634,apache_dubbo,ApplicationConfigTest.testMonitor,"    @Test
    public void testMonitor() throws Exception {
        ApplicationConfig application = new ApplicationConfig(""app"");
        application.setMonitor(new MonitorConfig(""monitor-addr""));
        assertThat(application.getMonitor().getAddress(), equalTo(""monitor-addr""));
        application.setMonitor(""monitor-addr"");
        assertThat(application.getMonitor().getAddress(), equalTo(""monitor-addr""));
    }
",non-flaky,5
92635,apache_dubbo,ApplicationConfigTest.testLogger,"    @Test
    public void testLogger() throws Exception {
        ApplicationConfig application = new ApplicationConfig(""app"");
        application.setLogger(""log4j"");
        assertThat(application.getLogger(), equalTo(""log4j""));
    }
",non-flaky,5
92636,apache_dubbo,ApplicationConfigTest.testDefault,"    @Test
    public void testDefault() throws Exception {
        ApplicationConfig application = new ApplicationConfig(""app"");
        application.setDefault(true);
        assertThat(application.isDefault(), is(true));
    }
",non-flaky,5
92637,apache_dubbo,ApplicationConfigTest.testDumpDirectory,"    @Test
    public void testDumpDirectory() throws Exception {
        ApplicationConfig application = new ApplicationConfig(""app"");
        application.setDumpDirectory(""/dump"");
        assertThat(application.getDumpDirectory(), equalTo(""/dump""));
        Map<String, String> parameters = new HashMap<String, String>();
        ApplicationConfig.appendParameters(parameters, application);
        assertThat(parameters, hasEntry(Constants.DUMP_DIRECTORY, ""/dump""));
    }
",non-flaky,5
92638,apache_dubbo,ApplicationConfigTest.testQosEnable,"    @Test
    public void testQosEnable() throws Exception {
        ApplicationConfig application = new ApplicationConfig(""app"");
        application.setQosEnable(true);
        assertThat(application.getQosEnable(), is(true));
        Map<String, String> parameters = new HashMap<String, String>();
        ApplicationConfig.appendParameters(parameters, application);
        assertThat(parameters, hasEntry(Constants.QOS_ENABLE, ""true""));
    }
",non-flaky,5
92639,apache_dubbo,ApplicationConfigTest.testQosPort,"    @Test
    public void testQosPort() throws Exception {
        ApplicationConfig application = new ApplicationConfig(""app"");
        application.setQosPort(8080);
        assertThat(application.getQosPort(), equalTo(8080));
    }
",non-flaky,5
92640,apache_dubbo,ApplicationConfigTest.testQosAcceptForeignIp,"    @Test
    public void testQosAcceptForeignIp() throws Exception {
        ApplicationConfig application = new ApplicationConfig(""app"");
        application.setQosAcceptForeignIp(true);
        assertThat(application.getQosAcceptForeignIp(), is(true));
        Map<String, String> parameters = new HashMap<String, String>();
        ApplicationConfig.appendParameters(parameters, application);
        assertThat(parameters, hasEntry(Constants.ACCEPT_FOREIGN_IP, ""true""));
    }
",non-flaky,5
92641,apache_dubbo,ApplicationConfigTest.testParameters,"    @Test
    public void testParameters() throws Exception {
        ApplicationConfig application = new ApplicationConfig(""app"");
        application.setQosAcceptForeignIp(true);
        Map<String, String> parameters = new HashMap<String, String>();
        parameters.put(""k1"", ""v1"");
        ApplicationConfig.appendParameters(parameters, application);
        assertThat(parameters, hasEntry(""k1"", ""v1""));
        assertThat(parameters, hasEntry(Constants.ACCEPT_FOREIGN_IP, ""true""));
    }
",non-flaky,5
92642,apache_dubbo,RegistryConfigTest.testProtocol,"    @Test
    public void testProtocol() throws Exception {
        RegistryConfig registry = new RegistryConfig();
        registry.setProtocol(""protocol"");
        assertThat(registry.getProtocol(), equalTo(registry.getProtocol()));
    }
",non-flaky,5
92643,apache_dubbo,RegistryConfigTest.testAddress,"    @Test
    public void testAddress() throws Exception {
        RegistryConfig registry = new RegistryConfig();
        registry.setAddress(""localhost"");
        assertThat(registry.getAddress(), equalTo(""localhost""));
        Map<String, String> parameters = new HashMap<String, String>();
        RegistryConfig.appendParameters(parameters, registry);
        assertThat(parameters, not(hasKey(""address"")));
    }
",non-flaky,5
92644,apache_dubbo,RegistryConfigTest.testUsername,"    @Test
    public void testUsername() throws Exception {
        RegistryConfig registry = new RegistryConfig();
        registry.setUsername(""username"");
        assertThat(registry.getUsername(), equalTo(""username""));
    }
",non-flaky,5
92645,apache_dubbo,RegistryConfigTest.testPassword,"    @Test
    public void testPassword() throws Exception {
        RegistryConfig registry = new RegistryConfig();
        registry.setPassword(""password"");
        assertThat(registry.getPassword(), equalTo(""password""));
    }
",non-flaky,5
92646,apache_dubbo,RegistryConfigTest.testWait,"    @Test
    public void testWait() throws Exception {
        RegistryConfig registry = new RegistryConfig();
        registry.setWait(10);
        assertThat(registry.getWait(), is(10));
        assertThat(System.getProperty(Constants.SHUTDOWN_WAIT_KEY), equalTo(""10""));
    }
",non-flaky,5
92647,apache_dubbo,RegistryConfigTest.testCheck,"    @Test
    public void testCheck() throws Exception {
        RegistryConfig registry = new RegistryConfig();
        registry.setCheck(true);
        assertThat(registry.isCheck(), is(true));
    }
",non-flaky,5
92648,apache_dubbo,RegistryConfigTest.testFile,"    @Test
    public void testFile() throws Exception {
        RegistryConfig registry = new RegistryConfig();
        registry.setFile(""file"");
        assertThat(registry.getFile(), equalTo(""file""));
    }
",non-flaky,5
92649,apache_dubbo,RegistryConfigTest.testTransporter,"    @Test
    public void testTransporter() throws Exception {
        RegistryConfig registry = new RegistryConfig();
        registry.setTransporter(""transporter"");
        assertThat(registry.getTransporter(), equalTo(""transporter""));
    }
",non-flaky,5
92650,apache_dubbo,RegistryConfigTest.testClient,"    @Test
    public void testClient() throws Exception {
        RegistryConfig registry = new RegistryConfig();
        registry.setClient(""client"");
        assertThat(registry.getClient(), equalTo(""client""));
    }
",non-flaky,5
92651,apache_dubbo,RegistryConfigTest.testTimeout,"    @Test
    public void testTimeout() throws Exception {
        RegistryConfig registry = new RegistryConfig();
        registry.setTimeout(10);
        assertThat(registry.getTimeout(), is(10));
    }
",non-flaky,5
92652,apache_dubbo,RegistryConfigTest.testSession,"    @Test
    public void testSession() throws Exception {
        RegistryConfig registry = new RegistryConfig();
        registry.setSession(10);
        assertThat(registry.getSession(), is(10));
    }
",non-flaky,5
92653,apache_dubbo,RegistryConfigTest.testDynamic,"    @Test
    public void testDynamic() throws Exception {
        RegistryConfig registry = new RegistryConfig();
        registry.setDynamic(true);
        assertThat(registry.isDynamic(), is(true));
    }
",non-flaky,5
92654,apache_dubbo,RegistryConfigTest.testRegister,"    @Test
    public void testRegister() throws Exception {
        RegistryConfig registry = new RegistryConfig();
        registry.setRegister(true);
        assertThat(registry.isRegister(), is(true));
    }
",non-flaky,5
92655,apache_dubbo,RegistryConfigTest.testSubscribe,"    @Test
    public void testSubscribe() throws Exception {
        RegistryConfig registry = new RegistryConfig();
        registry.setSubscribe(true);
        assertThat(registry.isSubscribe(), is(true));
    }
",non-flaky,5
92656,apache_dubbo,RegistryConfigTest.testCluster,"    @Test
    public void testCluster() throws Exception {
        RegistryConfig registry = new RegistryConfig();
        registry.setCluster(""cluster"");
        assertThat(registry.getCluster(), equalTo(""cluster""));
    }
",non-flaky,5
92657,apache_dubbo,RegistryConfigTest.testGroup,"    @Test
    public void testGroup() throws Exception {
        RegistryConfig registry = new RegistryConfig();
        registry.setGroup(""group"");
        assertThat(registry.getGroup(), equalTo(""group""));
    }
",non-flaky,5
92658,apache_dubbo,RegistryConfigTest.testVersion,"    @Test
    public void testVersion() throws Exception {
        RegistryConfig registry = new RegistryConfig();
        registry.setVersion(""1.0.0"");
        assertThat(registry.getVersion(), equalTo(""1.0.0""));
    }
",non-flaky,5
92659,apache_dubbo,RegistryConfigTest.testParameters,"    @Test
    public void testParameters() throws Exception {
        RegistryConfig registry = new RegistryConfig();
        registry.setParameters(Collections.singletonMap(""k1"", ""v1""));
        assertThat(registry.getParameters(), hasEntry(""k1"", ""v1""));
        Map<String, String> parameters = new HashMap<String, String>();
        RegistryConfig.appendParameters(parameters, registry);
        assertThat(parameters, hasEntry(""k1"", ""v1""));
    }
",non-flaky,5
92660,apache_dubbo,RegistryConfigTest.testDefault,"    @Test
    public void testDefault() throws Exception {
        RegistryConfig registry = new RegistryConfig();
        registry.setDefault(true);
        assertThat(registry.isDefault(), is(true));
    }
",non-flaky,5
92661,apache_dubbo,ConsumerConfigTest.testTimeout,"    @Test
    public void testTimeout() throws Exception {
        try {
            System.clearProperty(""sun.rmi.transport.tcp.responseTimeout"");
            ConsumerConfig consumer = new ConsumerConfig();
            consumer.setTimeout(10);
            assertThat(consumer.getTimeout(), is(10));
            assertThat(System.getProperty(""sun.rmi.transport.tcp.responseTimeout""), equalTo(""10""));
        } finally {
            System.clearProperty(""sun.rmi.transport.tcp.responseTimeout"");
        }
    }
",non-flaky,5
92662,apache_dubbo,ConsumerConfigTest.testDefault,"    @Test
    public void testDefault() throws Exception {
        ConsumerConfig consumer = new ConsumerConfig();
        consumer.setDefault(true);
        assertThat(consumer.isDefault(), is(true));
    }
",non-flaky,5
92663,apache_dubbo,ConsumerConfigTest.testClient,"    @Test
    public void testClient() throws Exception {
        ConsumerConfig consumer = new ConsumerConfig();
        consumer.setClient(""client"");
        assertThat(consumer.getClient(), equalTo(""client""));
    }
",non-flaky,5
92664,apache_dubbo,ModuleConfigTest.testName1,"    @Test(expected = IllegalStateException.class)
    public void testName1() throws Exception {
        ModuleConfig module = new ModuleConfig();
        Map<String, String> parameters = new HashMap<String, String>();
        ModuleConfig.appendParameters(parameters, module);
    }
",non-flaky,5
92665,apache_dubbo,ModuleConfigTest.testName2,"    @Test
    public void testName2() throws Exception {
        ModuleConfig module = new ModuleConfig();
        module.setName(""module-name"");
        assertThat(module.getName(), equalTo(""module-name""));
        assertThat(module.getId(), equalTo(""module-name""));
        Map<String, String> parameters = new HashMap<String, String>();
        ModuleConfig.appendParameters(parameters, module);
        assertThat(parameters, hasEntry(""module"", ""module-name""));
    }
",non-flaky,5
92666,apache_dubbo,ModuleConfigTest.testVersion,"    @Test
    public void testVersion() throws Exception {
        ModuleConfig module = new ModuleConfig();
        module.setName(""module-name"");
        module.setVersion(""1.0.0"");
        assertThat(module.getVersion(), equalTo(""1.0.0""));
        Map<String, String> parameters = new HashMap<String, String>();
        ModuleConfig.appendParameters(parameters, module);
        assertThat(parameters, hasEntry(""module.version"", ""1.0.0""));
    }
",non-flaky,5
92667,apache_dubbo,ModuleConfigTest.testOwner,"    @Test
    public void testOwner() throws Exception {
        ModuleConfig module = new ModuleConfig();
        module.setOwner(""owner"");
        assertThat(module.getOwner(), equalTo(""owner""));
    }
",non-flaky,5
92668,apache_dubbo,ModuleConfigTest.testOrganization,"    @Test
    public void testOrganization() throws Exception {
        ModuleConfig module = new ModuleConfig();
        module.setOrganization(""org"");
        assertThat(module.getOrganization(), equalTo(""org""));
    }
",non-flaky,5
92669,apache_dubbo,ModuleConfigTest.testRegistry,"    @Test
    public void testRegistry() throws Exception {
        ModuleConfig module = new ModuleConfig();
        RegistryConfig registry = new RegistryConfig();
        module.setRegistry(registry);
        assertThat(module.getRegistry(), sameInstance(registry));
    }
",non-flaky,5
92670,apache_dubbo,ModuleConfigTest.testRegistries,"    @Test
    public void testRegistries() throws Exception {
        ModuleConfig module = new ModuleConfig();
        RegistryConfig registry = new RegistryConfig();
        module.setRegistries(Collections.singletonList(registry));
        assertThat(module.getRegistries(), Matchers.<org.apache.dubbo.config.RegistryConfig>hasSize(1));
        assertThat(module.getRegistries(), contains(registry));
    }
",non-flaky,5
92671,apache_dubbo,ModuleConfigTest.testMonitor,"    @Test
    public void testMonitor() throws Exception {
        ModuleConfig module = new ModuleConfig();
        module.setMonitor(""monitor-addr1"");
        assertThat(module.getMonitor().getAddress(), equalTo(""monitor-addr1""));
        module.setMonitor(new MonitorConfig(""monitor-addr2""));
        assertThat(module.getMonitor().getAddress(), equalTo(""monitor-addr2""));
    }
",non-flaky,5
92672,apache_dubbo,ModuleConfigTest.testDefault,"    @Test
    public void testDefault() throws Exception {
        ModuleConfig module = new ModuleConfig();
        module.setDefault(true);
        assertThat(module.isDefault(), is(true));
    }
",non-flaky,5
92673,apache_dubbo,ArgumentConfigTest.testIndex,"    @Test
    public void testIndex() throws Exception {
        ArgumentConfig argument = new ArgumentConfig();
        argument.setIndex(1);
        assertThat(argument.getIndex(), is(1));
    }
",non-flaky,5
92674,apache_dubbo,ArgumentConfigTest.testType,"    @Test
    public void testType() throws Exception {
        ArgumentConfig argument = new ArgumentConfig();
        argument.setType(""int"");
        assertThat(argument.getType(), equalTo(""int""));
    }
",non-flaky,5
92675,apache_dubbo,ArgumentConfigTest.testCallback,"    @Test
    public void testCallback() throws Exception {
        ArgumentConfig argument = new ArgumentConfig();
        argument.setCallback(true);
        assertThat(argument.isCallback(), is(true));
    }
",non-flaky,5
92676,apache_dubbo,ArgumentConfigTest.testArguments,"    @Test
    public void testArguments() throws Exception {
        ArgumentConfig argument = new ArgumentConfig();
        argument.setIndex(1);
        argument.setType(""int"");
        argument.setCallback(true);
        Map<String, String> parameters = new HashMap<String, String>();
        AbstractServiceConfig.appendParameters(parameters, argument);
        assertThat(parameters, hasEntry(""callback"", ""true""));
        assertThat(parameters.size(), is(1));
    }
",non-flaky,5
92677,apache_dubbo,ProviderConfigTest.testProtocol,"    @Test
    public void testProtocol() throws Exception {
        ProviderConfig provider = new ProviderConfig();
        provider.setProtocol(""protocol"");
        assertThat(provider.getProtocol().getName(), equalTo(""protocol""));
    }
",non-flaky,5
92678,apache_dubbo,ProviderConfigTest.testDefault,"    @Test
    public void testDefault() throws Exception {
        ProviderConfig provider = new ProviderConfig();
        provider.setDefault(true);
        Map<String, String> parameters = new HashMap<String, String>();
        ProviderConfig.appendParameters(parameters, provider);
        assertThat(provider.isDefault(), is(true));
        assertThat(parameters, not(hasKey(""default"")));
    }
",non-flaky,5
92679,apache_dubbo,ProviderConfigTest.testHost,"    @Test
    public void testHost() throws Exception {
        ProviderConfig provider = new ProviderConfig();
        provider.setHost(""demo-host"");
        Map<String, String> parameters = new HashMap<String, String>();
        ProviderConfig.appendParameters(parameters, provider);
        assertThat(provider.getHost(), equalTo(""demo-host""));
        assertThat(parameters, not(hasKey(""host"")));
    }
",non-flaky,5
92680,apache_dubbo,ProviderConfigTest.testPort,"    @Test
    public void testPort() throws Exception {
        ProviderConfig provider = new ProviderConfig();
        provider.setPort(8080);
        Map<String, String> parameters = new HashMap<String, String>();
        ProviderConfig.appendParameters(parameters, provider);
        assertThat(provider.getPort(), is(8080));
        assertThat(parameters, not(hasKey(""port"")));
    }
",non-flaky,5
92681,apache_dubbo,ProviderConfigTest.testPath,"    @Test
    public void testPath() throws Exception {
        ProviderConfig provider = new ProviderConfig();
        provider.setPath(""/path"");
        Map<String, String> parameters = new HashMap<String, String>();
        ProviderConfig.appendParameters(parameters, provider);
        assertThat(provider.getPath(), equalTo(""/path""));
        assertThat(provider.getContextpath(), equalTo(""/path""));
        assertThat(parameters, not(hasKey(""path"")));
    }
",non-flaky,5
92682,apache_dubbo,ProviderConfigTest.testContextPath,"    @Test
    public void testContextPath() throws Exception {
        ProviderConfig provider = new ProviderConfig();
        provider.setContextpath(""/context-path"");
        Map<String, String> parameters = new HashMap<String, String>();
        ProviderConfig.appendParameters(parameters, provider);
        assertThat(provider.getContextpath(), equalTo(""/context-path""));
        assertThat(parameters, not(hasKey(""/context-path"")));
    }
",non-flaky,5
92683,apache_dubbo,ProviderConfigTest.testThreads,"    @Test
    public void testThreads() throws Exception {
        ProviderConfig provider = new ProviderConfig();
        provider.setThreads(10);
        assertThat(provider.getThreads(), is(10));
    }
",non-flaky,5
92684,apache_dubbo,ProviderConfigTest.testIothreads,"    @Test
    public void testIothreads() throws Exception {
        ProviderConfig provider = new ProviderConfig();
        provider.setIothreads(10);
        assertThat(provider.getIothreads(), is(10));
    }
",non-flaky,5
92685,apache_dubbo,ProviderConfigTest.testQueues,"    @Test
    public void testQueues() throws Exception {
        ProviderConfig provider = new ProviderConfig();
        provider.setQueues(10);
        assertThat(provider.getQueues(), is(10));
    }
",non-flaky,5
92686,apache_dubbo,ProviderConfigTest.testAccepts,"    @Test
    public void testAccepts() throws Exception {
        ProviderConfig provider = new ProviderConfig();
        provider.setAccepts(10);
        assertThat(provider.getAccepts(), is(10));
    }
",non-flaky,5
92687,apache_dubbo,ProviderConfigTest.testCharset,"    @Test
    public void testCharset() throws Exception {
        ProviderConfig provider = new ProviderConfig();
        provider.setCharset(""utf-8"");
        assertThat(provider.getCharset(), equalTo(""utf-8""));
    }
",non-flaky,5
92688,apache_dubbo,ProviderConfigTest.testPayload,"    @Test
    public void testPayload() throws Exception {
        ProviderConfig provider = new ProviderConfig();
        provider.setPayload(10);
        assertThat(provider.getPayload(), is(10));
    }
",non-flaky,5
92689,apache_dubbo,ProviderConfigTest.testBuffer,"    @Test
    public void testBuffer() throws Exception {
        ProviderConfig provider = new ProviderConfig();
        provider.setBuffer(10);
        assertThat(provider.getBuffer(), is(10));
    }
",non-flaky,5
92690,apache_dubbo,ProviderConfigTest.testServer,"    @Test
    public void testServer() throws Exception {
        ProviderConfig provider = new ProviderConfig();
        provider.setServer(""demo-server"");
        assertThat(provider.getServer(), equalTo(""demo-server""));
    }
",non-flaky,5
92691,apache_dubbo,ProviderConfigTest.testClient,"    @Test
    public void testClient() throws Exception {
        ProviderConfig provider = new ProviderConfig();
        provider.setClient(""client"");
        assertThat(provider.getClient(), equalTo(""client""));
    }
",non-flaky,5
92692,apache_dubbo,ProviderConfigTest.testPrompt,"    @Test
    public void testPrompt() throws Exception {
        ProviderConfig provider = new ProviderConfig();
        provider.setPrompt(""#"");
        Map<String, String> parameters = new HashMap<String, String>();
        ProviderConfig.appendParameters(parameters, provider);
        assertThat(provider.getPrompt(), equalTo(""#""));
        assertThat(parameters, hasEntry(""prompt"", ""%23""));
    }
",non-flaky,5
92693,apache_dubbo,ProviderConfigTest.testDispatcher,"    @Test
    public void testDispatcher() throws Exception {
        ProviderConfig provider = new ProviderConfig();
        provider.setDispatcher(""mockdispatcher"");
        assertThat(provider.getDispatcher(), equalTo(""mockdispatcher""));
    }
",non-flaky,5
92694,apache_dubbo,ProviderConfigTest.testNetworker,"    @Test
    public void testNetworker() throws Exception {
        ProviderConfig provider = new ProviderConfig();
        provider.setNetworker(""networker"");
        assertThat(provider.getNetworker(), equalTo(""networker""));
    }
",non-flaky,5
92695,apache_dubbo,ProviderConfigTest.testWait,"    @Test
    public void testWait() throws Exception {
        ProviderConfig provider = new ProviderConfig();
        provider.setWait(10);
        assertThat(provider.getWait(), equalTo(10));
    }
",non-flaky,5
92696,apache_dubbo,MethodConfigTest.testName,"    @Test
    public void testName() throws Exception {
        MethodConfig method = new MethodConfig();
        method.setName(""hello"");
        assertThat(method.getName(), equalTo(""hello""));
        Map<String, String> parameters = new HashMap<String, String>();
        MethodConfig.appendParameters(parameters, method);
        assertThat(parameters, not(hasKey(""name"")));
    }
",non-flaky,5
92697,apache_dubbo,MethodConfigTest.testStat,"    @Test
    public void testStat() throws Exception {
        MethodConfig method = new MethodConfig();
        method.setStat(10);
        assertThat(method.getStat(), equalTo(10));
    }
",non-flaky,5
92698,apache_dubbo,MethodConfigTest.testRetry,"    @Test
    public void testRetry() throws Exception {
        MethodConfig method = new MethodConfig();
        method.setRetry(true);
        assertThat(method.isRetry(), is(true));
    }
",non-flaky,5
92699,apache_dubbo,MethodConfigTest.testReliable,"    @Test
    public void testReliable() throws Exception {
        MethodConfig method = new MethodConfig();
        method.setReliable(true);
        assertThat(method.isReliable(), is(true));
    }
",non-flaky,5
92700,apache_dubbo,MethodConfigTest.testExecutes,"    @Test
    public void testExecutes() throws Exception {
        MethodConfig method = new MethodConfig();
        method.setExecutes(10);
        assertThat(method.getExecutes(), equalTo(10));
    }
",non-flaky,5
92701,apache_dubbo,MethodConfigTest.testDeprecated,"    @Test
    public void testDeprecated() throws Exception {
        MethodConfig method = new MethodConfig();
        method.setDeprecated(true);
        assertThat(method.getDeprecated(), is(true));
    }
",non-flaky,5
92702,apache_dubbo,MethodConfigTest.testArguments,"    @Test
    public void testArguments() throws Exception {
        MethodConfig method = new MethodConfig();
        ArgumentConfig argument = new ArgumentConfig();
        method.setArguments(Collections.singletonList(argument));
        assertThat(method.getArguments(), contains(argument));
        assertThat(method.getArguments(), Matchers.<org.apache.dubbo.config.ArgumentConfig>hasSize(1));
    }
",non-flaky,5
92703,apache_dubbo,MethodConfigTest.testSticky,"    @Test
    public void testSticky() throws Exception {
        MethodConfig method = new MethodConfig();
        method.setSticky(true);
        assertThat(method.getSticky(), is(true));
    }
",non-flaky,5
92704,apache_dubbo,MethodConfigTest.testOnreturn,"    @Test
    public void testOnreturn() throws Exception {
        MethodConfig method = new MethodConfig();
        method.setOnreturn(""on-return-object"");
        assertThat(method.getOnreturn(), equalTo((Object) ""on-return-object""));
        Map<Object, Object> attribute = new HashMap<Object, Object>();
        MethodConfig.appendAttributes(attribute, method);
        assertThat(attribute, hasEntry((Object) Constants.ON_RETURN_INSTANCE_KEY, (Object) ""on-return-object""));
        Map<String, String> parameters = new HashMap<String, String>();
        MethodConfig.appendParameters(parameters, method);
        assertThat(parameters.size(), is(0));
    }
",non-flaky,5
92705,apache_dubbo,MethodConfigTest.testOnreturnMethod,"    @Test
    public void testOnreturnMethod() throws Exception {
        MethodConfig method = new MethodConfig();
        method.setOnreturnMethod(""on-return-method"");
        assertThat(method.getOnreturnMethod(), equalTo(""on-return-method""));
        Map<Object, Object> attribute = new HashMap<Object, Object>();
        MethodConfig.appendAttributes(attribute, method);
        assertThat(attribute, hasEntry((Object) Constants.ON_RETURN_METHOD_KEY, (Object) ""on-return-method""));
        Map<String, String> parameters = new HashMap<String, String>();
        MethodConfig.appendParameters(parameters, method);
        assertThat(parameters.size(), is(0));
    }
",non-flaky,5
92706,apache_dubbo,MethodConfigTest.testOnthrow,"    @Test
    public void testOnthrow() throws Exception {
        MethodConfig method = new MethodConfig();
        method.setOnthrow(""on-throw-object"");
        assertThat(method.getOnthrow(), equalTo((Object) ""on-throw-object""));
        Map<Object, Object> attribute = new HashMap<Object, Object>();
        MethodConfig.appendAttributes(attribute, method);
        assertThat(attribute, hasEntry((Object) Constants.ON_THROW_INSTANCE_KEY, (Object) ""on-throw-object""));
        Map<String, String> parameters = new HashMap<String, String>();
        MethodConfig.appendParameters(parameters, method);
        assertThat(parameters.size(), is(0));
    }
",non-flaky,5
92707,apache_dubbo,MethodConfigTest.testOnthrowMethod,"    @Test
    public void testOnthrowMethod() throws Exception {
        MethodConfig method = new MethodConfig();
        method.setOnthrowMethod(""on-throw-method"");
        assertThat(method.getOnthrowMethod(), equalTo(""on-throw-method""));
        Map<Object, Object> attribute = new HashMap<Object, Object>();
        MethodConfig.appendAttributes(attribute, method);
        assertThat(attribute, hasEntry((Object) Constants.ON_THROW_METHOD_KEY, (Object) ""on-throw-method""));
        Map<String, String> parameters = new HashMap<String, String>();
        MethodConfig.appendParameters(parameters, method);
        assertThat(parameters.size(), is(0));
    }
",non-flaky,5
92708,apache_dubbo,MethodConfigTest.testOninvoke,"    @Test
    public void testOninvoke() throws Exception {
        MethodConfig method = new MethodConfig();
        method.setOninvoke(""on-invoke-object"");
        assertThat(method.getOninvoke(), equalTo((Object) ""on-invoke-object""));
        Map<Object, Object> attribute = new HashMap<Object, Object>();
        MethodConfig.appendAttributes(attribute, method);
        assertThat(attribute, hasEntry((Object) Constants.ON_INVOKE_INSTANCE_KEY, (Object) ""on-invoke-object""));
        Map<String, String> parameters = new HashMap<String, String>();
        MethodConfig.appendParameters(parameters, method);
        assertThat(parameters.size(), is(0));
    }
",non-flaky,5
92709,apache_dubbo,MethodConfigTest.testOninvokeMethod,"    @Test
    public void testOninvokeMethod() throws Exception {
        MethodConfig method = new MethodConfig();
        method.setOninvokeMethod(""on-invoke-method"");
        assertThat(method.getOninvokeMethod(), equalTo(""on-invoke-method""));
        Map<Object, Object> attribute = new HashMap<Object, Object>();
        MethodConfig.appendAttributes(attribute, method);
        assertThat(attribute, hasEntry((Object) Constants.ON_INVOKE_METHOD_KEY, (Object) ""on-invoke-method""));
        Map<String, String> parameters = new HashMap<String, String>();
        MethodConfig.appendParameters(parameters, method);
        assertThat(parameters.size(), is(0));
    }
",non-flaky,5
92710,apache_dubbo,MethodConfigTest.testReturn,"    @Test
    public void testReturn() throws Exception {
        MethodConfig method = new MethodConfig();
        method.setReturn(true);
        assertThat(method.isReturn(), is(true));
    }
",non-flaky,5
92711,apache_dubbo,ConfigTest.testConfig,"    @Test
    public void testConfig() {
        com.alibaba.dubbo.config.ServiceConfig<DemoService> service = new ServiceConfig<>();
        service.setApplication(new com.alibaba.dubbo.config.ApplicationConfig(""first-dubbo-provider""));
        service.setRegistry(new com.alibaba.dubbo.config.RegistryConfig(""multicast://224.5.6.7:1234""));
        service.setInterface(DemoService.class);
        service.setRef(new DemoServiceImpl());
        service.export();

        com.alibaba.dubbo.config.ReferenceConfig<DemoService> reference = new ReferenceConfig<>();
        reference.setApplication(new ApplicationConfig(""first-dubbo-client""));
        reference.setRegistry(new RegistryConfig(""multicast://224.5.6.7:1234""));
        reference.setInterface(DemoService.class);
        DemoService demoService = reference.get();
        String message = demoService.sayHello(""dubbo"");
        Assert.assertEquals(""hello dubbo"", message);
    }
",non-flaky,5
92712,apache_dubbo,ProtocolConfigTest.testName,"    @Test
    public void testName() throws Exception {
        ProtocolConfig protocol = new ProtocolConfig();
        protocol.setName(""name"");
        Map<String, String> parameters = new HashMap<String, String>();
        ProtocolConfig.appendParameters(parameters, protocol);
        assertThat(protocol.getName(), equalTo(""name""));
        assertThat(protocol.getId(), equalTo(""name""));
        assertThat(parameters.isEmpty(), is(true));
    }
",non-flaky,5
92713,apache_dubbo,ProtocolConfigTest.testHost,"    @Test
    public void testHost() throws Exception {
        ProtocolConfig protocol = new ProtocolConfig();
        protocol.setHost(""host"");
        Map<String, String> parameters = new HashMap<String, String>();
        ProtocolConfig.appendParameters(parameters, protocol);
        assertThat(protocol.getHost(), equalTo(""host""));
        assertThat(parameters.isEmpty(), is(true));
    }
",non-flaky,5
92714,apache_dubbo,ProtocolConfigTest.testPort,"    @Test
    public void testPort() throws Exception {
        ProtocolConfig protocol = new ProtocolConfig();
        protocol.setPort(8080);
        Map<String, String> parameters = new HashMap<String, String>();
        ProtocolConfig.appendParameters(parameters, protocol);
        assertThat(protocol.getPort(), equalTo(8080));
        assertThat(parameters.isEmpty(), is(true));
    }
",non-flaky,5
92715,apache_dubbo,ProtocolConfigTest.testPath,"    @Test
    public void testPath() throws Exception {
        ProtocolConfig protocol = new ProtocolConfig();
        protocol.setContextpath(""context-path"");
        Map<String, String> parameters = new HashMap<String, String>();
        ProtocolConfig.appendParameters(parameters, protocol);
        assertThat(protocol.getPath(), equalTo(""context-path""));
        assertThat(protocol.getContextpath(), equalTo(""context-path""));
        assertThat(parameters.isEmpty(), is(true));
        protocol.setPath(""path"");
        assertThat(protocol.getPath(), equalTo(""path""));
        assertThat(protocol.getContextpath(), equalTo(""path""));
    }
",non-flaky,5
92716,apache_dubbo,ProtocolConfigTest.testThreads,"    @Test
    public void testThreads() throws Exception {
        ProtocolConfig protocol = new ProtocolConfig();
        protocol.setThreads(10);
        assertThat(protocol.getThreads(), is(10));
    }
",non-flaky,5
92717,apache_dubbo,ProtocolConfigTest.testIothreads,"    @Test
    public void testIothreads() throws Exception {
        ProtocolConfig protocol = new ProtocolConfig();
        protocol.setIothreads(10);
        assertThat(protocol.getIothreads(), is(10));
    }
",non-flaky,5
92718,apache_dubbo,ProtocolConfigTest.testQueues,"    @Test
    public void testQueues() throws Exception {
        ProtocolConfig protocol = new ProtocolConfig();
        protocol.setQueues(10);
        assertThat(protocol.getQueues(), is(10));
    }
",non-flaky,5
92719,apache_dubbo,ProtocolConfigTest.testAccepts,"    @Test
    public void testAccepts() throws Exception {
        ProtocolConfig protocol = new ProtocolConfig();
        protocol.setAccepts(10);
        assertThat(protocol.getAccepts(), is(10));
    }
",non-flaky,5
92720,apache_dubbo,ProtocolConfigTest.testAccesslog,"    @Test
    public void testAccesslog() throws Exception {
        ProtocolConfig protocol = new ProtocolConfig();
        protocol.setAccesslog(""access.log"");
        assertThat(protocol.getAccesslog(), equalTo(""access.log""));
    }
",non-flaky,5
92721,apache_dubbo,ProtocolConfigTest.testRegister,"    @Test
    public void testRegister() throws Exception {
        ProtocolConfig protocol = new ProtocolConfig();
        protocol.setRegister(true);
        assertThat(protocol.isRegister(), is(true));
    }
",non-flaky,5
92722,apache_dubbo,ProtocolConfigTest.testParameters,"    @Test
    public void testParameters() throws Exception {
        ProtocolConfig protocol = new ProtocolConfig();
        protocol.setParameters(Collections.singletonMap(""k1"", ""v1""));
        assertThat(protocol.getParameters(), hasEntry(""k1"", ""v1""));
    }
",non-flaky,5
92723,apache_dubbo,ProtocolConfigTest.testDefault,"    @Test
    public void testDefault() throws Exception {
        ProtocolConfig protocol = new ProtocolConfig();
        protocol.setDefault(true);
        assertThat(protocol.isDefault(), is(true));
    }
",non-flaky,5
96007,stanfordnlp_CoreNLP,SemgrexPatternITest.testNERStanfordDependencies,"  @Test
  public void testNERStanfordDependencies() throws Exception{
    String sentence = ""John lives in Washington."";
    Properties props = new Properties();
    props.setProperty(""annotators"",""tokenize, ssplit, pos, lemma, ner, parse"");
    props.setProperty(""parse.originalDependencies"", ""true"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    Annotation doc = new Annotation(sentence);
    pipeline.annotate(doc);
    CoreMap sent = doc.get(CoreAnnotations.SentencesAnnotation.class).get(0);
    SemanticGraph graph = sent.get(SemanticGraphCoreAnnotations.CollapsedCCProcessedDependenciesAnnotation.class);
    graph.prettyPrint();
    String patStr = ""({word:/lives/} >/prep_in/ {word:/\\QCalifornia\\E|\\QWashington\\E/} >nsubj {ner:PERSON})"";
    SemgrexPattern pat = SemgrexPattern.compile(patStr);
    SemgrexMatcher mat = pat.matcher(graph, true);
    assertTrue(mat.find());
  }
",non-flaky,5
96008,stanfordnlp_CoreNLP,SemgrexPatternITest.testNERUniversalDependencies,"  @Test
  public void testNERUniversalDependencies() throws Exception{
    String sentence = ""John lives in Washington."";
    Properties props = new Properties();
    props.setProperty(""annotators"",""tokenize, ssplit, pos, lemma, ner, parse"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    props.setProperty(""parse.originalDependencies"", ""false"");
    Annotation doc = new Annotation(sentence);
    pipeline.annotate(doc);
    CoreMap sent = doc.get(CoreAnnotations.SentencesAnnotation.class).get(0);
    SemanticGraph graph = sent.get(SemanticGraphCoreAnnotations.CollapsedCCProcessedDependenciesAnnotation.class);
    graph.prettyPrint();
    String patStr = ""({word:/lives/} >/obl:in/ {word:/\\QCalifornia\\E|\\QWashington\\E/} >nsubj {ner:PERSON})"";
    SemgrexPattern pat = SemgrexPattern.compile(patStr);
    SemgrexMatcher mat = pat.matcher(graph, true);
    assertTrue(mat.find());
  }
",non-flaky,5
96009,stanfordnlp_CoreNLP,SUTimeSimpleParserITest.testWorking,"  @Test
  public void testWorking() throws SUTimeSimpleParser.SUTimeParsingError {
    String[] inputs = { ""1972"", ""1972-07-05"", ""Jan 12, 1975 5:30"", ""7:12"", ""0712"", ""1972-04"" };
    String[] outputs = { ""1972-XX-XX"", ""1972-07-05"", ""1975-01-12T05:30"", ""T07:12"", ""712-XX-XX"", ""1972-04"" };
    // todo: second last case is totally bad, but it's what it does at present. But I guess 1930 is ambiguous....
    assertEquals(inputs.length, outputs.length);

    for (int i = 0; i < inputs.length; i++) {
      // System.err.println(""String: "" + inputs[i]);
      SUTime.Temporal timeExpression = parse(inputs[i]);
      // System.err.println(""Parsed: "" + timeExpression);
      assertEquals(outputs[i], timeExpression.toString());
    }
  }
",non-flaky,5
96010,stanfordnlp_CoreNLP,HeidelTimeITest.runHeidelTimeEnglish,"  @Test
  public void runHeidelTimeEnglish() throws Exception {
    String text = ""On Monday, some cataclysmic news about a a release last Christmas was released."";

    Annotation ann = new Annotation(text);
    String date = ""2017-07-07"";
    ann.set(CoreAnnotations.DocDateAnnotation.class, date);

    String heideltimeEnv = System.getenv(""HEIDELTIME_PATH"");
    if (heideltimeEnv == null) {
      heideltimeEnv = DEFAULT_HEIDELTIME_LOCATION;
    }

    Properties defaultProps = new Properties();
    defaultProps.load(IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(""edu/stanford/nlp/pipeline/StanfordCoreNLP.properties""));

    Properties props = new Properties(defaultProps);
    props.setProperty(""customAnnotatorClass.heideltime"", ""edu.stanford.nlp.time.HeidelTimeAnnotator"");
    props.setProperty(HeidelTimeAnnotator.HEIDELTIME_PATH_PROPERTY, heideltimeEnv);
    props.setProperty(HeidelTimeAnnotator.HEIDELTIME_LANGUAGE_PROPERTY, ""english"");
    props.setProperty(""annotators"", ""tokenize,ssplit,heideltime"");

    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    pipeline.annotate(ann);

    List<CoreMap> outputs = ann.get(TimeAnnotations.TimexAnnotations.class);
    Assert.assertEquals(2, outputs.size());

    Assert.assertEquals(""Monday"", outputs.get(0).get(TimeAnnotations.TimexAnnotation.class).text());
    Assert.assertEquals(""2017-07-03"", outputs.get(0).get(TimeAnnotations.TimexAnnotation.class).value());

    Assert.assertEquals(""Christmas"", outputs.get(1).get(TimeAnnotations.TimexAnnotation.class).text());
    Assert.assertEquals(""2016-12-25"", outputs.get(1).get(TimeAnnotations.TimexAnnotation.class).value());
  }
",non-flaky,5
96011,stanfordnlp_CoreNLP,HeidelTimeITest.runHeidelTimeSpanish,"  @Test
  public void runHeidelTimeSpanish() throws Exception {
    String text = ""El lunes, algunas noticias cataclÃ­smicas sobre un lanzamiento de la Navidad pasada fueron liberadas."";

    Annotation ann = new Annotation(text);
    String date = ""2017-07-07"";
    ann.set(CoreAnnotations.DocDateAnnotation.class, date);

    String heideltimeEnv = System.getenv(""HEIDELTIME_PATH"");
    if (heideltimeEnv == null) {
      heideltimeEnv = DEFAULT_HEIDELTIME_LOCATION;
    }

    Properties defaultProps = new Properties();
    defaultProps.load(IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(""edu/stanford/nlp/pipeline/StanfordCoreNLP-spanish.properties""));

    Properties props = new Properties(defaultProps);
    props.setProperty(""customAnnotatorClass.heideltime"", ""edu.stanford.nlp.time.HeidelTimeAnnotator"");
    props.setProperty(HeidelTimeAnnotator.HEIDELTIME_PATH_PROPERTY, heideltimeEnv);
    props.setProperty(HeidelTimeAnnotator.HEIDELTIME_LANGUAGE_PROPERTY, ""spanish"");
    props.setProperty(""annotators"", ""tokenize,ssplit,heideltime"");

    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    pipeline.annotate(ann);

    List<CoreMap> outputs = ann.get(TimeAnnotations.TimexAnnotations.class);
    Assert.assertEquals(1, outputs.size()); // Unfortunately, HeidelTime doesn't get Navidad :-(

    Assert.assertEquals(""El lunes"", outputs.get(0).get(TimeAnnotations.TimexAnnotation.class).text());
    Assert.assertEquals(""2017-07-03"", outputs.get(0).get(TimeAnnotations.TimexAnnotation.class).value());

    //Assert.assertEquals(""Navidad"", outputs.get(1).get(TimeAnnotations.TimexAnnotation.class).text());
    //Assert.assertEquals(""2016-12-25"", outputs.get(1).get(TimeAnnotations.TimexAnnotation.class).value());
  }
",non-flaky,5
96012,stanfordnlp_CoreNLP,DcorefExactOutputITest.testCoref,"  @Test
  public void testCoref() throws IOException {
    String doc = IOUtils.slurpFile(""edu/stanford/nlp/dcoref/STILLALONEWOLF_20050102.1100.eng.LDC2005E83.sgm"");
    Annotation annotation = pipeline.process(doc);
    Map<Integer, CorefChain> chains = annotation.get(CorefCoreAnnotations.CorefChainAnnotation.class);
    Map<Integer, List<ExpectedMention>> expected = loadExpectedResults(""edu/stanford/nlp/dcoref/STILLALONEWOLF_20050102.1100.eng.LDC2005E83.expectedcoref"");
    compareResults(expected, chains);
  }
",non-flaky,5
96013,stanfordnlp_CoreNLP,DcorefBenchmarkSlowITest.testDcoref,"  @Test
  public void testDcoref() throws Exception {
    Counter<String> results = getCorefResults(runCorefTest(true));

    Counter<String> lowResults = new ClassicCounter<>();
    Counter<String> highResults = new ClassicCounter<>();
    Counter<String> expectedResults = new ClassicCounter<>();

    setLowHighExpected(lowResults, highResults, expectedResults, MENTION_TP, 12400, 12410, 12405);
    setLowHighExpected(lowResults, highResults, expectedResults, MENTION_F1, 50.4, 50.45, 50.42);

    setLowHighExpected(lowResults, highResults, expectedResults, MUC_TP, 6245, 6255, 6250);
    setLowHighExpected(lowResults, highResults, expectedResults, MUC_F1, 60.65, 60.7, 60.66);

    setLowHighExpected(lowResults, highResults, expectedResults, BCUBED_TP, 12440, 12452.25, 12452.25);
    setLowHighExpected(lowResults, highResults, expectedResults, BCUBED_F1, 70.75, 70.85, 70.80);

    setLowHighExpected(lowResults, highResults, expectedResults, CEAFM_TP, 10915, 10930, 10920);
    setLowHighExpected(lowResults, highResults, expectedResults, CEAFM_F1, 59.4, 59.5, 59.42);

    setLowHighExpected(lowResults, highResults, expectedResults, CEAFE_TP, 3830, 3840, 3831.36);
    setLowHighExpected(lowResults, highResults, expectedResults, CEAFE_F1, 47.4, 47.5, 47.45);

    setLowHighExpected(lowResults, highResults, expectedResults, BLANC_F1, 75.35, 75.44, 75.38);

    setLowHighExpected(lowResults, highResults, expectedResults, CONLL_SCORE, 59.6, 59.7, 59.64);

    BenchmarkingHelper.benchmarkResults(results, lowResults, highResults, expectedResults);
  }
",non-flaky,5
96014,stanfordnlp_CoreNLP,SentenceUtilsITest.testRebuildingMWTText,"  @Test
  public void testRebuildingMWTText() throws IOException {
    // set up French properties
    Properties frenchProperties = LanguageInfo.getLanguageProperties(""french"");
    frenchProperties.setProperty(""annotators"", ""tokenize,ssplit,mwt"");
    StanfordCoreNLP frenchPipeline = new StanfordCoreNLP(frenchProperties);
    String frenchText = ""Le but des bandes de roulement est d'augmenter la traction."";
    CoreDocument frenchDoc = new CoreDocument(frenchPipeline.process(frenchText));
    String rebuiltFrenchText = SentenceUtils.listToOriginalTextString(frenchDoc.tokens());
    assertTrue(frenchText.equals(rebuiltFrenchText));
  }
",non-flaky,5
96015,stanfordnlp_CoreNLP,SentenceUtilsITest.testRebuildingText,"  @Test
  public void testRebuildingText() {
    // set up basic English pipeline
    Properties basicProperties = new Properties();
    basicProperties.setProperty(""annotators"", ""tokenize,ssplit"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(basicProperties);
    String text = ""Let's hope this doesn't not work properly.  Especially across sentences. "";
    CoreDocument doc = new CoreDocument(pipeline.process(text));
    String rebuiltText = SentenceUtils.listToOriginalTextString(doc.tokens());
    assertTrue(text.equals(rebuiltText));
  }
",non-flaky,5
96016,stanfordnlp_CoreNLP,TokenSequenceMatcherITest.testTokenSequenceMatcherValue,"  @Test
  public void testTokenSequenceMatcherValue() throws IOException {
    CoreMap doc = createDocument(testText);

    // Test simple sequence with value
    TokenSequencePattern p = TokenSequencePattern.compile(getOrPatternExpr(
            new Pair<String,Object>(""one"", 1), new Pair<String,Object>(""two"", null), new Pair<String,Object>(""fifty"", 50)));
    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));

    boolean match = m.find();
    assertTrue(match);
    assertEquals(""one"", m.group());
    assertEquals(1, m.groupValue());

    match = m.find();
    assertTrue(match);
    assertEquals(""two"", m.group());
    assertNull(m.groupValue());

    match = m.find();
    assertTrue(match);
    assertEquals(""fifty"", m.group());
    assertEquals(50, m.groupValue());

    match = m.find();
    assertFalse(match);
  }
",non-flaky,5
96017,stanfordnlp_CoreNLP,TokenSequenceMatcherITest.testTokenSequenceMatcherBeginEnd,"  @Test
  public void testTokenSequenceMatcherBeginEnd() throws IOException {
    CoreMap doc = createDocument(testText);

    // Test simple sequence with begin sequence matching
    TokenSequencePattern p = TokenSequencePattern.compile(""^ [] []"");
    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));

    boolean match = m.find();
    assertTrue(match);
    assertEquals(""the number"", m.group());

    match = m.find();
    assertFalse(match);

    // Test simple sequence with end sequence matching
    p = TokenSequencePattern.compile(""[] [] $"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));

    match = m.find();
    assertTrue(match);
    assertEquals(""fifty."", m.group());

    match = m.find();
    assertFalse(match);

    // Test simple sequence with begin and end sequence matching
    p = TokenSequencePattern.compile(""^ [] [] $"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));

    match = m.find();
    assertFalse(match);

    // Test simple sequence with ^$ in a string regular expression
    p = TokenSequencePattern.compile(""/^number$/"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));

    match = m.find();
    assertTrue(match);
    assertEquals(""number"", m.group());

    match = m.find();
    assertFalse(match);
  }
",non-flaky,5
96018,stanfordnlp_CoreNLP,TokenSequenceMatcherITest.testTokenSequenceMatcher1,"  @Test
  public void testTokenSequenceMatcher1() throws IOException {
    CoreMap doc = createDocument(testText1);

    // Test simple sequence
    TokenSequencePattern p = TokenSequencePattern.compile(getSequencePatternExpr(""Archbishop"", ""of"", ""Canterbury""));
    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    boolean match = m.find();
    assertTrue(match);
    assertEquals(""Archbishop of Canterbury"", m.group());
    match = m.find();
    assertFalse(match);

    m.reset();
    match = m.find();
    assertTrue(match);
    assertEquals(""Archbishop of Canterbury"", m.group());

    m.reset();
    match = m.matches();
    assertFalse(match);

    // Test sequence with or
    p = TokenSequencePattern.compile(
            new SequencePattern.OrPatternExpr(
                    getSequencePatternExpr(""Archbishop"", ""of"", ""Canterbury""),
                    getSequencePatternExpr(""Bishop"", ""of"", ""London"")
            ));
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""Bishop of London"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""Archbishop of Canterbury"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""Bishop of London"", m.group());
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile(
              new SequencePattern.SequencePatternExpr(
                    SequencePattern.SEQ_BEGIN_PATTERN_EXPR,
                    getSequencePatternExpr(""Archbishop"", ""of"", ""Canterbury"")
            ));
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile(
            new SequencePattern.SequencePatternExpr(
                    SequencePattern.SEQ_BEGIN_PATTERN_EXPR,
                    getSequencePatternExpr(""Mellitus"", ""was"", ""the"")
            ));
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""Mellitus was the"", m.group());
    match = m.find();
    assertFalse(match);


    p = TokenSequencePattern.compile(
            new SequencePattern.SequencePatternExpr(
                    getSequencePatternExpr(""Archbishop"", ""of"", ""Canterbury""),
                    SequencePattern.SEQ_END_PATTERN_EXPR
                    ));
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile(
            new SequencePattern.SequencePatternExpr(
                    getSequencePatternExpr(""London"", ""in"", ""604"", "".""),
                    SequencePattern.SEQ_END_PATTERN_EXPR
                    ));
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""London in 604."", m.group());
    match = m.find();
    assertFalse(match);
  }
",non-flaky,5
96019,stanfordnlp_CoreNLP,TokenSequenceMatcherITest.testTokenSequenceMatcher2,"  @Test
  public void testTokenSequenceMatcher2() throws IOException {
    CoreMap doc = createDocument(testText1);
    TokenSequencePattern p = TokenSequencePattern.compile(
                    getSequencePatternExpr("".*"", "".*"", ""of"", "".*""));

    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    boolean match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""first Bishop of London"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""third Archbishop of Canterbury"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""a member of the"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""as Bishop of London"", m.group());
    match = m.find();
    assertFalse(match);

    // Test sequence with groups
    p = TokenSequencePattern.compile(
                    new SequencePattern.SequencePatternExpr(
                      new SequencePattern.GroupPatternExpr(
                            getSequencePatternExpr("".*"", "".*"")),
                      getNodePatternExpr(""of""),
                      new SequencePattern.GroupPatternExpr(
                            getSequencePatternExpr("".*""))));

    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""first Bishop of London"", m.group());
    assertEquals(""first Bishop"", m.group(1));
    assertEquals(""London"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""third Archbishop of Canterbury"", m.group());
    assertEquals(""third Archbishop"", m.group(1));
    assertEquals(""Canterbury"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""a member of the"", m.group());
    assertEquals(""a member"", m.group(1));
    assertEquals(""the"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""as Bishop of London"", m.group());
    assertEquals(""as Bishop"", m.group(1));
    assertEquals(""London"", m.group(2));
    match = m.find();
    assertFalse(match);

  }
",non-flaky,5
96020,stanfordnlp_CoreNLP,TokenSequenceMatcherITest.testTokenSequenceMatcher3,"  @Test
  public void testTokenSequenceMatcher3() throws IOException {
    CoreMap doc = createDocument(testText1);

    // Test sequence with groups
    TokenSequencePattern p = TokenSequencePattern.compile(
        new SequencePattern.SequencePatternExpr(
            new SequencePattern.GroupPatternExpr(
                new SequencePattern.RepeatPatternExpr(
                    getSequencePatternExpr(""[A-Za-z]+""), 1, 2)),
            getNodePatternExpr(""of""),
            new SequencePattern.GroupPatternExpr(
                new SequencePattern.RepeatPatternExpr(
                    getSequencePatternExpr(""[A-Za-z]+""), 1, 3))));

    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    boolean match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""first Bishop of London"", m.group());
    assertEquals(""first Bishop"", m.group(1));
    assertEquals(""London"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""third Archbishop of Canterbury"", m.group());
    assertEquals(""third Archbishop"", m.group(1));
    assertEquals(""Canterbury"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""a member of the Gregorian mission"", m.group());
    assertEquals(""a member"", m.group(1));
    assertEquals(""the Gregorian mission"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""as Bishop of London in"", m.group());
    assertEquals(""as Bishop"", m.group(1));
    assertEquals(""London in"", m.group(2));
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile(
        new SequencePattern.SequencePatternExpr(
            new SequencePattern.GroupPatternExpr(
                new SequencePattern.RepeatPatternExpr(
                    getNodePatternExpr(""[A-Za-z]+""), 2, 2)),
            getNodePatternExpr(""of""),
            new SequencePattern.GroupPatternExpr(
                new SequencePattern.RepeatPatternExpr(
                    getNodePatternExpr(""[A-Za-z]+""), 1, 3, false))));

    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""first Bishop of London"", m.group());
    assertEquals(""first Bishop"", m.group(1));
    assertEquals(""London"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""third Archbishop of Canterbury"", m.group());
    assertEquals(""third Archbishop"", m.group(1));
    assertEquals(""Canterbury"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""a member of the"", m.group());
    assertEquals(""a member"", m.group(1));
    assertEquals(""the"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""as Bishop of London"", m.group());
    assertEquals(""as Bishop"", m.group(1));
    assertEquals(""London"", m.group(2));
    match = m.find();
    assertFalse(match);
  }
",non-flaky,5
96021,stanfordnlp_CoreNLP,TokenSequenceMatcherITest.testTokenSequenceMatcherConj,"  @Test
  public void testTokenSequenceMatcherConj() throws IOException {
    CoreMap doc = createDocument(testText1);
    TokenSequencePattern p = TokenSequencePattern.compile(
                  new SequencePattern.AndPatternExpr(
                    new SequencePattern.SequencePatternExpr(
                      new SequencePattern.GroupPatternExpr(
                            new SequencePattern.RepeatPatternExpr(
                                    getNodePatternExpr(""[A-Za-z]+""), 2, 2)),
                      getNodePatternExpr(""of""),
                      new SequencePattern.GroupPatternExpr(
                            new SequencePattern.RepeatPatternExpr(
                                    getNodePatternExpr(""[A-Za-z]+""), 1, 3, false))),
                    new SequencePattern.SequencePatternExpr(
                      new SequencePattern.GroupPatternExpr(
                        new SequencePattern.RepeatPatternExpr(
                            getNodePatternExpr("".*""), 0, -1)),
                      getNodePatternExpr(""Bishop""),
                      new SequencePattern.RepeatPatternExpr(
                            getNodePatternExpr("".*""), 0, -1)
                    )));

    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    boolean match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    assertEquals(""first Bishop of London"", m.group());
    assertEquals(""first Bishop"", m.group(1));
    assertEquals(""London"", m.group(2));
    assertEquals(""first"", m.group(3));
    match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    // TODO: This conjunction has both a greedy and nongreedy pattern
    //  - the greedy will try to match as much as possible
    //  - while the non greedy will try to match less
    //  - currently the greedy overrides the nongreedy so we get an additional in...
    assertEquals(""as Bishop of London in"", m.group());
    assertEquals(""as Bishop"", m.group(1));
    assertEquals(""London in"", m.group(2));
    assertEquals(""as"", m.group(3));
    match = m.find();
    assertFalse(match);


    // Same as before, but both non-greedy now...
    p = TokenSequencePattern.compile(
                  new SequencePattern.AndPatternExpr(
                    new SequencePattern.SequencePatternExpr(
                      new SequencePattern.GroupPatternExpr(
                            new SequencePattern.RepeatPatternExpr(
                                    getNodePatternExpr(""[A-Za-z]+""), 2, 2)),
                      getNodePatternExpr(""of""),
                      new SequencePattern.GroupPatternExpr(
                            new SequencePattern.RepeatPatternExpr(
                                    getNodePatternExpr(""[A-Za-z]+""), 1, 3, false))),
                    new SequencePattern.SequencePatternExpr(
                      new SequencePattern.GroupPatternExpr(
                        new SequencePattern.RepeatPatternExpr(
                            getNodePatternExpr("".*""), 0, -1)),
                      getNodePatternExpr(""Bishop""),
                      new SequencePattern.RepeatPatternExpr(
                            getNodePatternExpr("".*""), 0, -1, false)
                    )));

    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    assertEquals(""first Bishop of London"", m.group());
    assertEquals(""first Bishop"", m.group(1));
    assertEquals(""London"", m.group(2));
    assertEquals(""first"", m.group(3));
    match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    assertEquals(""as Bishop of London"", m.group());
    assertEquals(""as Bishop"", m.group(1));
    assertEquals(""London"", m.group(2));
    assertEquals(""as"", m.group(3));
    match = m.find();
    assertFalse(match);


    // Same as before, but compiled from string
    p = TokenSequencePattern.compile(
            ""(?: (/[A-Za-z]+/{2,2}) /of/ (/[A-Za-z]+/{1,3}?) ) & (?: (/.*/*) /Bishop/ /.*/*? )"");

    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    assertEquals(""first Bishop of London"", m.group());
    assertEquals(""first Bishop"", m.group(1));
    assertEquals(""London"", m.group(2));
    assertEquals(""first"", m.group(3));
    match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    assertEquals(""as Bishop of London"", m.group());
    assertEquals(""as Bishop"", m.group(1));
    assertEquals(""London"", m.group(2));
    assertEquals(""as"", m.group(3));
    match = m.find();
    assertFalse(match);
  }
",non-flaky,5
96022,stanfordnlp_CoreNLP,TokenSequenceMatcherITest.testTokenSequenceMatcherConj2,"  @Test
  public void testTokenSequenceMatcherConj2() throws IOException {
    String content = ""The cat is sleeping on the floor."";
    String greedyPattern = ""(?: ([]* cat []*) & ([]* sleeping []*))"";

    TokenizerFactory tf = PTBTokenizer.factory(new CoreLabelTokenFactory(), """");
    List<CoreLabel> tokens = tf.getTokenizer(new StringReader(content)).tokenize();
    TokenSequencePattern seqPattern = TokenSequencePattern.compile(greedyPattern);
    TokenSequenceMatcher matcher = seqPattern.getMatcher(tokens);

    boolean entireMatch = matcher.matches();
    assertTrue(entireMatch);

    boolean match = matcher.find();
    assertTrue(match);
    assertEquals(""The cat is sleeping on the floor."", matcher.group());

    String reluctantPattern = ""(?: ([]*? cat []*?) & ([]*? sleeping []*?))"";
    TokenSequencePattern seqPattern2 = TokenSequencePattern.compile(reluctantPattern);
    TokenSequenceMatcher matcher2 = seqPattern2.getMatcher(tokens);

    match = matcher2.find();
    assertTrue(match);
    assertEquals(""The cat is sleeping"", matcher2.group());
  }
",non-flaky,5
96023,stanfordnlp_CoreNLP,TokenSequenceMatcherITest.testTokenSequenceMatcherConjAll,"  @Test
  public void testTokenSequenceMatcherConjAll() throws IOException {
    CoreMap doc = createDocument(testText1);
    TokenSequencePattern p = TokenSequencePattern.compile(
            ""(?: (/[A-Za-z]+/{1,2}) /of/ (/[A-Za-z]+/{1,3}?) ) & (?: (/.*/*) /Bishop/ /.*/*? )"");

    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    m.setFindType(SequenceMatcher.FindType.FIND_ALL);
    // Test finding of ALL matching sequences with conjunctions
    // todo: Not all sequences are found for some reason - missing sequences starting with just Bishop....
    boolean match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    assertEquals(""first Bishop of London"", m.group());
    assertEquals(""first Bishop"", m.group(1));
    assertEquals(""London"", m.group(2));
    assertEquals(""first"", m.group(3));
    match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    assertEquals(""Bishop of London"", m.group());
    assertEquals(""Bishop"", m.group(1));
    assertEquals(""London"", m.group(2));
    assertEquals("""", m.group(3));
    match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    assertEquals(""as Bishop of London"", m.group());
    assertEquals(""as Bishop"", m.group(1));
    assertEquals(""London"", m.group(2));
    assertEquals(""as"", m.group(3));
    match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    assertEquals(""as Bishop of London in"", m.group());
    assertEquals(""as Bishop"", m.group(1));
    assertEquals(""London in"", m.group(2));
    assertEquals(""as"", m.group(3));
    match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    assertEquals(""Bishop of London"", m.group());
    assertEquals(""Bishop"", m.group(1));
    assertEquals(""London"", m.group(2));
    assertEquals("""", m.group(3));
    match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    assertEquals(""Bishop of London in"", m.group());
    assertEquals(""Bishop"", m.group(1));
    assertEquals(""London in"", m.group(2));
    assertEquals("""", m.group(3));
    match = m.find();
    assertFalse(match);
  }
",non-flaky,5
96024,stanfordnlp_CoreNLP,TokenSequenceMatcherITest.testTokenSequenceMatcherAll,"  @Test
  public void testTokenSequenceMatcherAll() throws IOException {
    CoreMap doc = createDocument(testText1);
    TokenSequencePattern p = TokenSequencePattern.compile(
            ""(/[A-Za-z]+/{1,2}) /of/ (/[A-Za-z]+/{1,3}?) "");

    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    m.setFindType(SequenceMatcher.FindType.FIND_ALL);
    // Test finding of ALL matching sequences
    // NOTE: when using FIND_ALL greedy/reluctant modifiers are not enforced
    //       perhaps should add syntax where some of them are enforced...
    boolean match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""first Bishop of London"", m.group());
    assertEquals(""first Bishop"", m.group(1));
    assertEquals(""London"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""Bishop of London"", m.group());
    assertEquals(""Bishop"", m.group(1));
    assertEquals(""London"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""third Archbishop of Canterbury"", m.group());
    assertEquals(""third Archbishop"", m.group(1));
    assertEquals(""Canterbury"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""Archbishop of Canterbury"", m.group());
    assertEquals(""Archbishop"", m.group(1));
    assertEquals(""Canterbury"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""a member of the"", m.group());
    assertEquals(""a member"", m.group(1));
    assertEquals(""the"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""a member of the Gregorian"", m.group());
    assertEquals(""a member"", m.group(1));
    assertEquals(""the Gregorian"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""a member of the Gregorian mission"", m.group());
    assertEquals(""a member"", m.group(1));
    assertEquals(""the Gregorian mission"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""member of the"", m.group());
    assertEquals(""member"", m.group(1));
    assertEquals(""the"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""member of the Gregorian"", m.group());
    assertEquals(""member"", m.group(1));
    assertEquals(""the Gregorian"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""member of the Gregorian mission"", m.group());
    assertEquals(""member"", m.group(1));
    assertEquals(""the Gregorian mission"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""as Bishop of London"", m.group());
    assertEquals(""as Bishop"", m.group(1));
    assertEquals(""London"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""as Bishop of London in"", m.group());
    assertEquals(""as Bishop"", m.group(1));
    assertEquals(""London in"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""Bishop of London"", m.group());
    assertEquals(""Bishop"", m.group(1));
    assertEquals(""London"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""Bishop of London in"", m.group());
    assertEquals(""Bishop"", m.group(1));
    assertEquals(""London in"", m.group(2));
    match = m.find();
    assertFalse(match);
  }
",non-flaky,5
96025,stanfordnlp_CoreNLP,TokenSequenceMatcherITest.testTokenSequenceMatcherAll2,"  @Test
  public void testTokenSequenceMatcherAll2() throws IOException {
    String text = ""DATE1 PROD1 PRICE1 PROD2 PRICE2 PROD3 PRICE3 DATE2 PROD4 PRICE4 PROD5 PRICE5 PROD6 PRICE6"";
    CoreMap doc = createDocument(text);
    TokenSequencePattern p = TokenSequencePattern.compile(
        ""(/DATE.*/) (?: /PROD.*/ /PRICE.*/)* (/PROD.*/) (/PRICE.*/)"");

    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    m.setFindType(SequenceMatcher.FindType.FIND_ALL);
    // Test finding of ALL matching sequences
    boolean match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    assertEquals(""DATE1"", m.group(1));
    assertEquals(""PROD3"", m.group(2));
    assertEquals(""PRICE3"", m.group(3));
    match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    assertEquals(""DATE1"", m.group(1));
    assertEquals(""PROD2"", m.group(2));
    assertEquals(""PRICE2"", m.group(3));
    match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    assertEquals(""DATE1"", m.group(1));
    assertEquals(""PROD1"", m.group(2));
    assertEquals(""PRICE1"", m.group(3));
    match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    assertEquals(""DATE2"", m.group(1));
    assertEquals(""PROD6"", m.group(2));
    assertEquals(""PRICE6"", m.group(3));
    match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    assertEquals(""DATE2"", m.group(1));
    assertEquals(""PROD5"", m.group(2));
    assertEquals(""PRICE5"", m.group(3));
    match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    assertEquals(""DATE2"", m.group(1));
    assertEquals(""PROD4"", m.group(2));
    assertEquals(""PRICE4"", m.group(3));
    match = m.find();
    assertFalse(match);
  }
",non-flaky,5
96026,stanfordnlp_CoreNLP,TokenSequenceMatcherITest.testTokenSequenceMatcherNonOverlapping,"  @Test
  public void testTokenSequenceMatcherNonOverlapping() throws IOException {
    String text = ""DATE1 PROD1 PRICE1 PROD2 PRICE2 PROD3 PRICE3 DATE2 PROD4 PRICE4 PROD5 PRICE5 PROD6 PRICE6"";
    CoreMap doc = createDocument(text);
    TokenSequencePattern p = TokenSequencePattern.compile(
        ""(/DATE.*/) ((/PROD.*/ /PRICE.*/)+)"");

    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    boolean match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    assertEquals(""DATE1"", m.group(1));
    assertEquals(""PROD1 PRICE1 PROD2 PRICE2 PROD3 PRICE3"", m.group(2));
    assertEquals(""PROD3 PRICE3"", m.group(3));
    match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    assertEquals(""DATE2"", m.group(1));
    assertEquals(""PROD4 PRICE4 PROD5 PRICE5 PROD6 PRICE6"", m.group(2));
    assertEquals(""PROD6 PRICE6"", m.group(3));
    match = m.find();
    assertFalse(match);
  }
",non-flaky,5
96027,stanfordnlp_CoreNLP,TokenSequenceMatcherITest.testTokenSequenceMatcher4,"  @Test
  public void testTokenSequenceMatcher4() throws IOException {
    CoreMap doc = createDocument(testText1);

    // Test sequence with groups
    TokenSequencePattern p = TokenSequencePattern.compile(
                      new SequencePattern.RepeatPatternExpr(
                                    getSequencePatternExpr(""[A-Za-z]+""), 1, -1));

    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    boolean match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""Mellitus was the first Bishop of London"", m.group());

    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""the third Archbishop of Canterbury"", m.group());

    p = TokenSequencePattern.compile(
            new SequencePattern.SequencePatternExpr(
                      new SequencePattern.RepeatPatternExpr(
                              getSequencePatternExpr(""[A-Za-z]+""), 0, -1),
                      getSequencePatternExpr(""Mellitus"", ""was"")));

    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""Mellitus was"", m.group());
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile(
            new SequencePattern.SequencePatternExpr(
                      new SequencePattern.RepeatPatternExpr(
                              getSequencePatternExpr(""[A-Za-z]+""), 1, -1),
                      getSequencePatternExpr(""Mellitus"", ""was"")));

    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertFalse(match);

  }
",non-flaky,5
96028,stanfordnlp_CoreNLP,TokenSequenceMatcherITest.testTokenSequenceMatcher5,"  @Test
  public void testTokenSequenceMatcher5() throws IOException {
    CoreMap doc = createDocument(testText1);

    // Test simple sequence
    TokenSequencePattern p = TokenSequencePattern.compile("" [ { word:\""Archbishop\"" } ]  [ { word:\""of\"" } ]  [ { word:\""Canterbury\"" } ]"");
    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    boolean match = m.find();
    assertTrue(match);
    assertEquals(""Archbishop of Canterbury"", m.group());
    match = m.find();
    assertFalse(match);

    m.reset();
    match = m.find();
    assertTrue(match);
    assertEquals(""Archbishop of Canterbury"", m.group());

    m.reset();
    match = m.matches();
    assertFalse(match);


    p = TokenSequencePattern.compile("" [ \""Archbishop\"" ]  [ \""of\""  ]  [ \""Canterbury\""  ]"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(""Archbishop of Canterbury"", m.group());
    match = m.find();
    assertFalse(match);

    m.reset();
    match = m.find();
    assertTrue(match);
    assertEquals(""Archbishop of Canterbury"", m.group());

    m.reset();
    match = m.matches();
    assertFalse(match);

    // Test sequence with or
    p = TokenSequencePattern.compile("" [ \""Archbishop\""] [\""of\""] [\""Canterbury\""] |  [ \""Bishop\"" ] [ \""of\"" ]  [ \""London\"" ] "");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""Bishop of London"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""Archbishop of Canterbury"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""Bishop of London"", m.group());
    match = m.find();
    assertFalse(match);

  }
",non-flaky,5
96029,stanfordnlp_CoreNLP,TokenSequenceMatcherITest.testTokenSequenceMatcher6,"  @Test
  public void testTokenSequenceMatcher6() throws IOException {
    CoreMap doc = createDocument(testText1);
    TokenSequencePattern p = TokenSequencePattern.compile(""[ /.*/ ] [ /.*/ ] [/of/] [/.*/]"");
    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    boolean match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""first Bishop of London"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""third Archbishop of Canterbury"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""a member of the"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""as Bishop of London"", m.group());
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile(""([ /.*/ ] [ /.*/ ]) [/of/] ([/.*/])"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""first Bishop of London"", m.group());
    assertEquals(""first Bishop"", m.group(1));
    assertEquals(""London"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""third Archbishop of Canterbury"", m.group());
    assertEquals(""third Archbishop"", m.group(1));
    assertEquals(""Canterbury"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""a member of the"", m.group());
    assertEquals(""a member"", m.group(1));
    assertEquals(""the"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""as Bishop of London"", m.group());
    assertEquals(""as Bishop"", m.group(1));
    assertEquals(""London"", m.group(2));
    match = m.find();
    assertFalse(match);

  }
",non-flaky,5
96030,stanfordnlp_CoreNLP,TokenSequenceMatcherITest.testTokenSequenceMatcher7,"  @Test
  public void testTokenSequenceMatcher7() throws IOException {
    CoreMap doc = createDocument(testText1);

    // Test sequence with groups
    TokenSequencePattern p = TokenSequencePattern.compile("" ( [ /[A-Za-z]+/ ]{1,2} )  [ /of/ ] ( [ /[A-Za-z]+/ ]{1,3} )"");
    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    boolean match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""first Bishop of London"", m.group());
    assertEquals(""first Bishop"", m.group(1));
    assertEquals(""London"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""third Archbishop of Canterbury"", m.group());
    assertEquals(""third Archbishop"", m.group(1));
    assertEquals(""Canterbury"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""a member of the Gregorian mission"", m.group());
    assertEquals(""a member"", m.group(1));
    assertEquals(""the Gregorian mission"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""as Bishop of London in"", m.group());
    assertEquals(""as Bishop"", m.group(1));
    assertEquals(""London in"", m.group(2));
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile( "" ( [ /[A-Za-z]+/ ]{2,2} )  [ /of/ ] ( [ /[A-Za-z]+/ ]{1,3}? )"");

    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""first Bishop of London"", m.group());
    assertEquals(""first Bishop"", m.group(1));
    assertEquals(""London"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""third Archbishop of Canterbury"", m.group());
    assertEquals(""third Archbishop"", m.group(1));
    assertEquals(""Canterbury"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""a member of the"", m.group());
    assertEquals(""a member"", m.group(1));
    assertEquals(""the"", m.group(2));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""as Bishop of London"", m.group());
    assertEquals(""as Bishop"", m.group(1));
    assertEquals(""London"", m.group(2));
    match = m.find();
    assertFalse(match);
  }
",non-flaky,5
96031,stanfordnlp_CoreNLP,TokenSequenceMatcherITest.testTokenSequenceMatcher8,"  @Test
  public void testTokenSequenceMatcher8() throws IOException {
    CoreMap doc = createDocument(testText1);

    // Test sequence with groups
    TokenSequencePattern p = TokenSequencePattern.compile( ""[ /[A-Za-z]+/ ]*"");

    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    boolean match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""Mellitus was the first Bishop of London"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""the third Archbishop of Canterbury"", m.group());

    p = TokenSequencePattern.compile( ""[ /[A-Za-z]+/ ]*  [\""Mellitus\""] [ \""was\""]"");

    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""Mellitus was"", m.group());
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile( ""[ /[A-Za-z]+/ ]+  [\""Mellitus\""] [ \""was\""]"");

    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertFalse(match);

  }
",non-flaky,5
96032,stanfordnlp_CoreNLP,TokenSequenceMatcherITest.testTokenSequenceMatcher9,"  @Test
  public void testTokenSequenceMatcher9() throws IOException {
    CoreMap doc = createDocument(testText1);

    // Test sequence with groups
//    TokenSequencePattern p = TokenSequencePattern.compile( ""(?$contextprev /.*/) (?$treat [{{treat}} & /.*/]) (?$contextnext [/.*/])"");
    TokenSequencePattern p = TokenSequencePattern.compile(""(?$contextprev /.*/) (?$test [{tag:NNP} & /.*/]) (?$contextnext [/.*/])"");

    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    boolean match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    assertEquals(""first Bishop of"", m.group());

    assertEquals(""first"", m.group(1));
    assertEquals(""Bishop"", m.group(2));
    assertEquals(""of"", m.group(3));
    assertEquals(""first"", m.group(""$contextprev""));
    assertEquals(""Bishop"", m.group(""$test""));
    assertEquals(""of"", m.group(""$contextnext""));
    assertEquals(""first"", m.group("" $contextprev""));
    assertEquals(""Bishop"", m.group(""$test ""));
    assertEquals(null, m.group(""$contex tnext""));

    assertEquals(3, m.start(""$contextprev""));
    assertEquals(4, m.end(""$contextprev""));
    assertEquals(4, m.start(""$test""));
    assertEquals(5, m.end(""$test""));
    assertEquals(5, m.start(""$contextnext""));
    assertEquals(6, m.end(""$contextnext""));
  }
",non-flaky,5
96033,stanfordnlp_CoreNLP,TokenSequenceMatcherITest.testTokenSequenceMatcher10,"  @Test
  public void testTokenSequenceMatcher10() throws IOException {
    CoreMap doc = createDocument(""the number is five or 5 or 5.0 or but not 5x or -5 or 5L."");

    // Test simplified pattern with number
    TokenSequencePattern p = TokenSequencePattern.compile( ""(five|5|5x|5.0|-5|5L)"");

    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    boolean match = m.find();
    assertTrue(match);
    assertEquals(1, m.groupCount());
    assertEquals(""five"", m.group(1));

    match = m.find();
    assertTrue(match);
    assertEquals(1, m.groupCount());
    assertEquals(""5"", m.group(1));

    match = m.find();
    assertTrue(match);
    assertEquals(1, m.groupCount());
    assertEquals(""5.0"", m.group(1));

    match = m.find();
    assertTrue(match);
    assertEquals(1, m.groupCount());
    assertEquals(""5x"", m.group(1));

    match = m.find();
    assertTrue(match);
    assertEquals(1, m.groupCount());
    assertEquals(""-5"", m.group(1));

    match = m.find();
    assertTrue(match);
    assertEquals(1, m.groupCount());
    assertEquals(""5L"", m.group(1));

    match = m.find();
    assertFalse(match);
  }
",non-flaky,5
96034,stanfordnlp_CoreNLP,TokenSequenceMatcherITest.testTokenSequenceOptimizeOrString,"  @Test
  public void testTokenSequenceOptimizeOrString() throws IOException {
    CoreMap doc = createDocument(""atropine we need to have many many words here but we don't sweating"");

    // Test simplified pattern with number
    TokenSequencePattern p = TokenSequencePattern.compile( ""(?$dt \""atropine\"") []{0,15} "" +
            ""(?$se  \""social\"" \""avoidant\"" \""behaviour\""|\""dysuria\""|\""hyperglycaemia\""| \""mental\"" \""disorder\""|\""vertigo\""|\""flutter\""| \""chest\"" \""pain\""| \""elevated\"" \""blood\"" \""pressure\""|\""mania\""| \""rash\"" \""erythematous\""|\""manic\""| \""papular\"" \""rash\""|\""death\""| \""atrial\"" \""arrhythmia\""| \""dry\"" \""eyes\""| \""loss\"" \""of\"" \""libido\""| \""rash\"" \""papular\""|\""hypersensitivity\""| \""blood\"" \""pressure\"" \""increased\""|\""dyspepsia\""| \""accommodation\"" \""disorder\""| \""reflexes\"" \""increased\""|\""lesions\""|\""asthenia\""| \""gastrointestinal\"" \""pain\""|\""excitement\""| \""breast\"" \""feeding\""|\""hypokalaemia\""| \""cerebellar\"" \""syndrome\""|\""nervousness\""| \""pulmonary\"" \""oedema\""| \""inspiratory\"" \""stridor\""| \""taste\"" \""altered\""|\""paranoia\""| \""psychotic\"" \""disorder\""| \""open\"" \""angle\"" \""glaucoma\""|\""photophobia\""| \""dry\"" \""eye\""|\""osteoarthritis\""| \""keratoconjunctivitis\"" \""sicca\""| \""haemoglobin\"" \""increased\""| \""ventricular\"" \""extrasystoles\""|\""hallucinations\""|\""conjunctivitis\""|\""paralysis\""| \""qrs\"" \""complex\""|\""anxiety\""| \""conjunctival\"" \""disorder\""|\""coma\""|\""strabismus\""|\""thirst\""|\""para\""| \""sicca\"" \""syndrome\""| \""atrioventricular\"" \""dissociation\""|\""desquamation\""|\""crusting\""| \""abdominal\"" \""distension\""|\""blindness\""|\""hypotension\""|\""dermatitis\""| \""sinus\"" \""tachycardia\""| \""abdominal\"" \""distention\""| \""lacrimation\"" \""decreased\""|\""sicca\""| \""paralytic\"" \""ileus\""| \""urinary\"" \""hesitation\""|\""withdrawn\""| \""erectile\"" \""dysfunction\""|\""keratoconjunctivitis\""|\""anaphylaxis\""| \""psychiatric\"" \""disorders\""| \""altered\"" \""taste\""|\""somnolence\""|\""extrasystoles\""|\""ageusia\""| \""intraocular\"" \""pressure\"" \""increased\""| \""left\"" \""ventricular\"" \""failure\""|\""impotence\""|\""drowsiness\""|\""conjunctiva\""| \""delayed\"" \""gastric\"" \""emptying\""| \""gastrointestinal\"" \""sounds\"" \""abnormal\""| \""qt\"" \""prolonged\""| \""supraventricular\"" \""tachycardia\""|\""weakness\""|\""hypertonia\""| \""confusional\"" \""state\""|\""anhidrosis\""|\""myopia\""|\""dyspnoea\""| \""speech\"" \""impairment\"" \""nos\""| \""rash\"" \""maculo\"" \""papular\""|\""petechiae\""|\""tachypnea\""| \""acute\"" \""angle\"" \""closure\"" \""glaucoma\""| \""gastrooesophageal\"" \""reflux\"" \""disease\""|\""hypokalemia\""| \""left\"" \""heart\"" \""failure\""| \""myocardial\"" \""infarction\""| \""site\"" \""reaction\""| \""ventricular\"" \""fibrillation\""|\""fibrillation\""| \""maculopapular\"" \""rash\""| \""impaired\"" \""gastric\"" \""emptying\""|\""amnesia\""| \""labored\"" \""respirations\""| \""decreased\"" \""lacrimation\""|\""mydriasis\""|\""headache\""| \""dry\"" \""mouth\""|\""scab\""| \""cardiac\"" \""syncope\""| \""visual\"" \""acuity\"" \""reduced\""|\""tension\""| \""blurred\"" \""vision\""| \""bloated\"" \""feeling\""| \""labored\"" \""breathing\""| \""stridor\"" \""inspiratory\""| \""skin\"" \""exfoliation\""| \""memory\"" \""loss\""|\""syncope\""| \""rash\"" \""scarlatiniform\""|\""hyperpyrexia\""| \""cardiac\"" \""flutter\""|\""heartburn\""| \""bowel\"" \""sounds\"" \""decreased\""|\""blepharitis\""|\""tachycardia\""| \""excessive\"" \""thirst\""|\""confusion\""| \""rash\"" \""macular\""| \""taste\"" \""loss\""| \""respiratory\"" \""failure\""|\""hesitancy\""|\""dysmetria\""|\""disorientation\""| \""decreased\"" \""hemoglobin\""| \""atrial\"" \""fibrillation\""| \""urinary\"" \""retention\""| \""dry\"" \""skin\""|\""dehydration\""|\""hyponatraemia\""|\""dysgeusia\""|\""disorder\""| \""increased\"" \""intraocular\"" \""pressure\""| \""speech\"" \""disorder\""| \""feeling\"" \""abnormal\""|\""pain\""| \""anaphylactic\"" \""shock\""|\""hallucination\""| \""abdominal\"" \""pain\""| \""junctional\"" \""tachycardia\""| \""bun\"" \""increased\""| \""ventricular\"" \""flutter\""| \""scarlatiniform\"" \""rash\""|\""agitation\""| \""feeling\"" \""hot\""|\""hyponatremia\""| \""decreased\"" \""bowel\"" \""sounds\""|\""cyanosis\""|\""dysarthria\""| \""heat\"" \""intolerance\""|\""hyperglycemia\""|\""reflux\""| \""angle\"" \""closure\"" \""glaucoma\""| \""electrocardiogram\"" \""qt\"" \""prolonged\""| \""vision\"" \""blurred\""| \""blood\"" \""urea\"" \""increased\""|\""dizziness\""|\""arrhythmia\""|\""erythema\""|\""vomiting\""| \""difficulty\"" \""in\"" \""micturition\""|\""infarction\""|\""laryngospasm\""|\""hypoglycaemia\""|\""hypoglycemia\""| \""elevated\"" \""hemoglobin\""| \""skin\"" \""warm\""| \""ventricular\"" \""arrhythmia\""|\""dissociation\""| \""warm\"" \""skin\""| \""follicular\"" \""conjunctivitis\""|\""urticaria\""|\""fatigue\""| \""cardiac\"" \""fibrillation\""| \""decreased\"" \""sweating\""| \""decreased\"" \""visual\"" \""acuity\""|\""lethargy\""| \""acute\"" \""angle\"" \""closure\"" \""glaucoma\""| \""nodal\"" \""rhythm\""|\""borborygmi\""|\""hyperreflexia\""| \""respiratory\"" \""depression\""|\""diarrhea\""|\""leukocytosis\""| \""speech\"" \""disturbance\""|\""ataxia\""|\""cycloplegia\""|\""tachypnoea\""|\""eczema\""| \""supraventricular\"" \""extrasystoles\""|\""ileus\""| \""cardiac\"" \""arrest\""| \""ventricular\"" \""tachycardia\""|\""laryngitis\""|\""delirium\""|\""lactation\""|\""glaucoma\""|\""obstruction\""|\""hypohidrosis\""|\""parity\""|\""palpitations\""| \""temperature\"" \""intolerance\""|\""constipation\""|\""cyclophoria\""| \""acute\"" \""coronary\"" \""syndrome\""| \""arrhythmia\"" \""supraventricular\""|\""arrest\""|\""lesion\""|\""nausea\""| \""sweating\"" \""decreased\""|\""keratitis\""|\""dyskinesia\""| \""pulmonary\"" \""function\"" \""test\"" \""decreased\""|\""stridor\""|\""swelling\""|\""dysphagia\""| \""haemoglobin\"" \""decreased\""|\""diarrhoea\""| \""ileus\"" \""paralytic\""|\""clonus\""|\""insomnia\""| \""electrocardiogram\"" \""qrs\"" \""complex\""| \""nasal\"" \""congestion\""| \""nasal\"" \""dryness\""|\""sweating\""|\""rash\""| \""nodal\"" \""arrhythmia\""|\""irritability\""|\""hyperhidrosis\""| \""ventricular\"" \""failure\"")"");

    Timing timing = new Timing();
    timing.start();
    for (int i = 0; i < 100; i++) {
      TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
      boolean match = m.find();
      assertTrue(match);
      assertEquals(""atropine we need to have many many words here but we don't sweating"", m.group(0));

      match = m.find();
      assertFalse(match);
    }
    timing.stop(""testTokenSequenceOptimizeOrString matched"");


    CoreMap docNoMatch = createDocument(""atropine we need to have many many words here but we don't, many many many words but still no match"");
    timing.start();
    for (int i = 0; i < 100; i++) {
      TokenSequenceMatcher m = p.getMatcher(docNoMatch.get(CoreAnnotations.TokensAnnotation.class));
      boolean match = m.find();
      assertFalse(match);
    }
    timing.stop(""testTokenSequenceOptimizeOrString no match"");
  }
",non-flaky,5
96035,stanfordnlp_CoreNLP,TokenSequenceMatcherITest.testMultiplePatterns,"  @Test
  public void testMultiplePatterns() throws IOException {
    TokenSequencePattern p1 = TokenSequencePattern.compile(""(?$dt \""atropine\"") []{0,15} "" +
        ""(?$se  \""social\"" \""avoidant\"" \""behaviour\""|\""dysuria\""|\""hyperglycaemia\""| \""mental\"" \""disorder\""|\""vertigo\""|\""flutter\""| \""chest\"" \""pain\""| \""elevated\"" \""blood\"" \""pressure\""|\""mania\""| \""rash\"" \""erythematous\""|\""manic\""| \""papular\"" \""rash\""|\""death\""| \""atrial\"" \""arrhythmia\""| \""dry\"" \""eyes\""| \""loss\"" \""of\"" \""libido\""| \""rash\"" \""papular\""|\""hypersensitivity\""| \""blood\"" \""pressure\"" \""increased\""|\""dyspepsia\""| \""accommodation\"" \""disorder\""| \""reflexes\"" \""increased\""|\""lesions\""|\""asthenia\""| \""gastrointestinal\"" \""pain\""|\""excitement\""| \""breast\"" \""feeding\""|\""hypokalaemia\""| \""cerebellar\"" \""syndrome\""|\""nervousness\""| \""pulmonary\"" \""oedema\""| \""inspiratory\"" \""stridor\""| \""taste\"" \""altered\""|\""paranoia\""| \""psychotic\"" \""disorder\""| \""open\"" \""angle\"" \""glaucoma\""|\""photophobia\""| \""dry\"" \""eye\""|\""osteoarthritis\""| \""keratoconjunctivitis\"" \""sicca\""| \""haemoglobin\"" \""increased\""| \""ventricular\"" \""extrasystoles\""|\""hallucinations\""|\""conjunctivitis\""|\""paralysis\""| \""qrs\"" \""complex\""|\""anxiety\""| \""conjunctival\"" \""disorder\""|\""coma\""|\""strabismus\""|\""thirst\""|\""para\""| \""sicca\"" \""syndrome\""| \""atrioventricular\"" \""dissociation\""|\""desquamation\""|\""crusting\""| \""abdominal\"" \""distension\""|\""blindness\""|\""hypotension\""|\""dermatitis\""| \""sinus\"" \""tachycardia\""| \""abdominal\"" \""distention\""| \""lacrimation\"" \""decreased\""|\""sicca\""| \""paralytic\"" \""ileus\""| \""urinary\"" \""hesitation\""|\""withdrawn\""| \""erectile\"" \""dysfunction\""|\""keratoconjunctivitis\""|\""anaphylaxis\""| \""psychiatric\"" \""disorders\""| \""altered\"" \""taste\""|\""somnolence\""|\""extrasystoles\""|\""ageusia\""| \""intraocular\"" \""pressure\"" \""increased\""| \""left\"" \""ventricular\"" \""failure\""|\""impotence\""|\""drowsiness\""|\""conjunctiva\""| \""delayed\"" \""gastric\"" \""emptying\""| \""gastrointestinal\"" \""sounds\"" \""abnormal\""| \""qt\"" \""prolonged\""| \""supraventricular\"" \""tachycardia\""|\""weakness\""|\""hypertonia\""| \""confusional\"" \""state\""|\""anhidrosis\""|\""myopia\""|\""dyspnoea\""| \""speech\"" \""impairment\"" \""nos\""| \""rash\"" \""maculo\"" \""papular\""|\""petechiae\""|\""tachypnea\""| \""acute\"" \""angle\"" \""closure\"" \""glaucoma\""| \""gastrooesophageal\"" \""reflux\"" \""disease\""|\""hypokalemia\""| \""left\"" \""heart\"" \""failure\""| \""myocardial\"" \""infarction\""| \""site\"" \""reaction\""| \""ventricular\"" \""fibrillation\""|\""fibrillation\""| \""maculopapular\"" \""rash\""| \""impaired\"" \""gastric\"" \""emptying\""|\""amnesia\""| \""labored\"" \""respirations\""| \""decreased\"" \""lacrimation\""|\""mydriasis\""|\""headache\""| \""dry\"" \""mouth\""|\""scab\""| \""cardiac\"" \""syncope\""| \""visual\"" \""acuity\"" \""reduced\""|\""tension\""| \""blurred\"" \""vision\""| \""bloated\"" \""feeling\""| \""labored\"" \""breathing\""| \""stridor\"" \""inspiratory\""| \""skin\"" \""exfoliation\""| \""memory\"" \""loss\""|\""syncope\""| \""rash\"" \""scarlatiniform\""|\""hyperpyrexia\""| \""cardiac\"" \""flutter\""|\""heartburn\""| \""bowel\"" \""sounds\"" \""decreased\""|\""blepharitis\""|\""tachycardia\""| \""excessive\"" \""thirst\""|\""confusion\""| \""rash\"" \""macular\""| \""taste\"" \""loss\""| \""respiratory\"" \""failure\""|\""hesitancy\""|\""dysmetria\""|\""disorientation\""| \""decreased\"" \""hemoglobin\""| \""atrial\"" \""fibrillation\""| \""urinary\"" \""retention\""| \""dry\"" \""skin\""|\""dehydration\""|\""hyponatraemia\""|\""dysgeusia\""|\""disorder\""| \""increased\"" \""intraocular\"" \""pressure\""| \""speech\"" \""disorder\""| \""feeling\"" \""abnormal\""|\""pain\""| \""anaphylactic\"" \""shock\""|\""hallucination\""| \""abdominal\"" \""pain\""| \""junctional\"" \""tachycardia\""| \""bun\"" \""increased\""| \""ventricular\"" \""flutter\""| \""scarlatiniform\"" \""rash\""|\""agitation\""| \""feeling\"" \""hot\""|\""hyponatremia\""| \""decreased\"" \""bowel\"" \""sounds\""|\""cyanosis\""|\""dysarthria\""| \""heat\"" \""intolerance\""|\""hyperglycemia\""|\""reflux\""| \""angle\"" \""closure\"" \""glaucoma\""| \""electrocardiogram\"" \""qt\"" \""prolonged\""| \""vision\"" \""blurred\""| \""blood\"" \""urea\"" \""increased\""|\""dizziness\""|\""arrhythmia\""|\""erythema\""|\""vomiting\""| \""difficulty\"" \""in\"" \""micturition\""|\""infarction\""|\""laryngospasm\""|\""hypoglycaemia\""|\""hypoglycemia\""| \""elevated\"" \""hemoglobin\""| \""skin\"" \""warm\""| \""ventricular\"" \""arrhythmia\""|\""dissociation\""| \""warm\"" \""skin\""| \""follicular\"" \""conjunctivitis\""|\""urticaria\""|\""fatigue\""| \""cardiac\"" \""fibrillation\""| \""decreased\"" \""sweating\""| \""decreased\"" \""visual\"" \""acuity\""|\""lethargy\""| \""acute\"" \""angle\"" \""closure\"" \""glaucoma\""| \""nodal\"" \""rhythm\""|\""borborygmi\""|\""hyperreflexia\""| \""respiratory\"" \""depression\""|\""diarrhea\""|\""leukocytosis\""| \""speech\"" \""disturbance\""|\""ataxia\""|\""cycloplegia\""|\""tachypnoea\""|\""eczema\""| \""supraventricular\"" \""extrasystoles\""|\""ileus\""| \""cardiac\"" \""arrest\""| \""ventricular\"" \""tachycardia\""|\""laryngitis\""|\""delirium\""|\""lactation\""|\""glaucoma\""|\""obstruction\""|\""hypohidrosis\""|\""parity\""|\""palpitations\""| \""temperature\"" \""intolerance\""|\""constipation\""|\""cyclophoria\""| \""acute\"" \""coronary\"" \""syndrome\""| \""arrhythmia\"" \""supraventricular\""|\""arrest\""|\""lesion\""|\""nausea\""| \""sweating\"" \""decreased\""|\""keratitis\""|\""dyskinesia\""| \""pulmonary\"" \""function\"" \""test\"" \""decreased\""|\""stridor\""|\""swelling\""|\""dysphagia\""| \""haemoglobin\"" \""decreased\""|\""diarrhoea\""| \""ileus\"" \""paralytic\""|\""clonus\""|\""insomnia\""| \""electrocardiogram\"" \""qrs\"" \""complex\""| \""nasal\"" \""congestion\""| \""nasal\"" \""dryness\""|\""sweating\""|\""rash\""| \""nodal\"" \""arrhythmia\""|\""irritability\""|\""hyperhidrosis\""| \""ventricular\"" \""failure\"")"");
    TokenSequencePattern p2 = TokenSequencePattern.compile( ""(?$dt \""disease\"") []{0,15} "" +
            ""(?$se  \""social\"" \""avoidant\"" \""behaviour\""|\""dysuria\""|\""hyperglycaemia\""| \""mental\"" \""disorder\""|\""vertigo\""|\""flutter\""| \""chest\"" \""pain\""| \""elevated\"" \""blood\"" \""pressure\""|\""mania\""| \""rash\"" \""erythematous\""|\""manic\""| \""papular\"" \""rash\""|\""death\""| \""atrial\"" \""arrhythmia\""| \""dry\"" \""eyes\""| \""loss\"" \""of\"" \""libido\""| \""rash\"" \""papular\""|\""hypersensitivity\""| \""blood\"" \""pressure\"" \""increased\""|\""dyspepsia\""| \""accommodation\"" \""disorder\""| \""reflexes\"" \""increased\""|\""lesions\""|\""asthenia\""| \""gastrointestinal\"" \""pain\""|\""excitement\""| \""breast\"" \""feeding\""|\""hypokalaemia\""| \""cerebellar\"" \""syndrome\""|\""nervousness\""| \""pulmonary\"" \""oedema\""| \""inspiratory\"" \""stridor\""| \""taste\"" \""altered\""|\""paranoia\""| \""psychotic\"" \""disorder\""| \""open\"" \""angle\"" \""glaucoma\""|\""photophobia\""| \""dry\"" \""eye\""|\""osteoarthritis\""| \""keratoconjunctivitis\"" \""sicca\""| \""haemoglobin\"" \""increased\""| \""ventricular\"" \""extrasystoles\""|\""hallucinations\""|\""conjunctivitis\""|\""paralysis\""| \""qrs\"" \""complex\""|\""anxiety\""| \""conjunctival\"" \""disorder\""|\""coma\""|\""strabismus\""|\""thirst\""|\""para\""| \""sicca\"" \""syndrome\""| \""atrioventricular\"" \""dissociation\""|\""desquamation\""|\""crusting\""| \""abdominal\"" \""distension\""|\""blindness\""|\""hypotension\""|\""dermatitis\""| \""sinus\"" \""tachycardia\""| \""abdominal\"" \""distention\""| \""lacrimation\"" \""decreased\""|\""sicca\""| \""paralytic\"" \""ileus\""| \""urinary\"" \""hesitation\""|\""withdrawn\""| \""erectile\"" \""dysfunction\""|\""keratoconjunctivitis\""|\""anaphylaxis\""| \""psychiatric\"" \""disorders\""| \""altered\"" \""taste\""|\""somnolence\""|\""extrasystoles\""|\""ageusia\""| \""intraocular\"" \""pressure\"" \""increased\""| \""left\"" \""ventricular\"" \""failure\""|\""impotence\""|\""drowsiness\""|\""conjunctiva\""| \""delayed\"" \""gastric\"" \""emptying\""| \""gastrointestinal\"" \""sounds\"" \""abnormal\""| \""qt\"" \""prolonged\""| \""supraventricular\"" \""tachycardia\""|\""weakness\""|\""hypertonia\""| \""confusional\"" \""state\""|\""anhidrosis\""|\""myopia\""|\""dyspnoea\""| \""speech\"" \""impairment\"" \""nos\""| \""rash\"" \""maculo\"" \""papular\""|\""petechiae\""|\""tachypnea\""| \""acute\"" \""angle\"" \""closure\"" \""glaucoma\""| \""gastrooesophageal\"" \""reflux\"" \""disease\""|\""hypokalemia\""| \""left\"" \""heart\"" \""failure\""| \""myocardial\"" \""infarction\""| \""site\"" \""reaction\""| \""ventricular\"" \""fibrillation\""|\""fibrillation\""| \""maculopapular\"" \""rash\""| \""impaired\"" \""gastric\"" \""emptying\""|\""amnesia\""| \""labored\"" \""respirations\""| \""decreased\"" \""lacrimation\""|\""mydriasis\""|\""headache\""| \""dry\"" \""mouth\""|\""scab\""| \""cardiac\"" \""syncope\""| \""visual\"" \""acuity\"" \""reduced\""|\""tension\""| \""blurred\"" \""vision\""| \""bloated\"" \""feeling\""| \""labored\"" \""breathing\""| \""stridor\"" \""inspiratory\""| \""skin\"" \""exfoliation\""| \""memory\"" \""loss\""|\""syncope\""| \""rash\"" \""scarlatiniform\""|\""hyperpyrexia\""| \""cardiac\"" \""flutter\""|\""heartburn\""| \""bowel\"" \""sounds\"" \""decreased\""|\""blepharitis\""|\""tachycardia\""| \""excessive\"" \""thirst\""|\""confusion\""| \""rash\"" \""macular\""| \""taste\"" \""loss\""| \""respiratory\"" \""failure\""|\""hesitancy\""|\""dysmetria\""|\""disorientation\""| \""decreased\"" \""hemoglobin\""| \""atrial\"" \""fibrillation\""| \""urinary\"" \""retention\""| \""dry\"" \""skin\""|\""dehydration\""|\""hyponatraemia\""|\""dysgeusia\""|\""disorder\""| \""increased\"" \""intraocular\"" \""pressure\""| \""speech\"" \""disorder\""| \""feeling\"" \""abnormal\""|\""pain\""| \""anaphylactic\"" \""shock\""|\""hallucination\""| \""abdominal\"" \""pain\""| \""junctional\"" \""tachycardia\""| \""bun\"" \""increased\""| \""ventricular\"" \""flutter\""| \""scarlatiniform\"" \""rash\""|\""agitation\""| \""feeling\"" \""hot\""|\""hyponatremia\""| \""decreased\"" \""bowel\"" \""sounds\""|\""cyanosis\""|\""dysarthria\""| \""heat\"" \""intolerance\""|\""hyperglycemia\""|\""reflux\""| \""angle\"" \""closure\"" \""glaucoma\""| \""electrocardiogram\"" \""qt\"" \""prolonged\""| \""vision\"" \""blurred\""| \""blood\"" \""urea\"" \""increased\""|\""dizziness\""|\""arrhythmia\""|\""erythema\""|\""vomiting\""| \""difficulty\"" \""in\"" \""micturition\""|\""infarction\""|\""laryngospasm\""|\""hypoglycaemia\""|\""hypoglycemia\""| \""elevated\"" \""hemoglobin\""| \""skin\"" \""warm\""| \""ventricular\"" \""arrhythmia\""|\""dissociation\""| \""warm\"" \""skin\""| \""follicular\"" \""conjunctivitis\""|\""urticaria\""|\""fatigue\""| \""cardiac\"" \""fibrillation\""| \""decreased\"" \""sweating\""| \""decreased\"" \""visual\"" \""acuity\""|\""lethargy\""| \""acute\"" \""angle\"" \""closure\"" \""glaucoma\""| \""nodal\"" \""rhythm\""|\""borborygmi\""|\""hyperreflexia\""| \""respiratory\"" \""depression\""|\""diarrhea\""|\""leukocytosis\""| \""speech\"" \""disturbance\""|\""ataxia\""|\""cycloplegia\""|\""tachypnoea\""|\""eczema\""| \""supraventricular\"" \""extrasystoles\""|\""ileus\""| \""cardiac\"" \""arrest\""| \""ventricular\"" \""tachycardia\""|\""laryngitis\""|\""delirium\""|\""lactation\""|\""glaucoma\""|\""obstruction\""|\""hypohidrosis\""|\""parity\""|\""palpitations\""| \""temperature\"" \""intolerance\""|\""constipation\""|\""cyclophoria\""| \""acute\"" \""coronary\"" \""syndrome\""| \""arrhythmia\"" \""supraventricular\""|\""arrest\""|\""lesion\""|\""nausea\""| \""sweating\"" \""decreased\""|\""keratitis\""|\""dyskinesia\""| \""pulmonary\"" \""function\"" \""test\"" \""decreased\""|\""stridor\""|\""swelling\""|\""dysphagia\""| \""haemoglobin\"" \""decreased\""|\""diarrhoea\""| \""ileus\"" \""paralytic\""|\""clonus\""|\""insomnia\""| \""electrocardiogram\"" \""qrs\"" \""complex\""| \""nasal\"" \""congestion\""| \""nasal\"" \""dryness\""|\""sweating\""|\""rash\""| \""nodal\"" \""arrhythmia\""|\""irritability\""|\""hyperhidrosis\""| \""ventricular\"" \""failure\"")"");
    CoreMap doc = createDocument(""atropine we need to have many many words here but we don't sweating"");
    MultiPatternMatcher<CoreMap> multiPatternMatcher = TokenSequencePattern.getMultiPatternMatcher(p1, p2);
    List<String> expected = new ArrayList<String>();
    expected.add(""atropine we need to have many many words here but we don't sweating"");
    Iterator<String> expectedIter = expected.iterator();

    Iterable<SequenceMatchResult<CoreMap>> matches =
            multiPatternMatcher.findAllNonOverlappingMatchesPerPattern(doc.get(CoreAnnotations.TokensAnnotation.class));
    for (SequenceMatchResult<CoreMap> match:matches) {
     assertEquals(expectedIter.next(), match.group());
    }
    assertFalse(expectedIter.hasNext());
  }
",non-flaky,5
96036,stanfordnlp_CoreNLP,TokenSequenceMatcherITest.testTokenSequenceMatcherPosNNP,"  @Test
  public void testTokenSequenceMatcherPosNNP() throws IOException {
    CoreMap doc = createDocument(testText1);

    // Test sequence with groups
    TokenSequencePattern p = TokenSequencePattern.compile( ""[ { tag:\""NNP\"" } ]+"");
    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    boolean match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""Mellitus"", m.group());

    p = TokenSequencePattern.compile( ""[ { tag:\""NNP\"" } ] [ /is|was/ ] []*? [ { tag:\""NNP\"" } ]+ "");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""Mellitus was the first Bishop"", m.group());

    TokenSequencePattern nnpPattern = TokenSequencePattern.compile( ""[ { tag:\""NNP\"" } ]"" );
    Env env = TokenSequencePattern.getNewEnv();
    env.bind(""$NNP"", nnpPattern);
    p = TokenSequencePattern.compile(env, "" $NNP [ /is|was/ ] []*? $NNP+ [ \""of\"" ] $NNP+ "");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""Mellitus was the first Bishop of London"", m.group());

    p = TokenSequencePattern.compile(env, "" ($NNP) /is|was/ []*? ($NNP)+ \""of\"" ($NNP)+ "");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    assertEquals(""Mellitus was the first Bishop of London"", m.group());
    assertEquals(""Mellitus"", m.group(1));
    assertEquals(""Bishop"", m.group(2));
    assertEquals(""London"", m.group(3));


    nnpPattern = TokenSequencePattern.compile( "" ( [ { tag:\""NNP\"" } ] )"" );
    env.bind(""$NNP"", nnpPattern);
    p = TokenSequencePattern.compile(env, "" $NNP /is|was/ []*? $NNP+ \""of\"" $NNP+ "");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    assertEquals(""Mellitus was the first Bishop of London"", m.group());
    assertEquals(""Mellitus"", m.group(1));
    assertEquals(""Bishop"", m.group(2));
    assertEquals(""London"", m.group(3));


    // Same as above but without extra ""{}""
    nnpPattern = TokenSequencePattern.compile( "" ( [ tag:\""NNP\"" ] )"" );
    env.bind(""$NNP"", nnpPattern);
    p = TokenSequencePattern.compile(env, "" $NNP /is|was/ []*? $NNP+ \""of\"" $NNP+ "");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    assertEquals(""Mellitus was the first Bishop of London"", m.group());
    assertEquals(""Mellitus"", m.group(1));
    assertEquals(""Bishop"", m.group(2));
    assertEquals(""London"", m.group(3));

    // Same as above but using ""pos""
    nnpPattern = TokenSequencePattern.compile( "" ( [ pos:\""NNP\"" ] )"" );
    env.bind(""$NNP"", nnpPattern);
    p = TokenSequencePattern.compile(env, "" $NNP /is|was/ []*? $NNP+ \""of\"" $NNP+ "");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(3, m.groupCount());
    assertEquals(""Mellitus was the first Bishop of London"", m.group());
    assertEquals(""Mellitus"", m.group(1));
    assertEquals(""Bishop"", m.group(2));
    assertEquals(""London"", m.group(3));
  }
",non-flaky,5
96037,stanfordnlp_CoreNLP,TokenSequenceMatcherITest.testTokenSequenceMatcherNumber,"  @Test
  public void testTokenSequenceMatcherNumber() throws IOException {
    CoreMap doc = createDocument(""It happened on January 3, 2002"");

    // Test sequence with groups
    TokenSequencePattern p = TokenSequencePattern.compile( ""[ { word::IS_NUM } ]+"");
    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    boolean match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""3"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""2002"", m.group());
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile( ""[ { word>=2002 } ]+"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""2002"", m.group());
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile( ""[ { word>2002 } ]+"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertFalse(match);

    // Check no {} with or
    p = TokenSequencePattern.compile( ""[ word > 2002 | word==2002 ]+"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""2002"", m.group());
    match = m.find();
    assertFalse(match);

    // Check no {} with and
    p = TokenSequencePattern.compile( ""[ word>2002 & word==2002 ]+"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile( ""[ { word>2000 } ]+"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""2002"", m.group());
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile( ""[ { word<=2002 } ]+"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""3"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""2002"", m.group());
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile( ""[ { word<2002 } ]+"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""3"", m.group());
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile( ""[ { word==2002 } ]+"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""2002"", m.group());
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile( ""[ { ner:DATE } ]+"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""January 3, 2002"", m.group());
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile( ""[ { ner::NOT_NIL } ]+"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""January 3, 2002"", m.group());
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile( ""[ { ner::IS_NIL } ]+"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""It happened on"", m.group());
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile( ""[ {{ word=~/2002/ }} ]+"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""2002"", m.group());
    match = m.find();
    assertFalse(match);
  }
",non-flaky,5
96038,stanfordnlp_CoreNLP,TokenSequenceMatcherITest.testTokenSequenceMatcherNested,"  @Test
  public void testTokenSequenceMatcherNested() throws IOException {
    CoreMap doc = createDocument(""A A A B B B B B B C C"");

    // Test sequence with groups
    TokenSequencePattern p = TokenSequencePattern.compile( ""( /B/+ )+"");
    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    boolean match = m.find();
    assertTrue(match);
    assertEquals(1, m.groupCount());
    assertEquals(""B B B B B B"", m.group());
    assertEquals(""B B B B B B"", m.group(1));
    match = m.find();
    assertFalse(match);
  }
",non-flaky,5
96039,stanfordnlp_CoreNLP,TokenSequenceMatcherITest.testTokenSequenceMatcherAAs,"  @Test
  public void testTokenSequenceMatcherAAs() throws IOException {
    StringBuilder s = new StringBuilder();
 //   Timing timing = new Timing();
    for (int i = 1; i <= 10; i++) {
      s.append(""A "");
      CoreMap doc = createDocument(s.toString());
      TokenSequencePattern p = TokenSequencePattern.compile(""(A?)"" + ""{"" + i + ""} "" + ""A"" + ""{"" + i + ""}"");
//      TokenSequencePattern p = TokenSequencePattern.compile( ""(A?)"" + ""{"" + i + ""}"");
      TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
//      timing.start();
      boolean match = m.matches();
      assertTrue(match);
//      timing.stop(""matched: "" + match + "" "" + i);
    }
  }
",non-flaky,5
96040,stanfordnlp_CoreNLP,TokenSequenceMatcherITest.testTokenSequenceFindsWildcard,"  @Test
  public void testTokenSequenceFindsWildcard() throws IOException {
    CoreMap doc = createDocument(""word1 word2"");

    // Test sequence with groups
    TokenSequencePattern p = TokenSequencePattern.compile( ""[]{2}|[]"");
    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    boolean match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""word1 word2"", m.group());
    match = m.find();
    assertFalse(match);

    // Reverse order
    p = TokenSequencePattern.compile( ""[]|[]{2}"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""word1 word2"", m.group());
    match = m.find();
    assertFalse(match);

    // Using {1,2}
    p = TokenSequencePattern.compile( ""[]{2}"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""word1 word2"", m.group());
    match = m.find();
    assertFalse(match);
  }
",non-flaky,5
96041,stanfordnlp_CoreNLP,TokenSequenceMatcherITest.testTokenSequenceMatchesWildcard,"  @Test
  public void testTokenSequenceMatchesWildcard() throws IOException {
    CoreMap doc = createDocument(""word1 word2"");

    // Test sequence with groups
    TokenSequencePattern p = TokenSequencePattern.compile( ""[]{2}|[]"");
    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    boolean matches = m.matches();
    assertTrue(matches);

    // Reverse order
    p = TokenSequencePattern.compile( ""[]|[]{2}"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    matches = m.matches();
    assertTrue(matches);

    // Using {1,2}
    p = TokenSequencePattern.compile( ""[]{1,2}"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    matches = m.matches();
    assertTrue(matches);
  }
",non-flaky,5
96042,stanfordnlp_CoreNLP,TokenSequenceMatcherITest.testTokenSequenceMatcherABs,"  @Test
  public void testTokenSequenceMatcherABs() throws IOException {
    CoreMap doc = createDocument(""A A A A A A A B A A B A C A E A A A A A A A A A A A B A A A"");

    // Test sequence with groups
    TokenSequencePattern p = TokenSequencePattern.compile( ""/A/+ B"");
    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    boolean match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""A A A A A A A B"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""A A B"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""A A A A A A A A A A A B"", m.group());
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile( ""(/A/+ B)+"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(1, m.groupCount());
    assertEquals(""A A A A A A A B A A B"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(1, m.groupCount());
    assertEquals(""A A A A A A A A A A A B"", m.group());
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile( ""( A+ ( /B/+ )? )*"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""A A A A A A A B A A B A"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""A"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(2, m.groupCount());
    assertEquals(""A A A A A A A A A A A B A A A"", m.group());
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile( ""(/A/+ /B/+ )+"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(1, m.groupCount());
    assertEquals(""A A A A A A A B A A B"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(1, m.groupCount());
    assertEquals(""A A A A A A A A A A A B"", m.group());
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile( ""(/A/+ /C/? /A/* )+"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(1, m.groupCount());
    assertEquals(""A A A A A A A"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(1, m.groupCount());
    assertEquals(""A A"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(1, m.groupCount());
    assertEquals(""A C A"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(1, m.groupCount());
    assertEquals(""A A A A A A A A A A A"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(1, m.groupCount());
    assertEquals(""A A A"", m.group());
    match = m.find();
    assertFalse(match);
  }
",non-flaky,5
96043,stanfordnlp_CoreNLP,TokenSequenceMatcherITest.testTokenSequenceMatcherMultiNodePattern,"  @Test
  public void testTokenSequenceMatcherMultiNodePattern() throws IOException {
    CoreMap doc = createDocument(""blah four-years blah blah four - years"");

    // Test sequence with groups
    CoreMapNodePattern nodePattern  = CoreMapNodePattern.valueOf(""four\\s*-?\\s*years"");
    SequencePattern.MultiNodePatternExpr expr = new SequencePattern.MultiNodePatternExpr(
            new MultiCoreMapNodePattern(nodePattern));
    TokenSequencePattern p = TokenSequencePattern.compile(expr);
    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    boolean match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""four-years"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""four - years"", m.group());
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile(""(?m) /four\\s*-?\\s*years/"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""four-years"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""four - years"", m.group());
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile(""(?m){2,3} /four\\s*-?\\s*years/"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""four - years"", m.group());
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile( ""(?m){1,2} /four\\s*-?\\s*years/"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""four-years"", m.group());
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile(""(?m){1,3} /four\\s*-?\\s*years/ ==> &annotate( { ner=YEAR } )"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""four-years"", m.group());
    p.getAction().apply(m, 0);
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""four - years"", m.group());
    SequenceMatchResult<CoreMap> res = p.getAction().apply(m, 0);
    match = m.find();
    assertFalse(match);

    p = TokenSequencePattern.compile( ""[ { ner:YEAR } ]+"");
    m = p.getMatcher(res.elements());
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""four-years"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(0, m.groupCount());
    assertEquals(""four - years"", m.group());
    match = m.find();
    assertFalse(match);
  }
",non-flaky,5
96044,stanfordnlp_CoreNLP,TokenSequenceMatcherITest.testTokenSequenceMatcherMultiNodePattern2,"  @Test
  public void testTokenSequenceMatcherMultiNodePattern2() throws IOException {
    CoreMap doc = createDocument(""Replace the lamp with model wss.32dc55c3e945384dbc5e533ab711fd24"");

    // Greedy
    TokenSequencePattern p = TokenSequencePattern.compile(""/model/ ((?m){1,4}/\\w+\\.\\w+/)"");
    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    boolean match = m.find();
    assertTrue(match);
    assertEquals(1, m.groupCount());
    assertEquals(""model wss.32dc55c3e945384dbc5e533ab711fd24"", m.group());
    assertEquals(""wss.32dc55c3e945384dbc5e533ab711fd24"", m.group(1));
    match = m.find();
    assertFalse(match);

    // Reluctant
    p = TokenSequencePattern.compile(""/model/ ((?m){1,4}?/\\w+\\.\\w+/)"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    match = m.find();
    assertTrue(match);
    assertEquals(1, m.groupCount());
    assertEquals(""model wss.32"", m.group());
    assertEquals(""wss.32"", m.group(1));
    match = m.find();
    assertFalse(match);
  }
",non-flaky,5
96045,stanfordnlp_CoreNLP,TokenSequenceMatcherITest.testTokenSequenceMatcherBackRef,"  @Test
  public void testTokenSequenceMatcherBackRef() throws IOException {
    CoreMap doc = createDocument(""A A A A A A A B A A B A C A E A A A A A A A A A A A B A A A"");

    // Test sequence with groups
    TokenSequencePattern p = TokenSequencePattern.compile( ""(/A/+) B \\1"");
    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    boolean match = m.find();
    assertTrue(match);
    assertEquals(1, m.groupCount());
    assertEquals(""A A B A A"", m.group());
    match = m.find();
    assertTrue(match);
    assertEquals(1, m.groupCount());
    assertEquals(""A A A B A A A"", m.group());
    match = m.find();
    assertFalse(match);

  }
",non-flaky,5
96046,stanfordnlp_CoreNLP,TokenSequenceMatcherITest.testMultiPatternMatcher,"  @Test
  public void testMultiPatternMatcher() throws IOException {
    CoreMap doc = createDocument(testText1);

    // Test simple sequence
    TokenSequencePattern p1 = TokenSequencePattern.compile(""/Archbishop/ /of/ /Canterbury/"");
    p1.setPriority(1);
    TokenSequencePattern p2 = TokenSequencePattern.compile(""/[a-zA-Z]+/{1,2}  /of/ /[a-zA-Z]+/+"");
    MultiPatternMatcher<CoreMap> m = new MultiPatternMatcher<CoreMap>(p2,p1);
    List<SequenceMatchResult<CoreMap>> matched = m.findNonOverlapping(doc.get(CoreAnnotations.TokensAnnotation.class));
    assertEquals(4, matched.size());
    assertEquals(""first Bishop of London"", matched.get(0).group());
    assertEquals(""Archbishop of Canterbury"", matched.get(1).group());
    assertEquals(""a member of the Gregorian mission sent to England to convert the"", matched.get(2).group());
    assertEquals(""as Bishop of London in"", matched.get(3).group());
  }
",non-flaky,5
96047,stanfordnlp_CoreNLP,TokenSequenceMatcherITest.testStringPatternMatchCaseInsensitive,"  @Test
  public void testStringPatternMatchCaseInsensitive() throws IOException {
    CoreMap doc = createDocument(testText1);

    // Test simple sequence
    Env env = TokenSequencePattern.getNewEnv();
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE);
    TokenSequencePattern p = TokenSequencePattern.compile(env, ""/archbishop/ /of/ /canterbury/"");
    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    assertTrue(m.find());
    assertEquals(""Archbishop of Canterbury"", m.group());
    assertFalse(m.find());

    p = TokenSequencePattern.compile(env, ""/ARCHBISHOP/ /OF/ /CANTERBURY/"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    assertTrue(m.find());
    assertEquals(""Archbishop of Canterbury"", m.group());
    assertFalse(m.find());
  }
",non-flaky,5
96048,stanfordnlp_CoreNLP,TokenSequenceMatcherITest.testStringMatchCaseInsensitive,"  @Test
  public void testStringMatchCaseInsensitive() throws IOException {
    CoreMap doc = createDocument(testText1);

    // Test simple sequence
    Env env = TokenSequencePattern.getNewEnv();
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE);
    TokenSequencePattern p = TokenSequencePattern.compile(env, ""archbishop of canterbury"");
    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    assertTrue(m.find());
    assertEquals(""Archbishop of Canterbury"", m.group());
    assertFalse(m.find());

    p = TokenSequencePattern.compile(env, ""ARCHBISHOP OF CANTERBURY"");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    assertTrue(m.find());
    assertEquals(""Archbishop of Canterbury"", m.group());
    assertFalse(m.find());
  }
",non-flaky,5
96049,stanfordnlp_CoreNLP,TokenSequenceMatcherITest.testCompile,"  @Test
  public void testCompile() {
    String s = ""(?$se \""matching\"" \""this\""|\""don't\"")"";
    CoreMap doc = createDocument(""does this do matching this"");
    TokenSequencePattern p = TokenSequencePattern.compile(s);
    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    boolean match = m.find();
    assertTrue(match);
    //assertEquals(m.group(), ""matching this"");
  }
",non-flaky,5
96050,stanfordnlp_CoreNLP,TokenSequenceMatcherITest.testBindingCompile,"  @Test
  public void testBindingCompile(){
    Env env = TokenSequencePattern.getNewEnv();
    env.bind(""wordname"",CoreAnnotations.TextAnnotation.class);
    String s = ""[wordname:\""name\""]{1,2}"";
    TokenSequencePattern p = TokenSequencePattern.compile(env, s);
  }
",non-flaky,5
96051,stanfordnlp_CoreNLP,TokenSequenceMatcherITest.testNoBindingCompile,"//  @Test
//  public void testNoBindingCompile(){
//    Env env = TokenSequencePattern.getNewEnv();
//    String s = ""["" + CoreAnnotations.TextAnnotation.class.getName()+"":\""name\""]{1,2}"";
//    TokenSequencePattern p = TokenSequencePattern.compile(env, s);
//  }
",non-flaky,5
96052,stanfordnlp_CoreNLP,TokenSequenceMatcherITest.testCaseInsensitive1,"  @Test
  public void testCaseInsensitive1(){
    Env env = TokenSequencePattern.getNewEnv();
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE);
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE);
    String s = ""for /President/"";
    CoreMap doc = createDocument(""for president"");
    TokenSequencePattern p = TokenSequencePattern.compile(env, s);
    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    boolean match = m.find();
    assertTrue(match);
  }
",non-flaky,5
96053,stanfordnlp_CoreNLP,TokenSequenceMatcherITest.testCaseInsensitive2,"  @Test
  public void testCaseInsensitive2(){
    Env env = TokenSequencePattern.getNewEnv();
    env.setDefaultStringPatternFlags(Pattern.CASE_INSENSITIVE);
    env.setDefaultStringMatchFlags(NodePattern.CASE_INSENSITIVE);

    String s = ""for president"";
    CoreMap doc = createDocument(""for President"");

    TokenSequencePattern p = TokenSequencePattern.compile(env, s);
    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));
    boolean match = m.find();
    assertTrue(match);
  }
",non-flaky,5
96054,stanfordnlp_CoreNLP,TSVSentenceIteratorITest.testOnlyGloss,"  // @Test
  public void testOnlyGloss() {
    List<List<String>> entries = Collections.singletonList(
            Arrays.asList(""124"", ""docid1"", ""1"", ""This is a test document.""));

    TSVSentenceIterator it = new TSVSentenceIterator(entries.iterator(),
            Arrays.asList(SentenceField.ID, SentenceField.DOC_ID, SentenceField.SENTENCE_INDEX, SentenceField.GLOSS));
    Sentence sentence = it.next();
    Assert.assertEquals(1, sentence.sentenceIndex());
    Assert.assertEquals(""This is a test document.""  , sentence.text());
    Assert.assertEquals(""docid1"", sentence.asCoreMap().get(CoreAnnotations.DocIDAnnotation.class));
    Assert.assertEquals(""124"", sentence.asCoreMap().get(CoreAnnotations.SentenceIDAnnotation.class));
  }
",non-flaky,5
96055,stanfordnlp_CoreNLP,TSVSentenceIteratorITest.testFullTokens,"  @Test
  public void testFullTokens() {
    List<List<String>> entries = Collections.singletonList(
            Arrays.asList(
                    ""3424"",
                    ""d2-s1-a1"",
                    ""0"",
                    ""{Chess,is,not,a,predominantly,physical,sport,\""\"",\""\"",yet,neither,are,shooting,and,curling,-LRB-,which,\""\"",\""\"",in,fact,\""\"",\""\"",has,been,nicknamed,``,chess,on,ice,'',5,),.}"",
                    ""{chess,be,not,a,predominantly,physical,sport,\""\"",\""\"",yet,neither,be,shooting,and,curling,-lrb-,which,\""\"",\""\"",in,fact,\""\"",\""\"",have,be,nickname,``,chess,on,ice,'',5,),.}"",
                    ""{NN,VBZ,RB,DT,RB,JJ,NN,\""\"",\""\"",RB,DT,VBP,JJ,CC,NN,-LRB-,WDT,\""\"",\""\"",IN,NN,\""\"",\""\"",VBZ,VBN,VBN,``,NN,IN,NN,'',LS,-RRB-,.}"",
                    ""{O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,NUMBER,O,O}"",
                    ""{0,6,9,13,15,29,38,43,45,49,57,61,70,74,82,83,88,90,93,97,99,103,108,118,119,125,128,131,132,133,134}"",
                    ""{5,8,12,14,28,37,43,44,48,56,60,69,73,81,83,88,89,92,97,98,102,107,117,119,124,127,131,132,133,134,135}""	,
                    //""[{\""\""dependent\""\"": 7, \""\""dep\""\"": \""\""ROOT\""\"", \""\""governorGloss\""\"": \""\""ROOT\""\"", \""\""governor\""\"": 0, \""\""dependentGloss\""\"": \""\""sport\""\""}, {\""\""dependent\""\"": 1, \""\""dep\""\"": \""\""nsubj\""\"", \""\""governorGloss\""\"": \""\""sport\""\"", \""\""governor\""\"": 7, \""\""dependentGloss\""\"": \""\""Chess\""\""}, {\""\""dependent\""\"": 2, \""\""dep\""\"": \""\""cop\""\"", \""\""governorGloss\""\"": \""\""sport\""\"", \""\""governor\""\"": 7, \""\""dependentGloss\""\"": \""\""is\""\""}, {\""\""dependent\""\"": 3, \""\""dep\""\"": \""\""neg\""\"", \""\""governorGloss\""\"": \""\""sport\""\"", \""\""governor\""\"": 7, \""\""dependentGloss\""\"": \""\""not\""\""}, {\""\""dependent\""\"": 4, \""\""dep\""\"": \""\""det\""\"", \""\""governorGloss\""\"": \""\""sport\""\"", \""\""governor\""\"": 7, \""\""dependentGloss\""\"": \""\""a\""\""}, {\""\""dependent\""\"": 5, \""\""dep\""\"": \""\""advmod\""\"", \""\""governorGloss\""\"": \""\""physical\""\"", \""\""governor\""\"": 6, \""\""dependentGloss\""\"": \""\""predominantly\""\""}, {\""\""dependent\""\"": 6, \""\""dep\""\"": \""\""amod\""\"", \""\""governorGloss\""\"": \""\""sport\""\"", \""\""governor\""\"": 7, \""\""dependentGloss\""\"": \""\""physical\""\""}, {\""\""dependent\""\"": 9, \""\""dep\""\"": \""\""advmod\""\"", \""\""governorGloss\""\"": \""\""sport\""\"", \""\""governor\""\"": 7, \""\""dependentGloss\""\"": \""\""yet\""\""}, {\""\""dependent\""\"": 10, \""\""dep\""\"": \""\""nsubj\""\"", \""\""governorGloss\""\"": \""\""shooting\""\"", \""\""governor\""\"": 12, \""\""dependentGloss\""\"": \""\""neither\""\""}, {\""\""dependent\""\"": 11, \""\""dep\""\"": \""\""cop\""\"", \""\""governorGloss\""\"": \""\""shooting\""\"", \""\""governor\""\"": 12, \""\""dependentGloss\""\"": \""\""are\""\""}, {\""\""dependent\""\"": 12, \""\""dep\""\"": \""\""parataxis\""\"", \""\""governorGloss\""\"": \""\""sport\""\"", \""\""governor\""\"": 7, \""\""dependentGloss\""\"": \""\""shooting\""\""}, {\""\""dependent\""\"": 13, \""\""dep\""\"": \""\""cc\""\"", \""\""governorGloss\""\"": \""\""shooting\""\"", \""\""governor\""\"": 12, \""\""dependentGloss\""\"": \""\""and\""\""}, {\""\""dependent\""\"": 14, \""\""dep\""\"": \""\""parataxis\""\"", \""\""governorGloss\""\"": \""\""sport\""\"", \""\""governor\""\"": 7, \""\""dependentGloss\""\"": \""\""curling\""\""}, {\""\""dependent\""\"": 14, \""\""dep\""\"": \""\""conj:and\""\"", \""\""governorGloss\""\"": \""\""shooting\""\"", \""\""governor\""\"": 12, \""\""dependentGloss\""\"": \""\""curling\""\""}, {\""\""dependent\""\"": 16, \""\""dep\""\"": \""\""nsubjpass\""\"", \""\""governorGloss\""\"": \""\""nicknamed\""\"", \""\""governor\""\"": 23, \""\""dependentGloss\""\"": \""\""which\""\""}, {\""\""dependent\""\"": 18, \""\""dep\""\"": \""\""case\""\"", \""\""governorGloss\""\"": \""\""fact\""\"", \""\""governor\""\"": 19, \""\""dependentGloss\""\"": \""\""in\""\""}, {\""\""dependent\""\"": 19, \""\""dep\""\"": \""\""nmod:in\""\"", \""\""governorGloss\""\"": \""\""nicknamed\""\"", \""\""governor\""\"": 23, \""\""dependentGloss\""\"": \""\""fact\""\""}, {\""\""dependent\""\"": 21, \""\""dep\""\"": \""\""aux\""\"", \""\""governorGloss\""\"": \""\""nicknamed\""\"", \""\""governor\""\"": 23, \""\""dependentGloss\""\"": \""\""has\""\""}, {\""\""dependent\""\"": 22, \""\""dep\""\"": \""\""auxpass\""\"", \""\""governorGloss\""\"": \""\""nicknamed\""\"", \""\""governor\""\"": 23, \""\""dependentGloss\""\"": \""\""been\""\""}, {\""\""dependent\""\"": 23, \""\""dep\""\"": \""\""dep\""\"", \""\""governorGloss\""\"": \""\""shooting\""\"", \""\""governor\""\"": 12, \""\""dependentGloss\""\"": \""\""nicknamed\""\""}, {\""\""dependent\""\"": 25, \""\""dep\""\"": \""\""dobj\""\"", \""\""governorGloss\""\"": \""\""nicknamed\""\"", \""\""governor\""\"": 23, \""\""dependentGloss\""\"": \""\""chess\""\""}, {\""\""dependent\""\"": 26, \""\""dep\""\"": \""\""case\""\"", \""\""governorGloss\""\"": \""\""ice\""\"", \""\""governor\""\"": 27, \""\""dependentGloss\""\"": \""\""on\""\""}, {\""\""dependent\""\"": 27, \""\""dep\""\"": \""\""nmod:on\""\"", \""\""governorGloss\""\"": \""\""chess\""\"", \""\""governor\""\"": 25, \""\""dependentGloss\""\"": \""\""ice\""\""}, {\""\""dependent\""\"": 29, \""\""dep\""\"": \""\""amod\""\"", \""\""governorGloss\""\"": \""\""chess\""\"", \""\""governor\""\"": 25, \""\""dependentGloss\""\"": \""\""5\""\""}]"",
                    ""Chess is not a predominantly physical sport, yet neither are shooting and curling (which, in fact, has been nicknamed âchess on iceâ5).""
            ));

    TSVSentenceIterator it = new TSVSentenceIterator(entries.iterator(), Arrays.asList(
            SentenceField.ID,
            SentenceField.DOC_ID,
            SentenceField.SENTENCE_INDEX,
            SentenceField.WORDS,
            SentenceField.LEMMAS,
            SentenceField.POS_TAGS,
            SentenceField.NER_TAGS,
            SentenceField.DOC_CHAR_BEGIN,
            SentenceField.DOC_CHAR_END,
            SentenceField.GLOSS
    ));

    Sentence sentence = it.next();
    Assert.assertEquals(""3424"", sentence.sentenceid().orElse(""-1""));
    Assert.assertEquals(""d2-s1-a1"", sentence.document.docid().orElse(""???""));
    Assert.assertEquals(0, sentence.sentenceIndex());
    Assert.assertEquals(""Chess is not a predominantly physical sport, yet neither are shooting and curling (which, in fact, has been nicknamed âchess on iceâ5)."" , sentence.text());
    Assert.assertArrayEquals(new String[]{
            ""Chess"",""is"",""not"",""a"",""predominantly"",""physical"",""sport"","","",""yet"",""neither"",""are"",""shooting"",""and"",""curling"",""-LRB-"",""which"","","",""in"",""fact"","","",""has"",""been"",""nicknamed"",""``"",""chess"",""on"",""ice"",""''"",""5"","")"","".""
    }, sentence.words().toArray());
    Assert.assertArrayEquals(new String[]{
            ""chess"",""be"",""not"",""a"",""predominantly"",""physical"",""sport"","","",""yet"",""neither"",""be"",""shooting"",""and"",""curling"",""-lrb-"",""which"","","",""in"",""fact"","","",""have"",""be"",""nickname"",""``"",""chess"",""on"",""ice"",""''"",""5"","")"","".""
    }, sentence.lemmas().toArray());
    Assert.assertArrayEquals(new String[]{
            ""NN"",""VBZ"",""RB"",""DT"",""RB"",""JJ"",""NN"","","",""RB"",""DT"",""VBP"",""JJ"",""CC"",""NN"",""-LRB-"",""WDT"","","",""IN"",""NN"","","",""VBZ"",""VBN"",""VBN"",""``"",""NN"",""IN"",""NN"",""''"",""LS"",""-RRB-"","".""
    }, sentence.posTags().toArray());
    Assert.assertArrayEquals(new String[]{
            ""O"",""O"",""O"",""O"",""O"",""O"",""O"",""O"",""O"",""O"",""O"",""O"",""O"",""O"",""O"",""O"",""O"",""O"",""O"",""O"",""O"",""O"",""O"",""O"",""O"",""O"",""O"",""O"",""NUMBER"",""O"",""O""
    }, sentence.nerTags().toArray());
    Assert.assertArrayEquals(new Integer[]{
            0,6,9,13,15,29,38,43,45,49,57,61,70,74,82,83,88,90,93,97,99,103,108,118,119,125,128,131,132,133,134
    }, sentence.characterOffsetBegin().toArray());
    Assert.assertArrayEquals(new Integer[]{
            5,8,12,14,28,37,43,44,48,56,60,69,73,81,83,88,89,92,97,98,102,107,117,119,124,127,131,132,133,134,135
    }, sentence.characterOffsetEnd().toArray());
  }
",non-flaky,5
96056,stanfordnlp_CoreNLP,TSVSentenceIteratorITest.testParseArray,"  @Test
  public void testParseArray() {
    String in = ""{Chess,is,not,a,predominantly,physical,sport,\""\"",\""\"",yet,neither,are,shooting,and,curling,-LRB-,which,\""\"",\""\"",in,fact,\""\"",\""\"",has,been,nicknamed,``,chess,on,ice,'',5,-RRB-,.}"";
    String[] out = {""Chess"",""is"",""not"",""a"",""predominantly"",""physical"",""sport"","","",""yet"",""neither"",""are"",""shooting"",""and"",""curling"",""-LRB-"",""which"","","",""in"",""fact"","","",""has"",""been"",""nicknamed"",""``"",""chess"",""on"",""ice"",""''"",""5"",""-RRB-"","".""};

    // System.err.println(in);
    // System.err.println(Arrays.asList(out));
    // System.err.println(Arrays.asList(TSVUtils.parseArray(in).toArray()));
    Assert.assertArrayEquals(out, TSVUtils.parseArray(in).toArray());

    String in2 = ""{Chess,is,not,a,predominantly,physical,sport,\""\"",\""\"",yet,neither,are,shooting,and,curling,(,which,\""\"",\""\"",in,fact,\""\"",\""\"",has,been,nicknamed,``,chess,on,ice,'',5,),.}"";
    String[] out2 = {""Chess"",""is"",""not"",""a"",""predominantly"",""physical"",""sport"","","",""yet"",""neither"",""are"",""shooting"",""and"",""curling"",""("",""which"","","",""in"",""fact"","","",""has"",""been"",""nicknamed"",""``"",""chess"",""on"",""ice"",""''"",""5"","")"","".""};

    Assert.assertArrayEquals(out2, TSVUtils.parseArray(in2).toArray());
  }
",non-flaky,5
96057,stanfordnlp_CoreNLP,TSVSentenceIteratorITest.testParseTrees,"  @Test
  public void testParseTrees() {
    List<List<String>> entries = Collections.singletonList(
            Arrays.asList(
                    ""3424"",
                    ""d2-s1-a1"",
                    ""0"",
                    ""{Chess,is,not,a,predominantly,physical,sport,\""\"",\""\"",yet,neither,are,shooting,and,curling,-LRB-,which,\""\"",\""\"",in,fact,\""\"",\""\"",has,been,nicknamed,``,chess,on,ice,'',5,-RRB-,.}"",
                    ""{chess,be,not,a,predominantly,physical,sport,\""\"",\""\"",yet,neither,be,shooting,and,curling,-lrb-,which,\""\"",\""\"",in,fact,\""\"",\""\"",have,be,nickname,``,chess,on,ice,'',5,-rrb-,.}"",
                    ""{NN,VBZ,RB,DT,RB,JJ,NN,\""\"",\""\"",RB,DT,VBP,JJ,CC,NN,-LRB-,WDT,\""\"",\""\"",IN,NN,\""\"",\""\"",VBZ,VBN,VBN,``,NN,IN,NN,'',LS,-RRB-,.}"",
                    ""{O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,NUMBER,O,O}"",
                    ""{0,6,9,13,15,29,38,43,45,49,57,61,70,74,82,83,88,90,93,97,99,103,108,118,119,125,128,131,132,133,134}"",
                    ""{5,8,12,14,28,37,43,44,48,56,60,69,73,81,83,88,89,92,97,98,102,107,117,119,124,127,131,132,133,134,135}""	,
                    ""[{\""\""dependent\""\"": 7, \""\""dep\""\"": \""\""ROOT\""\"", \""\""governorGloss\""\"": \""\""ROOT\""\"", \""\""governor\""\"": 0, \""\""dependentGloss\""\"": \""\""sport\""\""}, {\""\""dependent\""\"": 1, \""\""dep\""\"": \""\""nsubj\""\"", \""\""governorGloss\""\"": \""\""sport\""\"", \""\""governor\""\"": 7, \""\""dependentGloss\""\"": \""\""Chess\""\""}, {\""\""dependent\""\"": 2, \""\""dep\""\"": \""\""cop\""\"", \""\""governorGloss\""\"": \""\""sport\""\"", \""\""governor\""\"": 7, \""\""dependentGloss\""\"": \""\""is\""\""}, {\""\""dependent\""\"": 3, \""\""dep\""\"": \""\""neg\""\"", \""\""governorGloss\""\"": \""\""sport\""\"", \""\""governor\""\"": 7, \""\""dependentGloss\""\"": \""\""not\""\""}, {\""\""dependent\""\"": 4, \""\""dep\""\"": \""\""det\""\"", \""\""governorGloss\""\"": \""\""sport\""\"", \""\""governor\""\"": 7, \""\""dependentGloss\""\"": \""\""a\""\""}, {\""\""dependent\""\"": 5, \""\""dep\""\"": \""\""advmod\""\"", \""\""governorGloss\""\"": \""\""physical\""\"", \""\""governor\""\"": 6, \""\""dependentGloss\""\"": \""\""predominantly\""\""}, {\""\""dependent\""\"": 6, \""\""dep\""\"": \""\""amod\""\"", \""\""governorGloss\""\"": \""\""sport\""\"", \""\""governor\""\"": 7, \""\""dependentGloss\""\"": \""\""physical\""\""}, {\""\""dependent\""\"": 9, \""\""dep\""\"": \""\""advmod\""\"", \""\""governorGloss\""\"": \""\""sport\""\"", \""\""governor\""\"": 7, \""\""dependentGloss\""\"": \""\""yet\""\""}, {\""\""dependent\""\"": 10, \""\""dep\""\"": \""\""nsubj\""\"", \""\""governorGloss\""\"": \""\""shooting\""\"", \""\""governor\""\"": 12, \""\""dependentGloss\""\"": \""\""neither\""\""}, {\""\""dependent\""\"": 11, \""\""dep\""\"": \""\""cop\""\"", \""\""governorGloss\""\"": \""\""shooting\""\"", \""\""governor\""\"": 12, \""\""dependentGloss\""\"": \""\""are\""\""}, {\""\""dependent\""\"": 12, \""\""dep\""\"": \""\""parataxis\""\"", \""\""governorGloss\""\"": \""\""sport\""\"", \""\""governor\""\"": 7, \""\""dependentGloss\""\"": \""\""shooting\""\""}, {\""\""dependent\""\"": 13, \""\""dep\""\"": \""\""cc\""\"", \""\""governorGloss\""\"": \""\""shooting\""\"", \""\""governor\""\"": 12, \""\""dependentGloss\""\"": \""\""and\""\""}, {\""\""dependent\""\"": 14, \""\""dep\""\"": \""\""parataxis\""\"", \""\""governorGloss\""\"": \""\""sport\""\"", \""\""governor\""\"": 7, \""\""dependentGloss\""\"": \""\""curling\""\""}, {\""\""dependent\""\"": 14, \""\""dep\""\"": \""\""conj:and\""\"", \""\""governorGloss\""\"": \""\""shooting\""\"", \""\""governor\""\"": 12, \""\""dependentGloss\""\"": \""\""curling\""\""}, {\""\""dependent\""\"": 16, \""\""dep\""\"": \""\""nsubjpass\""\"", \""\""governorGloss\""\"": \""\""nicknamed\""\"", \""\""governor\""\"": 23, \""\""dependentGloss\""\"": \""\""which\""\""}, {\""\""dependent\""\"": 18, \""\""dep\""\"": \""\""case\""\"", \""\""governorGloss\""\"": \""\""fact\""\"", \""\""governor\""\"": 19, \""\""dependentGloss\""\"": \""\""in\""\""}, {\""\""dependent\""\"": 19, \""\""dep\""\"": \""\""nmod:in\""\"", \""\""governorGloss\""\"": \""\""nicknamed\""\"", \""\""governor\""\"": 23, \""\""dependentGloss\""\"": \""\""fact\""\""}, {\""\""dependent\""\"": 21, \""\""dep\""\"": \""\""aux\""\"", \""\""governorGloss\""\"": \""\""nicknamed\""\"", \""\""governor\""\"": 23, \""\""dependentGloss\""\"": \""\""has\""\""}, {\""\""dependent\""\"": 22, \""\""dep\""\"": \""\""auxpass\""\"", \""\""governorGloss\""\"": \""\""nicknamed\""\"", \""\""governor\""\"": 23, \""\""dependentGloss\""\"": \""\""been\""\""}, {\""\""dependent\""\"": 23, \""\""dep\""\"": \""\""dep\""\"", \""\""governorGloss\""\"": \""\""shooting\""\"", \""\""governor\""\"": 12, \""\""dependentGloss\""\"": \""\""nicknamed\""\""}, {\""\""dependent\""\"": 25, \""\""dep\""\"": \""\""dobj\""\"", \""\""governorGloss\""\"": \""\""nicknamed\""\"", \""\""governor\""\"": 23, \""\""dependentGloss\""\"": \""\""chess\""\""}, {\""\""dependent\""\"": 26, \""\""dep\""\"": \""\""case\""\"", \""\""governorGloss\""\"": \""\""ice\""\"", \""\""governor\""\"": 27, \""\""dependentGloss\""\"": \""\""on\""\""}, {\""\""dependent\""\"": 27, \""\""dep\""\"": \""\""nmod:on\""\"", \""\""governorGloss\""\"": \""\""chess\""\"", \""\""governor\""\"": 25, \""\""dependentGloss\""\"": \""\""ice\""\""}, {\""\""dependent\""\"": 29, \""\""dep\""\"": \""\""amod\""\"", \""\""governorGloss\""\"": \""\""chess\""\"", \""\""governor\""\"": 25, \""\""dependentGloss\""\"": \""\""5\""\""}]"",
                    ""Chess is not a predominantly physical sport, yet neither are shooting and curling (which, in fact, has been nicknamed âchess on iceâ5)."")
    );

    TSVSentenceIterator it = new TSVSentenceIterator(entries.iterator(), Arrays.asList(
            SentenceField.ID,
            SentenceField.DOC_ID,
            SentenceField.SENTENCE_INDEX,
            SentenceField.WORDS,
            SentenceField.LEMMAS,
            SentenceField.POS_TAGS,
            SentenceField.NER_TAGS,
            SentenceField.DOC_CHAR_BEGIN,
            SentenceField.DOC_CHAR_END,
            SentenceField.DEPENDENCIES_BASIC,
            SentenceField.GLOSS
    ));
    Sentence sentence = it.next();
    sentence.dependencyGraph();
    sentence.openieTriples();
  }
",non-flaky,5
96058,stanfordnlp_CoreNLP,MorphaAnnotatorITest.testMorphaAnnotator,"  @Test
  public void testMorphaAnnotator() {
    Annotation document = new Annotation(text);
    fullPipeline.annotate(document);
    checkResult(document.get(CoreAnnotations.TokensAnnotation.class));
  }
",non-flaky,5
96059,stanfordnlp_CoreNLP,MorphaAnnotatorITest.testSentencesAnnotation,"  @Test
  public void testSentencesAnnotation() {
    List<CoreLabel> words = getTestWords();

    CoreMap sentence = new ArrayCoreMap();
    sentence.set(CoreAnnotations.TokensAnnotation.class, words);
    List<CoreMap> sentences = new ArrayList<>();
    sentences.add(sentence);
    Annotation document = new Annotation(text);
    document.set(CoreAnnotations.SentencesAnnotation.class, sentences);

    shortPipeline.annotate(document);
    checkResult(words);
  }
",non-flaky,5
96060,stanfordnlp_CoreNLP,CoNLLUOutputterITest.testInvalidOutputter,"    @Test
    public void testInvalidOutputter() throws IOException {
        try {
            Annotation ann = new Annotation(""CoNLL-U is neat. Better than XML."");
            pipeline.annotate(ann);
            String actual = new CoNLLUOutputter(""this should fail"").print(ann);
            throw new AssertionError(""This should have failed"");
        } catch (IllegalArgumentException e) {
            // yay
        }
    }
",non-flaky,5
96061,stanfordnlp_CoreNLP,CoNLLUOutputterITest.testSimpleSentence,"    @Test
    public void testSimpleSentence() throws IOException {
        Annotation ann = new Annotation(""CoNLL-U is neat. Better than XML."");
        pipeline.annotate(ann);
        String actual = new CoNLLUOutputter(""enhanced"").print(ann);
        String expected = ""1\tCoNLL\tconll\tNOUN\tNN\tNumber=Sing\t3\tcompound\t3:compound\t_\n"" +
            ""2\t-\t-\tPUNCT\tHYPH\t_\t3\tpunct\t3:punct\t_\n"" +
            ""3\tU\tu\tNOUN\tNN\tNumber=Sing\t5\tnsubj\t5:nsubj\t_\n"" +
            ""4\tis\tbe\tVERB\tVBZ\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\t5\tcop\t5:cop\t_\n"" +
            ""5\tneat\tneat\tADJ\tJJ\tDegree=Pos\t0\troot\t0:root\t_\n"" +
            ""6\t.\t.\tPUNCT\t.\t_\t5\tpunct\t5:punct\t_\n"" +
            ""\n"" +
            ""1\tBetter\tbetter\tADJ\tJJR\tDegree=Cmp\t0\troot\t0:root\t_\n"" +
            ""2\tthan\tthan\tADP\tIN\t_\t3\tcase\t3:case\t_\n"" +
            ""3\tXML\txml\tNOUN\tNN\tNumber=Sing\t1\tobl\t1:obl:than\t_\n"" +
            ""4\t.\t.\tPUNCT\t.\t_\t1\tpunct\t1:punct\t_\n\n"";
        assertEquals(expected, actual);
    }
",non-flaky,5
96062,stanfordnlp_CoreNLP,ChineseSerializationITest.testChineseSerialization,"  @Test
  public void testChineseSerialization() {

    try {
      AnnotationSerializer serializer = new ProtobufAnnotationSerializer();
      // write Chinese doc
      String sampleChineseDocument = ""å·´æåÂ·å¥¥å·´é©¬æ¯ç¾å½æ»ç»ãä»å¨2008å¹´å½é"";
      Properties chineseProperties = StringUtils.argsToProperties(""-props"",
              ""StanfordCoreNLP-chinese.properties"");
      Annotation doc = new StanfordCoreNLP(chineseProperties).process(sampleChineseDocument);

      // fake having a section in the annotation so the test passes.
      // todo [2017] clean up the status of sections.
      doc.set(CoreAnnotations.SectionsAnnotation.class, new ArrayList<CoreMap>());

      ByteArrayOutputStream ks = new ByteArrayOutputStream();
      serializer.write(doc, ks).close();
      // read
      InputStream kis = new ByteArrayInputStream(ks.toByteArray());
      Pair<Annotation, InputStream> pair = serializer.read(kis);
      pair.second.close();
      Annotation readDoc = pair.first;
      kis.close();
      // check characters are equal
      List<CoreLabel> docChars = doc.get(SegmenterCoreAnnotations.CharactersAnnotation.class);
      List<CoreLabel> readDocChars = doc.get(SegmenterCoreAnnotations.CharactersAnnotation.class);
      assertEquals(docChars.size(),readDocChars.size());
      int numChars = docChars.size();
      int currChar = 0;
      while (currChar < numChars) {
        assertEquals(docChars.get(currChar),readDocChars.get(currChar));
        currChar++;
      }
      // check that sentences are equal
      /*int sentenceCount = 0;
      while (sentenceCount < doc.get(CoreAnnotations.SentencesAnnotation.class).size()) {
        assertEquals(doc.get(CoreAnnotations.SentencesAnnotation.class).get(sentenceCount),
                readDoc.get(CoreAnnotations.SentencesAnnotation.class).get(sentenceCount));
        sentenceCount++;
      }*/
      // check JSON output is same
      String docJSON = JSONOutputter.jsonPrint(doc);
      String readDocJSON = JSONOutputter.jsonPrint(readDoc);
      assertEquals(docJSON,readDocJSON);
    } catch (Exception e) { throw new RuntimeException(e); }
  }
",non-flaky,5
96063,stanfordnlp_CoreNLP,TokensRegexNERAnnotatorITest.testTokensRegexSyntax,"  @Test
  public void testTokensRegexSyntax() throws Exception {
    String[][] regexes =
      new String[][]{
        new String[]{""( /University/ /of/ [ {ner:LOCATION} ] )"", ""SCHOOL""}
        // TODO: TokensRegex literal string patterns ignores ignoreCase settings
        //new String[]{""( University of [ {ner:LOCATION} ] )"", ""SCHOOL""}
    };
    Annotator annotatorCased = getTokensRegexNerAnnotator(regexes, false);

    String str = ""University of Alaska is located in Alaska."";
    Annotation document = createDocument(str);
    annotatorCased.annotate(document);
    List<CoreLabel> tokens = document.get(CoreAnnotations.TokensAnnotation.class);

    checkNerTags(tokens,
      ""ORGANIZATION"", ""ORGANIZATION"", ""ORGANIZATION"", ""O"", ""O"", ""O"", ""LOCATION"", ""O"");

    reannotate(tokens, CoreAnnotations.NamedEntityTagAnnotation.class,
            ""O"", ""O"", ""LOCATION"", ""O"", ""O"", ""O"", ""LOCATION"", ""O"");
    annotatorCased.annotate(document);

    checkNerTags(tokens,
      ""SCHOOL"", ""SCHOOL"", ""SCHOOL"", ""O"", ""O"", ""O"", ""LOCATION"", ""O"");

    // Try lowercase
    Annotator annotatorCaseless = getTokensRegexNerAnnotator(regexes, true);

    str = ""university of alaska is located in alaska."";
    document = createDocument(str);
    tokens = document.get(CoreAnnotations.TokensAnnotation.class);
    checkNerTags(tokens,
      ""O"", ""O"", ""LOCATION"", ""O"", ""O"", ""O"", ""LOCATION"", ""O"");
    annotatorCased.annotate(document);
    checkNerTags(tokens,
      ""O"", ""O"", ""LOCATION"", ""O"", ""O"", ""O"", ""LOCATION"", ""O"");
    annotatorCaseless.annotate(document);
    checkNerTags(tokens,
      ""SCHOOL"", ""SCHOOL"", ""SCHOOL"", ""O"", ""O"", ""O"", ""LOCATION"", ""O"");
  }
",non-flaky,5
96064,stanfordnlp_CoreNLP,TokensRegexNERAnnotatorITest.testTokensRegexMatchGroup,"  @Test
  public void testTokensRegexMatchGroup() throws Exception {
    String[][] regexes =
      new String[][]{
        new String[]{""( /the/? /movie/ (/[A-Z].*/+) )"", ""MOVIE"", """", ""0"", ""1""}
      };
    Annotator annotatorCased = getTokensRegexNerAnnotator(regexes, false);

    String str = ""the movie Mud was very muddy"";
    Annotation document = createDocument(str);
    annotatorCased.annotate(document);
    List<CoreLabel> tokens = document.get(CoreAnnotations.TokensAnnotation.class);

    checkNerTags(tokens,
      ""O"", ""O"", ""MOVIE"", ""O"", ""O"", ""O"");

  }
",non-flaky,5
96065,stanfordnlp_CoreNLP,TokensRegexNERAnnotatorITest.testTokensRegexNormalizedAnnotate,"  @Test
  public void testTokensRegexNormalizedAnnotate() throws Exception {
    Properties props = new Properties();
    props.setProperty(REGEX_ANNOTATOR_NAME + "".mapping.header"", ""pattern,ner,normalized,overwrite,priority,group"");

    String[][] regexes =
      new String[][]{
        new String[]{""blue"",  ""COLOR"", ""B"", """", ""0""},
        new String[]{""red"",   ""COLOR"", ""R"", """", ""0""},
        new String[]{""green"", ""COLOR"", ""G"", """", ""0""}
      };
    Annotator annotatorCased = getTokensRegexNerAnnotator(props, regexes, false);

    String str = ""These are all colors: blue, red, and green."";
    Annotation document = createDocument(str);
    annotatorCased.annotate(document);
    List<CoreLabel> tokens = document.get(CoreAnnotations.TokensAnnotation.class);

    checkTags(tokens, CoreAnnotations.TextAnnotation.class, ""These"", ""are"", ""all"", ""colors"", "":"", ""blue"", "","", ""red"", "","", ""and"", ""green"", ""."");
    checkTags(tokens, CoreAnnotations.NamedEntityTagAnnotation.class,  ""O"", ""O"", ""O"", ""O"", ""O"", ""COLOR"", ""O"", ""COLOR"", ""O"", ""O"", ""COLOR"", ""O"");
    checkTags(tokens, CoreAnnotations.NormalizedNamedEntityTagAnnotation.class,  null, null, null, null, null, ""B"", null, ""R"", null, null, ""G"", null);
  }
",non-flaky,5
96066,stanfordnlp_CoreNLP,TokensRegexNERAnnotatorITest.testTokensRegexCustomAnnotate,"  @Test
  public void testTokensRegexCustomAnnotate() throws Exception {

    Properties props = new Properties();
    props.setProperty(REGEX_ANNOTATOR_NAME + "".mapping.header"", ""pattern,test,overwrite,priority,group"");
    props.setProperty(REGEX_ANNOTATOR_NAME + "".mapping.field.test"", ""edu.stanford.nlp.pipeline.TokensRegexNERAnnotatorITest$TestAnnotation"");
    String[][] regexes =
      new String[][]{
        new String[]{""test"", ""TEST"", """", ""0""}
      };
    Annotator annotatorCased = getTokensRegexNerAnnotator(props, regexes, true);

    String str = ""Marking all test as test"";
    Annotation document = createDocument(str);
    annotatorCased.annotate(document);
    List<CoreLabel> tokens = document.get(CoreAnnotations.TokensAnnotation.class);

    checkTags(tokens, CoreAnnotations.TextAnnotation.class, ""Marking"", ""all"", ""test"", ""as"", ""test"");
    checkTags(tokens, TestAnnotation.class, null, null, ""TEST"", null, ""TEST"");
  }
",non-flaky,5
96067,stanfordnlp_CoreNLP,TokensRegexNERAnnotatorITest.testBasicMatching,"  @Test
  public void testBasicMatching() {
    String str = ""President Barack Obama lives in Chicago , Illinois , "" +
    ""and is a practicing Christian ."";
    Annotation document = createDocument(str);
    annotator.annotate(document);
    List<CoreLabel> tokens = document.get(CoreAnnotations.TokensAnnotation.class);

    checkNerTags(tokens,
      ""TITLE"", ""PERSON"", ""PERSON"", ""O"", ""O"", ""LOCATION"", ""O"", ""STATE_OR_PROVINCE"",
      ""O"", ""O"", ""O"", ""O"", ""O"", ""IDEOLOGY"", ""O"");

  }
",non-flaky,5
96068,stanfordnlp_CoreNLP,TokensRegexNERAnnotatorITest.testOverwrite,"  @Test
  public void testOverwrite() {
    String str = ""I like Ontario Bank and Ontario Lake , and I like the Native American Church , too ."";
    Annotation document = createDocument(str);
    annotator.annotate(document);
    List<CoreLabel> tokens = document.get(CoreAnnotations.TokensAnnotation.class);

    checkNerTags(tokens, ""O"", ""O"", ""ORGANIZATION"", ""ORGANIZATION"", ""O"", ""STATE_OR_PROVINCE"", ""LOCATION"", ""O"", ""O"", ""O"", ""O"", ""O"", ""RELIGION"",
      ""RELIGION"", ""RELIGION"", ""O"", ""O"", ""O"");

  }
",non-flaky,5
96069,stanfordnlp_CoreNLP,TokensRegexNERAnnotatorITest.testPriority,"  @Test
  public void testPriority() {
    String str = ""Christianity is of higher regex priority than Early Christianity . "";
    Annotation document = createDocument(str);
    annotator.annotate(document);
    List<CoreLabel> tokens = document.get(CoreAnnotations.TokensAnnotation.class);
    checkNerTags(tokens, ""RELIGION"", ""O"", ""O"", ""O"", ""O"", ""O"", ""O"", ""O"", ""RELIGION"", ""O"");
  }
",non-flaky,5
96070,stanfordnlp_CoreNLP,TokensRegexNERAnnotatorITest.testEmptyAnnotation,"  @Test
  public void testEmptyAnnotation() {
    try {
      annotator.annotate(new Annotation(""""));
    } catch(RuntimeException e) {
      return;
    }
    Assert.fail(""Never expected to get this far... the annotator should have thrown an exception by now"");
  }
",non-flaky,5
96071,stanfordnlp_CoreNLP,QuantifiableEntityNormalizingAnnotatorITest.testQuantifiableEntityNormalizingAnnotator,"  @Test
  public void testQuantifiableEntityNormalizingAnnotator() {
    Annotation document = new Annotation(text);
    pipeline.annotate(document);

    int i = 0;
    for (CoreMap sentence: document.get(CoreAnnotations.SentencesAnnotation.class)) {
        List<CoreLabel> tokens = sentence.get(CoreAnnotations.TokensAnnotation.class);
        for (CoreLabel token : tokens) {
          System.out.println(token.get(CoreAnnotations.TextAnnotation.class) + "": "" + token.get(CoreAnnotations.NamedEntityTagAnnotation.class) + "", "" + token.get(CoreAnnotations.NormalizedNamedEntityTagAnnotation.class));
        }
      for (CoreLabel token : tokens) {
        String normalization = token.get(CoreAnnotations.NormalizedNamedEntityTagAnnotation.class);
        if (normalization != null) {
          Assert.assertEquals(answer_text[i], token.get(CoreAnnotations.OriginalTextAnnotation.class));
          Assert.assertEquals(answer_time[i], normalization);
          i++;
        }
      }
    }
    Assert.assertEquals(answer_text.length, i);
    Assert.assertEquals(answer_time.length, i);
  }
",non-flaky,5
96072,stanfordnlp_CoreNLP,RequirementsCorrectSlowITest.testDefaultPipeline,"  @Test
  public void testDefaultPipeline() {
    testAnnotatorSequence(Arrays.asList(""tokenize"", ""ssplit"", ""pos"", ""lemma"", ""ner"", ""gender"", ""parse"", ""coref""));
  }
",non-flaky,5
96073,stanfordnlp_CoreNLP,RequirementsCorrectSlowITest.testDepparsePipeline,"  @Test
  public void testDepparsePipeline() {
    testAnnotatorSequence(Arrays.asList(""tokenize"", ""ssplit"", ""pos"", ""depparse""));
  }
",non-flaky,5
96074,stanfordnlp_CoreNLP,RequirementsCorrectSlowITest.testQuotePipeline,"  @Test
  public void testQuotePipeline() {
    testAnnotatorSequence(Arrays.asList(""tokenize"",""ssplit"",""pos"",""lemma"",""ner"",""depparse"",""coref"",""quote""));
  }
",non-flaky,5
96075,stanfordnlp_CoreNLP,RequirementsCorrectSlowITest.testTrueCasePipeline,"   @Test
   public void testTrueCasePipeline() {
     testAnnotatorSequence(Arrays.asList(""tokenize"",""ssplit"",""pos"",""lemma"",""truecase""));
   }
",non-flaky,5
96076,stanfordnlp_CoreNLP,RequirementsCorrectSlowITest.testOpenIEPipeline,"  @Test
  public void testOpenIEPipeline() {
    testAnnotatorSequence(Arrays.asList(""tokenize"",""ssplit"",""pos"",""lemma"",""depparse"",""natlog"",""openie""));
  }
",non-flaky,5
96077,stanfordnlp_CoreNLP,RequirementsCorrectSlowITest.testMentionRegression,"  @Test
  public void testMentionRegression() {
    testAnnotatorSequence(Arrays.asList());
  }
",non-flaky,5
96078,stanfordnlp_CoreNLP,PipelineITest.testPipeline,"  @Test
  public void testPipeline() throws Exception {
    // create pipeline
    AnnotationPipeline pipeline = new AnnotationPipeline();
    pipeline.addAnnotator(new TokenizerAnnotator(false, ""en""));
    pipeline.addAnnotator(new WordsToSentencesAnnotator(false));
    pipeline.addAnnotator(new POSTaggerAnnotator(false));
    pipeline.addAnnotator(new MorphaAnnotator(false));
    pipeline.addAnnotator(new NERCombinerAnnotator(false));
    pipeline.addAnnotator(new ParserAnnotator(false, -1));
    //pipeline.addAnnotator(new CorefAnnotator(null, null, null, false));
    //pipeline.addAnnotator(new SRLAnnotator(false));

    // create annotation with text
    String text = ""Dan Ramage is working for\nMicrosoft. He's in Seattle! \n"";
    Annotation document = new Annotation(text);
    Assert.assertEquals(text, document.toString());
    Assert.assertEquals(text, document.get(CoreAnnotations.TextAnnotation.class));

    // annotate text with pipeline
    pipeline.annotate(document);

    // demonstrate typical usage
    for (CoreMap sentence: document.get(CoreAnnotations.SentencesAnnotation.class)) {

      // get the tree for the sentence
      Tree tree = sentence.get(TreeCoreAnnotations.TreeAnnotation.class);

      // get the tokens for the sentence and iterate over them
      for (CoreLabel token: sentence.get(CoreAnnotations.TokensAnnotation.class)) {

        // get token attributes
        String tokenText = token.get(CoreAnnotations.TextAnnotation.class);
        String tokenPOS = token.get(CoreAnnotations.PartOfSpeechAnnotation.class);
        String tokenLemma = token.get(CoreAnnotations.LemmaAnnotation.class);
        String tokenNE = token.get(CoreAnnotations.NamedEntityTagAnnotation.class);

        // text, pos, lemma and name entity tag should be defined
        Assert.assertNotNull(tokenText);
        Assert.assertNotNull(tokenPOS);
        Assert.assertNotNull(tokenLemma);
        Assert.assertNotNull(tokenNE);
      }
      // tree should be defined
      Assert.assertNotNull(tree);
    }

    // get tokens
    List<CoreLabel> tokens = document.get(CoreAnnotations.TokensAnnotation.class);
    String tokensText = ""Dan Ramage is working for Microsoft . He 's in Seattle !"";
    Assert.assertNotNull(tokens);
    Assert.assertEquals(12, tokens.size());
    Assert.assertEquals(tokensText, join(tokens));
    Assert.assertEquals(0, (int)tokens.get(0).get(CoreAnnotations.CharacterOffsetBeginAnnotation.class));
    Assert.assertEquals(3, (int)tokens.get(0).get(CoreAnnotations.CharacterOffsetEndAnnotation.class));
    Assert.assertEquals(""NNP"", tokens.get(0).get(CoreAnnotations.PartOfSpeechAnnotation.class));
    Assert.assertEquals(""VBZ"", tokens.get(2).get(CoreAnnotations.PartOfSpeechAnnotation.class));
    Assert.assertEquals(""."", tokens.get(11).get(CoreAnnotations.PartOfSpeechAnnotation.class));
    Assert.assertEquals(""Ramage"", tokens.get(1).get(CoreAnnotations.LemmaAnnotation.class));
    Assert.assertEquals(""be"", tokens.get(2).get(CoreAnnotations.LemmaAnnotation.class));
    Assert.assertEquals(""PERSON"", tokens.get(0).get(CoreAnnotations.NamedEntityTagAnnotation.class));
    Assert.assertEquals(""PERSON"", tokens.get(1).get(CoreAnnotations.NamedEntityTagAnnotation.class));
    Assert.assertEquals(""CITY"", tokens.get(10).get(CoreAnnotations.NamedEntityTagAnnotation.class));

    // get sentences
    List<CoreMap> sentences = document.get(CoreAnnotations.SentencesAnnotation.class);
    Assert.assertNotNull(sentences);
    Assert.assertEquals(2, sentences.size());

    // sentence 1
    String text1 = ""Dan Ramage is working for\nMicrosoft."";
    CoreMap sentence1 = sentences.get(0);
    Assert.assertEquals(text1, sentence1.toString());
    Assert.assertEquals(text1, sentence1.get(CoreAnnotations.TextAnnotation.class));
    Assert.assertEquals(0, (int)sentence1.get(CoreAnnotations.CharacterOffsetBeginAnnotation.class));
    Assert.assertEquals(36, (int)sentence1.get(CoreAnnotations.CharacterOffsetEndAnnotation.class));
    Assert.assertEquals(0, (int)sentence1.get(CoreAnnotations.TokenBeginAnnotation.class));
    Assert.assertEquals(7, (int)sentence1.get(CoreAnnotations.TokenEndAnnotation.class));

    // sentence 1 tree
    Tree tree1 = Tree.valueOf(""(ROOT (S (NP (NNP Dan) (NNP Ramage)) (VP (VBZ is) "" +
        ""(VP (VBG working) (PP (IN for) (NP (NNP Microsoft))))) (. .)))"");
    Assert.assertEquals(tree1, sentence1.get(TreeCoreAnnotations.TreeAnnotation.class));

    // sentence 1 tokens
    String tokenText1 = ""Dan Ramage is working for Microsoft ."";
    List<CoreLabel> tokens1 = sentence1.get(CoreAnnotations.TokensAnnotation.class);
    Assert.assertNotNull(tokens1);
    Assert.assertEquals(7, tokens1.size());
    Assert.assertEquals(tokenText1, join(tokens1));
    Assert.assertEquals(4, (int)tokens1.get(1).get(CoreAnnotations.CharacterOffsetBeginAnnotation.class));
    Assert.assertEquals(10, (int)tokens1.get(1).get(CoreAnnotations.CharacterOffsetEndAnnotation.class));
    Assert.assertEquals(""IN"", tokens1.get(4).get(CoreAnnotations.PartOfSpeechAnnotation.class));
    Assert.assertEquals(""NNP"", tokens1.get(5).get(CoreAnnotations.PartOfSpeechAnnotation.class));
    Assert.assertEquals(""work"", tokens1.get(3).get(CoreAnnotations.LemmaAnnotation.class));
    Assert.assertEquals(""."", tokens1.get(6).get(CoreAnnotations.LemmaAnnotation.class));
    Assert.assertEquals(""ORGANIZATION"", tokens1.get(5).get(CoreAnnotations.NamedEntityTagAnnotation.class));

    // sentence 2
    String text2 = ""He's in Seattle!"";
    CoreMap sentence2 = sentences.get(1);
    Assert.assertEquals(text2, sentence2.toString());
    Assert.assertEquals(text2, sentence2.get(CoreAnnotations.TextAnnotation.class));
    Assert.assertEquals(37, (int)sentence2.get(CoreAnnotations.CharacterOffsetBeginAnnotation.class));
    Assert.assertEquals(53, (int)sentence2.get(CoreAnnotations.CharacterOffsetEndAnnotation.class));
    Assert.assertEquals(7, (int)sentence2.get(CoreAnnotations.TokenBeginAnnotation.class));
    Assert.assertEquals(12, (int)sentence2.get(CoreAnnotations.TokenEndAnnotation.class));

    // sentence 2 tree (note error on Seattle, caused by part of speech tagger)
    Tree tree2 = Tree.valueOf(""(ROOT (S (NP (PRP He)) (VP (VBZ 's) (PP (IN in) "" +
        ""(NP (NNP Seattle)))) (. !)))"");
    Assert.assertEquals(tree2, sentence2.get(TreeCoreAnnotations.TreeAnnotation.class));

    // sentence 2 tokens
    String tokenText2 = ""He 's in Seattle !"";
    List<CoreLabel> tokens2 = sentence2.get(CoreAnnotations.TokensAnnotation.class);
    Assert.assertNotNull(tokens2);
    Assert.assertEquals(5, tokens2.size());
    Assert.assertEquals(tokenText2, join(tokens2));
    Assert.assertEquals(39, (int)tokens2.get(1).get(CoreAnnotations.CharacterOffsetBeginAnnotation.class));
    Assert.assertEquals(41, (int)tokens2.get(1).get(CoreAnnotations.CharacterOffsetEndAnnotation.class));
    Assert.assertEquals(""VBZ"", tokens2.get(1).get(CoreAnnotations.PartOfSpeechAnnotation.class));
    Assert.assertEquals(""be"", tokens2.get(1).get(CoreAnnotations.LemmaAnnotation.class));
    Assert.assertEquals(""CITY"", tokens2.get(3).get(CoreAnnotations.NamedEntityTagAnnotation.class));
  }
",non-flaky,5
96079,stanfordnlp_CoreNLP,PipelinePropertiesITest.buildFrenchPipeline,"  @Test
  public void buildFrenchPipeline() {
    // expected output
    List<String> expectedTokens = Arrays.asList(""Emmanuel"", ""Macron"", ""est"", ""le"", ""prÃ©sident"", ""de"", ""la"", ""France"", ""."");
    List<String> expectedTags = Arrays.asList(""PROPN"", ""PROPN"", ""AUX"", ""DET"", ""NOUN"", ""ADP"", ""DET"", ""PROPN"", ""PUNCT"");
    List<String> expectedNER = Arrays.asList(""I-PER"", ""I-PER"", ""O"", ""O"", ""O"", ""O"", ""I-LOC"", ""I-LOC"", ""O"");
    String expectedDependencyParse = ""root(ROOT-0, prÃ©sident-5)\n"" +
        ""nsubj(prÃ©sident-5, Emmanuel-1)\n"" +
        ""flat:name(Emmanuel-1, Macron-2)\n"" +
        ""cop(prÃ©sident-5, est-3)\n"" +
        ""det(prÃ©sident-5, le-4)\n"" +
        ""case(France-8, de-6)\n"" +
        ""det(France-8, la-7)\n"" +
        ""nmod:de(prÃ©sident-5, France-8)\n"" +
        ""punct(prÃ©sident-5, .-9)\n"";
    // build doc
    CoreDocument doc = new CoreDocument(""Emmanuel Macron est le prÃ©sident de la France."");
    // build pipeline with language name
    StanfordCoreNLP frenchPipeline = new StanfordCoreNLP(""french"");
    // annotate
    frenchPipeline.annotate(doc);
    // compare results
    assertEquals(expectedTokens, doc.tokens().stream().map(w -> w.word()).collect(Collectors.toList()));
    assertEquals(expectedTags, doc.tokens().stream().map(w -> w.tag()).collect(Collectors.toList()));
    assertEquals(expectedNER, doc.tokens().stream().map(w -> w.ner()).collect(Collectors.toList()));
    assertEquals(expectedDependencyParse, doc.sentences().get(0).dependencyParse().toList());
  }
",non-flaky,5
96080,stanfordnlp_CoreNLP,CoreQuoteSanityITest.testCoreQuote,"  @Test
  public void testCoreQuote() {
    // make the core document
    CoreDocument testDoc = new CoreDocument(testDocText);
    // annotate
    pipeline.annotate(testDoc);
    // test canonical entity mention is correct
    // ""Joe Smith"" should be first entity mention
    CoreMap canonicalEntityMention =
        testDoc.annotation().get(CoreAnnotations.MentionsAnnotation.class).get(1);
    // test canonical mention is correct
    assertEquals(""Joe Smith"", canonicalEntityMention.get(CoreAnnotations.TextAnnotation.class));
    assertEquals(14,
        canonicalEntityMention.get(CoreAnnotations.CharacterOffsetBeginAnnotation.class).intValue());
    assertEquals(23,
        canonicalEntityMention.get(CoreAnnotations.CharacterOffsetEndAnnotation.class).intValue());
    // test the CoreQuote has the correct entity mention for the canonical speaker
    assertEquals(canonicalEntityMention, testDoc.quotes().get(0).canonicalSpeakerEntityMention().get().coreMap());
  }
",non-flaky,5
96081,stanfordnlp_CoreNLP,CustomAnnotationSerializerITest.testSimple,"  @Test
  public void testSimple() throws IOException {
    Annotation annotation = new Annotation(""This is a test"");
    fullPipeline.annotate(annotation);
    runTest(annotation);
  }
",non-flaky,5
96082,stanfordnlp_CoreNLP,CustomAnnotationSerializerITest.testCollapsedGraphs,"  @Test
  public void testCollapsedGraphs() throws IOException {
    Annotation annotation = new Annotation(""I bought a bone for my dog."");
    fullPipeline.annotate(annotation);
    runTest(annotation);
  }
",non-flaky,5
96083,stanfordnlp_CoreNLP,CustomAnnotationSerializerITest.testTwoSentences,"  @Test
  public void testTwoSentences() throws IOException {
    Annotation annotation = new Annotation(""I bought a bone for my dog.  He chews it every day."");
    fullPipeline.annotate(annotation);
    runTest(annotation);
  }
",non-flaky,5
96084,stanfordnlp_CoreNLP,CustomAnnotationSerializerITest.testCopyWordGraphs,"  @Test
  public void testCopyWordGraphs() throws IOException {
    Annotation annotation = new Annotation(""I went over the river and through the woods"");
    fullPipeline.annotate(annotation);
    runTest(annotation);
  }
",non-flaky,5
96085,stanfordnlp_CoreNLP,StanfordCoreNLPServerITest.testLive,"  @Test
  public void testLive() {
    String result = IOUtils.slurpURLNoExceptions(""http://localhost:"" + port + ""/live"");
    Assert.assertNotNull(result);
    Assert.assertEquals(""live"", result.trim());
  }
",non-flaky,5
96086,stanfordnlp_CoreNLP,StanfordCoreNLPServerITest.testReady,"  @Test
  public void testReady() {
    String result = IOUtils.slurpURLNoExceptions(""http://localhost:"" + port + ""/ready"");
    Assert.assertNotNull(result);
    Assert.assertEquals(""ready"", result.trim());
  }
",non-flaky,5
96087,stanfordnlp_CoreNLP,StanfordCoreNLPServerITest.testClient,"  @Test
  public void testClient() {
    String query = ""The dog ate a fish"";
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,parse"");
    StanfordCoreNLPClient client = new StanfordCoreNLPClient(props, ""http://localhost"", port);
    // if something goes wrong, we don't want the unittest waiting forever for a response
    client.setTimeoutMilliseconds(30 * 1000);
    Annotation annotation = client.process(query);
    Throwable t = annotation.get(CoreAnnotations.ExceptionAnnotation.class);
    Assert.assertNull(t);

    List<CoreMap> sentences = annotation.get(CoreAnnotations.SentencesAnnotation.class);
    Assert.assertEquals(1, sentences.size());
  }
",non-flaky,5
96088,stanfordnlp_CoreNLP,StanfordCoreNLPServerITest.testClientFailure,"  @Test
  public void testClientFailure() {
    String query = ""The dog ate a fish"";
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,parse"");
    //StanfordCoreNLPClient client = new StanfordCoreNLPClient(props, ""http://localhost"", port);
    StanfordCoreNLPClient client = new StanfordCoreNLPClient(props, ""localhost"", port);
    client.setTimeoutMilliseconds(1000);
    Annotation annotation = client.process(query);
    Throwable t = annotation.get(CoreAnnotations.ExceptionAnnotation.class);
    Assert.assertNotNull(t);
  }
",non-flaky,5
96089,stanfordnlp_CoreNLP,StanfordCoreNLPServerITest.testTregexJson,"  @Test
  public void testTregexJson() throws IOException {

    String expected=""{\""sentences\"":[{\""0\"":{\""sentIndex\"":0,\""characterOffsetBegin\"":4,\""characterOffsetEnd\"":7,\""match\"":\""(NNdog)\\n\"",\""spanString\"":\""dog\"",\""namedNodes\"":[]},\""1\"":{\""sentIndex\"":0,\""characterOffsetBegin\"":14,\""characterOffsetEnd\"":18,\""match\"":\""(NNfish)\\n\"",\""spanString\"":\""fish\"",\""namedNodes\"":[]}}]}"".replaceAll("" "", """");

    String query = ""The dog ate a fish"";
    byte[] message = query.getBytes(""utf-8"");
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,parse"");
    String queryParams = String.format(""pattern=NN&properties=%s"",
                                       URLEncoder.encode(PropertiesUtils.propsAsJsonString(props), ""utf-8""));
    URL serverURL = new URL(""http"", ""localhost"", port, ""/tregex?"" + queryParams);
    String response = slurpURL(serverURL, message);

    Assert.assertEquals(expected, response.replaceAll("" "", """").replaceAll(""\n"", """"));
  }
",non-flaky,5
96090,stanfordnlp_CoreNLP,StanfordCoreNLPServerITest.testSemgrexJson,"  @Test
  public void testSemgrexJson() throws IOException {
    String expected=""{ \""sentences\"": [ { \""0\"": { \""text\"": \""ate\"", \""begin\"": 2, \""end\"": 3, \""$obj\"": { \""text\"": \""fish\"", \""begin\"": 4, \""end\"": 5 }, \""$verb\"": { \""text\"": \""ate\"", \""begin\"": 2, \""end\"": 3 } }, \""length\"": 1 }  ]}"".replaceAll("" "", """");

    String query = ""The dog ate a fish"";
    byte[] message = query.getBytes(""utf-8"");
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,parse"");
    String queryParams = String.format(""pattern=%s&properties=%s"",
                                       URLEncoder.encode(""{}=verb >obj {}=obj"", ""utf-8""),
                                       URLEncoder.encode(PropertiesUtils.propsAsJsonString(props), ""utf-8""));
    URL serverURL = new URL(""http"", ""localhost"", port, ""/semgrex?"" + queryParams);
    String response = slurpURL(serverURL, message);

    Assert.assertEquals(expected, response.replaceAll("" "", """").replaceAll(""\n"", """"));
  }
",non-flaky,5
96091,stanfordnlp_CoreNLP,StanfordCoreNLPServerITest.testSemgrexAnnotation,"  @Test
  public void testSemgrexAnnotation() throws IOException {
    String expected = ""result { result { match { matchIndex: 3 node { name: \""obj\"" matchIndex: 5 } node { name: \""verb\"" matchIndex: 3 } } }}"".replaceAll("" "", """");
    String query = ""The dog ate a fish"";
    byte[] message = query.getBytes(""utf-8"");
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,parse"");
    String queryParams = String.format(""pattern=%s&properties=%s&outputFormat=serialized"",
                                       URLEncoder.encode(""{}=verb >obj {}=obj"", ""utf-8""),
                                       URLEncoder.encode(PropertiesUtils.propsAsJsonString(props), ""utf-8""));
    URL serverURL = new URL(""http"", ""localhost"", port, ""/semgrex?"" + queryParams);
    InputStream is = postURL(serverURL, message);
    CoreNLPProtos.SemgrexResponse response = CoreNLPProtos.SemgrexResponse.parseFrom(is);

    Assert.assertEquals(expected, response.toString().replaceAll("" "", """").replaceAll(""\n"", """"));
  }
",non-flaky,5
96092,stanfordnlp_CoreNLP,StanfordCoreNLPServerITest.testSemgrexFilter,"  @Test
  public void testSemgrexFilter() throws IOException {
    String expected=""{ \""sentences\"": [ true, false ]}"".replaceAll("" "", """");

    String query = ""The dog ate a fish.  He went home."";
    byte[] message = query.getBytes(""utf-8"");
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,pos,parse"");
    String queryParams = String.format(""pattern=%s&properties=%s&filter=true"",
                                       URLEncoder.encode(""{}=verb >obj {}=obj"", ""utf-8""),
                                       URLEncoder.encode(PropertiesUtils.propsAsJsonString(props), ""utf-8""));
    URL serverURL = new URL(""http"", ""localhost"", port, ""/semgrex?"" + queryParams);
    String response = slurpURL(serverURL, message);

    Assert.assertEquals(expected, response.replaceAll("" "", """").replaceAll(""\n"", """"));
  }
",non-flaky,5
96093,stanfordnlp_CoreNLP,MWTProtobufSerializationITest.testBasicExample,"  @Test
  public void testBasicExample() throws ClassNotFoundException, IOException {
    // set up document
    CoreDocument sampleDocument = new CoreDocument(sampleText);
    // annotate
    pipeline.annotate(sampleDocument);
    // serialize
    ByteArrayOutputStream ks = new ByteArrayOutputStream();
    serializer.writeCoreDocument(sampleDocument, ks).close();
    // Read
    InputStream kis = new ByteArrayInputStream(ks.toByteArray());
    Pair<Annotation, InputStream> pair = serializer.read(kis);
    pair.second.close();
    Annotation readAnnotation = pair.first;
    kis.close();
    ProtobufAnnotationSerializerSlowITest.sameAsRead(sampleDocument.annotation(), readAnnotation);
  }
",non-flaky,5
96094,stanfordnlp_CoreNLP,ProtobufAnnotationSerializerSlowITest.testMentions,"  @Test
  public void testMentions() {
    testAnnotators(""tokenize,ssplit,pos,lemma,ner,entitymentions"");
  }
",non-flaky,5
96095,stanfordnlp_CoreNLP,ProtobufAnnotationSerializerSlowITest.testSentiment,"  @Test
  public void testSentiment() {
    testAnnotators(""tokenize,ssplit,pos,parse,sentiment"");
  }
",non-flaky,5
96096,stanfordnlp_CoreNLP,ProtobufAnnotationSerializerSlowITest.testOpenie,"  @Test
  public void testOpenie() {
    testAnnotators(""tokenize,ssplit,pos,lemma,depparse,natlog,openie"");
  }
",non-flaky,5
96097,stanfordnlp_CoreNLP,ProtobufAnnotationSerializerSlowITest.testQuote,"  @Test
  public void testQuote() {
    testAnnotators(""quote"");
    testAnnotators(""tokenize,quote"");
    testAnnotators(""tokenize,ssplit,quote"");
    testAnnotators(""tokenize,ssplit,quote"");
    testAnnotators(""tokenize,ssplit,pos,lemma,ner,depparse,coref,quote"");
  }
",non-flaky,5
96098,stanfordnlp_CoreNLP,ProtobufAnnotationSerializerSlowITest.testGetPossibleAnnotators,"  @Test
  public void testGetPossibleAnnotators() {
    assertNotNull(possibleAnnotators());
    assertNotEquals(0, possibleAnnotators().length);
  }
",non-flaky,5
96099,stanfordnlp_CoreNLP,ProtobufAnnotationSerializerSlowITest.testSave,"  @Test
  public void testSave() throws IOException {
    ByteArrayOutputStream os = new ByteArrayOutputStream();
    new ProtobufAnnotationSerializer().write(mkAnnotation(), os).close();
    String json = new String(os.toByteArray(), ""UTF-8"").trim();
    assertNotNull(json);
  }
",non-flaky,5
96100,stanfordnlp_CoreNLP,ProtobufAnnotationSerializerSlowITest.testSaveLarge,"  @Test
  public void testSaveLarge() throws IOException {
    ByteArrayOutputStream os = new ByteArrayOutputStream();
    new ProtobufAnnotationSerializer().write(mkLargeAnnotation(), os).close();
    String json = new String(os.toByteArray(), ""UTF-8"");
    assertNotNull(json);
  }
",non-flaky,5
96101,stanfordnlp_CoreNLP,ProtobufAnnotationSerializerSlowITest.testSaveSize,"  @Test
  public void testSaveSize() throws IOException {
    // Annotate
    ByteArrayOutputStream os = new ByteArrayOutputStream();
    ByteArrayOutputStream compressedImpl = new ByteArrayOutputStream();
    GZIPOutputStream compressed = new GZIPOutputStream(compressedImpl);
    new ProtobufAnnotationSerializer().write(mkLargeAnnotation(), os).close();
    new ProtobufAnnotationSerializer().write(mkLargeAnnotation(), compressed).close();
    byte[] uncompressedProto = os.toByteArray();
    byte[] compressedProto = compressedImpl.toByteArray();
    assertNotNull(uncompressedProto);
    assertNotNull(compressedProto);

    // Check size
    assertTrue(""Length too long: "" + compressedProto.length, compressedProto.length < 460000);
    assertTrue(""Length too long: "" + uncompressedProto.length, uncompressedProto.length < 2800000);
  }
",non-flaky,5
96102,stanfordnlp_CoreNLP,ProtobufAnnotationSerializerSlowITest.testCanWriteRead,"  @Test
  public void testCanWriteRead() {
    try {
      AnnotationSerializer serializer = new ProtobufAnnotationSerializer();
      // Write
      StanfordCoreNLP pipe = new StanfordCoreNLP(new Properties());
      Annotation doc = pipe.process(prideAndPrejudiceFirstBit);
      ByteArrayOutputStream ks = new ByteArrayOutputStream();
      serializer.write(doc, ks).close();

      // Read
      InputStream kis = new ByteArrayInputStream(ks.toByteArray());
      Pair<Annotation, InputStream> pair = serializer.read(kis);
      pair.second.close();
      Annotation readDoc = pair.first;
      kis.close();

      sameAsRead(doc, readDoc);
    } catch (Exception e) { throw new RuntimeException(e); }
  }
",non-flaky,5
96103,stanfordnlp_CoreNLP,ProtobufAnnotationSerializerSlowITest.testCanWriteReadCleanXML,"  @Test
  public void testCanWriteReadCleanXML() {
    try {
      AnnotationSerializer serializer = new ProtobufAnnotationSerializer();
      // Write
      Properties props = new Properties();
      props.setProperty(""annotators"", ""tokenize,cleanxml"");
      StanfordCoreNLP pipe = new StanfordCoreNLP(props);
      Annotation doc = pipe.process(prideAndPrejudiceFirstBit);
      ByteArrayOutputStream ks = new ByteArrayOutputStream();
      serializer.write(doc, ks).close();

      // Read
      InputStream kis = new ByteArrayInputStream(ks.toByteArray());
      Pair<Annotation, InputStream> pair = serializer.read(kis);
      pair.second.close();
      Annotation readDoc = pair.first;
      kis.close();

      sameAsRead(doc, readDoc);
    } catch (Exception e) {
      throw new RuntimeException(e);
    }
  }
",non-flaky,5
96104,stanfordnlp_CoreNLP,ProtobufAnnotationSerializerSlowITest.testCanWriteReadWriteReadLargeFile,"  @Test
  public void testCanWriteReadWriteReadLargeFile() {
    try {
      AnnotationSerializer serializer = new ProtobufAnnotationSerializer();
      // Write
      StanfordCoreNLP pipe = new StanfordCoreNLP(new Properties());
      Annotation doc = pipe.process(prideAndPrejudiceChapters1to5);

      ByteArrayOutputStream ks = new ByteArrayOutputStream();
      serializer.write(doc, ks).close();

      // Read
      InputStream kis = new ByteArrayInputStream(ks.toByteArray());
      Pair<Annotation, InputStream> pair1 = serializer.read(kis);
      pair1.second.close();
      Annotation readDoc = pair1.first;
      kis.close();

      for (int i = 0 ; i < doc.get(CoreAnnotations.MentionsAnnotation.class).size() ; i++) {
        CoreMap cm1 = doc.get(CoreAnnotations.MentionsAnnotation.class).get(i);
        CoreMap cm2 = readDoc.get(CoreAnnotations.MentionsAnnotation.class).get(i);
        diffCoreMaps(i,cm1,cm2);
      }

      sameAsRead(doc, readDoc);

      // Write 2
      ByteArrayOutputStream ks2 = new ByteArrayOutputStream();
      serializer.write(readDoc, ks2).close();

      // Read 2
      InputStream kis2 = new ByteArrayInputStream(ks2.toByteArray());
      Pair<Annotation, InputStream> pair = serializer.read(kis2);
      pair.second.close();
      Annotation readDoc2 = pair.first;
      kis2.close();

      sameAsRead(readDoc, readDoc2);
      sameAsRead(doc, readDoc2);
    } catch (Exception e) {
      throw new RuntimeException(e);
    }
  }
",non-flaky,5
96105,stanfordnlp_CoreNLP,ProtobufAnnotationSerializerSlowITest.testSerializeLanguage,"  @Test
  public void testSerializeLanguage() {
    testAnnotators(""tokenize,ssplit,parse"");
    testAnnotators(""tokenize,ssplit,pos,depparse"");
  }
",non-flaky,5
96106,stanfordnlp_CoreNLP,ProtobufAnnotationSerializerSlowITest.testRelation,"  @Test
  public void testRelation() {
    testAnnotators(""tokenize,ssplit,pos,lemma,ner,parse,relation"");
  }
",non-flaky,5
96860,apache_avro,TestSpecificCompiler.testCanReadTemplateFilesOnTheFilesystem,"  @Test
  public void testCanReadTemplateFilesOnTheFilesystem() throws IOException {
    SpecificCompiler compiler = createCompiler();
    compiler.compileToDestination(this.src, OUTPUT_DIR.getRoot());
    assertTrue(new File(OUTPUT_DIR.getRoot(),""SimpleRecord.java"").exists());
  }
",non-flaky,5
96861,apache_avro,TestSpecificCompiler.testPublicFieldVisibility,"  @Test
  public void testPublicFieldVisibility() throws IOException {
    SpecificCompiler compiler = createCompiler();
    compiler.setFieldVisibility(SpecificCompiler.FieldVisibility.PUBLIC);
    assertFalse(compiler.deprecatedFields());
    assertTrue(compiler.publicFields());
    assertFalse(compiler.privateFields());
    compiler.compileToDestination(this.src, this.OUTPUT_DIR.getRoot());
    assertTrue(this.outputFile.exists());
    try(BufferedReader reader = new BufferedReader(new FileReader(this.outputFile))) {
      String line;
      while ((line = reader.readLine()) != null) {
        // No line, once trimmed, should start with a deprecated field declaration
        // nor a private field declaration.  Since the nested builder uses private
        // fields, we cannot do the second check.
        line = line.trim();
        assertFalse(""Line started with a deprecated field declaration: "" + line,
                line.startsWith(""@Deprecated public int value""));
      }
    }
  }
",non-flaky,5
96862,apache_avro,TestSpecificCompiler.testCreateAllArgsConstructor,"  @Test
  public void testCreateAllArgsConstructor() throws Exception {
    SpecificCompiler compiler = createCompiler();
    compiler.compileToDestination(this.src, this.OUTPUT_DIR.getRoot());
    assertTrue(this.outputFile.exists());
    boolean foundAllArgsConstructor = false;
    try(BufferedReader reader = new BufferedReader(new FileReader(this.outputFile))) {
      String line;
      while (!foundAllArgsConstructor && (line = reader.readLine()) != null) {
        foundAllArgsConstructor = line.contains(""All-args constructor"");
      }
    }
    assertTrue(foundAllArgsConstructor);
  }
",non-flaky,5
96863,apache_avro,TestSpecificCompiler.testMaxValidParameterCounts,"  @Test
  public void testMaxValidParameterCounts() throws Exception {
    Schema validSchema1 = createSampleRecordSchema(SpecificCompiler.MAX_FIELD_PARAMETER_UNIT_COUNT, 0);
    assertCompilesWithJavaCompiler(new File(OUTPUT_DIR.getRoot(), name.getMethodName() + ""1""), new SpecificCompiler(validSchema1).compile());

    Schema validSchema2 = createSampleRecordSchema(SpecificCompiler.MAX_FIELD_PARAMETER_UNIT_COUNT - 2, 1);
    assertCompilesWithJavaCompiler(new File(OUTPUT_DIR.getRoot(), name.getMethodName() + ""2""), new SpecificCompiler(validSchema1).compile());
  }
",non-flaky,5
96864,apache_avro,TestSpecificCompiler.testInvalidParameterCounts,"  @Test
  public void testInvalidParameterCounts() throws Exception {
    Schema invalidSchema1 = createSampleRecordSchema(SpecificCompiler.MAX_FIELD_PARAMETER_UNIT_COUNT + 1, 0);
    SpecificCompiler compiler = new SpecificCompiler(invalidSchema1);
    assertCompilesWithJavaCompiler(new File(OUTPUT_DIR.getRoot(), name.getMethodName() + ""1""), compiler.compile());

    Schema invalidSchema2 = createSampleRecordSchema(SpecificCompiler.MAX_FIELD_PARAMETER_UNIT_COUNT, 10);
    compiler = new SpecificCompiler(invalidSchema2);
    assertCompilesWithJavaCompiler(new File(OUTPUT_DIR.getRoot(), name.getMethodName() + ""2""), compiler.compile());
  }
",non-flaky,5
96865,apache_avro,TestSpecificCompiler.testMaxParameterCounts,"  @Test
  public void testMaxParameterCounts() throws Exception {
    Schema validSchema1 = createSampleRecordSchema(SpecificCompiler.MAX_FIELD_PARAMETER_UNIT_COUNT, 0);
    assertTrue(new SpecificCompiler(validSchema1).compile().size() > 0);

    Schema validSchema2 = createSampleRecordSchema(SpecificCompiler.MAX_FIELD_PARAMETER_UNIT_COUNT - 2, 1);
    assertTrue(new SpecificCompiler(validSchema2).compile().size() > 0);

    Schema validSchema3 = createSampleRecordSchema(SpecificCompiler.MAX_FIELD_PARAMETER_UNIT_COUNT - 1, 1);
    assertTrue(new SpecificCompiler(validSchema3).compile().size() > 0);

    Schema validSchema4 = createSampleRecordSchema(SpecificCompiler.MAX_FIELD_PARAMETER_UNIT_COUNT + 1, 0);
    assertTrue(new SpecificCompiler(validSchema4).compile().size() > 0);
  }
",non-flaky,5
96866,apache_avro,TestSpecificCompiler.testCalcAllArgConstructorParameterUnitsFailure,"  @Test(expected=RuntimeException.class)
  public void testCalcAllArgConstructorParameterUnitsFailure() {
    Schema nonRecordSchema = SchemaBuilder.array().items().booleanType();
    new SpecificCompiler().calcAllArgConstructorParameterUnits(nonRecordSchema);
  }
",non-flaky,5
96867,apache_avro,TestSpecificCompiler.testPublicDeprecatedFieldVisibility,"  @Test
  public void testPublicDeprecatedFieldVisibility() throws IOException {
    SpecificCompiler compiler = createCompiler();
    assertTrue(compiler.deprecatedFields());
    assertTrue(compiler.publicFields());
    assertFalse(compiler.privateFields());
    compiler.compileToDestination(this.src, this.OUTPUT_DIR.getRoot());
    assertTrue(this.outputFile.exists());
    BufferedReader reader = new BufferedReader(new FileReader(this.outputFile));
    String line;
    while ((line = reader.readLine()) != null) {
      // No line, once trimmed, should start with a public field declaration
      line = line.trim();
      assertFalse(""Line started with a public field declaration: "" + line,
        line.startsWith(""public int value""));
    }
    reader.close();
  }
",non-flaky,5
96868,apache_avro,TestSpecificCompiler.testPrivateFieldVisibility,"  @Test
  public void testPrivateFieldVisibility() throws IOException {
    SpecificCompiler compiler = createCompiler();
    compiler.setFieldVisibility(SpecificCompiler.FieldVisibility.PRIVATE);
    assertFalse(compiler.deprecatedFields());
    assertFalse(compiler.publicFields());
    assertTrue(compiler.privateFields());
    compiler.compileToDestination(this.src, this.OUTPUT_DIR.getRoot());
    assertTrue(this.outputFile.exists());
    BufferedReader reader = new BufferedReader(new FileReader(this.outputFile));
    String line = null;
    while ((line = reader.readLine()) != null) {
      // No line, once trimmed, should start with a public field declaration
      // or with a deprecated public field declaration
      line = line.trim();
      assertFalse(""Line started with a public field declaration: "" + line,
        line.startsWith(""public int value""));
      assertFalse(""Line started with a deprecated field declaration: "" + line,
        line.startsWith(""@Deprecated public int value""));
    }
    reader.close();
  }
",non-flaky,5
96869,apache_avro,TestSpecificCompiler.setValue,"  @Test
  public void testSettersCreatedByDefault() throws IOException {
    SpecificCompiler compiler = createCompiler();
    assertTrue(compiler.isCreateSetters());
    compiler.compileToDestination(this.src, this.OUTPUT_DIR.getRoot());
    assertTrue(this.outputFile.exists());
    int foundSetters = 0;
    try(BufferedReader reader = new BufferedReader(new FileReader(this.outputFile))) {
      String line;
      while ((line = reader.readLine()) != null) {
        // We should find the setter in the main class
        line = line.trim();
        if (line.startsWith(""public void setValue("")) {
          foundSetters++;
        }
",non-flaky,5
96870,apache_avro,TestSpecificCompiler.testSettingOutputCharacterEncoding,"  @Test
  public void testSettingOutputCharacterEncoding() throws Exception {
    SpecificCompiler compiler = createCompiler();
    // Generated file in default encoding
    compiler.compileToDestination(this.src, this.OUTPUT_DIR.getRoot());
    byte[] fileInDefaultEncoding = new byte[(int) this.outputFile.length()];
    FileInputStream is = new FileInputStream(this.outputFile);
    is.read(fileInDefaultEncoding);
    is.close(); //close input stream otherwise delete might fail
    if (!this.outputFile.delete()) {
      throw new IllegalStateException(""unable to delete "" + this.outputFile); //delete otherwise compiler might not overwrite because src timestamp hasn't changed.
    }
    // Generate file in another encoding (make sure it has different number of bytes per character)
    String differentEncoding = Charset.defaultCharset().equals(Charset.forName(""UTF-16"")) ? ""UTF-32"" : ""UTF-16"";
    compiler.setOutputCharacterEncoding(differentEncoding);
    compiler.compileToDestination(this.src, this.OUTPUT_DIR.getRoot());
    byte[] fileInDifferentEncoding = new byte[(int) this.outputFile.length()];
    is = new FileInputStream(this.outputFile);
    is.read(fileInDifferentEncoding);
    is.close();
    // Compare as bytes
    assertThat(""Generated file should contain different bytes after setting non-default encoding"",
      fileInDefaultEncoding, not(equalTo(fileInDifferentEncoding)));
    // Compare as strings
    assertThat(""Generated files should contain the same characters in the proper encodings"",
      new String(fileInDefaultEncoding), equalTo(new String(fileInDifferentEncoding, differentEncoding)));
  }
",non-flaky,5
96871,apache_avro,TestSpecificCompiler.testJavaTypeWithDecimalLogicalTypeEnabled,"  @Test
  public void testJavaTypeWithDecimalLogicalTypeEnabled() throws Exception {
    SpecificCompiler compiler = createCompiler();
    compiler.setEnableDecimalLogicalType(true);

    Schema dateSchema = LogicalTypes.date()
        .addToSchema(Schema.create(Schema.Type.INT));
    Schema timeSchema = LogicalTypes.timeMillis()
        .addToSchema(Schema.create(Schema.Type.INT));
    Schema timestampSchema = LogicalTypes.timestampMillis()
        .addToSchema(Schema.create(Schema.Type.LONG));
    Schema decimalSchema = LogicalTypes.decimal(9,2)
        .addToSchema(Schema.create(Schema.Type.BYTES));
    Schema uuidSchema = LogicalTypes.uuid()
        .addToSchema(Schema.create(Schema.Type.STRING));

    // Date/time types should always use upper level java classes
    // Decimal type target class depends on configuration
    // UUID should always be CharSequence since we haven't added its
    // support in SpecificRecord
    Assert.assertEquals(""Should use Joda LocalDate for date type"",
        ""org.joda.time.LocalDate"", compiler.javaType(dateSchema));
    Assert.assertEquals(""Should use Joda LocalTime for time-millis type"",
        ""org.joda.time.LocalTime"", compiler.javaType(timeSchema));
    Assert.assertEquals(""Should use Joda DateTime for timestamp-millis type"",
        ""org.joda.time.DateTime"", compiler.javaType(timestampSchema));
    Assert.assertEquals(""Should use Java BigDecimal type"",
        ""java.math.BigDecimal"", compiler.javaType(decimalSchema));
    Assert.assertEquals(""Should use Java CharSequence type"",
        ""java.lang.CharSequence"", compiler.javaType(uuidSchema));
  }
",non-flaky,5
96872,apache_avro,TestSpecificCompiler.testJavaTypeWithDecimalLogicalTypeDisabled,"  @Test
  public void testJavaTypeWithDecimalLogicalTypeDisabled() throws Exception {
    SpecificCompiler compiler = createCompiler();
    compiler.setEnableDecimalLogicalType(false);

    Schema dateSchema = LogicalTypes.date()
        .addToSchema(Schema.create(Schema.Type.INT));
    Schema timeSchema = LogicalTypes.timeMillis()
        .addToSchema(Schema.create(Schema.Type.INT));
    Schema timestampSchema = LogicalTypes.timestampMillis()
        .addToSchema(Schema.create(Schema.Type.LONG));
    Schema decimalSchema = LogicalTypes.decimal(9,2)
        .addToSchema(Schema.create(Schema.Type.BYTES));
    Schema uuidSchema = LogicalTypes.uuid()
        .addToSchema(Schema.create(Schema.Type.STRING));

    // Date/time types should always use upper level java classes
    // Decimal type target class depends on configuration
    // UUID should always be CharSequence since we haven't added its
    // support in SpecificRecord
    Assert.assertEquals(""Should use Joda LocalDate for date type"",
        ""org.joda.time.LocalDate"", compiler.javaType(dateSchema));
    Assert.assertEquals(""Should use Joda LocalTime for time-millis type"",
        ""org.joda.time.LocalTime"", compiler.javaType(timeSchema));
    Assert.assertEquals(""Should use Joda DateTime for timestamp-millis type"",
        ""org.joda.time.DateTime"", compiler.javaType(timestampSchema));
    Assert.assertEquals(""Should use ByteBuffer type"",
        ""java.nio.ByteBuffer"", compiler.javaType(decimalSchema));
    Assert.assertEquals(""Should use Java CharSequence type"",
        ""java.lang.CharSequence"", compiler.javaType(uuidSchema));
  }
",non-flaky,5
96873,apache_avro,TestSpecificCompiler.testJavaTypeWithJsr310DateTimeTypes,"  @Test
  public void testJavaTypeWithJsr310DateTimeTypes() throws Exception {
    SpecificCompiler compiler = createCompiler(JSR310);

    Schema dateSchema = LogicalTypes.date()
        .addToSchema(Schema.create(Schema.Type.INT));
    Schema timeSchema = LogicalTypes.timeMillis()
        .addToSchema(Schema.create(Schema.Type.INT));
    Schema timestampSchema = LogicalTypes.timestampMillis()
        .addToSchema(Schema.create(Schema.Type.LONG));

    // Date/time types should always use upper level java classes
    Assert.assertEquals(""Should use java.time.LocalDate for date type"",
        ""java.time.LocalDate"", compiler.javaType(dateSchema));
    Assert.assertEquals(""Should use java.time.LocalTime for time-millis type"",
        ""java.time.LocalTime"", compiler.javaType(timeSchema));
    Assert.assertEquals(""Should use java.time.Instant for timestamp-millis type"",
        ""java.time.Instant"", compiler.javaType(timestampSchema));
  }
",non-flaky,5
96874,apache_avro,TestSpecificCompiler.testJavaUnbox,"  @Test
  public void testJavaUnbox() throws Exception {
    SpecificCompiler compiler = createCompiler();
    compiler.setEnableDecimalLogicalType(false);

    Schema intSchema = Schema.create(Schema.Type.INT);
    Schema longSchema = Schema.create(Schema.Type.LONG);
    Schema floatSchema = Schema.create(Schema.Type.FLOAT);
    Schema doubleSchema = Schema.create(Schema.Type.DOUBLE);
    Schema boolSchema = Schema.create(Schema.Type.BOOLEAN);
    Assert.assertEquals(""Should use int for Type.INT"",
        ""int"", compiler.javaUnbox(intSchema));
    Assert.assertEquals(""Should use long for Type.LONG"",
        ""long"", compiler.javaUnbox(longSchema));
    Assert.assertEquals(""Should use float for Type.FLOAT"",
        ""float"", compiler.javaUnbox(floatSchema));
    Assert.assertEquals(""Should use double for Type.DOUBLE"",
        ""double"", compiler.javaUnbox(doubleSchema));
    Assert.assertEquals(""Should use boolean for Type.BOOLEAN"",
        ""boolean"", compiler.javaUnbox(boolSchema));

    Schema dateSchema = LogicalTypes.date()
        .addToSchema(Schema.create(Schema.Type.INT));
    Schema timeSchema = LogicalTypes.timeMillis()
        .addToSchema(Schema.create(Schema.Type.INT));
    Schema timestampSchema = LogicalTypes.timestampMillis()
        .addToSchema(Schema.create(Schema.Type.LONG));
    // Date/time types should always use upper level java classes, even though
    // their underlying representations are primitive types
    Assert.assertEquals(""Should use Joda LocalDate for date type"",
        ""org.joda.time.LocalDate"", compiler.javaUnbox(dateSchema));
    Assert.assertEquals(""Should use Joda LocalTime for time-millis type"",
        ""org.joda.time.LocalTime"", compiler.javaUnbox(timeSchema));
    Assert.assertEquals(""Should use Joda DateTime for timestamp-millis type"",
        ""org.joda.time.DateTime"", compiler.javaUnbox(timestampSchema));
  }
",non-flaky,5
96875,apache_avro,TestSpecificCompiler.testJavaUnboxJsr310DateTime,"  @Test
  public void testJavaUnboxJsr310DateTime() throws Exception {
    SpecificCompiler compiler = createCompiler(JSR310);

    Schema dateSchema = LogicalTypes.date()
        .addToSchema(Schema.create(Schema.Type.INT));
    Schema timeSchema = LogicalTypes.timeMillis()
        .addToSchema(Schema.create(Schema.Type.INT));
    Schema timestampSchema = LogicalTypes.timestampMillis()
        .addToSchema(Schema.create(Schema.Type.LONG));
    // Date/time types should always use upper level java classes, even though
    // their underlying representations are primitive types
    Assert.assertEquals(""Should use java.time.LocalDate for date type"",
        ""java.time.LocalDate"", compiler.javaUnbox(dateSchema));
    Assert.assertEquals(""Should use java.time.LocalTime for time-millis type"",
        ""java.time.LocalTime"", compiler.javaUnbox(timeSchema));
    Assert.assertEquals(""Should use java.time.Instant for timestamp-millis type"",
        ""java.time.Instant"", compiler.javaUnbox(timestampSchema));
  }
",non-flaky,5
96876,apache_avro,TestSpecificCompiler.testNullableTypesJavaUnbox,"  @Test
  public void testNullableTypesJavaUnbox() throws Exception {
    SpecificCompiler compiler = createCompiler();
    compiler.setEnableDecimalLogicalType(false);

    // Nullable types should return boxed types instead of primitive types
    Schema nullableIntSchema1 = Schema.createUnion(
        Schema.create(Schema.Type.NULL), Schema.create(Schema.Type.INT));
    Schema nullableIntSchema2 = Schema.createUnion(
        Schema.create(Schema.Type.INT), Schema.create(Schema.Type.NULL));
    Assert.assertEquals(""Should return boxed type"",
        compiler.javaUnbox(nullableIntSchema1), ""java.lang.Integer"");
    Assert.assertEquals(""Should return boxed type"",
        compiler.javaUnbox(nullableIntSchema2), ""java.lang.Integer"");

    Schema nullableLongSchema1 = Schema.createUnion(
        Schema.create(Schema.Type.NULL), Schema.create(Schema.Type.LONG));
    Schema nullableLongSchema2 = Schema.createUnion(
        Schema.create(Schema.Type.LONG), Schema.create(Schema.Type.NULL));
    Assert.assertEquals(""Should return boxed type"",
        compiler.javaUnbox(nullableLongSchema1), ""java.lang.Long"");
    Assert.assertEquals(""Should return boxed type"",
        compiler.javaUnbox(nullableLongSchema2), ""java.lang.Long"");

    Schema nullableFloatSchema1 = Schema.createUnion(
        Schema.create(Schema.Type.NULL), Schema.create(Schema.Type.FLOAT));
    Schema nullableFloatSchema2 = Schema.createUnion(
        Schema.create(Schema.Type.FLOAT), Schema.create(Schema.Type.NULL));
    Assert.assertEquals(""Should return boxed type"",
        compiler.javaUnbox(nullableFloatSchema1), ""java.lang.Float"");
    Assert.assertEquals(""Should return boxed type"",
        compiler.javaUnbox(nullableFloatSchema2), ""java.lang.Float"");

    Schema nullableDoubleSchema1 = Schema.createUnion(
        Schema.create(Schema.Type.NULL), Schema.create(Schema.Type.DOUBLE));
    Schema nullableDoubleSchema2 = Schema.createUnion(
        Schema.create(Schema.Type.DOUBLE), Schema.create(Schema.Type.NULL));
    Assert.assertEquals(""Should return boxed type"",
        compiler.javaUnbox(nullableDoubleSchema1), ""java.lang.Double"");
    Assert.assertEquals(""Should return boxed type"",
        compiler.javaUnbox(nullableDoubleSchema2), ""java.lang.Double"");

    Schema nullableBooleanSchema1 = Schema.createUnion(
        Schema.create(Schema.Type.NULL), Schema.create(Schema.Type.BOOLEAN));
    Schema nullableBooleanSchema2 = Schema.createUnion(
        Schema.create(Schema.Type.BOOLEAN), Schema.create(Schema.Type.NULL));
    Assert.assertEquals(""Should return boxed type"",
        compiler.javaUnbox(nullableBooleanSchema1), ""java.lang.Boolean"");
    Assert.assertEquals(""Should return boxed type"",
        compiler.javaUnbox(nullableBooleanSchema2), ""java.lang.Boolean"");
  }
",non-flaky,5
96877,apache_avro,TestSpecificCompiler.testLogicalTypesWithMultipleFields,"  @Test
  public void testLogicalTypesWithMultipleFields() throws Exception {
    Schema logicalTypesWithMultipleFields = new Schema.Parser().parse(
        new File(""src/test/resources/logical_types_with_multiple_fields.avsc""));
    assertCompilesWithJavaCompiler(new File(OUTPUT_DIR.getRoot(), name.getMethodName()),
        new SpecificCompiler(logicalTypesWithMultipleFields).compile());
  }
",non-flaky,5
96878,apache_avro,TestSpecificCompiler.testUnionAndFixedFields,"  @Test
  public void testUnionAndFixedFields() throws Exception {
    Schema unionTypesWithMultipleFields = new Schema.Parser().parse(
        new File(""src/test/resources/union_and_fixed_fields.avsc""));
    assertCompilesWithJavaCompiler(new File(this.outputFile, name.getMethodName()),
        new SpecificCompiler(unionTypesWithMultipleFields).compile());
  }
",non-flaky,5
96879,apache_avro,TestSpecificCompiler.testLogicalTypesWithMultipleFieldsJsr310DateTime,"  @Test
  public void testLogicalTypesWithMultipleFieldsJsr310DateTime() throws Exception {
    Schema logicalTypesWithMultipleFields = new Schema.Parser().parse(
        new File(""src/test/resources/logical_types_with_multiple_fields.avsc""));
    assertCompilesWithJavaCompiler(new File(this.outputFile, name.getMethodName()),
        new SpecificCompiler(logicalTypesWithMultipleFields, JSR310).compile());
  }
",non-flaky,5
96880,apache_avro,TestSpecificCompiler.testConversionInstanceWithDecimalLogicalTypeDisabled,"  @Test
  public void testConversionInstanceWithDecimalLogicalTypeDisabled() throws Exception {
    SpecificCompiler compiler = createCompiler();
    compiler.setEnableDecimalLogicalType(false);

    Schema dateSchema = LogicalTypes.date()
        .addToSchema(Schema.create(Schema.Type.INT));
    Schema timeSchema = LogicalTypes.timeMillis()
        .addToSchema(Schema.create(Schema.Type.INT));
    Schema timestampSchema = LogicalTypes.timestampMillis()
        .addToSchema(Schema.create(Schema.Type.LONG));
    Schema decimalSchema = LogicalTypes.decimal(9,2)
        .addToSchema(Schema.create(Schema.Type.BYTES));
    Schema uuidSchema = LogicalTypes.uuid()
        .addToSchema(Schema.create(Schema.Type.STRING));

    Assert.assertEquals(""Should use DATE_CONVERSION for date type"",
        ""DATE_CONVERSION"", compiler.conversionInstance(dateSchema));
    Assert.assertEquals(""Should use TIME_CONVERSION for time type"",
        ""TIME_CONVERSION"", compiler.conversionInstance(timeSchema));
    Assert.assertEquals(""Should use TIMESTAMP_CONVERSION for date type"",
        ""TIMESTAMP_CONVERSION"", compiler.conversionInstance(timestampSchema));
    Assert.assertEquals(""Should use null for decimal if the flag is off"",
        ""null"", compiler.conversionInstance(decimalSchema));
    Assert.assertEquals(""Should use null for decimal if the flag is off"",
        ""null"", compiler.conversionInstance(uuidSchema));
  }
",non-flaky,5
96881,apache_avro,TestSpecificCompiler.testConversionInstanceWithDecimalLogicalTypeEnabled,"  @Test
  public void testConversionInstanceWithDecimalLogicalTypeEnabled() throws Exception {
    SpecificCompiler compiler = createCompiler();
    compiler.setEnableDecimalLogicalType(true);

    Schema dateSchema = LogicalTypes.date()
        .addToSchema(Schema.create(Schema.Type.INT));
    Schema timeSchema = LogicalTypes.timeMillis()
        .addToSchema(Schema.create(Schema.Type.INT));
    Schema timestampSchema = LogicalTypes.timestampMillis()
        .addToSchema(Schema.create(Schema.Type.LONG));
    Schema decimalSchema = LogicalTypes.decimal(9,2)
        .addToSchema(Schema.create(Schema.Type.BYTES));
    Schema uuidSchema = LogicalTypes.uuid()
        .addToSchema(Schema.create(Schema.Type.STRING));

    Assert.assertEquals(""Should use DATE_CONVERSION for date type"",
        ""DATE_CONVERSION"", compiler.conversionInstance(dateSchema));
    Assert.assertEquals(""Should use TIME_CONVERSION for time type"",
        ""TIME_CONVERSION"", compiler.conversionInstance(timeSchema));
    Assert.assertEquals(""Should use TIMESTAMP_CONVERSION for date type"",
        ""TIMESTAMP_CONVERSION"", compiler.conversionInstance(timestampSchema));
    Assert.assertEquals(""Should use null for decimal if the flag is off"",
        ""DECIMAL_CONVERSION"", compiler.conversionInstance(decimalSchema));
    Assert.assertEquals(""Should use null for decimal if the flag is off"",
        ""null"", compiler.conversionInstance(uuidSchema));
  }
",non-flaky,5
96882,apache_avro,TestSchemas.textCloning,"  @Test
  public void textCloning() {
    Schema recSchema = new Schema.Parser().parse(SCHEMA);
    Schemas.visit(recSchema, new PrintingVisitor());


    CloningVisitor cv = new CloningVisitor(recSchema);
    Schema trimmed = Schemas.visit(recSchema, cv);
    Assert.assertNull(trimmed.getDoc());
    Assert.assertNotNull(recSchema.getDoc());

    SchemaCompatibility.SchemaCompatibilityType compat =
        SchemaCompatibility.checkReaderWriterCompatibility(trimmed, recSchema).getType();
    Assert.assertEquals(SchemaCompatibility.SchemaCompatibilityType.COMPATIBLE, compat);
    compat = SchemaCompatibility.checkReaderWriterCompatibility(recSchema, trimmed).getType();
    Assert.assertEquals(SchemaCompatibility.SchemaCompatibilityType.COMPATIBLE, compat);
    Assert.assertNotNull(cv.toString());
  }
",non-flaky,5
96883,apache_avro,TestSchemas.copy,"  @Test
  public void textCloningCopyDocs() {
    Schema recSchema = new Schema.Parser().parse(SCHEMA);
    Schemas.visit(recSchema, new PrintingVisitor());


    Schema trimmed = Schemas.visit(recSchema, new CloningVisitor(new CloningVisitor.PropertyCopier() {
      @Override
      public void copy(final Schema first, final Schema second) {
        Schemas.copyLogicalTypes(first, second);
        Schemas.copyAliases(first, second);
      }
",non-flaky,5
96884,apache_avro,TestSchemas.testCloningError1,"  @Test(expected = IllegalStateException.class)
  public void testCloningError1() {
    // Visit Terminal with union
    Schema recordSchema = new Schema.Parser().parse(
        ""{\""type\"": \""record\"", \""name\"": \""R\"", \""fields\"":[{\""name\"": \""f1\"", \""type\"": [\""int\"", \""long\""]}]}"");
    new CloningVisitor(recordSchema).visitTerminal(recordSchema.getField(""f1"").schema());
  }
",non-flaky,5
96885,apache_avro,TestSchemas.testCloningError2,"  @Test(expected = IllegalStateException.class)
  public void testCloningError2() {
    // After visit Non-terminal with int
    Schema recordSchema = new Schema.Parser().parse(
        ""{\""type\"": \""record\"", \""name\"": \""R\"", \""fields\"":[{\""name\"": \""f1\"", \""type\"": \""int\""}]}"");
    new CloningVisitor(recordSchema).afterVisitNonTerminal(recordSchema.getField(""f1"").schema());
  }
",non-flaky,5
96886,apache_avro,TestSchemas.testHasGeneratedJavaClass,"  @Test
  public void testHasGeneratedJavaClass() {
    Assert.assertTrue(Schemas.hasGeneratedJavaClass(
        new Schema.Parser().parse(""{\""type\"": \""fixed\"", \""name\"": \""N\"", \""size\"": 10}"")));
    Assert.assertFalse(Schemas.hasGeneratedJavaClass(new Schema.Parser().parse(""{\""type\"": \""int\""}"")));
  }
",non-flaky,5
96887,apache_avro,TestSchemas.testGetJavaClassName,"  @Test
  public void testGetJavaClassName() {
    Assert.assertEquals(""N"", Schemas.getJavaClassName(
        new Schema.Parser().parse(""{\""type\"": \""fixed\"", \""name\"": \""N\"", \""size\"": 10}"")));
    Assert.assertEquals(""N"", Schemas.getJavaClassName(
        new Schema.Parser().parse(""{\""type\"": \""fixed\"", \""name\"": \""N\"", \""size\"": 10, \""namespace\"": \""\""}"")));
    Assert.assertEquals(""com.example.N"", Schemas.getJavaClassName(
        new Schema.Parser().parse(""{\""type\"": \""fixed\"", \""name\"": \""N\"", \""size\"": 10, \""namespace\"": \""com.example\""}"")));
  }
",non-flaky,5
96888,apache_avro,TestSchemas.testVisit1,"  @Test
  public void testVisit1() {
    String s1 = ""{\""type\"": \""record\"", \""name\"": \""t1\"", \""fields\"": ["" +
        ""{\""name\"": \""f1\"", \""type\"": \""int\""}"" +
        ""]}"";
    Assert.assertEquals(""t1."", Schemas.visit(new Schema.Parser().parse(s1), new TestVisitor()));
  }
",non-flaky,5
96889,apache_avro,TestSchemas.testVisit2,"  @Test
  public void testVisit2() {
    String s2 = ""{\""type\"": \""record\"", \""name\"": \""c1\"", \""fields\"": ["" +
        ""{\""name\"": \""f1\"", \""type\"": \""int\""}"" +
        ""]}"";
    Assert.assertEquals(""c1.\""int\""!"", Schemas.visit(new Schema.Parser().parse(s2), new TestVisitor()));

  }
",non-flaky,5
96890,apache_avro,TestSchemas.testVisit3,"  @Test
  public void testVisit3() {
    String s3 = ""{\""type\"": \""record\"", \""name\"": \""ss1\"", \""fields\"": ["" +
        ""{\""name\"": \""f1\"", \""type\"": \""int\""}"" +
        ""]}"";
    Assert.assertEquals(""ss1."", Schemas.visit(new Schema.Parser().parse(s3), new TestVisitor()));

  }
",non-flaky,5
96891,apache_avro,TestSchemas.testVisit4,"  @Test
  public void testVisit4() {
    String s4 = ""{\""type\"": \""record\"", \""name\"": \""st1\"", \""fields\"": ["" +
        ""{\""name\"": \""f1\"", \""type\"": \""int\""}"" +
        ""]}"";
    Assert.assertEquals(""st1.!"", Schemas.visit(new Schema.Parser().parse(s4), new TestVisitor()));

  }
",non-flaky,5
96892,apache_avro,TestSchemas.testVisit5,"  @Test
  public void testVisit5() {
    String s5 = ""{\""type\"": \""record\"", \""name\"": \""c1\"", \""fields\"": ["" +
        ""{\""name\"": \""f1\"", \""type\"": {\""type\"": \""record\"", \""name\"": \""c2\"", \""fields\"": "" +
        ""[{\""name\"": \""f11\"", \""type\"": \""int\""}]}},"" +
        ""{\""name\"": \""f2\"", \""type\"": \""long\""}"" +
        ""]}"";
    Assert.assertEquals(""c1.c2.\""int\""!\""long\""!"",
        Schemas.visit(new Schema.Parser().parse(s5), new TestVisitor()));

  }
",non-flaky,5
96893,apache_avro,TestSchemas.testVisit6,"  @Test
  public void testVisit6() {
    String s6 = ""{\""type\"": \""record\"", \""name\"": \""c1\"", \""fields\"": ["" +
        ""{\""name\"": \""f1\"", \""type\"": {\""type\"": \""record\"", \""name\"": \""ss2\"", \""fields\"": "" +
        ""[{\""name\"": \""f11\"", \""type\"": \""int\""}]}},"" +
        ""{\""name\"": \""f2\"", \""type\"": \""long\""}"" +
        ""]}"";
    Assert.assertEquals(""c1.ss2.!"",
        Schemas.visit(new Schema.Parser().parse(s6), new TestVisitor()));

  }
",non-flaky,5
96894,apache_avro,TestSchemas.testVisit7,"  @Test
  public void testVisit7() {
    String s7 = ""{\""type\"": \""record\"", \""name\"": \""c1\"", \""fields\"": ["" +
        ""{\""name\"": \""f1\"", \""type\"": {\""type\"": \""record\"", \""name\"": \""css2\"", \""fields\"": "" +
        ""[{\""name\"": \""f11\"", \""type\"": \""int\""}]}},"" +
        ""{\""name\"": \""f2\"", \""type\"": \""long\""}"" +
        ""]}"";
    Assert.assertEquals(""c1.css2.\""int\""!!"",
        Schemas.visit(new Schema.Parser().parse(s7), new TestVisitor()));
  }
",non-flaky,5
96895,apache_avro,TestSchemas.testVisit8,"  @Test(expected = UnsupportedOperationException.class)
  public void testVisit8() {
    String s8 = ""{\""type\"": \""record\"", \""name\"": \""c1\"", \""fields\"": ["" +
        ""{\""name\"": \""f1\"", \""type\"": {\""type\"": \""record\"", \""name\"": \""cst2\"", \""fields\"": "" +
        ""[{\""name\"": \""f11\"", \""type\"": \""int\""}]}},"" +
        ""{\""name\"": \""f2\"", \""type\"": \""int\""}"" +
        ""]}"";
    Schemas.visit(new Schema.Parser().parse(s8), new TestVisitor());
  }
",non-flaky,5
96896,apache_avro,TestSchemas.testVisit9,"  @Test
  public void testVisit9() {
    String s9 = ""{\""type\"": \""record\"", \""name\"": \""c1\"", \""fields\"": ["" +
        ""{\""name\"": \""f1\"", \""type\"": {\""type\"": \""record\"", \""name\"": \""ct2\"", \""fields\"": "" +
        ""[{\""name\"": \""f11\"", \""type\"": \""int\""}]}},"" +
        ""{\""name\"": \""f2\"", \""type\"": \""long\""}"" +
        ""]}"";
    Assert.assertEquals(""c1.ct2.\""int\""!"", Schemas.visit(new Schema.Parser().parse(s9), new TestVisitor()));
  }
",non-flaky,5
96897,apache_avro,TestSchemas.visitTerminal,"  @Test(expected = UnsupportedOperationException.class)
  public void testVisit10() {
    String s10 = ""{\""type\"": \""record\"", \""name\"": \""c1\"", \""fields\"": ["" +
        ""{\""name\"": \""f1\"", \""type\"": {\""type\"": \""record\"", \""name\"": \""ct2\"", \""fields\"": "" +
        ""[{\""name\"": \""f11\"", \""type\"": \""int\""}]}},"" +
        ""{\""name\"": \""f2\"", \""type\"": \""int\""}"" +
        ""]}"";
    Schemas.visit(new Schema.Parser().parse(s10),
        new TestVisitor() {
          public SchemaVisitorAction visitTerminal(Schema terminal) {
            return SchemaVisitorAction.SKIP_SUBTREE;
          }
",non-flaky,5
96898,apache_avro,TestSchemas.visitTerminal,"  @Test
  public void testVisit11() {
    String s11 = ""{\""type\"": \""record\"", \""name\"": \""c1\"", \""fields\"": ["" +
        ""{\""name\"": \""f1\"", \""type\"": {\""type\"": \""record\"", \""name\"": \""c2\"", \""fields\"": "" +
        ""[{\""name\"": \""f11\"", \""type\"": \""int\""},{\""name\"": \""f12\"", \""type\"": \""double\""}"" +
        ""]}},"" +
        ""{\""name\"": \""f2\"", \""type\"": \""long\""}"" +
        ""]}"";
    Assert.assertEquals(""c1.c2.\""int\"".!\""long\"".!"", Schemas.visit(new Schema.Parser().parse(s11),
        new TestVisitor() {
          public SchemaVisitorAction visitTerminal(Schema terminal) {
            sb.append(terminal).append('.');
            return SchemaVisitorAction.SKIP_SIBLINGS;
          }
",non-flaky,5
96899,apache_avro,TestSchemas.visitTerminal,"  @Test
  public void testVisit12() {
    String s12 = ""{\""type\"": \""record\"", \""name\"": \""c1\"", \""fields\"": ["" +
        ""{\""name\"": \""f1\"", \""type\"": {\""type\"": \""record\"", \""name\"": \""ct2\"", \""fields\"": "" +
        ""[{\""name\"": \""f11\"", \""type\"": \""int\""}]}},"" +
        ""{\""name\"": \""f2\"", \""type\"": \""long\""}"" +
        ""]}"";
    Assert.assertEquals(""c1.ct2.\""int\""."", Schemas.visit(new Schema.Parser().parse(s12),
        new TestVisitor() {
          public SchemaVisitorAction visitTerminal(Schema terminal) {
            sb.append(terminal).append('.');
            return SchemaVisitorAction.TERMINATE;
          }
",non-flaky,5
96900,apache_avro,TestSchemas.visitTerminal,"  @Test
  public void testVisit13() {
    String s12 = ""{\""type\"": \""int\""}"";
    Assert.assertEquals(""\""int\""."", Schemas.visit(new Schema.Parser().parse(s12),
        new TestVisitor() {
          public SchemaVisitorAction visitTerminal(Schema terminal) {
            sb.append(terminal).append('.');
            return SchemaVisitorAction.SKIP_SIBLINGS;
          }
",non-flaky,5
96901,apache_avro,TestIdl.runTests,"  @Test
  public void runTests() throws Exception {
    if (! ""run"".equals(TEST_MODE)) return;

    int passed = 0, failed = 0;

    for (GenTest t : tests) {
      try {
        t.run();
        passed++;
      } catch (Exception e) {
        failed++;
        System.err.println(""Failed: "" + t.testName());
        e.printStackTrace(System.err);
      }
    }

    if (failed > 0) {
      fail(String.valueOf(failed) + "" tests failed"");
    }
  }
",non-flaky,5
96902,apache_avro,TestIdl.writeTests,"  @Test
  public void writeTests() throws Exception {
    if (! ""write"".equals(TEST_MODE)) return;

    for (GenTest t : tests) {
      t.write();
    }
  }
",non-flaky,5
96903,apache_avro,TestSchemaResolver.testResolving,"  @Test
  public void testResolving() throws ParseException, MalformedURLException, IOException {
    File file = new File(""."");
    String currentWorkPath = file.getAbsolutePath();
    String testIdl = currentWorkPath + File.separator + ""src"" + File.separator + ""test""
        + File.separator + ""idl"" + File.separator + ""cycle.avdl"";
    Idl compiler = new Idl(new File(testIdl));
    Protocol protocol = compiler.CompilationUnit();
    System.out.println(protocol);
    Assert.assertEquals(5, protocol.getTypes().size());
  }
",non-flaky,5
96904,apache_avro,TestSchemaResolver.testIsUnresolvedSchemaError1,"  @Test(expected = IllegalArgumentException.class)
  public void testIsUnresolvedSchemaError1() {
    // No ""org.apache.avro.compiler.idl.unresolved.name"" property
    Schema s = SchemaBuilder.record(""R"").fields().endRecord();
    SchemaResolver.getUnresolvedSchemaName(s);
  }
",non-flaky,5
96905,apache_avro,TestSchemaResolver.testIsUnresolvedSchemaError2,"  @Test(expected = IllegalArgumentException.class)
  public void testIsUnresolvedSchemaError2() {
    // No ""UnresolvedSchema"" property
    Schema s = SchemaBuilder.record(""R"")
        .prop(""org.apache.avro.compiler.idl.unresolved.name"", ""x"").fields().endRecord();
    SchemaResolver.getUnresolvedSchemaName(s);
  }
",non-flaky,5
96906,apache_avro,TestSchemaResolver.testIsUnresolvedSchemaError3,"  @Test(expected = IllegalArgumentException.class)
  public void testIsUnresolvedSchemaError3() {
    // Namespace not ""org.apache.avro.compiler"".
    Schema s = SchemaBuilder.record(""UnresolvedSchema"")
        .prop(""org.apache.avro.compiler.idl.unresolved.name"", ""x"")
        .fields().endRecord();
    SchemaResolver.getUnresolvedSchemaName(s);
  }
",non-flaky,5
96907,apache_avro,TestSchemaResolver.testGetUnresolvedSchemaNameError,"  @Test(expected = IllegalArgumentException.class)
  public void testGetUnresolvedSchemaNameError() {
    Schema s = SchemaBuilder.fixed(""a"").size(10);
    SchemaResolver.getUnresolvedSchemaName(s);
  }
",non-flaky,5
96908,apache_avro,TestCycle.testCycleGeneration,"  @Test
  public void testCycleGeneration() throws ParseException, IOException {
    final ClassLoader cl = Thread.currentThread().getContextClassLoader();
    Idl idl = new Idl(cl.getResourceAsStream(""input/cycle.avdl""),
            ""UTF-8"");
    Protocol protocol = idl.CompilationUnit();
    String json = protocol.toString();
    LOG.info(json);

    SpecificCompiler compiler = new SpecificCompiler(protocol);
    compiler.setStringType(GenericData.StringType.String);
    File output = new File(""./target"");
    compiler.compileToDestination(null, output);

    Map<String, Schema> schemas = new HashMap<>();
    for (Schema schema : protocol.getTypes()) {
      final String name = schema.getName();
      schemas.put(name, schema);
    }

    GenericRecordBuilder rb2 = new GenericRecordBuilder(schemas.get(""SampleNode""));
    rb2.set(""count"", 10);
    rb2.set(""subNodes"", Collections.EMPTY_LIST);
    GenericData.Record node = rb2.build();

    GenericRecordBuilder mb = new GenericRecordBuilder(schemas.get(""Method""));
    mb.set(""declaringClass"", ""Test"");
    mb.set(""methodName"", ""test"");
    GenericData.Record method = mb.build();

    GenericRecordBuilder spb = new GenericRecordBuilder(schemas.get(""SamplePair""));
    spb.set(""method"", method);
    spb.set(""node"", node);
    GenericData.Record sp = spb.build();

    GenericRecordBuilder rb = new GenericRecordBuilder(schemas.get(""SampleNode""));
    rb.set(""count"", 10);
    rb.set(""subNodes"", Arrays.asList(sp));
    GenericData.Record record = rb.build();

    serDeserRecord(record);

  }
",non-flaky,5
96909,apache_avro,TestGeneratedCode.withoutSchemaMigration,"  @Test
  public void withoutSchemaMigration() throws IOException {
    FullRecordV1 src = new FullRecordV1(true, 87231, 731L, 54.2832F, 38.321, ""Hi there"", null);
    Assert.assertTrue(""Test schema must allow for custom coders."",
                      ((SpecificRecordBase)src).hasCustomCoders());

    ByteArrayOutputStream out = new ByteArrayOutputStream(1024);
    Encoder e = EncoderFactory.get().directBinaryEncoder(out, null);
    DatumWriter<FullRecordV1> w = (DatumWriter<FullRecordV1>)MODEL.createDatumWriter(V1S);
    w.write(src, e);
    e.flush();

    ByteArrayInputStream in = new ByteArrayInputStream(out.toByteArray());
    Decoder d = DecoderFactory.get().directBinaryDecoder(in, null);
    DatumReader<FullRecordV1> r = (DatumReader<FullRecordV1>)MODEL.createDatumReader(V1S);
    FullRecordV1 dst = r.read(null, d);

    Assert.assertEquals(src, dst);
  }
",non-flaky,5
96910,apache_avro,TestGeneratedCode.withSchemaMigration,"  @Test
  public void withSchemaMigration() throws IOException {
    FullRecordV2 src = new FullRecordV2(true, 731, 87231, 38L, 54.2832F, ""Hi there"",
                                        ByteBuffer.wrap(Utf8.getBytesFor(""Hello, world!"")));
    Assert.assertTrue(""Test schema must allow for custom coders."",
                      ((SpecificRecordBase)src).hasCustomCoders());

    ByteArrayOutputStream out = new ByteArrayOutputStream(1024);
    Encoder e = EncoderFactory.get().directBinaryEncoder(out, null);
    DatumWriter<FullRecordV2> w = (DatumWriter<FullRecordV2>)MODEL.createDatumWriter(V2S);
    w.write(src, e);
    e.flush();

    ByteArrayInputStream in = new ByteArrayInputStream(out.toByteArray());
    Decoder d = DecoderFactory.get().directBinaryDecoder(in, null);
    DatumReader<FullRecordV1> r = (DatumReader<FullRecordV1>)MODEL.createDatumReader(V2S, V1S);
    FullRecordV1 dst = r.read(null, d);

    FullRecordV1 expected = new FullRecordV1(true, 87231, 731L, 54.2832F, 38.0, null,
                                             ""Hello, world!"");
    Assert.assertEquals(expected, dst);
  }
",non-flaky,5
96911,apache_avro,TestAvroOutputFormat.testSetSyncInterval,"  @Test
  public void testSetSyncInterval() {
    JobConf jobConf = new JobConf();
    int newSyncInterval = 100000;
    AvroOutputFormat.setSyncInterval(jobConf, newSyncInterval);

    assertEquals(newSyncInterval, jobConf.getInt(
            AvroOutputFormat.SYNC_INTERVAL_KEY, -1));
  }
",non-flaky,5
96912,apache_avro,TestAvroOutputFormat.testNoCodec,"  @Test
  public void testNoCodec() {
    JobConf job = new JobConf();
    assertNull(AvroOutputFormat.getCodecFactory(job));

    job = new JobConf();
    job.set(""mapred.output.compress"", ""false"");
    job.set(""mapred.output.compression.codec"", ""org.apache.hadoop.io.compress.BZip2Codec"");
    assertNull(AvroOutputFormat.getCodecFactory(job));

    job = new JobConf();
    job.set(""mapred.output.compress"", ""false"");
    job.set(AvroJob.OUTPUT_CODEC, ""bzip2"");
    assertNull(AvroOutputFormat.getCodecFactory(job));
  }
",non-flaky,5
96913,apache_avro,TestAvroOutputFormat.testBZip2CodecUsingHadoopClass,"  @Test
  public void testBZip2CodecUsingHadoopClass() {
    CodecFactory avroBZip2Codec = CodecFactory.fromString(""bzip2"");

    JobConf job = new JobConf();
    job.set(""mapred.output.compress"", ""true"");
    job.set(""mapred.output.compression.codec"", ""org.apache.hadoop.io.compress.BZip2Codec"");
    CodecFactory factory = AvroOutputFormat.getCodecFactory(job);
    assertNotNull(factory);
    assertEquals(factory.getClass(), avroBZip2Codec.getClass());
  }
",non-flaky,5
96914,apache_avro,TestAvroOutputFormat.testBZip2CodecUsingAvroCodec,"  @Test
  public void testBZip2CodecUsingAvroCodec() {
    CodecFactory avroBZip2Codec = CodecFactory.fromString(""bzip2"");

    JobConf job = new JobConf();
    job.set(""mapred.output.compress"", ""true"");
    job.set(AvroJob.OUTPUT_CODEC, ""bzip2"");
    CodecFactory factory = AvroOutputFormat.getCodecFactory(job);
    assertNotNull(factory);
    assertEquals(factory.getClass(), avroBZip2Codec.getClass());
  }
",non-flaky,5
96915,apache_avro,TestAvroOutputFormat.testDeflateCodecUsingHadoopClass,"  @Test
  public void testDeflateCodecUsingHadoopClass() {
    CodecFactory avroDeflateCodec = CodecFactory.fromString(""deflate"");

    JobConf job = new JobConf();
    job.set(""mapred.output.compress"", ""true"");
    job.set(""mapred.output.compression.codec"", ""org.apache.hadoop.io.compress.DeflateCodec"");
    CodecFactory factory = AvroOutputFormat.getCodecFactory(job);
    assertNotNull(factory);
    assertEquals(factory.getClass(), avroDeflateCodec.getClass());
  }
",non-flaky,5
96916,apache_avro,TestAvroOutputFormat.testDeflateCodecUsingAvroCodec,"  @Test
  public void testDeflateCodecUsingAvroCodec() {
    CodecFactory avroDeflateCodec = CodecFactory.fromString(""deflate"");

    JobConf job = new JobConf();
    job.set(""mapred.output.compress"", ""true"");
    job.set(AvroJob.OUTPUT_CODEC, ""deflate"");
    CodecFactory factory = AvroOutputFormat.getCodecFactory(job);
    assertNotNull(factory);
    assertEquals(factory.getClass(), avroDeflateCodec.getClass());
  }
",non-flaky,5
96917,apache_avro,TestAvroOutputFormat.testSnappyCodecUsingHadoopClass,"  @Test
  public void testSnappyCodecUsingHadoopClass() {
    CodecFactory avroSnappyCodec = CodecFactory.fromString(""snappy"");

    JobConf job = new JobConf();
    job.set(""mapred.output.compress"", ""true"");
    job.set(""mapred.output.compression.codec"", ""org.apache.hadoop.io.compress.SnappyCodec"");
    CodecFactory factory = AvroOutputFormat.getCodecFactory(job);
    assertNotNull(factory);
    assertEquals(factory.getClass(), avroSnappyCodec.getClass());
  }
",non-flaky,5
96918,apache_avro,TestAvroOutputFormat.testSnappyCodecUsingAvroCodec,"  @Test
  public void testSnappyCodecUsingAvroCodec() {
    CodecFactory avroSnappyCodec = CodecFactory.fromString(""snappy"");

    JobConf job = new JobConf();
    job.set(""mapred.output.compress"", ""true"");
    job.set(AvroJob.OUTPUT_CODEC, ""snappy"");
    CodecFactory factory = AvroOutputFormat.getCodecFactory(job);
    assertNotNull(factory);
    assertEquals(factory.getClass(), avroSnappyCodec.getClass());
  }
",non-flaky,5
96919,apache_avro,TestAvroOutputFormat.testGZipCodecUsingHadoopClass,"  @Test
  public void testGZipCodecUsingHadoopClass() {
    CodecFactory avroDeflateCodec = CodecFactory.fromString(""deflate"");

    JobConf job = new JobConf();
    job.set(""mapred.output.compress"", ""true"");
    job.set(""mapred.output.compression.codec"", ""org.apache.hadoop.io.compress.GZipCodec"");
    CodecFactory factory = AvroOutputFormat.getCodecFactory(job);
    assertNotNull(factory);
    assertEquals(factory.getClass(), avroDeflateCodec.getClass());
  }
",non-flaky,5
96920,apache_avro,TestReflectJob.testJob,"  @Test
  public void testJob() throws Exception {
    JobConf job = new JobConf();
    String dir = ""target/testReflectJob"";
    Path inputPath = new Path(dir + ""/in"");
    Path outputPath = new Path(dir + ""/out"");

    outputPath.getFileSystem(job).delete(outputPath);
    inputPath.getFileSystem(job).delete(inputPath);

    writeLinesFile(new File(dir+""/in""));

    job.setJobName(""reflect"");

    AvroJob.setInputSchema(job, ReflectData.get().getSchema(Text.class));
    AvroJob.setMapOutputSchema
      (job, new Pair(new Text(""""), new Count(0L)).getSchema());
    AvroJob.setOutputSchema(job, ReflectData.get().getSchema(WordCount.class));

    AvroJob.setMapperClass(job, MapImpl.class);
    //AvroJob.setCombinerClass(job, ReduceImpl.class);
    AvroJob.setReducerClass(job, ReduceImpl.class);

    FileInputFormat.setInputPaths(job, inputPath);
    FileOutputFormat.setOutputPath(job, outputPath);

    AvroJob.setReflect(job); // use reflection

    JobClient.runJob(job);

    validateCountsFile(new File(new File(dir, ""out""), ""part-00000.avro""));
  }
",non-flaky,5
96921,apache_avro,TestAvroInputFormat.testIgnoreFilesWithoutExtension,"  @Test
  public void testIgnoreFilesWithoutExtension() throws Exception {
    fs.mkdirs(inputDir);
    Path avroFile = new Path(inputDir, ""somefile.avro"");
    Path textFile = new Path(inputDir, ""someotherfile.txt"");
    fs.create(avroFile).close();
    fs.create(textFile).close();

    FileInputFormat.setInputPaths(conf, inputDir);

    AvroInputFormat inputFormat = new AvroInputFormat();
    FileStatus[] statuses = inputFormat.listStatus(conf);
    Assert.assertEquals(1, statuses.length);
    Assert.assertEquals(""somefile.avro"", statuses[0].getPath().getName());

    conf.setBoolean(AvroInputFormat.IGNORE_FILES_WITHOUT_EXTENSION_KEY, false);
    statuses = inputFormat.listStatus(conf);
    Assert.assertEquals(2, statuses.length);
    Set<String> names = new HashSet<>();
    names.add(statuses[0].getPath().getName());
    names.add(statuses[1].getPath().getName());
    Assert.assertTrue(names.contains(""somefile.avro""));
    Assert.assertTrue(names.contains(""someotherfile.txt""));
  }
",non-flaky,5
96922,apache_avro,TestAvroWrapper.testToString,"  @Test
  public void testToString() {
    String datum = ""my string"";
    AvroWrapper<CharSequence> wrapper = new AvroWrapper<>(datum);
    assertEquals(datum, wrapper.toString());
  }
",non-flaky,5
96923,apache_avro,TestWordCount.runTestsInOrder,"  @Test
  public void runTestsInOrder() throws Exception {
    String pathOut = OUTPUT_DIR.getRoot().getPath();
    testJob(pathOut);
    testProjection(pathOut);
  }
",non-flaky,5
96924,apache_avro,TestAvroTextSort.testSort,"  @Test
  public void testSort() throws Exception {
    JobConf job = new JobConf();
    String inputPath = INPUT_DIR.getRoot().getPath();
    Path outputPath = new Path(OUTPUT_DIR.getRoot().getPath());
    outputPath.getFileSystem(job).delete(outputPath);

    WordCountUtil.writeLinesBytesFile(inputPath);

    job.setInputFormat(AvroAsTextInputFormat.class);
    job.setOutputFormat(AvroTextOutputFormat.class);
    job.setOutputKeyClass(Text.class);

    FileInputFormat.setInputPaths(job, new Path(inputPath));
    FileOutputFormat.setOutputPath(job, outputPath);

    JobClient.runJob(job);

    WordCountUtil.validateSortedFile(outputPath.toString() + ""/part-00000.avro"");
  }
",non-flaky,5
96925,apache_avro,TestAvroMultipleInputs.testJob,"  @Test
  public void testJob() throws Exception {
    JobConf job = new JobConf();
    Path inputPath1 = new Path(INPUT_DIR_1.getRoot().getPath());
    Path inputPath2 = new Path(INPUT_DIR_2.getRoot().getPath());
    Path outputPath = new Path(OUTPUT_DIR.getRoot().getPath());

    outputPath.getFileSystem(job).delete(outputPath);

    writeNamesFiles(new File(inputPath1.toUri().getPath()));
    writeBalancesFiles(new File(inputPath2.toUri().getPath()));

    job.setJobName(""multiple-inputs-join"");
    AvroMultipleInputs.addInputPath(job, inputPath1, NamesMapImpl.class,
            ReflectData.get().getSchema(NamesRecord.class));
    AvroMultipleInputs.addInputPath(job, inputPath2, BalancesMapImpl.class,
            ReflectData.get().getSchema(BalancesRecord.class));

    Schema keySchema = ReflectData.get().getSchema(KeyRecord.class);
    Schema valueSchema = ReflectData.get().getSchema(JoinableRecord.class);
    AvroJob.setMapOutputSchema(job,
            Pair.getPairSchema(keySchema, valueSchema));
    AvroJob.setOutputSchema(job,
            ReflectData.get().getSchema(CompleteRecord.class));

    AvroJob.setReducerClass(job, ReduceImpl.class);
    job.setNumReduceTasks(1);

    FileOutputFormat.setOutputPath(job, outputPath);

    AvroJob.setReflect(job);

    JobClient.runJob(job);

    validateCompleteFile(new File(OUTPUT_DIR.getRoot(), ""part-00000.avro""));
  }
",non-flaky,5
96926,apache_avro,TestAvroTextOutputFormat.testAvroTextRecordWriter,"  @Test
  public void testAvroTextRecordWriter() throws Exception {
    File file = new File(tmpFolder.getRoot().getPath(), ""writer"");
    Schema schema = Schema.create(Schema.Type.BYTES);
    DatumWriter<ByteBuffer> datumWriter =
      new GenericDatumWriter<>(schema);
    DataFileWriter<ByteBuffer> fileWriter =
      new DataFileWriter<>(datumWriter);
    fileWriter.create(schema, file);
    RecordWriter<Object, Object> rw = new AvroTextOutputFormat<>()
      .new AvroTextRecordWriter(fileWriter, ""\t"".getBytes(UTF8));

    rw.write(null, null);
    rw.write(null, NullWritable.get());
    rw.write(NullWritable.get(), null);
    rw.write(NullWritable.get(), NullWritable.get());

    rw.write(""k1"", null);
    rw.write(""k2"", NullWritable.get());

    rw.write(null, ""v1"");
    rw.write(NullWritable.get(), ""v2"");

    rw.write(""k3"", ""v3"");
    rw.write(new Text(""k4""), new Text(""v4""));

    rw.close(null);

    DatumReader<ByteBuffer> reader = new GenericDatumReader<>();
    DataFileReader<ByteBuffer> fileReader =
      new DataFileReader<>(file, reader);
    assertEquals(""k1"", asString(fileReader.next()));
    assertEquals(""k2"", asString(fileReader.next()));
    assertEquals(""v1"", asString(fileReader.next()));
    assertEquals(""v2"", asString(fileReader.next()));
    assertEquals(""k3\tv3"", asString(fileReader.next()));
    assertEquals(""k4\tv4"", asString(fileReader.next()));
    assertFalse(""End"", fileReader.hasNext());
  }
",non-flaky,5
96927,apache_avro,TestAvroMultipleOutputs.runTestsInOrder,"  @Test
  public void runTestsInOrder() throws Exception {
    String avroPath = OUTPUT_DIR.getRoot().getPath();
    testJob(avroPath);
    testProjection(avroPath);
    testProjectionNewMethodsOne(avroPath);
    testProjectionNewMethodsTwo(avroPath);
    testProjection1(avroPath);
    testJobNoreducer();
    testProjectionNoreducer(avroPath);
  }
",non-flaky,5
96928,apache_avro,TestSequenceFileReader.testReadSequenceFile,"  @Test
  public void testReadSequenceFile() throws Exception {
    checkFile(new SequenceFileReader<>(file()));
  }
",non-flaky,5
96929,apache_avro,TestSequenceFileReader.testSequenceFileInputFormat,"  @Test
  public void testSequenceFileInputFormat() throws Exception {
    JobConf job = new JobConf();
    Path outputPath = new Path(OUTPUT_DIR.getRoot().getPath());
    outputPath.getFileSystem(job).delete(outputPath);

    // configure input for Avro from sequence file
    AvroJob.setInputSequenceFile(job);
    FileInputFormat.setInputPaths(job, file().toURI().toString());
    AvroJob.setInputSchema(job, SCHEMA);

    // mapper is default, identity
    // reducer is default, identity

    // configure output for avro
    AvroJob.setOutputSchema(job, SCHEMA);
    FileOutputFormat.setOutputPath(job, outputPath);

    JobClient.runJob(job);

    checkFile(new DataFileReader<>
              (new File(outputPath.toString() + ""/part-00000.avro""),
               new SpecificDatumReader<>()));
  }
",non-flaky,5
96930,apache_avro,TestSequenceFileReader.testNonAvroMapper,"  @Test
  public void testNonAvroMapper() throws Exception {
    JobConf job = new JobConf();
    Path outputPath = new Path(OUTPUT_DIR.getRoot().getPath());
    outputPath.getFileSystem(job).delete(outputPath);

    // configure input for non-Avro sequence file
    job.setInputFormat(SequenceFileInputFormat.class);
    FileInputFormat.setInputPaths(job, file().toURI().toString());

    // use a hadoop mapper that emits Avro output
    job.setMapperClass(NonAvroMapper.class);

    // reducer is default, identity

    // configure output for avro
    FileOutputFormat.setOutputPath(job, outputPath);
    AvroJob.setOutputSchema(job, SCHEMA);

    JobClient.runJob(job);

    checkFile(new DataFileReader<>
              (new File(outputPath.toString() + ""/part-00000.avro""),
               new SpecificDatumReader<>()));
  }
",non-flaky,5
96931,apache_avro,TestSequenceFileReader.testNonAvroMapOnly,"  @Test
  public void testNonAvroMapOnly() throws Exception {
    JobConf job = new JobConf();
    Path outputPath = new Path(OUTPUT_DIR.getRoot().getPath());
    outputPath.getFileSystem(job).delete(outputPath);

    // configure input for non-Avro sequence file
    job.setInputFormat(SequenceFileInputFormat.class);
    FileInputFormat.setInputPaths(job, file().toURI().toString());

    // use a hadoop mapper that emits Avro output
    job.setMapperClass(NonAvroOnlyMapper.class);

    // configure output for avro
    job.setNumReduceTasks(0);                     // map-only
    FileOutputFormat.setOutputPath(job, outputPath);
    AvroJob.setOutputSchema(job, SCHEMA);

    JobClient.runJob(job);

    checkFile(new DataFileReader<>
              (new File(outputPath.toString() + ""/part-00000.avro""),
               new SpecificDatumReader<>()));
  }
",non-flaky,5
96932,apache_avro,TestSequenceFileReader.testNonAvroReducer,"  @Test
  public void testNonAvroReducer() throws Exception {
    JobConf job = new JobConf();
    Path outputPath = new Path(OUTPUT_DIR.getRoot().getPath());
    outputPath.getFileSystem(job).delete(outputPath);

    // configure input for Avro from sequence file
    AvroJob.setInputSequenceFile(job);
    AvroJob.setInputSchema(job, SCHEMA);
    FileInputFormat.setInputPaths(job, file().toURI().toString());

    // mapper is default, identity

    // use a hadoop reducer that consumes Avro input
    AvroJob.setMapOutputSchema(job, SCHEMA);
    job.setReducerClass(NonAvroReducer.class);

    // configure outputPath for non-Avro SequenceFile
    job.setOutputFormat(SequenceFileOutputFormat.class);
    FileOutputFormat.setOutputPath(job, outputPath);

    // output key/value classes are default, LongWritable/Text

    JobClient.runJob(job);

    checkFile(new SequenceFileReader<>
              (new File(outputPath.toString() + ""/part-00000"")));
  }
",non-flaky,5
96933,apache_avro,TestWeather.testMapOnly,"  @Test
  public void testMapOnly() throws Exception {
    JobConf job = new JobConf();
    String inDir = System.getProperty(""share.dir"",""../../../share"")+""/test/data"";
    Path input = new Path(inDir+""/weather.avro"");
    Path output = new Path(""target/test/weather-ident"");

    output.getFileSystem(job).delete(output);

    job.setJobName(""identity map weather"");

    AvroJob.setInputSchema(job, Weather.SCHEMA$);
    AvroJob.setOutputSchema(job, Weather.SCHEMA$);

    FileInputFormat.setInputPaths(job, input);
    FileOutputFormat.setOutputPath(job, output);
    FileOutputFormat.setCompressOutput(job, true);

    job.setNumReduceTasks(0);                     // map-only

    JobClient.runJob(job);

    // check output is correct
    DatumReader<Weather> reader = new SpecificDatumReader<>();
    DataFileReader<Weather> check = new DataFileReader<>
      (new File(inDir + ""/weather.avro""), reader);
    DataFileReader<Weather> sorted = new DataFileReader<>
      (new File(output.toString() + ""/part-00000.avro""), reader);

    for (Weather w : sorted)
      assertEquals(check.next(), w);

    check.close();
    sorted.close();
  }
",non-flaky,5
96934,apache_avro,TestWeather.testSort,"  @Test
  public void testSort() throws Exception {
    JobConf job = new JobConf();
    String inDir = ""../../../share/test/data"";
    Path input = new Path(inDir+""/weather.avro"");
    Path output = new Path(""target/test/weather-sort"");

    output.getFileSystem(job).delete(output);

    job.setJobName(""sort weather"");

    AvroJob.setInputSchema(job, Weather.SCHEMA$);
    AvroJob.setMapOutputSchema
      (job, Pair.getPairSchema(Weather.SCHEMA$, Schema.create(Type.NULL)));
    AvroJob.setOutputSchema(job, Weather.SCHEMA$);

    AvroJob.setMapperClass(job, SortMapper.class);
    AvroJob.setReducerClass(job, SortReducer.class);

    FileInputFormat.setInputPaths(job, input);
    FileOutputFormat.setOutputPath(job, output);
    FileOutputFormat.setCompressOutput(job, true);
    AvroJob.setOutputCodec(job, SNAPPY_CODEC);

    JobClient.runJob(job);

    // check output is correct
    DatumReader<Weather> reader = new SpecificDatumReader<>();
    DataFileReader<Weather> check = new DataFileReader<>
      (new File(inDir + ""/weather-sorted.avro""), reader);
    DataFileReader<Weather> sorted = new DataFileReader<>
      (new File(output.toString() + ""/part-00000.avro""), reader);

    for (Weather w : sorted)
      assertEquals(check.next(), w);

    check.close();
    sorted.close();

    // check that AvroMapper and AvroReducer get close() and configure() called
    assertEquals(1, mapCloseCalls.get());
    assertEquals(1, reducerCloseCalls.get());
    assertEquals(1, mapConfigureCalls.get());
    assertEquals(1, reducerConfigureCalls.get());


  }
",non-flaky,5
96935,apache_avro,TestGenericJob.testJob,"  @Test
    public void testJob() throws Exception {
    JobConf job = new JobConf();
    Path outputPath = new Path(DIR.getRoot().getPath() + ""/out"");
    outputPath.getFileSystem(job).delete(outputPath);

    job.setInputFormat(TextInputFormat.class);
    FileInputFormat.setInputPaths(job, DIR.getRoot().getPath() + ""/in"");

    job.setMapperClass(AvroTestConverter.class);
    job.setNumReduceTasks(0);

    FileOutputFormat.setOutputPath(job, outputPath);
    System.out.println(createSchema());
    AvroJob.setOutputSchema(job,
                            Pair.getPairSchema(Schema.create(Schema.Type.LONG),
                                               createSchema()));
    job.setOutputFormat(AvroOutputFormat.class);

    JobClient.runJob(job);
  }
",non-flaky,5
96936,apache_avro,TestWordCountTether.testJob,"  @Test
  public void testJob() throws Exception {
    _runjob(""sasl"");
  }
",non-flaky,5
96937,apache_avro,TestWordCountTether.testhtp,"  @Test
  public void testhtp() throws Exception {
    _runjob(""http"");
  }
",non-flaky,5
96938,apache_avro,TestSortedKeyValueFile.testWriteOutOfSortedOrder,"  @Test(expected=IllegalArgumentException.class)
  public void testWriteOutOfSortedOrder() throws IOException {
    LOG.debug(""Writing some records to a SortedKeyValueFile..."");

    Configuration conf = new Configuration();
    SortedKeyValueFile.Writer.Options options = new SortedKeyValueFile.Writer.Options()
        .withKeySchema(Schema.create(Schema.Type.STRING))
        .withValueSchema(Schema.create(Schema.Type.STRING))
        .withConfiguration(conf)
        .withPath(new Path(mTempDir.getRoot().getPath(), ""myfile""))
        .withIndexInterval(2);  // Index every other record.

    SortedKeyValueFile.Writer<CharSequence, CharSequence> writer
        = new SortedKeyValueFile.Writer<>(options);

    Utf8 key = new Utf8();                        // re-use key, to test copied

    try {
      writer.append(key.set(""banana""), ""Banana"");
      writer.append(key.set(""apple""), ""Apple"");  // Ruh, roh!
    } finally {
      writer.close();
    }
  }
",non-flaky,5
96939,apache_avro,TestSortedKeyValueFile.testNamedCodecs,"  @Test
  public void testNamedCodecs() throws IOException {
    Configuration conf = new Configuration();
    Path myfile = new Path(mTempDir.getRoot().getPath(), ""myfile"");
    Schema key = Schema.create(Schema.Type.STRING);
    Schema value = Schema.create(Schema.Type.STRING);
    Schema recordSchema = AvroKeyValue.getSchema(key, value);
    DatumReader<GenericRecord> datumReader = SpecificData.get().createDatumReader(recordSchema);
    DataFileReader<GenericRecord> reader;

    SortedKeyValueFile.Writer.Options options = new SortedKeyValueFile.Writer.Options()
        .withKeySchema(key)
        .withValueSchema(value)
        .withConfiguration(conf)
        .withPath(myfile);

    SortedKeyValueFile.Writer<CharSequence, CharSequence> writer;

    for(String codec : new String[]{""null"", ""deflate"", ""snappy"", ""bzip2""}) {
        LOG.debug(""Using "" + codec + ""codec for a SortedKeyValueFile..."");

        options.withCodec(codec);

        writer = new SortedKeyValueFile.Writer<>(options);
        writer.close();

        reader = new DataFileReader<>(
            new FsInput(new Path(myfile, SortedKeyValueFile.DATA_FILENAME), conf),
            datumReader);

        assertEquals(codec, reader.getMetaString(""avro.codec""));
        reader.close();
    }
  }
",non-flaky,5
96940,apache_avro,TestSortedKeyValueFile.testDeflateClassCodec,"  @Test
  public void testDeflateClassCodec() throws IOException {
    Configuration conf = new Configuration();
    Path myfile = new Path(mTempDir.getRoot().getPath(), ""myfile"");
    Schema key = Schema.create(Schema.Type.STRING);
    Schema value = Schema.create(Schema.Type.STRING);
    Schema recordSchema = AvroKeyValue.getSchema(key, value);
    DatumReader<GenericRecord> datumReader = SpecificData.get().createDatumReader(recordSchema);
    DataFileReader<GenericRecord> reader;

    LOG.debug(""Using CodecFactory.deflateCodec() for a SortedKeyValueFile..."");
    SortedKeyValueFile.Writer.Options options = new SortedKeyValueFile.Writer.Options()
        .withKeySchema(key)
        .withValueSchema(value)
        .withConfiguration(conf)
        .withPath(myfile)
        .withCodec(CodecFactory.deflateCodec(9));

    SortedKeyValueFile.Writer<CharSequence, CharSequence> writer =
        new SortedKeyValueFile.Writer<>(options);
    writer.close();

    reader = new DataFileReader<>(
        new FsInput(new Path(myfile, SortedKeyValueFile.DATA_FILENAME), conf),
        datumReader);

    assertEquals(""deflate"", reader.getMetaString(""avro.codec""));
    reader.close();
  }
",non-flaky,5
96941,apache_avro,TestSortedKeyValueFile.testBadCodec,"  @Test
  public void testBadCodec() throws IOException {
    LOG.debug(""Using a bad codec for a SortedKeyValueFile..."");

    try {
      SortedKeyValueFile.Writer.Options options =
          new SortedKeyValueFile.Writer.Options().withCodec(""foobar"");
    } catch (AvroRuntimeException e) {
        assertEquals(""Unrecognized codec: foobar"", e.getMessage());
    }
  }
",non-flaky,5
96942,apache_avro,TestSortedKeyValueFile.testWriter,"  @Test
  public void testWriter() throws IOException {
    LOG.debug(""Writing some records to a SortedKeyValueFile..."");

    Configuration conf = new Configuration();
    SortedKeyValueFile.Writer.Options options = new SortedKeyValueFile.Writer.Options()
        .withKeySchema(Schema.create(Schema.Type.STRING))
        .withValueSchema(Schema.create(Schema.Type.STRING))
        .withConfiguration(conf)
        .withPath(new Path(mTempDir.getRoot().getPath(), ""myfile""))
        .withIndexInterval(2);  // Index every other record.

    SortedKeyValueFile.Writer<CharSequence, CharSequence> writer
        = new SortedKeyValueFile.Writer<>(options);

    try {
      writer.append(""apple"", ""Apple"");  // Will be indexed.
      writer.append(""banana"", ""Banana"");
      writer.append(""carrot"", ""Carrot"");  // Will be indexed.
      writer.append(""durian"", ""Durian"");
    } finally {
      writer.close();
    }


    LOG.debug(""Checking the generated directory..."");
    File directory = new File(mTempDir.getRoot().getPath(), ""myfile"");
    assertTrue(directory.exists());


    LOG.debug(""Checking the generated index file..."");
    File indexFile = new File(directory, SortedKeyValueFile.INDEX_FILENAME);
    DatumReader<GenericRecord> indexReader = new GenericDatumReader<>(
        AvroKeyValue.getSchema(options.getKeySchema(), Schema.create(Schema.Type.LONG)));
    FileReader<GenericRecord> indexFileReader = DataFileReader.openReader(indexFile, indexReader);

    List<AvroKeyValue<CharSequence, Long>> indexRecords
        = new ArrayList<>();
    try {
      for (GenericRecord indexRecord : indexFileReader) {
        indexRecords.add(new AvroKeyValue<>(indexRecord));
      }
    } finally {
      indexFileReader.close();
    }

    assertEquals(2, indexRecords.size());
    assertEquals(""apple"", indexRecords.get(0).getKey().toString());
    LOG.debug(""apple's position in the file: "" + indexRecords.get(0).getValue());
    assertEquals(""carrot"", indexRecords.get(1).getKey().toString());
    LOG.debug(""carrot's position in the file: "" + indexRecords.get(1).getValue());

    LOG.debug(""Checking the generated data file..."");
    File dataFile = new File(directory, SortedKeyValueFile.DATA_FILENAME);
    DatumReader<GenericRecord> dataReader = new GenericDatumReader<>(
        AvroKeyValue.getSchema(options.getKeySchema(), options.getValueSchema()));
    DataFileReader<GenericRecord> dataFileReader
        = new DataFileReader<>(dataFile, dataReader);

    try {
      dataFileReader.seek(indexRecords.get(0).getValue());
      assertTrue(dataFileReader.hasNext());
      AvroKeyValue<CharSequence, CharSequence> appleRecord
          = new AvroKeyValue<>(dataFileReader.next());
      assertEquals(""apple"", appleRecord.getKey().toString());
      assertEquals(""Apple"", appleRecord.getValue().toString());

      dataFileReader.seek(indexRecords.get(1).getValue());
      assertTrue(dataFileReader.hasNext());
      AvroKeyValue<CharSequence, CharSequence> carrotRecord
          = new AvroKeyValue<>(dataFileReader.next());
      assertEquals(""carrot"", carrotRecord.getKey().toString());
      assertEquals(""Carrot"", carrotRecord.getValue().toString());

      assertTrue(dataFileReader.hasNext());
      AvroKeyValue<CharSequence, CharSequence> durianRecord
          = new AvroKeyValue<>(dataFileReader.next());
      assertEquals(""durian"", durianRecord.getKey().toString());
      assertEquals(""Durian"", durianRecord.getValue().toString());
    } finally {
      dataFileReader.close();
    }
  }
",non-flaky,5
96943,apache_avro,TestSortedKeyValueFile.testReader,"  @Test
  public void testReader() throws IOException {
    Configuration conf = new Configuration();
    SortedKeyValueFile.Writer.Options writerOptions = new SortedKeyValueFile.Writer.Options()
        .withKeySchema(Schema.create(Schema.Type.STRING))
        .withValueSchema(Schema.create(Schema.Type.STRING))
        .withConfiguration(conf)
        .withPath(new Path(mTempDir.getRoot().getPath(), ""myfile""))
        .withIndexInterval(2);  // Index every other record.

    SortedKeyValueFile.Writer<CharSequence, CharSequence> writer
        = new SortedKeyValueFile.Writer<>(writerOptions);

    try {
      writer.append(""apple"", ""Apple"");  // Will be indexed.
      writer.append(""banana"", ""Banana"");
      writer.append(""carrot"", ""Carrot"");  // Will be indexed.
      writer.append(""durian"", ""Durian"");
    } finally {
      writer.close();
    }

    LOG.debug(""Reading the file back using a reader..."");
    SortedKeyValueFile.Reader.Options readerOptions = new SortedKeyValueFile.Reader.Options()
        .withKeySchema(Schema.create(Schema.Type.STRING))
        .withValueSchema(Schema.create(Schema.Type.STRING))
        .withConfiguration(conf)
        .withPath(new Path(mTempDir.getRoot().getPath(), ""myfile""));

    SortedKeyValueFile.Reader<CharSequence, CharSequence> reader
        = new SortedKeyValueFile.Reader<>(readerOptions);

    try {
      assertEquals(""Carrot"", reader.get(""carrot"").toString());
      assertEquals(""Banana"", reader.get(""banana"").toString());
      assertNull(reader.get(""a-vegetable""));
      assertNull(reader.get(""beet""));
      assertNull(reader.get(""zzz""));
    } finally {
      reader.close();
    }
  }
",non-flaky,5
96944,apache_avro,TestHadoopCodecFactory.testHadoopCodecFactoryDeflate,"  @Test
  public void testHadoopCodecFactoryDeflate(){
    CodecFactory hadoopDeflateCodec = HadoopCodecFactory.fromHadoopString(""org.apache.hadoop.io.compress.DeflateCodec"");
    CodecFactory avroDeflateCodec = CodecFactory.fromString(""deflate"");
    assertTrue(hadoopDeflateCodec.getClass().equals(avroDeflateCodec.getClass()));
  }
",non-flaky,5
96945,apache_avro,TestHadoopCodecFactory.testHadoopCodecFactorySnappy,"  @Test
  public void testHadoopCodecFactorySnappy(){
    CodecFactory hadoopSnappyCodec = HadoopCodecFactory.fromHadoopString(""org.apache.hadoop.io.compress.SnappyCodec"");
    CodecFactory avroSnappyCodec = CodecFactory.fromString(""snappy"");
    assertTrue(hadoopSnappyCodec.getClass().equals(avroSnappyCodec.getClass()));
  }
",non-flaky,5
96946,apache_avro,TestHadoopCodecFactory.testHadoopCodecFactoryBZip2,"  @Test
  public void testHadoopCodecFactoryBZip2(){
    CodecFactory hadoopSnappyCodec = HadoopCodecFactory.fromHadoopString(""org.apache.hadoop.io.compress.BZip2Codec"");
    CodecFactory avroSnappyCodec = CodecFactory.fromString(""bzip2"");
    assertTrue(hadoopSnappyCodec.getClass().equals(avroSnappyCodec.getClass()));
  }
",non-flaky,5
96947,apache_avro,TestHadoopCodecFactory.testHadoopCodecFactoryGZip,"  @Test
  public void testHadoopCodecFactoryGZip(){
    CodecFactory hadoopSnappyCodec = HadoopCodecFactory.fromHadoopString(""org.apache.hadoop.io.compress.GZipCodec"");
    CodecFactory avroSnappyCodec = CodecFactory.fromString(""deflate"");
    assertTrue(hadoopSnappyCodec.getClass().equals(avroSnappyCodec.getClass()));
  }
",non-flaky,5
96948,apache_avro,TestHadoopCodecFactory.testHadoopCodecFactoryFail,"  @Test
  public void testHadoopCodecFactoryFail(){
    CodecFactory hadoopSnappyCodec = HadoopCodecFactory.fromHadoopString(""org.apache.hadoop.io.compress.FooCodec"");
    assertTrue(hadoopSnappyCodec == null);
  }
",non-flaky,5
96949,apache_avro,TestAvroCharSequenceComparator.testCompareString,"  @Test
  public void testCompareString() {
    assertEquals(0, mComparator.compare("""", """"));
    assertThat(mComparator.compare("""", ""a""), lessThan(0));
    assertThat(mComparator.compare(""a"", """"), greaterThan(0));

    assertEquals(0, mComparator.compare(""a"", ""a""));
    assertThat(mComparator.compare(""a"", ""b""), lessThan(0));
    assertThat(mComparator.compare(""b"", ""a""), greaterThan(0));

    assertEquals(0, mComparator.compare(""ab"", ""ab""));
    assertThat(mComparator.compare(""a"", ""aa""), lessThan(0));
    assertThat(mComparator.compare(""aa"", ""a""), greaterThan(0));

    assertThat(mComparator.compare(""abc"", ""abcdef""), lessThan(0));
    assertThat(mComparator.compare(""abcdef"", ""abc""), greaterThan(0));
  }
",non-flaky,5
96950,apache_avro,TestAvroCharSequenceComparator.testCompareUtf8,"  @Test
  public void testCompareUtf8() {
    assertEquals(0, mComparator.compare(new Utf8(""""), new Utf8("""")));
    assertThat(mComparator.compare(new Utf8(""""), new Utf8(""a"")), lessThan(0));
    assertThat(mComparator.compare(new Utf8(""a""), new Utf8("""")), greaterThan(0));

    assertEquals(0, mComparator.compare(new Utf8(""a""), new Utf8(""a"")));
    assertThat(mComparator.compare(new Utf8(""a""), new Utf8(""b"")), lessThan(0));
    assertThat(mComparator.compare(new Utf8(""b""), new Utf8(""a"")), greaterThan(0));

    assertEquals(0, mComparator.compare(new Utf8(""ab""), new Utf8(""ab"")));
    assertThat(mComparator.compare(new Utf8(""a""), new Utf8(""aa"")), lessThan(0));
    assertThat(mComparator.compare(new Utf8(""aa""), new Utf8(""a"")), greaterThan(0));

    assertThat(mComparator.compare(new Utf8(""abc""), new Utf8(""abcdef"")), lessThan(0));
    assertThat(mComparator.compare(new Utf8(""abcdef""), new Utf8(""abc"")), greaterThan(0));
  }
",non-flaky,5
96951,apache_avro,TestAvroCharSequenceComparator.testCompareUtf8ToString,"  @Test
  public void testCompareUtf8ToString() {
    assertEquals(0, mComparator.compare(new Utf8(""""), """"));
    assertThat(mComparator.compare(new Utf8(""""), ""a""), lessThan(0));
    assertThat(mComparator.compare(new Utf8(""a""), """"), greaterThan(0));

    assertEquals(0, mComparator.compare(new Utf8(""a""), ""a""));
    assertThat(mComparator.compare(new Utf8(""a""), ""b""), lessThan(0));
    assertThat(mComparator.compare(new Utf8(""b""), ""a""), greaterThan(0));

    assertEquals(0, mComparator.compare(new Utf8(""ab""), ""ab""));
    assertThat(mComparator.compare(new Utf8(""a""), ""aa""), lessThan(0));
    assertThat(mComparator.compare(new Utf8(""aa""), ""a""), greaterThan(0));

    assertThat(mComparator.compare(new Utf8(""abc""), ""abcdef""), lessThan(0));
    assertThat(mComparator.compare(new Utf8(""abcdef""), ""abc""), greaterThan(0));
  }
",non-flaky,5
96952,apache_avro,TestAvroSerializer.testSerialize,"  @Test
  public void testSerialize() throws IOException {
    // Create a serializer.
    Schema writerSchema = Schema.create(Schema.Type.STRING);
    AvroSerializer<CharSequence> serializer = new AvroSerializer<>(writerSchema);

    // Check the writer schema.
    assertEquals(writerSchema, serializer.getWriterSchema());

    // Serialize two records, 'record1' and 'record2'.
    ByteArrayOutputStream outputStream = new ByteArrayOutputStream();
    serializer.open(outputStream);
    serializer.serialize(new AvroKey<>(""record1""));
    serializer.serialize(new AvroKey<>(""record2""));
    serializer.close();

    // Make sure the records were serialized correctly.
    ByteArrayInputStream inputStream = new ByteArrayInputStream(outputStream.toByteArray());
    Schema readerSchema = Schema.create(Schema.Type.STRING);
    DatumReader<CharSequence> datumReader = new GenericDatumReader<>(readerSchema);
    Decoder decoder = DecoderFactory.get().binaryDecoder(inputStream, null);
    CharSequence record = null;

    record = datumReader.read(record, decoder);
    assertEquals(""record1"", record.toString());

    record = datumReader.read(record, decoder);
    assertEquals(""record2"", record.toString());

    inputStream.close();
  }
",non-flaky,5
96953,apache_avro,TestAvroKeyDeserializer.testDeserialize,"  @Test
  public void testDeserialize() throws IOException {
    // Create a deserializer.
    Schema writerSchema = Schema.create(Schema.Type.STRING);
    Schema readerSchema = Schema.create(Schema.Type.STRING);
    ClassLoader classLoader = this.getClass().getClassLoader();
    AvroKeyDeserializer<CharSequence> deserializer =
        new AvroKeyDeserializer<>(writerSchema, readerSchema, classLoader);

    // Check the schemas.
    assertEquals(writerSchema, deserializer.getWriterSchema());
    assertEquals(readerSchema, deserializer.getReaderSchema());

    // Write some records to deserialize.
    DatumWriter<CharSequence> datumWriter = new GenericDatumWriter<>(writerSchema);
    ByteArrayOutputStream outputStream = new ByteArrayOutputStream();
    Encoder encoder = EncoderFactory.get().binaryEncoder(outputStream, null);
    datumWriter.write(""record1"", encoder);
    datumWriter.write(""record2"", encoder);
    encoder.flush();

    // Deserialize the records.
    ByteArrayInputStream inputStream = new ByteArrayInputStream(outputStream.toByteArray());
    deserializer.open(inputStream);
    AvroWrapper<CharSequence> record = null;

    record = deserializer.deserialize(record);
    assertEquals(""record1"", record.datum().toString());

    record = deserializer.deserialize(record);
    assertEquals(""record2"", record.datum().toString());

    deserializer.close();
  }
",non-flaky,5
96954,apache_avro,TestAvroSerialization.testAccept,"  @Test
  public void testAccept() {
    AvroSerialization<CharSequence> serialization = new AvroSerialization<>();

    assertTrue(serialization.accept(AvroKey.class));
    assertTrue(serialization.accept(AvroValue.class));
    assertFalse(serialization.accept(AvroWrapper.class));
    assertFalse(serialization.accept(String.class));
  }
",non-flaky,5
96955,apache_avro,TestAvroSerialization.testGetSerializerForKey,"  @Test
  public void testGetSerializerForKey() throws IOException {
    // Set the writer schema in the job configuration.
    Schema writerSchema = Schema.create(Schema.Type.STRING);
    Job job = new Job();
    AvroJob.setMapOutputKeySchema(job, writerSchema);

    // Get a serializer from the configuration.
    AvroSerialization serialization
        = ReflectionUtils.newInstance(AvroSerialization.class, job.getConfiguration());
    @SuppressWarnings(""unchecked"")
    Serializer<AvroWrapper> serializer = serialization.getSerializer(AvroKey.class);
    assertTrue(serializer instanceof AvroSerializer);
    AvroSerializer avroSerializer = (AvroSerializer) serializer;

    // Check that the writer schema is set correctly on the serializer.
    assertEquals(writerSchema, avroSerializer.getWriterSchema());
  }
",non-flaky,5
96956,apache_avro,TestAvroSerialization.testGetSerializerForValue,"  @Test
  public void testGetSerializerForValue() throws IOException {
    // Set the writer schema in the job configuration.
    Schema writerSchema = Schema.create(Schema.Type.STRING);
    Job job = new Job();
    AvroJob.setMapOutputValueSchema(job, writerSchema);

    // Get a serializer from the configuration.
    AvroSerialization serialization
        = ReflectionUtils.newInstance(AvroSerialization.class, job.getConfiguration());
    @SuppressWarnings(""unchecked"")
    Serializer<AvroWrapper> serializer = serialization.getSerializer(AvroValue.class);
    assertTrue(serializer instanceof AvroSerializer);
    AvroSerializer avroSerializer = (AvroSerializer) serializer;

    // Check that the writer schema is set correctly on the serializer.
    assertEquals(writerSchema, avroSerializer.getWriterSchema());
  }
",non-flaky,5
96957,apache_avro,TestAvroSerialization.testGetDeserializerForKey,"  @Test
  public void testGetDeserializerForKey() throws IOException {
    // Set the reader schema in the job configuration.
    Schema readerSchema = Schema.create(Schema.Type.STRING);
    Job job = new Job();
    AvroJob.setMapOutputKeySchema(job, readerSchema);

    // Get a deserializer from the configuration.
    AvroSerialization serialization
        = ReflectionUtils.newInstance(AvroSerialization.class, job.getConfiguration());
    @SuppressWarnings(""unchecked"")
    Deserializer<AvroWrapper> deserializer = serialization.getDeserializer(AvroKey.class);
    assertTrue(deserializer instanceof AvroKeyDeserializer);
    AvroKeyDeserializer avroDeserializer = (AvroKeyDeserializer) deserializer;

    // Check that the reader schema is set correctly on the deserializer.
    assertEquals(readerSchema, avroDeserializer.getReaderSchema());
  }
",non-flaky,5
96958,apache_avro,TestAvroSerialization.testGetDeserializerForValue,"  @Test
  public void testGetDeserializerForValue() throws IOException {
    // Set the reader schema in the job configuration.
    Schema readerSchema = Schema.create(Schema.Type.STRING);
    Job job = new Job();
    AvroJob.setMapOutputValueSchema(job, readerSchema);

    // Get a deserializer from the configuration.
    AvroSerialization serialization
        = ReflectionUtils.newInstance(AvroSerialization.class, job.getConfiguration());
    @SuppressWarnings(""unchecked"")
    Deserializer<AvroWrapper> deserializer = serialization.getDeserializer(AvroValue.class);
    assertTrue(deserializer instanceof AvroValueDeserializer);
    AvroValueDeserializer avroDeserializer = (AvroValueDeserializer) deserializer;

    // Check that the reader schema is set correctly on the deserializer.
    assertEquals(readerSchema, avroDeserializer.getReaderSchema());
  }
",non-flaky,5
96959,apache_avro,TestAvroSerialization.testRoundTrip,"  @Test
  public void testRoundTrip() throws Exception {
    Schema schema = Schema.create(Schema.Type.STRING);
    assertTrue(roundTrip(schema, ""record"", null) instanceof String);
    assertTrue(roundTrip(schema, ""record"", GenericData.class) instanceof Utf8);
  }
",non-flaky,5
98369,ONSdigital_rm-collection-exercise-service,MultipleMandatoryEventsValidatorTest.testValidMpsEventCreation,"  @Test
  public void testValidMpsEventCreation() throws CTPException {
    List<Event> events = getExistingEvents();
    Instant timestamp = Instant.now().plus(2, ChronoUnit.DAYS);
    Event newMpsEvent = new Event();
    newMpsEvent.setTag((EventService.Tag.mps.toString()));
    newMpsEvent.setTimestamp(Timestamp.from(timestamp));
    validator.validate(events, newMpsEvent, CollectionExerciseDTO.CollectionExerciseState.CREATED);
  }
",non-flaky,5
98370,ONSdigital_rm-collection-exercise-service,MultipleMandatoryEventsValidatorTest.testValidGoLiveEventCreation,"  @Test
  public void testValidGoLiveEventCreation() throws CTPException {
    List<Event> events = getExistingEvents();
    Instant timestamp = Instant.now().plus(4, ChronoUnit.DAYS);
    Event newGoLiveEvent = new Event();
    newGoLiveEvent.setTag((EventService.Tag.go_live.toString()));
    newGoLiveEvent.setTimestamp(Timestamp.from(timestamp));
    validator.validate(
        events, newGoLiveEvent, CollectionExerciseDTO.CollectionExerciseState.CREATED);
  }
",non-flaky,5
98371,ONSdigital_rm-collection-exercise-service,MultipleMandatoryEventsValidatorTest.testValidReturnByEventCreation,"  @Test
  public void testValidReturnByEventCreation() throws CTPException {
    List<Event> events = getExistingEvents();
    Instant timestamp = Instant.now().plus(6, ChronoUnit.DAYS);
    Event newReturnByEvent = new Event();
    newReturnByEvent.setTag((EventService.Tag.return_by.toString()));
    newReturnByEvent.setTimestamp(Timestamp.from(timestamp));
    validator.validate(
        events, newReturnByEvent, CollectionExerciseDTO.CollectionExerciseState.CREATED);
  }
",non-flaky,5
98372,ONSdigital_rm-collection-exercise-service,MultipleMandatoryEventsValidatorTest.testValidExerciseEndEventCreation,"  @Test
  public void testValidExerciseEndEventCreation() throws CTPException {
    List<Event> events = getExistingEvents();
    Instant timestamp = Instant.now().plus(8, ChronoUnit.DAYS);
    Event newExerciseEndEvent = new Event();
    newExerciseEndEvent.setTag((EventService.Tag.exercise_end.toString()));
    newExerciseEndEvent.setTimestamp(Timestamp.from(timestamp));
    validator.validate(
        events, newExerciseEndEvent, CollectionExerciseDTO.CollectionExerciseState.CREATED);
  }
",non-flaky,5
98373,ONSdigital_rm-collection-exercise-service,ReminderEventValidatorTest.isEventValidator,"  @Test
  public void isEventValidator() {
    assertThat(reminderValidator, instanceOf(EventValidator.class));
  }
",non-flaky,5
98374,ONSdigital_rm-collection-exercise-service,ReminderEventValidatorTest.returnTrueAndDoNothingIfNotReminder,"  @Test
  public void returnTrueAndDoNothingIfNotReminder() throws CTPException {
    final Event mpsEvent = new Event();
    mpsEvent.setTag((Tag.mps.toString()));
    mpsEvent.setTimestamp(Timestamp.from(Instant.now()));
    final List<Event> events = new ArrayList<>();
    reminderValidator.validate(events, mpsEvent, CollectionExerciseState.CREATED);

    verify(eventDateOrderChecker, never()).isEventDatesInOrder(anyList());
  }
",non-flaky,5
98375,ONSdigital_rm-collection-exercise-service,ReminderEventValidatorTest.testCanUpdateReminderWhenReadyForLive,"  @Test
  public void testCanUpdateReminderWhenReadyForLive() throws CTPException {
    final Event reminderEvent = new Event();
    reminderEvent.setTag(Tag.reminder.toString());
    reminderEvent.setTimestamp(Timestamp.from(Instant.now()));

    final List<Event> events = new ArrayList<>();
    reminderValidator.validate(events, reminderEvent, CollectionExerciseState.READY_FOR_LIVE);
  }
",non-flaky,5
98376,ONSdigital_rm-collection-exercise-service,ReminderEventValidatorTest.testCanUpdateReminderWhenLive,"  @Test
  public void testCanUpdateReminderWhenLive() throws CTPException {
    final Event reminderEvent = new Event();
    reminderEvent.setTag(Tag.reminder.toString());
    reminderEvent.setTimestamp(Timestamp.from(Instant.now()));

    final List<Event> events = new ArrayList<>();

    reminderValidator.validate(events, reminderEvent, CollectionExerciseState.LIVE);
  }
",non-flaky,5
98377,ONSdigital_rm-collection-exercise-service,ReminderEventValidatorTest.testCantUpdateReminderThatHasPastAndCollectionExerciseInLockedState,"  @Test
  public void testCantUpdateReminderThatHasPastAndCollectionExerciseInLockedState() {
    final Event reminder = new Event();
    reminder.setTag((Tag.reminder.toString()));
    reminder.setTimestamp(Timestamp.from(Instant.now().minus(2, ChronoUnit.DAYS)));

    final Event newReminder = new Event();
    newReminder.setTag((Tag.reminder.toString()));
    newReminder.setTimestamp(Timestamp.from(Instant.now().plus(2, ChronoUnit.DAYS)));

    final List<Event> events = Collections.singletonList(reminder);
    CTPException actualException = null;
    try {
      reminderValidator.validate(events, newReminder, CollectionExerciseState.LIVE);
    } catch (CTPException expectedException) {
      actualException = expectedException;
    }
    assertNotNull(actualException);
    assertEquals(""Reminder cannot be set in the past"", actualException.getMessage());
  }
",non-flaky,5
98378,ONSdigital_rm-collection-exercise-service,ReminderEventValidatorTest.testCanUpdateReminderThatHasPastAndCollectionExerciseNotInLockedState,"  @Test
  public void testCanUpdateReminderThatHasPastAndCollectionExerciseNotInLockedState()
      throws CTPException {
    final Event reminder = new Event();
    reminder.setTag((Tag.reminder.toString()));
    reminder.setTimestamp(Timestamp.from(Instant.now().minus(2, ChronoUnit.DAYS)));

    final Event newReminder = new Event();
    newReminder.setTag((Tag.reminder.toString()));
    newReminder.setTimestamp(Timestamp.from(Instant.now().plus(2, ChronoUnit.DAYS)));

    final List<Event> events = Collections.singletonList(reminder);

    reminderValidator.validate(events, newReminder, CollectionExerciseState.SCHEDULED);
  }
",non-flaky,5
98379,ONSdigital_rm-collection-exercise-service,ReminderEventValidatorTest.testValidReminderEventCreation,"  @Test
  public void testValidReminderEventCreation() throws CTPException {
    final Event goLive = new Event();
    goLive.setTag((Tag.go_live.toString()));
    goLive.setTimestamp(Timestamp.from(Instant.now()));

    final Event reminder = new Event();
    reminder.setTag((Tag.reminder.toString()));
    reminder.setTimestamp(Timestamp.from(Instant.now().plus(2, ChronoUnit.DAYS)));

    final Event exerciseEnd = new Event();
    exerciseEnd.setTag((Tag.exercise_end.toString()));
    exerciseEnd.setTimestamp(Timestamp.from(Instant.now().plus(4, ChronoUnit.DAYS)));

    final List<Event> events = Arrays.asList(goLive, exerciseEnd);

    reminderValidator.validate(events, reminder, CollectionExerciseState.CREATED);
  }
",non-flaky,5
98380,ONSdigital_rm-collection-exercise-service,ReminderEventValidatorTest.testReminderAfterExerciseEndInvalid,"  @Test
  public void testReminderAfterExerciseEndInvalid() {
    final Event reminderEvent = new Event();
    reminderEvent.setTag(Tag.reminder3.toString());
    reminderEvent.setTimestamp(Timestamp.from(Instant.now().plus(4, ChronoUnit.DAYS)));

    final Event exerciseEnd = new Event();
    exerciseEnd.setTag((Tag.exercise_end.toString()));
    exerciseEnd.setTimestamp(Timestamp.from(Instant.now().plus(3, ChronoUnit.DAYS)));

    final List<Event> events = Collections.singletonList(exerciseEnd);
    CTPException actualException = null;
    try {
      reminderValidator.validate(events, reminderEvent, CollectionExerciseState.SCHEDULED);
    } catch (CTPException expectedException) {
      actualException = expectedException;
    }
    assertNotNull(actualException);
    assertEquals(
        ""Reminder must take place during collection exercise period"", actualException.getMessage());
  }
",non-flaky,5
98381,ONSdigital_rm-collection-exercise-service,ReminderEventValidatorTest.testReminderBeforeGoliveInvalid,"  @Test
  public void testReminderBeforeGoliveInvalid() {
    final Event goLive = new Event();
    goLive.setTag((Tag.go_live.toString()));
    goLive.setTimestamp(Timestamp.from(Instant.now().plus(2, ChronoUnit.DAYS)));
    final List<Event> events = Collections.singletonList(goLive);

    final Event reminderEvent = new Event();
    reminderEvent.setTag(Tag.reminder.toString());
    reminderEvent.setTimestamp(Timestamp.from(Instant.now().plus(1, ChronoUnit.DAYS)));

    CTPException actualException = null;
    try {
      reminderValidator.validate(events, reminderEvent, CollectionExerciseState.SCHEDULED);
    } catch (CTPException expectedException) {
      actualException = expectedException;
    }
    assertNotNull(actualException);
    assertEquals(
        ""Reminder must take place during collection exercise period"", actualException.getMessage());
  }
",non-flaky,5
98382,ONSdigital_rm-collection-exercise-service,ReminderEventValidatorTest.testReminder2WrongOrderEventCreation,"  @Test
  public void testReminder2WrongOrderEventCreation() {
    final Event reminder = new Event();
    reminder.setTag((Tag.reminder.toString()));
    reminder.setTimestamp(Timestamp.from(Instant.now().plus(2, ChronoUnit.DAYS)));
    final Event reminder2 = new Event();
    reminder2.setTag((Tag.reminder2.toString()));
    reminder2.setTimestamp(Timestamp.from(Instant.now().plus(1, ChronoUnit.DAYS)));
    final List<Event> events = new ArrayList<>();
    events.add(reminder);
    CTPException actualException = null;
    try {
      reminderValidator.validate(events, reminder2, CollectionExerciseState.CREATED);
    } catch (CTPException expectedException) {
      actualException = expectedException;
    }
    assertNotNull(actualException);
    assertEquals(
        ""Collection exercise events must be set sequentially"", actualException.getMessage());
  }
",non-flaky,5
98383,ONSdigital_rm-collection-exercise-service,ReminderEventValidatorTest.testReminder3WrongOrderEventCreation,"  @Test
  public void testReminder3WrongOrderEventCreation() {
    final Event reminder2 = new Event();
    reminder2.setTag((Tag.reminder2.toString()));
    reminder2.setTimestamp(Timestamp.from(Instant.now().plus(2, ChronoUnit.DAYS)));
    final Event reminder3 = new Event();
    reminder3.setTag((Tag.reminder3.toString()));
    reminder3.setTimestamp(Timestamp.from(Instant.now().plus(1, ChronoUnit.DAYS)));
    final List<Event> events = new ArrayList<>();
    events.add(reminder2);
    CTPException actualException = null;
    try {
      reminderValidator.validate(events, reminder3, CollectionExerciseState.CREATED);
    } catch (CTPException expectedException) {
      actualException = expectedException;
    }
    assertNotNull(actualException);
    assertEquals(
        ""Collection exercise events must be set sequentially"", actualException.getMessage());
  }
",non-flaky,5
98384,ONSdigital_rm-collection-exercise-service,MandatoryEventValidatorTest.isEventValidator,"  @Test
  public void isEventValidator() {
    assertThat(mandatoryValidator, instanceOf(EventValidator.class));
  }
",non-flaky,5
98385,ONSdigital_rm-collection-exercise-service,MandatoryEventValidatorTest.returnTrueAndDoNothingIfNotReminder,"  @Test
  public void returnTrueAndDoNothingIfNotReminder() throws CTPException {
    final Event mpsEvent = new Event();
    mpsEvent.setTag((Tag.reminder.toString()));
    mpsEvent.setTimestamp(Timestamp.from(Instant.now()));
    final List<Event> events = new ArrayList<>();
    mandatoryValidator.validate(events, mpsEvent, CollectionExerciseState.CREATED);

    verify(eventDateOrderChecker, never()).isEventDatesInOrder(anyList());
  }
",non-flaky,5
98386,ONSdigital_rm-collection-exercise-service,MandatoryEventValidatorTest.testValidMpsEventCreation,"  @Test
  public void testValidMpsEventCreation() throws CTPException {
    final Event mpsEvent = new Event();
    mpsEvent.setTag((Tag.mps.toString()));
    mpsEvent.setTimestamp(Timestamp.from(Instant.now().plus(2, ChronoUnit.DAYS)));
    final List<Event> events = new ArrayList<>();
    mandatoryValidator.validate(events, mpsEvent, CollectionExerciseState.CREATED);
  }
",non-flaky,5
98387,ONSdigital_rm-collection-exercise-service,MandatoryEventValidatorTest.testValidGoLiveEventCreation,"  @Test
  public void testValidGoLiveEventCreation() throws CTPException {
    final Event mpsEvent = new Event();
    mpsEvent.setTag((Tag.mps.toString()));
    mpsEvent.setTimestamp(Timestamp.from(Instant.now().plus(2, ChronoUnit.DAYS)));

    final Event goLiveEvent = new Event();
    goLiveEvent.setTag((Tag.go_live.toString()));
    goLiveEvent.setTimestamp(Timestamp.from(Instant.now().plus(4, ChronoUnit.DAYS)));

    final List<Event> events = Arrays.asList(mpsEvent);

    mandatoryValidator.validate(events, goLiveEvent, CollectionExerciseState.CREATED);
  }
",non-flaky,5
98388,ONSdigital_rm-collection-exercise-service,MandatoryEventValidatorTest.testValidReturnByEventCreation,"  @Test
  public void testValidReturnByEventCreation() throws CTPException {
    final Event mpsEvent = new Event();
    mpsEvent.setTag((Tag.mps.toString()));
    mpsEvent.setTimestamp(Timestamp.from(Instant.now().plus(2, ChronoUnit.DAYS)));

    final Event goLiveEvent = new Event();
    goLiveEvent.setTag((Tag.go_live.toString()));
    goLiveEvent.setTimestamp(Timestamp.from(Instant.now().plus(4, ChronoUnit.DAYS)));

    final List<Event> events = Arrays.asList(mpsEvent, goLiveEvent);

    final Event returnByEvent = new Event();
    returnByEvent.setTag((Tag.return_by.toString()));
    returnByEvent.setTimestamp(Timestamp.from(Instant.now().plus(6, ChronoUnit.DAYS)));

    mandatoryValidator.validate(events, returnByEvent, CollectionExerciseState.CREATED);
  }
",non-flaky,5
98389,ONSdigital_rm-collection-exercise-service,MandatoryEventValidatorTest.testInvalidGoLiveEventCreation,"  @Test
  public void testInvalidGoLiveEventCreation() {
    final Event mpsEvent = new Event();
    mpsEvent.setTag((Tag.mps.toString()));
    mpsEvent.setTimestamp(Timestamp.from(Instant.now().plus(10, ChronoUnit.DAYS)));

    final Event goLive = new Event();
    goLive.setTag((Tag.go_live.toString()));
    goLive.setTimestamp(Timestamp.from(Instant.now().plus(2, ChronoUnit.DAYS)));

    final List<Event> events = Arrays.asList(mpsEvent);

    CTPException actualException = null;
    try {
      mandatoryValidator.validate(events, goLive, CollectionExerciseState.CREATED);
    } catch (CTPException expectedException) {
      actualException = expectedException;
    }
    assertNotNull(actualException);
    assertEquals(
        ""Collection exercise events must be set sequentially"", actualException.getMessage());
  }
",non-flaky,5
98390,ONSdigital_rm-collection-exercise-service,MandatoryEventValidatorTest.testInvalidReturnByEventCreation,"  @Test
  public void testInvalidReturnByEventCreation() {
    final Event mpsEvent = new Event();
    mpsEvent.setTag((Tag.mps.toString()));
    mpsEvent.setTimestamp(Timestamp.from(Instant.now().plus(2, ChronoUnit.DAYS)));

    final Event goLiveEvent = new Event();
    goLiveEvent.setTag((Tag.go_live.toString()));
    goLiveEvent.setTimestamp(Timestamp.from(Instant.now().plus(4, ChronoUnit.DAYS)));

    final List<Event> events = Arrays.asList(mpsEvent, goLiveEvent);

    final Event returnByEvent = new Event();
    returnByEvent.setTag((Tag.return_by.toString()));
    returnByEvent.setTimestamp(Timestamp.from(Instant.now().plus(2, ChronoUnit.DAYS)));

    CTPException actualException = null;
    try {
      mandatoryValidator.validate(events, returnByEvent, CollectionExerciseState.CREATED);
    } catch (CTPException expectedException) {
      actualException = expectedException;
    }
    assertNotNull(actualException);
    assertEquals(
        ""Collection exercise events must be set sequentially"", actualException.getMessage());
  }
",non-flaky,5
98391,ONSdigital_rm-collection-exercise-service,MandatoryEventValidatorTest.testMandatoryEventsCannotBeChangedIfCollectionExerciseIsLive,"  @Test
  public void testMandatoryEventsCannotBeChangedIfCollectionExerciseIsLive() {
    final List<Event> events = new ArrayList<>();

    final Event mpsEvent = new Event();
    mpsEvent.setTag((Tag.mps.toString()));
    mpsEvent.setTimestamp(Timestamp.from(Instant.now().plus(1, ChronoUnit.MINUTES)));

    CTPException actualException = null;
    try {
      mandatoryValidator.validate(events, mpsEvent, CollectionExerciseState.LIVE);
    } catch (CTPException expectedException) {
      actualException = expectedException;
    }
    assertNotNull(actualException);
    assertEquals(
        ""Mandatory events cannot be changed if collection exercise is set to live, executed, validated or locked"",
        actualException.getMessage());
  }
",non-flaky,5
98392,ONSdigital_rm-collection-exercise-service,MandatoryEventValidatorTest.testMandatoryEventsCannotBeChangedIfCollectionExerciseIsValidated,"  @Test
  public void testMandatoryEventsCannotBeChangedIfCollectionExerciseIsValidated() {
    final List<Event> events = new ArrayList<>();

    final Event mpsEvent = new Event();
    mpsEvent.setTag((Tag.mps.toString()));
    mpsEvent.setTimestamp(Timestamp.from(Instant.now().plus(1, ChronoUnit.MINUTES)));

    CTPException actualException = null;
    try {
      mandatoryValidator.validate(events, mpsEvent, CollectionExerciseState.VALIDATED);
    } catch (CTPException expectedException) {
      actualException = expectedException;
    }
    assertNotNull(actualException);
    assertEquals(
        ""Mandatory events cannot be changed if collection exercise is set to live, executed, validated or locked"",
        actualException.getMessage());
  }
",non-flaky,5
98393,ONSdigital_rm-collection-exercise-service,MandatoryEventValidatorTest.testMandatoryEventsCannotBeChangedIfCollectionExerciseIsExecutionStarted,"  @Test
  public void testMandatoryEventsCannotBeChangedIfCollectionExerciseIsExecutionStarted() {
    final List<Event> events = new ArrayList<>();

    final Event mpsEvent = new Event();
    mpsEvent.setTag((Tag.mps.toString()));
    mpsEvent.setTimestamp(Timestamp.from(Instant.now().plus(1, ChronoUnit.MINUTES)));

    CTPException actualException = null;
    try {
      mandatoryValidator.validate(events, mpsEvent, CollectionExerciseState.EXECUTION_STARTED);
    } catch (CTPException expectedException) {
      actualException = expectedException;
    }
    assertNotNull(actualException);
    assertEquals(
        ""Mandatory events cannot be changed if collection exercise is set to live, executed, validated or locked"",
        actualException.getMessage());
  }
",non-flaky,5
98394,ONSdigital_rm-collection-exercise-service,MandatoryEventValidatorTest.testMandatoryEventsCannotBeChangedIfCollectionExerciseIsReadyForLive,"  @Test
  public void testMandatoryEventsCannotBeChangedIfCollectionExerciseIsReadyForLive() {
    final List<Event> events = new ArrayList<>();

    final Event mpsEvent = new Event();
    mpsEvent.setTag((Tag.mps.toString()));
    mpsEvent.setTimestamp(Timestamp.from(Instant.now().plus(1, ChronoUnit.MINUTES)));

    CTPException actualException = null;
    try {
      mandatoryValidator.validate(events, mpsEvent, CollectionExerciseState.READY_FOR_LIVE);
    } catch (CTPException expectedException) {
      actualException = expectedException;
    }
    assertNotNull(actualException);
    assertEquals(
        ""Mandatory events cannot be changed if collection exercise is set to live, executed, validated or locked"",
        actualException.getMessage());
  }
",non-flaky,5
98395,ONSdigital_rm-collection-exercise-service,MandatoryEventValidatorTest.testMandatoryEventsCannotBeChangedIfCollectionExerciseIsExecuted,"  @Test
  public void testMandatoryEventsCannotBeChangedIfCollectionExerciseIsExecuted() {
    final List<Event> events = new ArrayList<>();

    final Event mpsEvent = new Event();
    mpsEvent.setTag((Tag.mps.toString()));
    mpsEvent.setTimestamp(Timestamp.from(Instant.now().plus(1, ChronoUnit.MINUTES)));

    CTPException actualException = null;
    try {
      mandatoryValidator.validate(events, mpsEvent, CollectionExerciseState.EXECUTED);
    } catch (CTPException expectedException) {
      actualException = expectedException;
    }
    assertNotNull(actualException);
    assertEquals(
        ""Mandatory events cannot be changed if collection exercise is set to live, executed, validated or locked"",
        actualException.getMessage());
  }
",non-flaky,5
98396,ONSdigital_rm-collection-exercise-service,MandatoryEventValidatorTest.testValidReturnByEventUpdate,"  @Test
  public void testValidReturnByEventUpdate() throws CTPException {
    final List<Event> events = createMandatoryEvents();

    final Event returnByEvent = new Event();
    returnByEvent.setTag(Tag.return_by.toString());
    returnByEvent.setTimestamp(Timestamp.from(Instant.now().plus(5, ChronoUnit.DAYS)));

    mandatoryValidator.validate(events, returnByEvent, CollectionExerciseState.SCHEDULED);
  }
",non-flaky,5
98397,ONSdigital_rm-collection-exercise-service,MandatoryEventValidatorTest.testValidExerciseEndEventUpdate,"  @Test
  public void testValidExerciseEndEventUpdate() throws CTPException {
    final List<Event> events = createMandatoryEvents();

    final Event exerciseEndEvent = new Event();
    exerciseEndEvent.setTag(Tag.exercise_end.toString());
    exerciseEndEvent.setTimestamp(Timestamp.from(Instant.now().plus(10, ChronoUnit.DAYS)));

    mandatoryValidator.validate(events, exerciseEndEvent, CollectionExerciseState.SCHEDULED);
  }
",non-flaky,5
98398,ONSdigital_rm-collection-exercise-service,MandatoryEventValidatorTest.testInvalidMpsEventUpdate,"  @Test
  public void testInvalidMpsEventUpdate() {
    final List<Event> events = createMandatoryEvents();

    final Event mpsEvent = new Event();
    mpsEvent.setTag(Tag.mps.toString());
    mpsEvent.setTimestamp(Timestamp.from(Instant.now().plus(6, ChronoUnit.DAYS)));

    CTPException actualException = null;
    try {
      mandatoryValidator.validate(events, mpsEvent, CollectionExerciseState.SCHEDULED);
    } catch (CTPException expectedException) {
      actualException = expectedException;
    }
    assertNotNull(actualException);
    assertEquals(
        ""Collection exercise events must be set sequentially"", actualException.getMessage());
  }
",non-flaky,5
98399,ONSdigital_rm-collection-exercise-service,MandatoryEventValidatorTest.testInvalidGoLiveEventUpdate,"  @Test
  public void testInvalidGoLiveEventUpdate() {
    final List<Event> events = createMandatoryEvents();

    final Event goLiveEvent = new Event();
    goLiveEvent.setTag(Tag.go_live.toString());
    goLiveEvent.setTimestamp(Timestamp.from(Instant.now().plus(8, ChronoUnit.DAYS)));

    CTPException actualException = null;
    try {
      mandatoryValidator.validate(events, goLiveEvent, CollectionExerciseState.SCHEDULED);
    } catch (CTPException expectedException) {
      actualException = expectedException;
    }
    assertNotNull(actualException);
    assertEquals(
        ""Collection exercise events must be set sequentially"", actualException.getMessage());
  }
",non-flaky,5
98400,ONSdigital_rm-collection-exercise-service,MandatoryEventValidatorTest.testInvalidReturnByEventUpdate,"  @Test
  public void testInvalidReturnByEventUpdate() {
    final List<Event> events = createMandatoryEvents();

    final Event returnByEvent = new Event();
    returnByEvent.setTag(Tag.return_by.toString());
    returnByEvent.setTimestamp(Timestamp.from(Instant.now().plus(1, ChronoUnit.DAYS)));

    CTPException actualException = null;
    try {
      mandatoryValidator.validate(events, returnByEvent, CollectionExerciseState.SCHEDULED);
    } catch (CTPException expectedException) {
      actualException = expectedException;
    }
    assertNotNull(actualException);
    assertEquals(
        ""Collection exercise events must be set sequentially"", actualException.getMessage());
  }
",non-flaky,5
98401,ONSdigital_rm-collection-exercise-service,MandatoryEventValidatorTest.testInvalidExerciseEndEventUpdate,"  @Test
  public void testInvalidExerciseEndEventUpdate() {
    final List<Event> events = createMandatoryEvents();

    final Event exerciseEndEvent = new Event();
    exerciseEndEvent.setTag(Tag.exercise_end.toString());
    exerciseEndEvent.setTimestamp(Timestamp.from(Instant.now().plus(1, ChronoUnit.DAYS)));

    CTPException actualException = null;
    try {
      mandatoryValidator.validate(events, exerciseEndEvent, CollectionExerciseState.SCHEDULED);
    } catch (CTPException expectedException) {
      actualException = expectedException;
    }
    assertNotNull(actualException);
    assertEquals(
        ""Collection exercise events must be set sequentially"", actualException.getMessage());
  }
",non-flaky,5
98402,ONSdigital_rm-collection-exercise-service,ReferencePeriodEventValidatorTest.isEventValidator,"  @Test
  public void isEventValidator() {
    assertThat(referencePeriodValidator, instanceOf(EventValidator.class));
  }
",non-flaky,5
98403,ONSdigital_rm-collection-exercise-service,ReferencePeriodEventValidatorTest.returnTrueAndDoNothingIfNotReferencePeriodEvent,"  @Test
  public void returnTrueAndDoNothingIfNotReferencePeriodEvent() throws CTPException {
    final Event mpsEvent = new Event();
    mpsEvent.setTag((Tag.mps.toString()));
    mpsEvent.setTimestamp(Timestamp.from(Instant.now()));
    final List<Event> events = new ArrayList<>();
    referencePeriodValidator.validate(events, mpsEvent, CollectionExerciseState.CREATED);
  }
",non-flaky,5
98404,ONSdigital_rm-collection-exercise-service,ReferencePeriodEventValidatorTest.canUpdateReferencePeriodWhenCollectionExerciseReadyForLive,"  @Test
  public void canUpdateReferencePeriodWhenCollectionExerciseReadyForLive() throws CTPException {
    final Event referencePeriodStart = new Event();
    referencePeriodStart.setTag(Tag.ref_period_end.toString());
    referencePeriodStart.setTimestamp(Timestamp.from(Instant.now()));

    final List<Event> events = new ArrayList<>();

    referencePeriodValidator.validate(
        events, referencePeriodStart, CollectionExerciseState.READY_FOR_LIVE);
  }
",non-flaky,5
98405,ONSdigital_rm-collection-exercise-service,ReferencePeriodEventValidatorTest.canUpdateReferencePeriodWhenCollectionExerciseLive,"  @Test
  public void canUpdateReferencePeriodWhenCollectionExerciseLive() throws CTPException {
    final Event referencePeriodStart = new Event();
    referencePeriodStart.setTag(Tag.ref_period_start.toString());
    referencePeriodStart.setTimestamp(Timestamp.from(Instant.now()));

    final List<Event> events = new ArrayList<>();

    referencePeriodValidator.validate(events, referencePeriodStart, CollectionExerciseState.LIVE);
  }
",non-flaky,5
98406,ONSdigital_rm-collection-exercise-service,ReferencePeriodEventValidatorTest.testReferenceStartCanBeSetInThePast,"  @Test
  public void testReferenceStartCanBeSetInThePast() throws CTPException {
    final Event refStart = new Event();
    refStart.setTag((Tag.ref_period_start.toString()));
    refStart.setTimestamp(Timestamp.from(Instant.now().minus(1, ChronoUnit.DAYS)));

    final List<Event> events = new ArrayList<>();
    referencePeriodValidator.validate(events, refStart, CollectionExerciseState.CREATED);
  }
",non-flaky,5
98407,ONSdigital_rm-collection-exercise-service,ReferencePeriodEventValidatorTest.testReferenceEndCanBeSetInThePast,"  @Test
  public void testReferenceEndCanBeSetInThePast() throws CTPException {
    final Event refEnd = new Event();
    refEnd.setTag((Tag.ref_period_end.toString()));
    refEnd.setTimestamp(Timestamp.from(Instant.now().minus(1, ChronoUnit.DAYS)));

    final List<Event> events = new ArrayList<>();
    referencePeriodValidator.validate(events, refEnd, CollectionExerciseState.CREATED);
  }
",non-flaky,5
98408,ONSdigital_rm-collection-exercise-service,ReferencePeriodEventValidatorTest.testReferenceEndBeforeReferenceStartIsInvalid,"  @Test
  public void testReferenceEndBeforeReferenceStartIsInvalid() {
    final Event refEnd = new Event();
    refEnd.setTag((Tag.ref_period_end.toString()));
    refEnd.setTimestamp(Timestamp.from(Instant.now().plus(1, ChronoUnit.DAYS)));
    final Event refStart = new Event();
    refStart.setTag((Tag.ref_period_start.toString()));
    refStart.setTimestamp(Timestamp.from(Instant.now().plus(2, ChronoUnit.DAYS)));
    final List<Event> events = new ArrayList<>();
    events.add(refEnd);
    CTPException actualException = null;
    try {
      referencePeriodValidator.validate(events, refStart, CollectionExerciseState.CREATED);
    } catch (CTPException expectedException) {
      actualException = expectedException;
    }
    assertNotNull(actualException);
    assertEquals(
        ""Reference period end date must be after start date"", actualException.getMessage());
  }
",non-flaky,5
98409,ONSdigital_rm-collection-exercise-service,NudgeEmailValidatorTest.isEventValidator,"  @Test
  public void isEventValidator() {
    assertThat(nudgeEmailValidator, instanceOf(EventValidator.class));
  }
",non-flaky,5
98410,ONSdigital_rm-collection-exercise-service,NudgeEmailValidatorTest.returnTrueAndDoNothingIfNotNudge,"  @Test
  public void returnTrueAndDoNothingIfNotNudge() throws CTPException {
    final Event mpsEvent = new Event();
    mpsEvent.setTag((EventService.Tag.mps.toString()));
    mpsEvent.setTimestamp(Timestamp.from(Instant.now()));
    final List<Event> events = new ArrayList<>();
    nudgeEmailValidator.validate(
        events, mpsEvent, CollectionExerciseDTO.CollectionExerciseState.CREATED);

    verify(eventDateOrderChecker, never()).isEventDatesInOrder(anyList());
  }
",non-flaky,5
98411,ONSdigital_rm-collection-exercise-service,NudgeEmailValidatorTest.testCanUpdateNudgeWhenReadyForLive,"  @Test
  public void testCanUpdateNudgeWhenReadyForLive() throws CTPException {
    final Event nudgeEvent = new Event();
    nudgeEvent.setTag(EventService.Tag.nudge_email_0.toString());
    nudgeEvent.setTimestamp(Timestamp.from(Instant.now()));

    final List<Event> events = new ArrayList<>();
    nudgeEmailValidator.validate(
        events, nudgeEvent, CollectionExerciseDTO.CollectionExerciseState.READY_FOR_LIVE);
  }
",non-flaky,5
98412,ONSdigital_rm-collection-exercise-service,NudgeEmailValidatorTest.testCanUpdateNudgeWhenLive,"  @Test
  public void testCanUpdateNudgeWhenLive() throws CTPException {
    final Event nudgeEvent = new Event();
    nudgeEvent.setTag(EventService.Tag.nudge_email_0.toString());
    nudgeEvent.setTimestamp(Timestamp.from(Instant.now()));

    final List<Event> events = new ArrayList<>();

    nudgeEmailValidator.validate(
        events, nudgeEvent, CollectionExerciseDTO.CollectionExerciseState.LIVE);
  }
",non-flaky,5
98413,ONSdigital_rm-collection-exercise-service,NudgeEmailValidatorTest.testCantUpdateNudgeThatHasPastAndCollectionExerciseInLockedState,"  @Test
  public void testCantUpdateNudgeThatHasPastAndCollectionExerciseInLockedState() {
    final Event nudge = new Event();
    nudge.setTag((EventService.Tag.nudge_email_0.toString()));
    nudge.setTimestamp(Timestamp.from(Instant.now().minus(2, ChronoUnit.DAYS)));

    final Event newNudge = new Event();
    newNudge.setTag((EventService.Tag.nudge_email_0.toString()));
    newNudge.setTimestamp(Timestamp.from(Instant.now().plus(2, ChronoUnit.DAYS)));

    final List<Event> events = Collections.singletonList(nudge);
    CTPException actualException = null;
    try {
      nudgeEmailValidator.validate(
          events, newNudge, CollectionExerciseDTO.CollectionExerciseState.LIVE);
    } catch (CTPException expectedException) {
      actualException = expectedException;
    }
    assertNotNull(actualException);
    assertEquals(""Nudge email cannot be set in the past"", actualException.getMessage());
  }
",non-flaky,5
98414,ONSdigital_rm-collection-exercise-service,NudgeEmailValidatorTest.testCanUpdateNudgeThatHasPastAndCollectionExerciseNotInLockedState,"  @Test
  public void testCanUpdateNudgeThatHasPastAndCollectionExerciseNotInLockedState()
      throws CTPException {
    final Event nudge = new Event();
    nudge.setTag((EventService.Tag.nudge_email_0.toString()));
    nudge.setTimestamp(Timestamp.from(Instant.now().minus(2, ChronoUnit.DAYS)));

    final Event newNudge = new Event();
    newNudge.setTag((EventService.Tag.nudge_email_0.toString()));
    newNudge.setTimestamp(Timestamp.from(Instant.now().plus(2, ChronoUnit.DAYS)));

    final List<Event> events = Collections.singletonList(nudge);

    nudgeEmailValidator.validate(
        events, newNudge, CollectionExerciseDTO.CollectionExerciseState.SCHEDULED);
  }
",non-flaky,5
98415,ONSdigital_rm-collection-exercise-service,NudgeEmailValidatorTest.testValidNudgeEventCreation,"  @Test
  public void testValidNudgeEventCreation() throws CTPException {
    final Event goLive = new Event();
    goLive.setTag((EventService.Tag.go_live.toString()));
    goLive.setTimestamp(Timestamp.from(Instant.now()));

    final Event nudge = new Event();
    nudge.setTag((EventService.Tag.nudge_email_0.toString()));
    nudge.setTimestamp(Timestamp.from(Instant.now().plus(2, ChronoUnit.DAYS)));

    final Event returnBy = new Event();
    returnBy.setTag((EventService.Tag.return_by.toString()));
    returnBy.setTimestamp(Timestamp.from(Instant.now().plus(4, ChronoUnit.DAYS)));

    final List<Event> events = Arrays.asList(goLive, returnBy);

    nudgeEmailValidator.validate(
        events, nudge, CollectionExerciseDTO.CollectionExerciseState.CREATED);
  }
",non-flaky,5
98416,ONSdigital_rm-collection-exercise-service,NudgeEmailValidatorTest.testNudgeAfterReturnByEndInvalid,"  @Test
  public void testNudgeAfterReturnByEndInvalid() {
    final Event goLive = new Event();
    goLive.setTag((EventService.Tag.go_live.toString()));
    goLive.setTimestamp(Timestamp.from(Instant.now()));

    final Event nudgeEvent = new Event();
    nudgeEvent.setTag(EventService.Tag.nudge_email_0.toString());
    nudgeEvent.setTimestamp(Timestamp.from(Instant.now().plus(4, ChronoUnit.DAYS)));

    final Event returnBy = new Event();
    returnBy.setTag((EventService.Tag.return_by.toString()));
    returnBy.setTimestamp(Timestamp.from(Instant.now().plus(3, ChronoUnit.DAYS)));

    final List<Event> events = Arrays.asList(goLive, returnBy);
    CTPException actualException = null;

    SimpleDateFormat sdf = new SimpleDateFormat(""yyyy-MM-dd HH:mm:ss.SSS"");
    sdf.setTimeZone(TimeZone.getTimeZone(""Europe/London""));
    Date goLiveDate = new Date(goLive.getTimestamp().getTime());
    Date returnByDate = new Date(returnBy.getTimestamp().getTime());

    try {
      nudgeEmailValidator.validate(
          events, nudgeEvent, CollectionExerciseDTO.CollectionExerciseState.SCHEDULED);
    } catch (CTPException expectedException) {
      actualException = expectedException;
    }
    assertNotNull(actualException);
    String expectedMessage =
        ""Nudge email must be set after the Go Live date (""
            + sdf.format(goLiveDate)
            + "") ""
            + ""and before Return by date (""
            + sdf.format(returnByDate)
            + "")"";
    assertEquals(expectedMessage, actualException.getMessage());
  }
",non-flaky,5
98417,ONSdigital_rm-collection-exercise-service,NudgeEmailValidatorTest.testNudgeBeforeGoliveInvalid,"  @Test
  public void testNudgeBeforeGoliveInvalid() {
    final Event goLive = new Event();
    goLive.setTag((EventService.Tag.go_live.toString()));
    goLive.setTimestamp(Timestamp.from(Instant.now().plus(2, ChronoUnit.DAYS)));
    final Event returnBy = new Event();
    returnBy.setTag((EventService.Tag.return_by.toString()));
    returnBy.setTimestamp(Timestamp.from(Instant.now().plus(3, ChronoUnit.DAYS)));
    final List<Event> events = Arrays.asList(goLive, returnBy);

    final Event nudgeEvent = new Event();
    nudgeEvent.setTag(EventService.Tag.nudge_email_0.toString());
    nudgeEvent.setTimestamp(Timestamp.from(Instant.now().plus(1, ChronoUnit.DAYS)));

    SimpleDateFormat sdf = new SimpleDateFormat(""yyyy-MM-dd HH:mm:ss.SSS"");
    sdf.setTimeZone(TimeZone.getTimeZone(""Europe/London""));
    Date goLiveDate = new Date(goLive.getTimestamp().getTime());
    Date returnByDate = new Date(returnBy.getTimestamp().getTime());

    CTPException actualException = null;
    try {
      nudgeEmailValidator.validate(
          events, nudgeEvent, CollectionExerciseDTO.CollectionExerciseState.SCHEDULED);
    } catch (CTPException expectedException) {
      actualException = expectedException;
    }
    assertNotNull(actualException);
    String expectedMessage =
        ""Nudge email must be set after the Go Live date (""
            + sdf.format(goLiveDate)
            + "") ""
            + ""and before Return by date (""
            + sdf.format(returnByDate)
            + "")"";
    assertEquals(expectedMessage, actualException.getMessage());
  }
",non-flaky,5
98418,ONSdigital_rm-collection-exercise-service,NudgeEmailValidatorTest.testNudgeWithSameDateAndTimeEventCreation,"  @Test
  public void testNudgeWithSameDateAndTimeEventCreation() {
    final Instant now = Instant.now();
    final Long nudgeTime = now.plus(2, ChronoUnit.DAYS).toEpochMilli();
    final Event goLive = new Event();
    goLive.setTag((EventService.Tag.go_live.toString()));
    goLive.setTimestamp(Timestamp.from(now));

    final Event nudge = new Event();
    nudge.setTag((EventService.Tag.nudge_email_0.toString()));
    nudge.setTimestamp(new Timestamp(nudgeTime));

    final Event returnBy = new Event();
    returnBy.setTag((EventService.Tag.return_by.toString()));
    returnBy.setTimestamp(Timestamp.from(now.plus(4, ChronoUnit.DAYS)));

    final List<Event> events = Arrays.asList(goLive, returnBy, nudge);

    final Event submittedEvent = new Event();
    submittedEvent.setTag((EventService.Tag.nudge_email_1.toString()));
    submittedEvent.setTimestamp(new Timestamp(nudgeTime));

    CTPException actualException = null;
    try {
      nudgeEmailValidator.validate(
          events, submittedEvent, CollectionExerciseDTO.CollectionExerciseState.CREATED);
    } catch (CTPException expectedException) {
      actualException = expectedException;
    }
    assertNotNull(actualException);
    assertEquals(
        ""A nudge email has already been scheduled for this date and time. Choose a different date or time."",
        actualException.getMessage());
  }
",non-flaky,5
98419,ONSdigital_rm-collection-exercise-service,NudgeEmailValidatorTest.testNudgeWithSameDateAndTimeEventCreationNotValidForTheSameEvent,"  @Test
  public void testNudgeWithSameDateAndTimeEventCreationNotValidForTheSameEvent() {
    final Instant now = Instant.now();
    final Long nudgeTime = now.plus(2, ChronoUnit.DAYS).toEpochMilli();
    final Event goLive = new Event();
    goLive.setTag((EventService.Tag.go_live.toString()));
    goLive.setTimestamp(Timestamp.from(now));

    final Event nudge = new Event();
    nudge.setTag((EventService.Tag.nudge_email_0.toString()));
    nudge.setTimestamp(new Timestamp(nudgeTime));

    final Event returnBy = new Event();
    returnBy.setTag((EventService.Tag.return_by.toString()));
    returnBy.setTimestamp(Timestamp.from(now.plus(4, ChronoUnit.DAYS)));

    final List<Event> events = Arrays.asList(goLive, returnBy, nudge);

    final Event submittedEvent = new Event();
    submittedEvent.setTag((EventService.Tag.nudge_email_0.toString()));
    submittedEvent.setTimestamp(new Timestamp(nudgeTime));

    CTPException actualException = null;
    try {
      nudgeEmailValidator.validate(
          events, submittedEvent, CollectionExerciseDTO.CollectionExerciseState.CREATED);
    } catch (CTPException expectedException) {
      actualException = expectedException;
    }
    assertNull(actualException);
  }
",non-flaky,5
98420,ONSdigital_rm-collection-exercise-service,CollectionExerciseServiceTest.testCreateCollectionExercise,"  @Test
  public void testCreateCollectionExercise() throws Exception {
    // Given
    CollectionExercise collectionExercise =
        FixtureHelper.loadClassFixtures(CollectionExercise[].class).get(0);
    when(collexRepo.saveAndFlush(any())).thenReturn(collectionExercise);

    SurveyDTO survey = FixtureHelper.loadClassFixtures(SurveyDTO[].class).get(0);
    CollectionExerciseDTO toCreate =
        FixtureHelper.loadClassFixtures(CollectionExerciseDTO[].class).get(0);

    // When
    this.collectionExerciseService.createCollectionExercise(toCreate, survey);

    // Then
    ArgumentCaptor<CollectionExercise> captor = ArgumentCaptor.forClass(CollectionExercise.class);
    verify(this.collexRepo).saveAndFlush(captor.capture());
    CollectionExercise collex = captor.getValue();
    assertEquals(toCreate.getUserDescription(), collex.getUserDescription());
    assertEquals(toCreate.getExerciseRef(), collex.getExerciseRef());
    assertEquals(toCreate.getSurveyId(), collex.getSurveyId().toString());
    assertNotNull(collex.getCreated());
  }
",non-flaky,5
98421,ONSdigital_rm-collection-exercise-service,CollectionExerciseServiceTest.testUpdateCollectionExercise,"  @Test
  public void testUpdateCollectionExercise() throws Exception {
    CollectionExerciseDTO toUpdate =
        FixtureHelper.loadClassFixtures(CollectionExerciseDTO[].class).get(0);
    CollectionExercise existing =
        FixtureHelper.loadClassFixtures(CollectionExercise[].class).get(0);
    SurveyDTO survey = FixtureHelper.loadClassFixtures(SurveyDTO[].class).get(0);
    UUID surveyId = UUID.fromString(survey.getId());
    existing.setSurveyId(surveyId);
    when(collexRepo.findOneById(existing.getId())).thenReturn(existing);
    when(surveyService.findSurvey(surveyId)).thenReturn(survey);

    this.collectionExerciseService.updateCollectionExercise(existing.getId(), toUpdate);

    ArgumentCaptor<CollectionExercise> captor = ArgumentCaptor.forClass(CollectionExercise.class);

    verify(collexRepo).saveAndFlush(captor.capture());
    CollectionExercise collex = captor.getValue();
    assertEquals(UUID.fromString(toUpdate.getSurveyId()), collex.getSurveyId());
    assertEquals(toUpdate.getExerciseRef(), collex.getExerciseRef());
    assertEquals(toUpdate.getUserDescription(), collex.getUserDescription());
    assertNotNull(collex.getUpdated());
  }
",non-flaky,5
98422,ONSdigital_rm-collection-exercise-service,CollectionExerciseServiceTest.testUpdateCollectionExerciseInvalidSurvey,"  @Test
  public void testUpdateCollectionExerciseInvalidSurvey() throws Exception {
    CollectionExerciseDTO toUpdate =
        FixtureHelper.loadClassFixtures(CollectionExerciseDTO[].class).get(0);
    CollectionExercise existing =
        FixtureHelper.loadClassFixtures(CollectionExercise[].class).get(0);
    existing.setSurveyId(UUID.randomUUID());
    when(collexRepo.findOneById(existing.getId())).thenReturn(existing);

    try {
      this.collectionExerciseService.updateCollectionExercise(existing.getId(), toUpdate);
      fail(""Update collection exercise with null survey succeeded"");
    } catch (CTPException e) {
      assertEquals(CTPException.Fault.BAD_REQUEST, e.getFault());
    }
  }
",non-flaky,5
98423,ONSdigital_rm-collection-exercise-service,CollectionExerciseServiceTest.testUpdateCollectionExerciseNonUnique,"  @Test
  public void testUpdateCollectionExerciseNonUnique() throws Exception {
    CollectionExerciseDTO toUpdate =
        FixtureHelper.loadClassFixtures(CollectionExerciseDTO[].class).get(0);
    CollectionExercise existing =
        FixtureHelper.loadClassFixtures(CollectionExercise[].class).get(0);
    existing.setSurveyId(UUID.randomUUID());
    // Set up the mock to return the one we are attempting to update
    when(collexRepo.findOneById(existing.getId())).thenReturn(existing);

    UUID uuid = UUID.fromString(""0f66744b-bfdb-458a-b495-1eb605462003"");
    CollectionExercise otherExisting = new CollectionExercise();
    otherExisting.setId(uuid);
    // Set up the mock to return a different one with the same exercise ref and survey id
    when(collexRepo.findByExerciseRefAndSurveyId(
            toUpdate.getExerciseRef(), UUID.fromString(toUpdate.getSurveyId())))
        .thenReturn(Collections.singletonList(otherExisting));

    try {
      this.collectionExerciseService.updateCollectionExercise(existing.getId(), toUpdate);

      fail(""Update to collection exercise breaking uniqueness constraint succeeded"");
    } catch (CTPException e) {
      assertEquals(CTPException.Fault.RESOURCE_VERSION_CONFLICT, e.getFault());
    }
  }
",non-flaky,5
98424,ONSdigital_rm-collection-exercise-service,CollectionExerciseServiceTest.testUpdateCollectionExerciseDoesNotExist,"  @Test
  public void testUpdateCollectionExerciseDoesNotExist() throws Exception {
    CollectionExerciseDTO toUpdate =
        FixtureHelper.loadClassFixtures(CollectionExerciseDTO[].class).get(0);
    UUID updateUuid = UUID.randomUUID();

    try {
      this.collectionExerciseService.updateCollectionExercise(updateUuid, toUpdate);
      fail(""Update of non-existent collection exercise succeeded"");
    } catch (CTPException e) {
      assertEquals(CTPException.Fault.RESOURCE_NOT_FOUND, e.getFault());
    }
  }
",non-flaky,5
98425,ONSdigital_rm-collection-exercise-service,CollectionExerciseServiceTest.testDeleteCollectionExercise,"  @Test
  public void testDeleteCollectionExercise() throws Exception {
    CollectionExercise existing =
        FixtureHelper.loadClassFixtures(CollectionExercise[].class).get(0);
    when(collexRepo.findOneById(existing.getId())).thenReturn(existing);

    this.collectionExerciseService.deleteCollectionExercise(existing.getId());

    ArgumentCaptor<CollectionExercise> captor = ArgumentCaptor.forClass(CollectionExercise.class);
    verify(this.collexRepo).saveAndFlush(captor.capture());

    assertEquals(true, captor.getValue().getDeleted());
  }
",non-flaky,5
98426,ONSdigital_rm-collection-exercise-service,CollectionExerciseServiceTest.testUndeleteCollectionExercise,"  @Test
  public void testUndeleteCollectionExercise() throws Exception {
    CollectionExercise existing =
        FixtureHelper.loadClassFixtures(CollectionExercise[].class).get(0);
    when(collexRepo.findOneById(existing.getId())).thenReturn(existing);

    this.collectionExerciseService.undeleteCollectionExercise(existing.getId());

    ArgumentCaptor<CollectionExercise> captor = ArgumentCaptor.forClass(CollectionExercise.class);
    verify(this.collexRepo).saveAndFlush(captor.capture());

    assertEquals(false, captor.getValue().getDeleted());
  }
",non-flaky,5
98427,ONSdigital_rm-collection-exercise-service,CollectionExerciseServiceTest.testPatchCollectionExerciseNotExists,"  @Test
  public void testPatchCollectionExerciseNotExists() throws Exception {
    CollectionExerciseDTO toUpdate =
        FixtureHelper.loadClassFixtures(CollectionExerciseDTO[].class).get(0);
    UUID updateUuid = UUID.randomUUID();

    try {
      this.collectionExerciseService.patchCollectionExercise(updateUuid, toUpdate);

      fail(""Attempt to patch non-existent collection exercise succeeded"");
    } catch (CTPException e) {
      assertEquals(CTPException.Fault.RESOURCE_NOT_FOUND, e.getFault());
    }
  }
",non-flaky,5
98428,ONSdigital_rm-collection-exercise-service,CollectionExerciseServiceTest.testPatchCollectionExerciseExerciseRef,"  @Test
  public void testPatchCollectionExerciseExerciseRef() throws Exception {
    CollectionExercise existing = setupCollectionExercise();
    CollectionExerciseDTO collex = new CollectionExerciseDTO();
    SurveyDTO survey = FixtureHelper.loadClassFixtures(SurveyDTO[].class).get(0);
    UUID surveyId = UUID.fromString(survey.getId());
    String exerciseRef = ""209966"";
    collex.setExerciseRef(exerciseRef);
    collex.setSurveyId(surveyId.toString());
    when(surveyService.findSurvey(surveyId)).thenReturn(survey);
    this.collectionExerciseService.patchCollectionExercise(existing.getId(), collex);

    ArgumentCaptor<CollectionExercise> captor = ArgumentCaptor.forClass(CollectionExercise.class);
    verify(this.collexRepo).saveAndFlush(captor.capture());

    CollectionExercise ce = captor.getValue();
    assertEquals(exerciseRef, ce.getExerciseRef());
    assertNotNull(ce.getUpdated());
  }
",non-flaky,5
98429,ONSdigital_rm-collection-exercise-service,CollectionExerciseServiceTest.testPatchCollectionExerciseName,"  @Test
  public void testPatchCollectionExerciseName() throws Exception {
    CollectionExercise existing = setupCollectionExercise();
    CollectionExerciseDTO collex = new CollectionExerciseDTO();
    String name = ""Not BRES"";
    SurveyDTO survey = FixtureHelper.loadClassFixtures(SurveyDTO[].class).get(0);
    when(surveyService.findSurvey(any())).thenReturn(survey);
    this.collectionExerciseService.patchCollectionExercise(existing.getId(), collex);

    ArgumentCaptor<CollectionExercise> captor = ArgumentCaptor.forClass(CollectionExercise.class);
    verify(this.collexRepo).saveAndFlush(captor.capture());

    CollectionExercise ce = captor.getValue();
    assertNotNull(ce.getUpdated());
  }
",non-flaky,5
98430,ONSdigital_rm-collection-exercise-service,CollectionExerciseServiceTest.testPatchCollectionExerciseUserDescription,"  @Test
  public void testPatchCollectionExerciseUserDescription() throws Exception {
    CollectionExercise existing = setupCollectionExercise();
    CollectionExerciseDTO collex = new CollectionExerciseDTO();
    String userDescription = ""Really odd description"";
    collex.setUserDescription(userDescription);
    SurveyDTO survey = FixtureHelper.loadClassFixtures(SurveyDTO[].class).get(0);
    when(surveyService.findSurvey(any())).thenReturn(survey);
    this.collectionExerciseService.patchCollectionExercise(existing.getId(), collex);

    ArgumentCaptor<CollectionExercise> captor = ArgumentCaptor.forClass(CollectionExercise.class);
    verify(this.collexRepo).saveAndFlush(captor.capture());

    CollectionExercise ce = captor.getValue();
    assertEquals(userDescription, ce.getUserDescription());
    assertNotNull(ce.getUpdated());
  }
",non-flaky,5
98431,ONSdigital_rm-collection-exercise-service,CollectionExerciseServiceTest.testPatchCollectionExerciseNonUnique,"  @Test
  public void testPatchCollectionExerciseNonUnique() throws Exception {
    CollectionExerciseDTO toUpdate =
        FixtureHelper.loadClassFixtures(CollectionExerciseDTO[].class).get(0);
    CollectionExercise existing =
        FixtureHelper.loadClassFixtures(CollectionExercise[].class).get(0);
    existing.setSurveyId(UUID.randomUUID());
    // Set up the mock to return the one we are attempting to update
    when(collexRepo.findOneById(existing.getId())).thenReturn(existing);

    UUID uuid = UUID.fromString(""0f66744b-bfdb-458a-b495-1eb605462003"");
    CollectionExercise otherExisting = new CollectionExercise();
    otherExisting.setId(uuid);
    // Set up the mock to return a different one with the same exercise ref and survey id
    when(collexRepo.findByExerciseRefAndSurveyId(
            toUpdate.getExerciseRef(), UUID.fromString(toUpdate.getSurveyId())))
        .thenReturn(Collections.singletonList(otherExisting));

    try {
      this.collectionExerciseService.patchCollectionExercise(existing.getId(), toUpdate);

      fail(""Update to collection exercise breaking uniqueness constraint succeeded"");
    } catch (CTPException e) {
      assertEquals(CTPException.Fault.RESOURCE_VERSION_CONFLICT, e.getFault());
    }
  }
",non-flaky,5
98432,ONSdigital_rm-collection-exercise-service,CollectionExerciseServiceTest.testTransitionToReadyToReviewWhenScheduledWithCIsAndSample,"  @Test
  public void testTransitionToReadyToReviewWhenScheduledWithCIsAndSample() throws Exception {
    // Given
    CollectionExercise exercise =
        FixtureHelper.loadClassFixtures(CollectionExercise[].class).get(0);
    exercise.setState(CollectionExerciseDTO.CollectionExerciseState.SCHEDULED);
    SampleLink testSampleLink = new SampleLink();
    testSampleLink.setSampleSummaryId(UUID.randomUUID());
    given(sampleLinkRepository.findByCollectionExerciseId(exercise.getId()))
        .willReturn(Collections.singletonList(testSampleLink));

    SampleSummaryDTO sampleSummary = new SampleSummaryDTO();
    sampleSummary.setState(SampleSummaryDTO.SampleState.ACTIVE);
    given(sampleSvcClient.getSampleSummary(testSampleLink.getSampleSummaryId()))
        .willReturn(sampleSummary);

    String searchStringJson =
        new JSONObject(Collections.singletonMap(""COLLECTION_EXERCISE"", exercise.getId().toString()))
            .toString();
    given(collectionInstrument.countCollectionInstruments(searchStringJson)).willReturn(1);

    // When
    collectionExerciseService.transitionScheduleCollectionExerciseToReadyToReview(exercise);

    // Then
    exercise.setState(CollectionExerciseDTO.CollectionExerciseState.READY_FOR_REVIEW);
    verify(collexRepo).saveAndFlush(exercise);
  }
",non-flaky,5
98433,ONSdigital_rm-collection-exercise-service,CollectionExerciseServiceTest.testDoNotTransitionToReadyToReviewWhenScheduledWithCIsAndNoSample,"  @Test
  public void testDoNotTransitionToReadyToReviewWhenScheduledWithCIsAndNoSample() throws Exception {
    // Given
    CollectionExercise exercise =
        FixtureHelper.loadClassFixtures(CollectionExercise[].class).get(0);
    exercise.setState(CollectionExerciseDTO.CollectionExerciseState.SCHEDULED);
    given(sampleLinkRepository.findByCollectionExerciseId(exercise.getId()))
        .willReturn(Collections.emptyList());
    String searchStringJson =
        new JSONObject(Collections.singletonMap(""COLLECTION_EXERCISE"", exercise.getId().toString()))
            .toString();
    given(collectionInstrument.countCollectionInstruments(searchStringJson)).willReturn(1);

    // When
    collectionExerciseService.transitionScheduleCollectionExerciseToReadyToReview(exercise);

    // Then
    exercise.setState(CollectionExerciseDTO.CollectionExerciseState.READY_FOR_REVIEW);
    verify(collexRepo, times(0)).saveAndFlush(exercise);
  }
",non-flaky,5
98434,ONSdigital_rm-collection-exercise-service,CollectionExerciseServiceTest.testDoNotTransitionToReadyToReviewWhenScheduledWithNoCIsAndSample,"  @Test
  public void testDoNotTransitionToReadyToReviewWhenScheduledWithNoCIsAndSample() throws Exception {
    // Given
    CollectionExercise exercise =
        FixtureHelper.loadClassFixtures(CollectionExercise[].class).get(0);
    exercise.setState(CollectionExerciseDTO.CollectionExerciseState.SCHEDULED);
    given(sampleLinkRepository.findByCollectionExerciseId(exercise.getId()))
        .willReturn(Collections.emptyList());
    String searchStringJson =
        new JSONObject(Collections.singletonMap(""COLLECTION_EXERCISE"", exercise.getId().toString()))
            .toString();
    given(collectionInstrument.countCollectionInstruments(searchStringJson)).willReturn(0);

    // When
    collectionExerciseService.transitionScheduleCollectionExerciseToReadyToReview(exercise);

    // Then
    exercise.setState(CollectionExerciseDTO.CollectionExerciseState.READY_FOR_REVIEW);
    verify(collexRepo, times(0)).saveAndFlush(exercise);
  }
",non-flaky,5
98435,ONSdigital_rm-collection-exercise-service,CollectionExerciseServiceTest.testDoNotTransitionToReadyToReviewWhenCIsCountFailsAndReturnsNull,"  @Test
  public void testDoNotTransitionToReadyToReviewWhenCIsCountFailsAndReturnsNull() throws Exception {
    // Given
    CollectionExercise exercise =
        FixtureHelper.loadClassFixtures(CollectionExercise[].class).get(0);
    exercise.setState(CollectionExerciseDTO.CollectionExerciseState.SCHEDULED);
    given(sampleLinkRepository.findByCollectionExerciseId(exercise.getId()))
        .willReturn(Collections.emptyList());
    String searchStringJson =
        new JSONObject(Collections.singletonMap(""COLLECTION_EXERCISE"", exercise.getId().toString()))
            .toString();
    given(collectionInstrument.countCollectionInstruments(searchStringJson)).willReturn(null);

    // When
    collectionExerciseService.transitionScheduleCollectionExerciseToReadyToReview(exercise);

    // Then
    exercise.setState(CollectionExerciseDTO.CollectionExerciseState.READY_FOR_REVIEW);
    verify(collexRepo, times(0)).saveAndFlush(exercise);
  }
",non-flaky,5
98436,ONSdigital_rm-collection-exercise-service,CollectionExerciseServiceTest.testCreateLink,"  @Test
  public void testCreateLink() {
    UUID sampleSummaryUuid = UUID.randomUUID(), collexUuid = UUID.randomUUID();

    when(this.sampleLinkRepository.saveAndFlush(any(SampleLink.class))).then(returnsFirstArg());

    SampleLink sampleLink =
        this.collectionExerciseService.createLink(sampleSummaryUuid, collexUuid);

    assertEquals(sampleSummaryUuid, sampleLink.getSampleSummaryId());
    assertEquals(collexUuid, sampleLink.getCollectionExerciseId());

    verify(sampleLinkRepository, times(1)).saveAndFlush(any());
  }
",non-flaky,5
98437,ONSdigital_rm-collection-exercise-service,CollectionExerciseServiceTest.testCreateLinkShouldAttemptToTransitionToReadyToReview,"  @Test
  public void testCreateLinkShouldAttemptToTransitionToReadyToReview() throws CTPException {
    // Given
    UUID sampleSummaryUuid = UUID.randomUUID();
    UUID collexUuid = UUID.randomUUID();
    CollectionExercise collectionExercise = new CollectionExercise();
    collectionExercise.setId(collexUuid);
    collectionExercise.setState(CollectionExerciseDTO.CollectionExerciseState.CREATED);
    given(collexRepo.findOneById(collexUuid)).willReturn(collectionExercise);
    given(collectionInstrument.countCollectionInstruments(any())).willReturn(1);
    SampleSummaryDTO sampleSummary = new SampleSummaryDTO();
    SampleLink sampleLink = new SampleLink();
    sampleLink.setSampleSummaryId(sampleSummaryUuid);
    given(sampleLinkRepository.findByCollectionExerciseId(collexUuid))
        .willReturn(Collections.singletonList(sampleLink));
    sampleSummary.setState(SampleSummaryDTO.SampleState.ACTIVE);
    given(sampleSvcClient.getSampleSummary(sampleSummaryUuid)).willReturn(sampleSummary);

    // When
    this.collectionExerciseService.linkSampleSummaryToCollectionExercise(
        collexUuid, Collections.singletonList(sampleSummaryUuid));

    // Then
    verify(stateManager)
        .transition(
            CollectionExerciseDTO.CollectionExerciseState.CREATED,
            CollectionExerciseDTO.CollectionExerciseEvent.CI_SAMPLE_ADDED);
  }
",non-flaky,5
98438,ONSdigital_rm-collection-exercise-service,CollectionExerciseServiceTest.testFindCollectionExercisesForSurveys,"  @Test
  public void testFindCollectionExercisesForSurveys() throws Exception {
    final UUID SURVEY_ID_1 = UUID.fromString(""31ec898e-f370-429a-bca4-eab1045aff4e"");

    List<UUID> surveys = Arrays.asList(SURVEY_ID_1);

    List<CollectionExercise> existing = FixtureHelper.loadClassFixtures(CollectionExercise[].class);

    given(collexRepo.findBySurveyIdInOrderBySurveyId(surveys)).willReturn(existing);

    HashMap<UUID, List<CollectionExercise>> result =
        this.collectionExerciseService.findCollectionExercisesForSurveys(surveys);

    assertEquals(result.get(SURVEY_ID_1).size(), 2);
  }
",non-flaky,5
98439,ONSdigital_rm-collection-exercise-service,CollectionExerciseServiceTest.testFindCollectionExercisesForSurveysByState,"  @Test
  public void testFindCollectionExercisesForSurveysByState() throws Exception {
    final UUID SURVEY_ID_1 = UUID.fromString(""31ec898e-f370-429a-bca4-eab1045aff4e"");

    List<UUID> surveys = Arrays.asList(SURVEY_ID_1);

    List<CollectionExercise> existing = FixtureHelper.loadClassFixtures(CollectionExercise[].class);

    given(
            collexRepo.findBySurveyIdInAndStateOrderBySurveyId(
                surveys, CollectionExerciseDTO.CollectionExerciseState.LIVE))
        .willReturn(existing);

    HashMap<UUID, List<CollectionExercise>> result =
        this.collectionExerciseService.findCollectionExercisesForSurveysByState(
            surveys, CollectionExerciseDTO.CollectionExerciseState.LIVE);

    assertEquals(result.get(SURVEY_ID_1).size(), 2);
  }
",non-flaky,5
98440,ONSdigital_rm-collection-exercise-service,SampleServiceTest.testAcceptSampleUnitAlreadyExists,"  @Test
  public void testAcceptSampleUnitAlreadyExists() throws CTPException {
    CollectionExercise collex = new CollectionExercise();
    collex.setId(COLLEX_ID);
    collex.setSampleSize(99);
    collex.setState(CollectionExerciseState.EXECUTION_STARTED);

    SampleUnit sampleUnit = new SampleUnit();
    sampleUnit.setCollectionExerciseId(COLLEX_ID.toString());
    sampleUnit.setFormType(""X"");
    sampleUnit.setId(SAMPLE_ID.toString());
    sampleUnit.setSampleUnitType(""B"");
    sampleUnit.setSampleUnitRef(""REF123"");
    when(collectRepo.findOneById(any())).thenReturn(collex);
    when(sampleUnitRepo.existsBySampleUnitRefAndSampleUnitTypeAndSampleUnitGroupCollectionExercise(
            any(), any(), any()))
        .thenReturn(true);

    underTest.acceptSampleUnit(sampleUnit);

    verify(collectionExerciseTransitionState, never()).transition(any(), any());
    verify(sampleUnitGroupRepo, never()).saveAndFlush(any());
    verify(sampleUnitRepo, never()).saveAndFlush(any());
    verify(collectRepo, never()).saveAndFlush(any());
  }
",non-flaky,5
98441,ONSdigital_rm-collection-exercise-service,SampleServiceTest.requestSampleUnitsHappyPath,"  @Test
  public void requestSampleUnitsHappyPath() throws CTPException {
    UUID collexId = UUID.randomUUID();
    UUID sampleSummaryId = UUID.randomUUID();
    CollectionExercise collectionExercise = new CollectionExercise();
    collectionExercise.setId(collexId);
    SampleLink sampleLink = new SampleLink();
    sampleLink.setSampleSummaryId(sampleSummaryId);
    List<SampleLink> sampleLinks = Collections.singletonList(sampleLink);
    SampleUnitsRequestDTO sampleUnitsRequestDTO = new SampleUnitsRequestDTO();
    sampleUnitsRequestDTO.setSampleUnitsTotal(666);

    // Given
    when(collectRepo.findOneById(eq(collexId))).thenReturn(collectionExercise);
    when(sampleLinkRepo.findByCollectionExerciseId(any())).thenReturn(sampleLinks);
    when(sampleSvcClient.getSampleUnitCount(any())).thenReturn(sampleUnitsRequestDTO);
    when(sampleSvcClient.requestSampleUnits(any())).thenReturn(sampleUnitsRequestDTO);

    // When
    underTest.requestSampleUnits(collexId);

    // Then
    verify(collexSampleUnitReceiptPreparer).prepareCollexToAcceptSampleUnits(eq(collexId), eq(666));
    verify(partySvcClient).linkSampleSummaryId(any(), any());
  }
",non-flaky,5
98442,ONSdigital_rm-collection-exercise-service,EventServiceTest.givenCollectionExcerciseDoesNotExistWhenEventIsCreatedThenExceptionIsThrown,"  @Test
  public void givenCollectionExcerciseDoesNotExistWhenEventIsCreatedThenExceptionIsThrown() {
    EventDTO eventDto = new EventDTO();
    UUID collexUuid = UUID.randomUUID();
    eventDto.setCollectionExerciseId(collexUuid);

    try {
      eventService.createEvent(eventDto);

      fail(""Created event with non-existent collection exercise"");
    } catch (CTPException e) {
      // Expected 404
      assertThat(e.getFault(), is(Fault.RESOURCE_NOT_FOUND));
    }
  }
",non-flaky,5
98443,ONSdigital_rm-collection-exercise-service,EventServiceTest.givenEventAlreadyExistsWhenEventIsCreatedThenExceptionIsThrown,"  @Test
  public void givenEventAlreadyExistsWhenEventIsCreatedThenExceptionIsThrown() throws CTPException {
    String tag = Tag.mps.name();
    EventDTO eventDto = new EventDTO();
    CollectionExercise collex = new CollectionExercise();
    UUID collexUuid = UUID.randomUUID();
    eventDto.setCollectionExerciseId(collexUuid);
    eventDto.setTag(tag);
    collex.setId(collexUuid);

    when(collectionExerciseService.findCollectionExercise(collexUuid)).thenReturn(collex);
    when(eventRepository.findOneByCollectionExerciseAndTag(collex, tag)).thenReturn(new Event());

    try {
      eventService.createEvent(eventDto);

      fail(""Created event with non-existent collection exercise"");
    } catch (CTPException e) {
      // Expected 409
      assertThat(e.getFault(), is(Fault.RESOURCE_VERSION_CONFLICT));
    }
  }
",non-flaky,5
98444,ONSdigital_rm-collection-exercise-service,EventServiceTest.givenNoEventsWhenScheduledIsCheckedThenFalse,"  @Test
  public void givenNoEventsWhenScheduledIsCheckedThenFalse() throws CTPException {
    UUID collexUuid = UUID.randomUUID();
    when(eventRepository.findByCollectionExerciseId(collexUuid)).thenReturn(new ArrayList<>());

    boolean scheduled = this.eventService.isScheduled(collexUuid);

    assertFalse(scheduled);
  }
",non-flaky,5
98445,ONSdigital_rm-collection-exercise-service,EventServiceTest.givenCollectionExcerciseDoesNotExistWhenEventIsUpdatedThenExceptionIsThrown,"  @Test
  public void givenCollectionExcerciseDoesNotExistWhenEventIsUpdatedThenExceptionIsThrown() {
    final UUID collexUuid = UUID.randomUUID();

    when(collectionExerciseService.findCollectionExercise(collexUuid)).thenReturn(null);

    try {
      eventService.updateEvent(collexUuid, Tag.mps.name(), new Date());

      Assert.fail(""Updated event with non-existent collection exercise"");
    } catch (final CTPException e) {
      assertThat(e.getFault(), is(Fault.BAD_REQUEST));
    }
  }
",non-flaky,5
98446,ONSdigital_rm-collection-exercise-service,EventServiceTest.givenCollectionExcerciseDoesNotExistWhenEventIsDeletedThenExceptionIsThrown,"  @Test
  public void givenCollectionExcerciseDoesNotExistWhenEventIsDeletedThenExceptionIsThrown() {
    final UUID collexUuid = UUID.randomUUID();

    when(collectionExerciseService.findCollectionExercise(collexUuid)).thenReturn(null);

    try {
      eventService.deleteEvent(collexUuid, Tag.mps.name());

      Assert.fail(""Deleted event with non-existent collection exercise"");
    } catch (final CTPException e) {
      assertThat(e.getFault(), is(Fault.BAD_REQUEST));
    }
  }
",non-flaky,5
98447,ONSdigital_rm-collection-exercise-service,EventServiceTest.givenEventDoesNotExistWhenEventIsUpdatedThenExceptionIsThrown,"  @Test
  public void givenEventDoesNotExistWhenEventIsUpdatedThenExceptionIsThrown() {
    final UUID collexUuid = UUID.randomUUID();

    final CollectionExercise collex = new CollectionExercise();
    collex.setId(collexUuid);

    when(collectionExerciseService.findCollectionExercise(collexUuid)).thenReturn(collex);
    when(eventRepository.findOneByCollectionExerciseAndTag(collex, Tag.mps.name()))
        .thenReturn(null);

    try {
      eventService.updateEvent(collexUuid, Tag.mps.name(), new Date());

      Assert.fail(""Updated non-existent event"");
    } catch (final CTPException e) {
      assertThat(e.getFault(), is(Fault.RESOURCE_NOT_FOUND));
    }
  }
",non-flaky,5
98448,ONSdigital_rm-collection-exercise-service,EventServiceTest.givenEventDoesNotExistWhenEventIsDeletedThenExceptionIsThrown,"  @Test
  public void givenEventDoesNotExistWhenEventIsDeletedThenExceptionIsThrown() {
    final UUID collexUuid = UUID.randomUUID();

    final CollectionExercise collex = new CollectionExercise();
    collex.setId(collexUuid);

    when(collectionExerciseService.findCollectionExercise(collexUuid)).thenReturn(collex);
    when(eventRepository.findOneByCollectionExerciseAndTag(collex, Tag.mps.name()))
        .thenReturn(null);

    try {
      eventService.deleteEvent(collexUuid, Tag.mps.name());

      Assert.fail(""Deleted non-existent event"");
    } catch (final CTPException e) {
      assertThat(e.getFault(), is(Fault.RESOURCE_NOT_FOUND));
    }
  }
",non-flaky,5
98449,ONSdigital_rm-collection-exercise-service,EventServiceTest.givenEventsForCollectionExerciseValidateWhenEventIsUpdatedItIsSaved,"  @Test
  public void givenEventsForCollectionExerciseValidateWhenEventIsUpdatedItIsSaved()
      throws CTPException {

    final CollectionExercise collex = new CollectionExercise();
    collex.setId(COLLEX_UUID);
    collex.setExercisePK(EXERCISE_PK);
    final CollectionExerciseState collectionExerciseState = CollectionExerciseState.SCHEDULED;
    collex.setState(collectionExerciseState);

    when(collectionExerciseService.findCollectionExercise(COLLEX_UUID)).thenReturn(collex);
    final Event existingEvent = new Event();
    when(eventRepository.findOneByCollectionExerciseAndTag(collex, Tag.mps.name()))
        .thenReturn(existingEvent);

    final List<Event> existingEvents = new ArrayList<>();

    when(eventRepository.findByCollectionExercise(collex)).thenReturn(existingEvents);
    eventValidators.add(eventValidator);

    eventService.updateEvent(COLLEX_UUID, Tag.mps.name(), new Date());

    verify(eventRepository, atLeastOnce()).save(eq(existingEvent));
  }
",non-flaky,5
98450,ONSdigital_rm-collection-exercise-service,EventServiceTest.givenEventsForCollectionExerciseValidateWhenEventIsDeletedItIsSaved,"  @Test
  public void givenEventsForCollectionExerciseValidateWhenEventIsDeletedItIsSaved()
      throws CTPException {

    final CollectionExercise collex = new CollectionExercise();
    collex.setId(COLLEX_UUID);
    collex.setExercisePK(EXERCISE_PK);
    final CollectionExerciseState collectionExerciseState = CollectionExerciseState.SCHEDULED;
    collex.setState(collectionExerciseState);

    when(collectionExerciseService.findCollectionExercise(COLLEX_UUID)).thenReturn(collex);
    final Event existingEvent = new Event();
    existingEvent.setTag(Tag.nudge_email_4.toString());
    existingEvent.setId(UUID.randomUUID());
    when(eventRepository.findOneByCollectionExerciseAndTag(collex, Tag.nudge_email_4.name()))
        .thenReturn(existingEvent);

    final List<Event> existingEvents = new ArrayList<>();

    eventValidators.add(eventValidator);

    eventService.deleteEvent(COLLEX_UUID, Tag.nudge_email_4.name());

    verify(eventRepository, atLeastOnce()).delete(eq(existingEvent));
  }
",non-flaky,5
98451,ONSdigital_rm-collection-exercise-service,EventServiceTest.givenReminderEmailIsDeletedItGetsPropagatedToActionSVC,"  @Test
  public void givenReminderEmailIsDeletedItGetsPropagatedToActionSVC() throws CTPException {

    final CollectionExercise collex = new CollectionExercise();
    collex.setId(COLLEX_UUID);
    collex.setExercisePK(EXERCISE_PK);
    final CollectionExerciseState collectionExerciseState = CollectionExerciseState.SCHEDULED;
    collex.setState(collectionExerciseState);

    when(collectionExerciseService.findCollectionExercise(COLLEX_UUID)).thenReturn(collex);
    final Event existingEvent = new Event();
    existingEvent.setTag(Tag.reminder.toString());
    existingEvent.setId(UUID.randomUUID());
    when(eventRepository.findOneByCollectionExerciseAndTag(collex, Tag.reminder.name()))
        .thenReturn(existingEvent);

    final List<Event> existingEvents = new ArrayList<>();

    eventValidators.add(eventValidator);

    eventService.deleteEvent(COLLEX_UUID, Tag.reminder.name());

    verify(eventRepository, atLeastOnce()).delete(eq(existingEvent));
  }
",non-flaky,5
98452,ONSdigital_rm-collection-exercise-service,EventServiceTest.givenSomeEventsWhenScheduledIsCheckedThenFalse,"  @Test
  public void givenSomeEventsWhenScheduledIsCheckedThenFalse() throws CTPException {
    UUID collexUuid = UUID.randomUUID();
    List<Event> events = createEventList(Tag.mps, Tag.exercise_end);
    when(eventRepository.findByCollectionExerciseId(collexUuid)).thenReturn(events);

    boolean scheduled = this.eventService.isScheduled(collexUuid);

    assertFalse(scheduled);
  }
",non-flaky,5
98453,ONSdigital_rm-collection-exercise-service,EventServiceTest.givenAllEventsWhenScheduledIsCheckedThenTrue,"  @Test
  public void givenAllEventsWhenScheduledIsCheckedThenTrue() throws CTPException {
    UUID collexUuid = UUID.randomUUID();
    List<Event> events = createEventList(Tag.values());
    when(eventRepository.findByCollectionExerciseId(collexUuid)).thenReturn(events);

    boolean scheduled = this.eventService.isScheduled(collexUuid);

    assertTrue(scheduled);
  }
",non-flaky,5
98454,ONSdigital_rm-collection-exercise-service,EventServiceTest.givenCollectionExerciseDoesNotExistWhenCreatingAnExceptionThrowError,"  @Test
  public void givenCollectionExerciseDoesNotExistWhenCreatingAnExceptionThrowError() {
    final String tag = Tag.mps.name();
    final EventDTO eventDto = new EventDTO();
    final UUID collexUuid = UUID.randomUUID();
    eventDto.setCollectionExerciseId(collexUuid);
    eventDto.setTag(tag);
    when(collectionExerciseService.findCollectionExercise(collexUuid)).thenReturn(null);
    try {
      eventService.createEvent(eventDto);
      fail(""Created event with non-existent collection exercise"");
    } catch (final CTPException e) {
      assertThat(e.getFault(), is(Fault.RESOURCE_NOT_FOUND));
    }
  }
",non-flaky,5
98455,ONSdigital_rm-collection-exercise-service,EventServiceTest.givenCollectionExerciseEventsAreInInvalidStateThrowException,"  @Test
  public void givenCollectionExerciseEventsAreInInvalidStateThrowException() {
    final String tag = Tag.mps.name();
    final EventDTO eventDto = new EventDTO();
    final CollectionExercise collex = new CollectionExercise();
    final UUID collexUuid = UUID.randomUUID();
    eventDto.setCollectionExerciseId(collexUuid);
    eventDto.setTag(tag);
    eventDto.setTimestamp(new Timestamp(Instant.now().toEpochMilli()));
    collex.setId(collexUuid);
    when(collectionExerciseService.findCollectionExercise(collexUuid)).thenReturn(collex);
    when(eventRepository.findOneByCollectionExerciseAndTag(collex, Tag.mps.name()))
        .thenReturn(null);
    final List<Event> existingEvents = new ArrayList<>();
    final Event event = new Event();
    existingEvents.add(event);
    when(eventRepository.findByCollectionExercise(collex)).thenReturn(existingEvents);
    eventValidators.add(eventValidator);
    try {
      eventService.createEvent(eventDto);
    } catch (final CTPException e) {
      assertThat(e.getFault(), is(Fault.BAD_REQUEST));
    }
  }
",non-flaky,5
98456,ONSdigital_rm-collection-exercise-service,EventServiceTest.testStatusIsSetToScheduledNewEventCreated,"  @Test
  public void testStatusIsSetToScheduledNewEventCreated() {
    final CollectionExercise collex = new CollectionExercise();
    String tag = Tag.mps.name();

    when(collectionExerciseService.findCollectionExercise(COLLEX_UUID)).thenReturn(collex);
    when(eventRepository.save(any(Event.class))).then(returnsFirstArg());

    EventDTO eventDto = new EventDTO();
    eventDto.setCollectionExerciseId(COLLEX_UUID);
    eventDto.setTag(tag);
    eventDto.setTimestamp(new Timestamp(new Date().getTime()));

    try {
      Event event = eventService.createEvent(eventDto);
      assertThat(event.getStatus(), is(EventDTO.Status.SCHEDULED));
    } catch (CTPException e) {
      fail();
    }
  }
",non-flaky,5
98457,ONSdigital_rm-collection-exercise-service,EventServiceTest.testProcessEventsNoScheduledEvents,"  @Test
  public void testProcessEventsNoScheduledEvents() {
    // Given
    List<Event> emptyList = Collections.emptyList();
    when(eventRepository.findByStatus(EventDTO.Status.SCHEDULED)).thenReturn(emptyList);

    // When
    eventService.processEvents();

    // Then
    verify(eventRepository, atMost(1)).findByStatus(EventDTO.Status.SCHEDULED);
    verify(actionSvcClient, never()).processEvent(any(), any());
  }
",non-flaky,5
98458,ONSdigital_rm-collection-exercise-service,EventServiceTest.testProcessEventsOnlyEventInFuture,"  @Test
  public void testProcessEventsOnlyEventInFuture() {
    // Given
    List<Event> list = new ArrayList<>();
    Event event = createEvent(Tag.mps, ""31/12/2999"");
    CollectionExercise collectionExercise = new CollectionExercise();
    collectionExercise.setState(CollectionExerciseState.LIVE);
    event.setCollectionExercise(collectionExercise);
    list.add(event);

    when(eventRepository.findByStatus(EventDTO.Status.SCHEDULED)).thenReturn(list);

    // When
    eventService.processEvents();

    // Then
    verify(eventRepository, atMost(1)).findByStatus(EventDTO.Status.SCHEDULED);
    verify(actionSvcClient, never()).processEvent(any(), any());
  }
",non-flaky,5
98459,ONSdigital_rm-collection-exercise-service,EventServiceTest.testProcessEventsTransitionGoLive,"  @Test
  public void testProcessEventsTransitionGoLive() {
    // Given
    List<Event> list = new ArrayList<>();
    Event event = createEvent(Tag.go_live);
    CollectionExercise collectionExercise = new CollectionExercise();
    collectionExercise.setState(CollectionExerciseState.LIVE);
    event.setCollectionExercise(collectionExercise);
    list.add(event);

    when(eventRepository.findByStatus(EventDTO.Status.SCHEDULED)).thenReturn(list);

    // When
    eventService.processEvents();

    // Then
    verify(eventRepository, atMost(1)).findByStatus(EventDTO.Status.SCHEDULED);
    verify(actionSvcClient, atMost(1)).processEvent(any(), any());
    try {
      verify(collectionExerciseService, atMost(1))
          .transitionCollectionExercise(
              any(CollectionExercise.class),
              any(CollectionExerciseDTO.CollectionExerciseEvent.class));
    } catch (CTPException e) {
      fail();
    }
  }
",non-flaky,5
98460,ONSdigital_rm-collection-exercise-service,EventServiceTest.testTagShouldHaveMpsAsIsActionable,"  @Test
  public void testTagShouldHaveMpsAsIsActionable() {
    assertThat(Tag.mps.isActionable(), Matchers.is(true));
  }
",non-flaky,5
98461,ONSdigital_rm-collection-exercise-service,EventServiceTest.testTagShouldHaveGoLiveAsIsAnActionableTag,"  @Test
  public void testTagShouldHaveGoLiveAsIsAnActionableTag() {
    assertThat(Tag.go_live.isActionable(), Matchers.is(true));
  }
",non-flaky,5
98462,ONSdigital_rm-collection-exercise-service,EventServiceTest.testTagShouldHaveExerciseEndAsNotIsAnActionableTag,"  @Test
  public void testTagShouldHaveExerciseEndAsNotIsAnActionableTag() {
    assertThat(Tag.exercise_end.isActionable(), Matchers.is(false));
  }
",non-flaky,5
98463,ONSdigital_rm-collection-exercise-service,EventServiceTest.testTagShouldHaveReminderAsAnActionableTag,"  @Test
  public void testTagShouldHaveReminderAsAnActionableTag() {
    assertThat(Tag.reminder.isActionable(), Matchers.is(true));
  }
",non-flaky,5
98464,ONSdigital_rm-collection-exercise-service,EventServiceTest.testTagShouldHaveReminder2AsAnActionableTag,"  @Test
  public void testTagShouldHaveReminder2AsAnActionableTag() {
    assertThat(Tag.reminder2.isActionable(), Matchers.is(true));
  }
",non-flaky,5
98465,ONSdigital_rm-collection-exercise-service,EventServiceTest.testTagShouldHaveReminder3AsAnActionableTag,"  @Test
  public void testTagShouldHaveReminder3AsAnActionableTag() {
    assertThat(Tag.reminder3.isActionable(), Matchers.is(true));
  }
",non-flaky,5
98466,ONSdigital_rm-collection-exercise-service,EventServiceTest.testTagShouldHaveNudge0AsAnActionableTag,"  @Test
  public void testTagShouldHaveNudge0AsAnActionableTag() {
    assertThat(Tag.nudge_email_0.isActionable(), Matchers.is(true));
  }
",non-flaky,5
98467,ONSdigital_rm-collection-exercise-service,EventServiceTest.testTagShouldHaveNudge1AsAnActionableTag,"  @Test
  public void testTagShouldHaveNudge1AsAnActionableTag() {
    assertThat(Tag.nudge_email_1.isActionable(), Matchers.is(true));
  }
",non-flaky,5
98468,ONSdigital_rm-collection-exercise-service,EventServiceTest.testTagShouldHaveNudge2AsAnActionableTag,"  @Test
  public void testTagShouldHaveNudge2AsAnActionableTag() {
    assertThat(Tag.nudge_email_2.isActionable(), Matchers.is(true));
  }
",non-flaky,5
104091,spring-cloud_spring-cloud-config,ServerNativeApplicationTests.contextLoads,"	@Test
	public void contextLoads() {
		// The remote config was bad so there is no bootstrap
		assertThat(this.environment.getPropertySources().contains(""bootstrap"")).isFalse();
	}
",non-flaky,5
104092,spring-cloud_spring-cloud-config,ApplicationBootstrapTests.contextLoads,"	@Test
	public void contextLoads() {
		Map res = new TestRestTemplate().getForObject(
				""http://localhost:"" + this.port + BASE_PATH + ""/env/info.foo"", Map.class);
		assertThat(res).containsKey(""propertySources"");
		Map<String, Object> property = (Map<String, Object>) res.get(""property"");
		assertThat(property).containsEntry(""value"", ""bar"");
	}
",non-flaky,5
104093,spring-cloud_spring-cloud-config,ApplicationTests.contextLoads,"	@Test
	public void contextLoads() {
		Map res = new TestRestTemplate().getForObject(
				""http://localhost:"" + this.port + BASE_PATH + ""/env/info.foo"", Map.class);
		assertThat(res).containsKey(""propertySources"");
		Map<String, Object> property = (Map<String, Object>) res.get(""property"");
		assertThat(property).containsEntry(""value"", ""bar"");
	}
",non-flaky,5
104094,spring-cloud_spring-cloud-config,ApplicationFailFastTests.contextFails,"	@Test
	public void contextFails() {
		try {
			new SpringApplicationBuilder().sources(Application.class).run(
					""--server.port=0"", ""--spring.cloud.config.enabled=true"",
					""--spring.cloud.config.fail-fast=true"",
					""--spring.cloud.config.uri=http://server-host-doesnt-exist:1234"");
			fail(""failFast option did not produce an exception"");
		}
		catch (Exception e) {
			assertThat(e.getMessage().contains(""fail fast""))
					.as(""Exception not caused by fail fast"").isTrue();
		}
	}
",non-flaky,5
104095,spring-cloud_spring-cloud-config,ConfigClientWatchTests.stateChangedWorks,"	@Test
	public void stateChangedWorks() {
		ConfigClientWatch watch = new ConfigClientWatch(null);
		assertThat(watch.stateChanged(null, ""1"")).isTrue();
		assertThat(watch.stateChanged(""1"", ""2"")).isTrue();
		assertThat(watch.stateChanged(""1"", null)).isTrue();
		assertThat(watch.stateChanged(""1"", ""1"")).isFalse();
		watch.close();
	}
",non-flaky,5
104096,spring-cloud_spring-cloud-config,ConfigServiceBootstrapConfigurationTest.overrideConfigServicePropertySourceLocatorWhenBeanIsProvided,"	@Test
	public void overrideConfigServicePropertySourceLocatorWhenBeanIsProvided() {
		TestPropertyValues.of(""spring.cloud.config.enabled=true"").applyTo(this.context);
		this.context.register(ConfigServicePropertySourceLocatorOverrideConfig.class);
		this.context.register(ConfigServiceBootstrapConfiguration.class);
		this.context.refresh();

		ConfigServicePropertySourceLocator locator = this.context
				.getBean(ConfigServicePropertySourceLocator.class);

		Field restTemplateField = ReflectionUtils
				.findField(ConfigServicePropertySourceLocator.class, ""restTemplate"");
		restTemplateField.setAccessible(true);

		RestTemplate restTemplate = (RestTemplate) ReflectionUtils
				.getField(restTemplateField, locator);

		assertThat(restTemplate).isNotNull();
	}
",non-flaky,5
104097,spring-cloud_spring-cloud-config,ConfigClientAutoConfigurationTests.sunnyDay,"	@Test
	public void sunnyDay() {
		AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(
				ConfigClientAutoConfiguration.class);
		assertThat(BeanFactoryUtils.beanNamesForTypeIncludingAncestors(context,
				ConfigClientProperties.class).length).isEqualTo(1);
		context.close();
	}
",non-flaky,5
104098,spring-cloud_spring-cloud-config,ConfigClientAutoConfigurationTests.withParent,"	@Test
	public void withParent() {
		ConfigurableApplicationContext context = new SpringApplicationBuilder(
				ConfigClientAutoConfiguration.class).child(Object.class)
						.web(WebApplicationType.NONE).run();
		assertThat(BeanFactoryUtils.beanNamesForTypeIncludingAncestors(context,
				ConfigClientProperties.class).length).isEqualTo(1);
		context.close();
	}
",non-flaky,5
104099,spring-cloud_spring-cloud-config,DiscoveryClientConfigServiceBootstrapConfigurationNoSpringRetryTests.shouldFailWithExceptionGetConfigServerInstanceFromDiscoveryClient,"	@Test
	public void shouldFailWithExceptionGetConfigServerInstanceFromDiscoveryClient()
			throws Exception {
		givenDiscoveryClientReturnsNoInfo();

		expectNoInstancesOfConfigServerException();

		setup(""spring.cloud.config.discovery.enabled=true"",
				""spring.cloud.config.fail-fast=true"");
	}
",non-flaky,5
104100,spring-cloud_spring-cloud-config,DiscoveryClientConfigServiceBootstrapConfigurationNoSpringRetryTests.shouldFailWithMessageGetConfigServerInstanceFromDiscoveryClient,"	@Test
	public void shouldFailWithMessageGetConfigServerInstanceFromDiscoveryClient()
			throws Exception {
		givenDiscoveryClientReturnsNoInfo();

		setup(""spring.cloud.config.discovery.enabled=true"",
				""spring.cloud.config.fail-fast=false"");

		expectDiscoveryClientConfigServiceBootstrapConfigurationIsSetup();
		expectConfigClientPropertiesHasDefaultConfiguration();
		verifyDiscoveryClientCalledOnce();
	}
",non-flaky,5
104101,spring-cloud_spring-cloud-config,DiscoveryClientConfigServiceBootstrapConfigurationNoSpringRetryTests.shouldSucceedGetConfigServerInstanceFromDiscoveryClient,"	@Test
	public void shouldSucceedGetConfigServerInstanceFromDiscoveryClient()
			throws Exception {
		givenDiscoveryClientReturnsInfo();

		setup(""spring.cloud.config.discovery.enabled=true"",
				""spring.cloud.config.fail-fast=true"");

		expectDiscoveryClientConfigServiceBootstrapConfigurationIsSetup();
		expectConfigClientPropertiesHasConfigurationFromEureka();
		verifyDiscoveryClientCalledOnce();
	}
",non-flaky,5
104102,spring-cloud_spring-cloud-config,DiscoveryClientConfigServiceBootstrapConfigurationTests.offByDefault,"	@Test
	public void offByDefault() throws Exception {
		this.context = new AnnotationConfigApplicationContext(
				DiscoveryClientConfigServiceBootstrapConfiguration.class);

		assertThat(this.context.getBeanNamesForType(DiscoveryClient.class).length)
				.isEqualTo(0);
		assertThat(this.context.getBeanNamesForType(
				DiscoveryClientConfigServiceBootstrapConfiguration.class).length)
						.isEqualTo(0);
	}
",non-flaky,5
104103,spring-cloud_spring-cloud-config,DiscoveryClientConfigServiceBootstrapConfigurationTests.onWhenRequested,"	@Test
	public void onWhenRequested() throws Exception {
		givenDiscoveryClientReturnsInfo();

		setup(""spring.cloud.config.discovery.enabled=true"");

		expectDiscoveryClientConfigServiceBootstrapConfigurationIsSetup();
		verifyDiscoveryClientCalledOnce();
		expectConfigClientPropertiesHasConfigurationFromEureka();
	}
",non-flaky,5
104104,spring-cloud_spring-cloud-config,DiscoveryClientConfigServiceBootstrapConfigurationTests.onWhenHeartbeat,"	@Test
	public void onWhenHeartbeat() throws Exception {
		setup(""spring.cloud.config.discovery.enabled=true"");

		expectDiscoveryClientConfigServiceBootstrapConfigurationIsSetup();

		givenDiscoveryClientReturnsInfo();
		verifyDiscoveryClientCalledOnce();

		this.context.publishEvent(new HeartbeatEvent(this.context, ""new""));

		expectConfigClientPropertiesHasConfigurationFromEureka();
	}
",non-flaky,5
104105,spring-cloud_spring-cloud-config,DiscoveryClientConfigServiceBootstrapConfigurationTests.secureWhenRequested,"	@Test
	public void secureWhenRequested() throws Exception {
		this.info = new DefaultServiceInstance(""app"", ""foo"", 443, true);
		givenDiscoveryClientReturnsInfo();

		setup(""spring.cloud.config.discovery.enabled=true"");

		expectDiscoveryClientConfigServiceBootstrapConfigurationIsSetup();

		verifyDiscoveryClientCalledOnce();
		expectConfigClientPropertiesHasConfiguration(""https://foo:443/"");
	}
",non-flaky,5
104106,spring-cloud_spring-cloud-config,DiscoveryClientConfigServiceBootstrapConfigurationTests.multipleInstancesReturnedFromDiscovery,"	@Test
	public void multipleInstancesReturnedFromDiscovery() {
		ServiceInstance info1 = new DefaultServiceInstance(""app"", ""localhost"", 8888,
				true);
		ServiceInstance info2 = new DefaultServiceInstance(""app"", ""localhost1"", 8888,
				false);
		givenDiscoveryClientReturnsInfoForMultipleInstances(info1, info2);

		setup(""spring.cloud.config.discovery.enabled=true"");

		expectDiscoveryClientConfigServiceBootstrapConfigurationIsSetup();

		verifyDiscoveryClientCalledOnce();
		expectConfigClientPropertiesHasMultipleUris(""https://localhost:8888/"",
				""http://localhost1:8888/"");

	}
",non-flaky,5
104107,spring-cloud_spring-cloud-config,DiscoveryClientConfigServiceBootstrapConfigurationTests.setsPasssword,"	@Test
	public void setsPasssword() throws Exception {
		this.info.getMetadata().put(""password"", ""bar"");
		givenDiscoveryClientReturnsInfo();

		setup(""spring.cloud.config.discovery.enabled=true"");

		ConfigClientProperties locator = this.context
				.getBean(ConfigClientProperties.class);
		Credentials credentials = locator.getCredentials(0);
		assertThat(credentials.getUri()).isEqualTo(""http://foo:8877/"");
		assertThat(credentials.getPassword()).isEqualTo(""bar"");
		assertThat(credentials.getUsername()).isEqualTo(""user"");
	}
",non-flaky,5
104108,spring-cloud_spring-cloud-config,DiscoveryClientConfigServiceBootstrapConfigurationTests.setsPath,"	@Test
	public void setsPath() throws Exception {
		this.info.getMetadata().put(""configPath"", ""/bar"");
		givenDiscoveryClientReturnsInfo();

		setup(""spring.cloud.config.discovery.enabled=true"");

		expectConfigClientPropertiesHasConfiguration(""http://foo:8877/bar"");
	}
",non-flaky,5
104109,spring-cloud_spring-cloud-config,DiscoveryClientConfigServiceBootstrapConfigurationTests.shouldFailGetConfigServerInstanceFromDiscoveryClient,"	@Test
	public void shouldFailGetConfigServerInstanceFromDiscoveryClient() throws Exception {
		givenDiscoveryClientReturnsNoInfo();

		setup(""spring.cloud.config.discovery.enabled=true"");

		expectDiscoveryClientConfigServiceBootstrapConfigurationIsSetup();
		verifyDiscoveryClientCalledOnce();
		expectConfigClientPropertiesHasDefaultConfiguration();
	}
",non-flaky,5
104110,spring-cloud_spring-cloud-config,DiscoveryClientConfigServiceBootstrapConfigurationTests.shouldRetryAndSucceedGetConfigServerInstanceFromDiscoveryClient,"	@Test
	public void shouldRetryAndSucceedGetConfigServerInstanceFromDiscoveryClient()
			throws Exception {
		givenDiscoveryClientReturnsInfoOnThirdTry();

		setup(""spring.cloud.config.discovery.enabled=true"",
				""spring.cloud.config.retry.maxAttempts=3"",
				""spring.cloud.config.retry.initialInterval=10"",
				""spring.cloud.config.fail-fast=true"");

		expectDiscoveryClientConfigServiceBootstrapConfigurationIsSetup();
		verifyDiscoveryClientCalledThreeTimes();

		this.context.publishEvent(new HeartbeatEvent(this.context, ""new""));

		expectConfigClientPropertiesHasConfigurationFromEureka();
	}
",non-flaky,5
104111,spring-cloud_spring-cloud-config,DiscoveryClientConfigServiceBootstrapConfigurationTests.shouldNotRetryIfNotFailFastPropertySet,"	@Test
	public void shouldNotRetryIfNotFailFastPropertySet() throws Exception {
		givenDiscoveryClientReturnsInfoOnThirdTry();

		setup(""spring.cloud.config.discovery.enabled=true"",
				""spring.cloud.config.retry.maxAttempts=3"",
				""spring.cloud.config.retry.initialInterval=10"");

		expectDiscoveryClientConfigServiceBootstrapConfigurationIsSetup();
		verifyDiscoveryClientCalledOnce();
		expectConfigClientPropertiesHasDefaultConfiguration();
	}
",non-flaky,5
104112,spring-cloud_spring-cloud-config,DiscoveryClientConfigServiceBootstrapConfigurationTests.shouldRetryAndFailWithExceptionGetConfigServerInstanceFromDiscoveryClient,"	@Test
	public void shouldRetryAndFailWithExceptionGetConfigServerInstanceFromDiscoveryClient()
			throws Exception {
		givenDiscoveryClientReturnsNoInfo();

		expectNoInstancesOfConfigServerException();

		setup(""spring.cloud.config.discovery.enabled=true"",
				""spring.cloud.config.retry.maxAttempts=3"",
				""spring.cloud.config.retry.initialInterval=10"",
				""spring.cloud.config.fail-fast=true"");
	}
",non-flaky,5
104113,spring-cloud_spring-cloud-config,DiscoveryClientConfigServiceBootstrapConfigurationTests.shouldRetryAndFailWithMessageGetConfigServerInstanceFromDiscoveryClient,"	@Test
	public void shouldRetryAndFailWithMessageGetConfigServerInstanceFromDiscoveryClient()
			throws Exception {
		givenDiscoveryClientReturnsNoInfo();

		setup(""spring.cloud.config.discovery.enabled=true"",
				""spring.cloud.config.retry.maxAttempts=3"",
				""spring.cloud.config.retry.initialInterval=10"",
				""spring.cloud.config.fail-fast=false"");

		expectDiscoveryClientConfigServiceBootstrapConfigurationIsSetup();
		expectConfigClientPropertiesHasDefaultConfiguration();
	}
",non-flaky,5
104114,spring-cloud_spring-cloud-config,ConfigServerHealthIndicatorTests.testDefaultStatus,"	@Test
	public void testDefaultStatus() {
		// UNKNOWN is better than DOWN since it doesn't stop the app from working
		assertThat(this.indicator.health().getStatus()).isEqualTo(Status.UNKNOWN);
	}
",non-flaky,5
104115,spring-cloud_spring-cloud-config,ConfigServerHealthIndicatorTests.testExceptionStatus,"	@Test
	public void testExceptionStatus() {
		doThrow(new IllegalStateException()).when(this.locator)
				.locate(any(Environment.class));
		assertThat(this.indicator.health().getStatus()).isEqualTo(Status.DOWN);
		verify(this.locator, times(1)).locate(any(Environment.class));
	}
",non-flaky,5
104116,spring-cloud_spring-cloud-config,ConfigServerHealthIndicatorTests.testServerUp,"	@Test
	public void testServerUp() {
		PropertySource<?> source = new MapPropertySource(""foo"",
				Collections.<String, Object>emptyMap());
		doReturn(source).when(this.locator).locate(any(Environment.class));
		assertThat(this.indicator.health().getStatus()).isEqualTo(Status.UP);
		verify(this.locator, times(1)).locate(any(Environment.class));
	}
",non-flaky,5
104117,spring-cloud_spring-cloud-config,ConfigServerHealthIndicatorTests.healthIsCached,"	@Test
	public void healthIsCached() {
		PropertySource<?> source = new MapPropertySource(""foo"",
				Collections.<String, Object>emptyMap());
		doReturn(source).when(this.locator).locate(any(Environment.class));

		// not cached
		assertThat(this.indicator.health().getStatus()).isEqualTo(Status.UP);

		// cached
		assertThat(this.indicator.health().getStatus()).isEqualTo(Status.UP);

		verify(this.locator, times(1)).locate(any(Environment.class));
	}
",non-flaky,5
104118,spring-cloud_spring-cloud-config,ConfigServicePropertySourceLocatorTests.sunnyDay,"	@Test
	public void sunnyDay() {
		Environment body = new Environment(""app"", ""master"");
		mockRequestResponseWithoutLabel(new ResponseEntity<>(body, HttpStatus.OK));
		this.locator.setRestTemplate(this.restTemplate);

		ArgumentCaptor<HttpEntity> argumentCaptor = ArgumentCaptor
				.forClass(HttpEntity.class);

		assertThat(this.locator.locate(this.environment)).isNotNull();

		Mockito.verify(this.restTemplate).exchange(anyString(), any(HttpMethod.class),
				argumentCaptor.capture(), any(Class.class), anyString(), anyString());

		HttpEntity httpEntity = argumentCaptor.getValue();
		assertThat(httpEntity.getHeaders().getAccept())
				.containsExactly(MediaType.APPLICATION_JSON);
	}
",non-flaky,5
104119,spring-cloud_spring-cloud-config,ConfigServicePropertySourceLocatorTests.sunnyDayWithLabel,"	@Test
	public void sunnyDayWithLabel() {
		Environment body = new Environment(""app"", ""master"");
		mockRequestResponseWithLabel(new ResponseEntity<>(body, HttpStatus.OK), ""v1.0.0"");
		this.locator.setRestTemplate(this.restTemplate);
		TestPropertyValues.of(""spring.cloud.config.label:v1.0.0"")
				.applyTo(this.environment);
		assertThat(this.locator.locate(this.environment)).isNotNull();
	}
",non-flaky,5
104120,spring-cloud_spring-cloud-config,ConfigServicePropertySourceLocatorTests.sunnyDayWithLabelThatContainsASlash,"	@Test
	public void sunnyDayWithLabelThatContainsASlash() {
		Environment body = new Environment(""app"", ""master"");
		mockRequestResponseWithLabel(new ResponseEntity<>(body, HttpStatus.OK),
				""release(_)v1.0.0"");
		this.locator.setRestTemplate(this.restTemplate);
		TestPropertyValues.of(""spring.cloud.config.label:release/v1.0.0"")
				.applyTo(this.environment);
		assertThat(this.locator.locate(this.environment)).isNotNull();
	}
",non-flaky,5
104121,spring-cloud_spring-cloud-config,ConfigServicePropertySourceLocatorTests.sunnyDayWithNoSuchLabel,"	@Test
	public void sunnyDayWithNoSuchLabel() {
		mockRequestResponseWithLabel(
				new ResponseEntity<Void>((Void) null, HttpStatus.NOT_FOUND),
				""nosuchlabel"");
		this.locator.setRestTemplate(this.restTemplate);
		assertThat(this.locator.locate(this.environment)).isNull();
	}
",non-flaky,5
104122,spring-cloud_spring-cloud-config,ConfigServicePropertySourceLocatorTests.failsQuietly,"	@Test
	public void failsQuietly() {
		mockRequestResponseWithoutLabel(
				new ResponseEntity<>(""Wah!"", HttpStatus.INTERNAL_SERVER_ERROR));
		this.locator.setRestTemplate(this.restTemplate);
		assertThat(this.locator.locate(this.environment)).isNull();
	}
",non-flaky,5
104123,spring-cloud_spring-cloud-config,ConfigServicePropertySourceLocatorTests.failFast,"	@Test
	public void failFast() throws Exception {
		ClientHttpRequestFactory requestFactory = Mockito
				.mock(ClientHttpRequestFactory.class);
		ClientHttpRequest request = Mockito.mock(ClientHttpRequest.class);
		ClientHttpResponse response = Mockito.mock(ClientHttpResponse.class);
		Mockito.when(requestFactory.createRequest(Mockito.any(URI.class),
				Mockito.any(HttpMethod.class))).thenReturn(request);
		RestTemplate restTemplate = new RestTemplate(requestFactory);
		ConfigClientProperties defaults = new ConfigClientProperties(this.environment);
		defaults.setFailFast(true);
		this.locator = new ConfigServicePropertySourceLocator(defaults);
		Mockito.when(request.getHeaders()).thenReturn(new HttpHeaders());
		Mockito.when(request.execute()).thenReturn(response);
		HttpHeaders headers = new HttpHeaders();
		headers.setContentType(MediaType.APPLICATION_JSON);
		Mockito.when(response.getHeaders()).thenReturn(headers);
		Mockito.when(response.getStatusCode())
				.thenReturn(HttpStatus.INTERNAL_SERVER_ERROR);
		Mockito.when(response.getBody())
				.thenReturn(new ByteArrayInputStream(""{}"".getBytes()));
		this.locator.setRestTemplate(restTemplate);
		this.expected
				.expectCause(IsInstanceOf.instanceOf(IllegalArgumentException.class));
		this.expected.expectMessage(""fail fast property is set"");
		this.locator.locate(this.environment);
	}
",non-flaky,5
104124,spring-cloud_spring-cloud-config,ConfigServicePropertySourceLocatorTests.failFastWhenNotFound,"	@Test
	public void failFastWhenNotFound() throws Exception {
		ClientHttpRequestFactory requestFactory = Mockito
				.mock(ClientHttpRequestFactory.class);
		ClientHttpRequest request = Mockito.mock(ClientHttpRequest.class);
		ClientHttpResponse response = Mockito.mock(ClientHttpResponse.class);
		Mockito.when(requestFactory.createRequest(Mockito.any(URI.class),
				Mockito.any(HttpMethod.class))).thenReturn(request);
		RestTemplate restTemplate = new RestTemplate(requestFactory);
		ConfigClientProperties defaults = new ConfigClientProperties(this.environment);
		defaults.setFailFast(true);
		this.locator = new ConfigServicePropertySourceLocator(defaults);
		Mockito.when(request.getHeaders()).thenReturn(new HttpHeaders());
		Mockito.when(request.execute()).thenReturn(response);
		HttpHeaders headers = new HttpHeaders();
		headers.setContentType(MediaType.APPLICATION_JSON);
		Mockito.when(response.getHeaders()).thenReturn(headers);
		Mockito.when(response.getStatusCode()).thenReturn(HttpStatus.NOT_FOUND);
		Mockito.when(response.getBody())
				.thenReturn(new ByteArrayInputStream("""".getBytes()));
		this.locator.setRestTemplate(restTemplate);
		this.expected
				.expectCause(IsInstanceOf.instanceOf(IllegalArgumentException.class));
		this.expected.expectMessage(""fail fast property is set"");
		this.locator.locate(this.environment);
	}
",non-flaky,5
104125,spring-cloud_spring-cloud-config,ConfigServicePropertySourceLocatorTests.failFastWhenBothPasswordAndAuthorizationPropertiesSet,"	@Test
	public void failFastWhenBothPasswordAndAuthorizationPropertiesSet() throws Exception {
		ClientHttpRequestFactory requestFactory = Mockito
				.mock(ClientHttpRequestFactory.class);
		ClientHttpRequest request = Mockito.mock(ClientHttpRequest.class);
		Mockito.when(requestFactory.createRequest(Mockito.any(URI.class),
				Mockito.any(HttpMethod.class))).thenReturn(request);
		ConfigClientProperties defaults = new ConfigClientProperties(this.environment);
		defaults.setFailFast(true);
		defaults.setUsername(""username"");
		defaults.setPassword(""password"");
		defaults.getHeaders().put(AUTHORIZATION, ""Basic dXNlcm5hbWU6cGFzc3dvcmQNCg=="");
		this.locator = new ConfigServicePropertySourceLocator(defaults);
		this.expected.expect(IllegalStateException.class);
		this.expected.expectMessage(
				""Could not locate PropertySource and the fail fast property is set, failing"");
		this.locator.locate(this.environment);
	}
",non-flaky,5
104126,spring-cloud_spring-cloud-config,ConfigServicePropertySourceLocatorTests.interceptorShouldAddHeadersWhenHeadersPropertySet,"	@Test
	public void interceptorShouldAddHeadersWhenHeadersPropertySet() throws Exception {
		MockClientHttpRequest request = new MockClientHttpRequest();
		ClientHttpRequestExecution execution = Mockito
				.mock(ClientHttpRequestExecution.class);
		byte[] body = new byte[] {};
		Map<String, String> headers = new HashMap<>();
		headers.put(""X-Example-Version"", ""2.1"");
		new ConfigServicePropertySourceLocator.GenericRequestHeaderInterceptor(headers)
				.intercept(request, body, execution);
		Mockito.verify(execution).execute(request, body);
		assertThat(request.getHeaders().getFirst(""X-Example-Version"")).isEqualTo(""2.1"");
	}
",non-flaky,5
104127,spring-cloud_spring-cloud-config,ConfigServicePropertySourceLocatorTests.shouldAddAuthorizationHeaderWhenPasswordSet,"	@Test
	public void shouldAddAuthorizationHeaderWhenPasswordSet() {
		HttpHeaders headers = new HttpHeaders();
		ConfigClientProperties defaults = new ConfigClientProperties(this.environment);
		this.locator = new ConfigServicePropertySourceLocator(defaults);
		String username = ""user"";
		String password = ""pass"";
		ReflectionTestUtils.invokeMethod(this.locator, ""addAuthorizationToken"", defaults,
				headers, username, password);
		assertThat(headers).hasSize(1);
	}
",non-flaky,5
104128,spring-cloud_spring-cloud-config,ConfigServicePropertySourceLocatorTests.shouldAddAuthorizationHeaderWhenAuthorizationSet,"	@Test
	public void shouldAddAuthorizationHeaderWhenAuthorizationSet() {
		HttpHeaders headers = new HttpHeaders();
		ConfigClientProperties defaults = new ConfigClientProperties(this.environment);
		defaults.getHeaders().put(AUTHORIZATION, ""Basic dXNlcm5hbWU6cGFzc3dvcmQNCg=="");
		this.locator = new ConfigServicePropertySourceLocator(defaults);
		String username = ""user"";
		String password = null;
		ReflectionTestUtils.invokeMethod(this.locator, ""addAuthorizationToken"", defaults,
				headers, username, password);
		assertThat(headers).hasSize(1);
	}
",non-flaky,5
104129,spring-cloud_spring-cloud-config,ConfigServicePropertySourceLocatorTests.shouldThrowExceptionWhenPasswordAndAuthorizationBothSet,"	@Test
	public void shouldThrowExceptionWhenPasswordAndAuthorizationBothSet() {
		HttpHeaders headers = new HttpHeaders();
		ConfigClientProperties defaults = new ConfigClientProperties(this.environment);
		defaults.getHeaders().put(AUTHORIZATION, ""Basic dXNlcm5hbWU6cGFzc3dvcmQNCg=="");
		this.locator = new ConfigServicePropertySourceLocator(defaults);
		String username = ""user"";
		String password = ""pass"";
		this.expected.expect(IllegalStateException.class);
		this.expected.expectMessage(""You must set either 'password' or 'authorization'"");
		ReflectionTestUtils.invokeMethod(this.locator, ""addAuthorizationToken"", defaults,
				headers, username, password);
	}
",non-flaky,5
104130,spring-cloud_spring-cloud-config,ConfigServicePropertySourceLocatorTests.shouldThrowExceptionWhenNegativeReadTimeoutSet,"	@Test
	public void shouldThrowExceptionWhenNegativeReadTimeoutSet() {
		ConfigClientProperties defaults = new ConfigClientProperties(this.environment);
		defaults.setRequestReadTimeout(-1);
		this.locator = new ConfigServicePropertySourceLocator(defaults);
		this.expected.expect(IllegalStateException.class);
		this.expected.expectMessage(""Invalid Value for Read Timeout set."");
		ReflectionTestUtils.invokeMethod(this.locator, ""getSecureRestTemplate"", defaults);
	}
",non-flaky,5
104131,spring-cloud_spring-cloud-config,ConfigServicePropertySourceLocatorTests.shouldThrowExceptionWhenNegativeConnectTimeoutSet,"	@Test
	public void shouldThrowExceptionWhenNegativeConnectTimeoutSet() {
		ConfigClientProperties defaults = new ConfigClientProperties(this.environment);
		defaults.setRequestConnectTimeout(-1);
		this.locator = new ConfigServicePropertySourceLocator(defaults);
		this.expected.expect(IllegalStateException.class);
		this.expected.expectMessage(""Invalid Value for Connect Timeout set."");
		ReflectionTestUtils.invokeMethod(this.locator, ""getSecureRestTemplate"", defaults);
	}
",non-flaky,5
104132,spring-cloud_spring-cloud-config,ConfigServicePropertySourceLocatorTests.checkInterceptorHasNoAuthorizationHeaderPresent,"	@Test
	public void checkInterceptorHasNoAuthorizationHeaderPresent() {
		ConfigClientProperties defaults = new ConfigClientProperties(this.environment);
		defaults.getHeaders().put(AUTHORIZATION, ""Basic dXNlcm5hbWU6cGFzc3dvcmQNCg=="");
		defaults.getHeaders().put(""key"", ""value"");
		this.locator = new ConfigServicePropertySourceLocator(defaults);
		RestTemplate restTemplate = ReflectionTestUtils.invokeMethod(this.locator,
				""getSecureRestTemplate"", defaults);
		Iterator<ClientHttpRequestInterceptor> iterator = restTemplate.getInterceptors()
				.iterator();
		while (iterator.hasNext()) {
			GenericRequestHeaderInterceptor genericRequestHeaderInterceptor = (GenericRequestHeaderInterceptor) iterator
					.next();
			assertThat(genericRequestHeaderInterceptor.getHeaders().get(AUTHORIZATION))
					.isEqualTo(null);
		}
	}
",non-flaky,5
104133,spring-cloud_spring-cloud-config,ConfigClientPropertiesTests.vanilla,"	@Test
	public void vanilla() {
		this.locator.setUri(new String[] { ""http://localhost:9999"" });
		this.locator.setPassword(""secret"");
		Credentials credentials = this.locator.getCredentials(0);
		assertThat(credentials.getUri()).isEqualTo(""http://localhost:9999"");
		assertThat(credentials.getUsername()).isEqualTo(""user"");
		assertThat(credentials.getPassword()).isEqualTo(""secret"");
	}
",non-flaky,5
104134,spring-cloud_spring-cloud-config,ConfigClientPropertiesTests.uriCreds,"	@Test
	public void uriCreds() {
		this.locator.setUri(new String[] { ""http://foo:bar@localhost:9999"" });
		Credentials credentials = this.locator.getCredentials(0);
		assertThat(credentials.getUri()).isEqualTo(""http://localhost:9999"");
		assertThat(credentials.getUsername()).isEqualTo(""foo"");
		assertThat(credentials.getPassword()).isEqualTo(""bar"");
	}
",non-flaky,5
104135,spring-cloud_spring-cloud-config,ConfigClientPropertiesTests.explicitPassword,"	@Test
	public void explicitPassword() {
		this.locator.setUri(new String[] { ""http://foo:bar@localhost:9999"" });
		this.locator.setPassword(""secret"");
		Credentials credentials = this.locator.getCredentials(0);
		assertThat(credentials.getUri()).isEqualTo(""http://localhost:9999"");
		assertThat(credentials.getUsername()).isEqualTo(""foo"");
		assertThat(credentials.getPassword()).isEqualTo(""secret"");
	}
",non-flaky,5
104136,spring-cloud_spring-cloud-config,ConfigClientPropertiesTests.testIfNoColonPresentInUriCreds,"	@Test
	public void testIfNoColonPresentInUriCreds() {
		this.locator.setUri(new String[] { ""http://foobar@localhost:9999"" });
		this.locator.setPassword(""secret"");
		Credentials credentials = this.locator.getCredentials(0);
		assertThat(credentials.getUri()).isEqualTo(""http://localhost:9999"");
		assertThat(credentials.getUsername()).isEqualTo(""foobar"");
		assertThat(credentials.getPassword()).isEqualTo(""secret"");
	}
",non-flaky,5
104137,spring-cloud_spring-cloud-config,ConfigClientPropertiesTests.testIfColonPresentAtTheEndInUriCreds,"	@Test
	public void testIfColonPresentAtTheEndInUriCreds() {
		this.locator.setUri(new String[] { ""http://foobar:@localhost:9999"" });
		this.locator.setPassword(""secret"");
		Credentials credentials = this.locator.getCredentials(0);
		assertThat(credentials.getUri()).isEqualTo(""http://localhost:9999"");
		assertThat(credentials.getUsername()).isEqualTo(""foobar"");
		assertThat(credentials.getPassword()).isEqualTo(""secret"");
	}
",non-flaky,5
104138,spring-cloud_spring-cloud-config,ConfigClientPropertiesTests.testIfColonPresentAtTheStartInUriCreds,"	@Test
	public void testIfColonPresentAtTheStartInUriCreds() {
		this.locator.setUri(new String[] { ""http://:foobar@localhost:9999"" });
		Credentials credentials = this.locator.getCredentials(0);
		assertThat(credentials.getUri()).isEqualTo(""http://localhost:9999"");
		assertThat(credentials.getUsername()).isEqualTo("""");
		assertThat(credentials.getPassword()).isEqualTo(""foobar"");
	}
",non-flaky,5
104139,spring-cloud_spring-cloud-config,ConfigClientPropertiesTests.testIfColonPresentAtTheStartAndEndInUriCreds,"	@Test
	public void testIfColonPresentAtTheStartAndEndInUriCreds() {
		this.locator.setUri(new String[] { ""http://:foobar:@localhost:9999"" });
		Credentials credentials = this.locator.getCredentials(0);
		assertThat(credentials.getUri()).isEqualTo(""http://localhost:9999"");
		assertThat(credentials.getUsername()).isEqualTo("""");
		assertThat(credentials.getPassword()).isEqualTo(""foobar:"");
	}
",non-flaky,5
104140,spring-cloud_spring-cloud-config,ConfigClientPropertiesTests.testIfSpacePresentAsUriCreds,"	@Test
	public void testIfSpacePresentAsUriCreds() {
		this.locator.setUri(new String[] { ""http://  @localhost:9999"" });
		this.locator.setPassword(""secret"");
		Credentials credentials = this.locator.getCredentials(0);
		assertThat(credentials.getUri()).isEqualTo(""http://localhost:9999"");
		assertThat(credentials.getUsername()).isEqualTo(""  "");
		assertThat(credentials.getPassword()).isEqualTo(""secret"");
	}
",non-flaky,5
104141,spring-cloud_spring-cloud-config,ConfigClientPropertiesTests.changeNameInOverride,"	@Test
	public void changeNameInOverride() {
		this.locator.setName(""one"");
		ConfigurableEnvironment environment = new StandardEnvironment();
		TestPropertyValues.of(""spring.application.name:two"").applyTo(environment);
		ConfigClientProperties override = this.locator.override(environment);
		assertThat(override.getName()).isEqualTo(""two"");
	}
",non-flaky,5
104142,spring-cloud_spring-cloud-config,ConfigClientPropertiesTests.testThatExplicitUsernamePasswordTakePrecedence,"	@Test
	public void testThatExplicitUsernamePasswordTakePrecedence() {
		ConfigClientProperties properties = new ConfigClientProperties(
				new MockEnvironment());

		properties.setUri(
				new String[] { ""https://userInfoName:userInfoPW@localhost:8888/"" });
		properties.setUsername(""explicitName"");
		properties.setPassword(""explicitPW"");
		Credentials credentials = properties.getCredentials(0);
		assertThat(credentials.getPassword()).isEqualTo(""explicitPW"");
		assertThat(credentials.getUsername()).isEqualTo(""explicitName"");
	}
",non-flaky,5
104143,spring-cloud_spring-cloud-config,ConfigClientPropertiesTests.checkIfExceptionThrownForNegativeIndex,"	@Test
	public void checkIfExceptionThrownForNegativeIndex() {
		this.locator.setUri(
				new String[] { ""http://localhost:8888"", ""http://localhost:8889"" });
		this.expected.expect(IllegalStateException.class);
		this.expected.expectMessage(""Trying to access an invalid array index"");
		Credentials credentials = this.locator.getCredentials(-1);
	}
",non-flaky,5
104144,spring-cloud_spring-cloud-config,ConfigClientPropertiesTests.checkIfExceptionThrownForPositiveInvalidIndex,"	@Test
	public void checkIfExceptionThrownForPositiveInvalidIndex() {
		this.locator.setUri(
				new String[] { ""http://localhost:8888"", ""http://localhost:8889"" });
		this.expected.expect(IllegalStateException.class);
		this.expected.expectMessage(""Trying to access an invalid array index"");
		Credentials credentials = this.locator.getCredentials(3);
	}
",non-flaky,5
104145,spring-cloud_spring-cloud-config,ConfigClientPropertiesTests.checkIfExceptionThrownForIndexEqualToLength,"	@Test
	public void checkIfExceptionThrownForIndexEqualToLength() {
		this.locator.setUri(
				new String[] { ""http://localhost:8888"", ""http://localhost:8889"" });
		this.expected.expect(IllegalStateException.class);
		this.expected.expectMessage(""Trying to access an invalid array index"");
		Credentials credentials = this.locator.getCredentials(2);
	}
",non-flaky,5
104146,spring-cloud_spring-cloud-config,ConfigServerBootstrapConfigurationTests.withHealthIndicator,"	@Test
	public void withHealthIndicator() {
		ConfigurableApplicationContext context = new SpringApplicationBuilder(
				PropertySourceBootstrapConfiguration.class,
				ConfigServiceBootstrapConfiguration.class)
						.child(ConfigClientAutoConfiguration.class)
						.web(WebApplicationType.NONE).run();
		assertThat(BeanFactoryUtils.beanNamesForTypeIncludingAncestors(context,
				ConfigClientProperties.class).length).isEqualTo(1);
		assertThat(BeanFactoryUtils.beanNamesForTypeIncludingAncestors(context,
				ConfigServerHealthIndicator.class).length).isEqualTo(1);
		context.close();
	}
",non-flaky,5
104147,spring-cloud_spring-cloud-config,EncryptionControllerTests.cannotDecryptWithoutKey,"	@Test(expected = EncryptionTooWeakException.class)
	public void cannotDecryptWithoutKey() {
		this.controller.decrypt(""foo"", MediaType.TEXT_PLAIN);
	}
",non-flaky,5
104148,spring-cloud_spring-cloud-config,EncryptionControllerTests.cannotDecryptWithNoopEncryptor,"	@Test(expected = EncryptionTooWeakException.class)
	public void cannotDecryptWithNoopEncryptor() {
		this.controller.decrypt(""foo"", MediaType.TEXT_PLAIN);
	}
",non-flaky,5
104149,spring-cloud_spring-cloud-config,EncryptionControllerTests.shouldThrowExceptionOnDecryptInvalidData,"	@Test(expected = InvalidCipherException.class)
	public void shouldThrowExceptionOnDecryptInvalidData() {
		this.controller = new EncryptionController(
				new SingleTextEncryptorLocator(new RsaSecretEncryptor()));
		this.controller.decrypt(""foo"", MediaType.TEXT_PLAIN);
	}
",non-flaky,5
104150,spring-cloud_spring-cloud-config,EncryptionControllerTests.shouldThrowExceptionOnDecryptWrongKey,"	@Test(expected = InvalidCipherException.class)
	public void shouldThrowExceptionOnDecryptWrongKey() {
		RsaSecretEncryptor encryptor = new RsaSecretEncryptor();
		this.controller = new EncryptionController(
				new SingleTextEncryptorLocator(new RsaSecretEncryptor()));
		this.controller.decrypt(encryptor.encrypt(""foo""), MediaType.TEXT_PLAIN);
	}
",non-flaky,5
104151,spring-cloud_spring-cloud-config,EncryptionControllerTests.sunnyDayRsaKey,"	@Test
	public void sunnyDayRsaKey() {
		this.controller = new EncryptionController(
				new SingleTextEncryptorLocator(new RsaSecretEncryptor()));
		String cipher = this.controller.encrypt(""foo"", MediaType.TEXT_PLAIN);
		assertThat(this.controller.decrypt(cipher, MediaType.TEXT_PLAIN))
				.isEqualTo(""foo"");
	}
",non-flaky,5
104152,spring-cloud_spring-cloud-config,EncryptionControllerTests.publicKey,"	@Test
	public void publicKey() {
		this.controller = new EncryptionController(
				new SingleTextEncryptorLocator(new RsaSecretEncryptor()));
		String key = this.controller.getPublicKey();
		assertThat(key.startsWith(""ssh-rsa"")).as(""Wrong key format: "" + key).isTrue();
	}
",non-flaky,5
104153,spring-cloud_spring-cloud-config,EncryptionControllerTests.appAndProfile,"	@Test
	public void appAndProfile() {
		this.controller = new EncryptionController(
				new SingleTextEncryptorLocator(new RsaSecretEncryptor()));
		// Add space to input
		String cipher = this.controller.encrypt(""app"", ""default"", ""foo bar"",
				MediaType.TEXT_PLAIN);
		String decrypt = this.controller.decrypt(""app"", ""default"", cipher,
				MediaType.TEXT_PLAIN);
		assertThat(decrypt).as(""Wrong decrypted plaintext: "" + decrypt)
				.isEqualTo(""foo bar"");
	}
",non-flaky,5
104154,spring-cloud_spring-cloud-config,EncryptionControllerTests.formDataIn,"	@Test
	public void formDataIn() {
		this.controller = new EncryptionController(
				new SingleTextEncryptorLocator(new RsaSecretEncryptor()));
		// Add space to input
		String cipher = this.controller.encrypt(""foo bar="",
				MediaType.APPLICATION_FORM_URLENCODED);
		String decrypt = this.controller.decrypt(cipher + ""="",
				MediaType.APPLICATION_FORM_URLENCODED);
		assertThat(decrypt).as(""Wrong decrypted plaintext: "" + decrypt)
				.isEqualTo(""foo bar"");
	}
",non-flaky,5
104155,spring-cloud_spring-cloud-config,EncryptionControllerTests.formDataInWithPrefix,"	@Test
	public void formDataInWithPrefix() {
		this.controller = new EncryptionController(
				new SingleTextEncryptorLocator(new RsaSecretEncryptor()));
		// Add space to input
		String cipher = this.controller.encrypt(""{key:test}foo bar="",
				MediaType.APPLICATION_FORM_URLENCODED);
		String decrypt = this.controller.decrypt(cipher + ""="",
				MediaType.APPLICATION_FORM_URLENCODED);
		assertThat(decrypt).as(""Wrong decrypted plaintext: "" + decrypt)
				.isEqualTo(""foo bar"");
	}
",non-flaky,5
104156,spring-cloud_spring-cloud-config,EncryptionControllerTests.prefixStrippedBeforeEncrypt,"	@Test
	public void prefixStrippedBeforeEncrypt() {
		TextEncryptor encryptor = mock(TextEncryptor.class);
		when(encryptor.encrypt(anyString())).thenReturn(""myEncryptedValue"");

		this.controller = new EncryptionController(
				new SingleTextEncryptorLocator(encryptor));
		this.controller.encrypt(""{key:test}foo"", MediaType.TEXT_PLAIN);

		ArgumentCaptor<String> captor = ArgumentCaptor.forClass(String.class);
		verify(encryptor, atLeastOnce()).encrypt(captor.capture());
		assertThat(captor.getValue()).doesNotContain(""{key:test}"")
				.as(""Prefix must be stripped prior to encrypt"");
	}
",non-flaky,5
104157,spring-cloud_spring-cloud-config,EncryptionControllerTests.encryptDecyptTextWithCurlyBrace,"	@Test
	public void encryptDecyptTextWithCurlyBrace() {
		this.controller = new EncryptionController(
				new SingleTextEncryptorLocator(new RsaSecretEncryptor()));

		String plain = ""textwith}brace"";
",non-flaky,5
104158,spring-cloud_spring-cloud-config,EncryptionControllerTests.locate,"	@Test
	public void addEnvironment() {
		TextEncryptorLocator locator = new TextEncryptorLocator() {

			private RsaSecretEncryptor encryptor = new RsaSecretEncryptor();

			@Override
			public TextEncryptor locate(Map<String, String> keys) {
				return this.encryptor;
			}
",non-flaky,5
104159,spring-cloud_spring-cloud-config,CipherEnvironmentEncryptorTests.shouldDecryptEnvironment,"	@Test
	public void shouldDecryptEnvironment() {
		// given
		String secret = randomUUID().toString();

		// when
		Environment environment = new Environment(""name"", ""profile"", ""label"");
		environment.add(new PropertySource(""a"", Collections.<Object, Object>singletonMap(
				environment.getName(), ""{cipher}"" + this.textEncryptor.encrypt(secret))));

		// then
		assertThat(this.encryptor.decrypt(environment).getPropertySources().get(0)
				.getSource().get(environment.getName())).isEqualTo(secret);
	}
",non-flaky,5
104160,spring-cloud_spring-cloud-config,CipherEnvironmentEncryptorTests.shouldDecryptEnvironmentWithKey,"	@Test
	public void shouldDecryptEnvironmentWithKey() {
		// given
		String secret = randomUUID().toString();

		// when
		Environment environment = new Environment(""name"", ""profile"", ""label"");
		environment.add(new PropertySource(""a"",
				Collections.<Object, Object>singletonMap(environment.getName(),
						""{cipher}{key:test}"" + this.textEncryptor.encrypt(secret))));

		// then
		assertThat(this.encryptor.decrypt(environment).getPropertySources().get(0)
				.getSource().get(environment.getName())).isEqualTo(secret);
	}
",non-flaky,5
104161,spring-cloud_spring-cloud-config,CipherEnvironmentEncryptorTests.shouldBeAbleToUseNullAsPropertyValue,"	@Test
	public void shouldBeAbleToUseNullAsPropertyValue() {

		// when
		Environment environment = new Environment(""name"", ""profile"", ""label"");
		environment.add(new PropertySource(""a"",
				Collections.<Object, Object>singletonMap(environment.getName(), null)));

		// then
		assertThat(this.encryptor.decrypt(environment).getPropertySources().get(0)
				.getSource().get(environment.getName())).isEqualTo(null);
	}
",non-flaky,5
104162,spring-cloud_spring-cloud-config,EncryptionIntegrationTests.symmetricEncryptionEnabled,"		@Test
		public void symmetricEncryptionEnabled() throws Exception {
			ResponseEntity<String> entity = this.testRestTemplate
					.getForEntity(""/encrypt/status"", String.class);
			assertThat(entity.getStatusCode()).isEqualTo(HttpStatus.OK);
		}
",non-flaky,5
104163,spring-cloud_spring-cloud-config,EncryptionIntegrationTests.symmetricEncryptionBootstrapConfig,"		@Test
		public void symmetricEncryptionBootstrapConfig() throws Exception {
			ResponseEntity<String> entity = this.testRestTemplate
					.getForEntity(""/encrypt/status"", String.class);
			assertThat(entity.getStatusCode()).isEqualTo(HttpStatus.OK);
		}
",non-flaky,5
104164,spring-cloud_spring-cloud-config,EncryptionIntegrationTests.keystoreBootstrapConfig,"		@Test
		public void keystoreBootstrapConfig() throws Exception {
			ResponseEntity<String> entity = this.testRestTemplate
					.getForEntity(""/encrypt/status"", String.class);
			assertThat(entity.getStatusCode()).isEqualTo(HttpStatus.OK);
		}
",non-flaky,5
104165,spring-cloud_spring-cloud-config,EnvironmentPrefixHelperTests.testAddPrefix,"	@Test
	public void testAddPrefix() {
		assertThat(this.helper.addPrefix(Collections.singletonMap(""bar"", ""spam""), ""foo""))
				.isEqualTo(""{bar:spam}foo"");
	}
",non-flaky,5
104166,spring-cloud_spring-cloud-config,EnvironmentPrefixHelperTests.testAddNoPrefix,"	@Test
	public void testAddNoPrefix() {
		assertThat(this.helper.addPrefix(Collections.<String, String>emptyMap(), ""foo""))
				.isEqualTo(""foo"");
	}
",non-flaky,5
104167,spring-cloud_spring-cloud-config,EnvironmentPrefixHelperTests.testStripNoPrefix,"	@Test
	public void testStripNoPrefix() {
		assertThat(this.helper.stripPrefix(""foo"")).isEqualTo(""foo"");
	}
",non-flaky,5
104168,spring-cloud_spring-cloud-config,EnvironmentPrefixHelperTests.testStripPrefix,"	@Test
	public void testStripPrefix() {
		assertThat(this.helper.stripPrefix(""{key:foo}foo"")).isEqualTo(""foo"");
	}
",non-flaky,5
104169,spring-cloud_spring-cloud-config,EnvironmentPrefixHelperTests.testStripPrefixWithEscape,"	@Test
	public void testStripPrefixWithEscape() {
		assertThat(this.helper.stripPrefix(""{plain}{key:foo}foo""))
				.isEqualTo(""{key:foo}foo"");
	}
",non-flaky,5
104170,spring-cloud_spring-cloud-config,EnvironmentPrefixHelperTests.testKeysDefaults,"	@Test
	public void testKeysDefaults() {
		Map<String, String> keys = this.helper.getEncryptorKeys(""foo"", ""bar"", ""spam"");
		assertThat(keys.get(""name"")).isEqualTo(""foo"");
		assertThat(keys.get(""profiles"")).isEqualTo(""bar"");
	}
",non-flaky,5
104171,spring-cloud_spring-cloud-config,EnvironmentPrefixHelperTests.testKeysWithPrefix,"	@Test
	public void testKeysWithPrefix() {
		Map<String, String> keys = this.helper.getEncryptorKeys(""foo"", ""bar"",
				""{key:mykey}foo"");
		assertThat(keys.size()).isEqualTo(3);
		assertThat(keys.get(""key"")).isEqualTo(""mykey"");
	}
",non-flaky,5
104172,spring-cloud_spring-cloud-config,EnvironmentPrefixHelperTests.testKeysWithPrefixAndEscape,"	@Test
	public void testKeysWithPrefixAndEscape() {
		Map<String, String> keys = this.helper.getEncryptorKeys(""foo"", ""bar"",
				""{key:mykey}{plain}{foo:bar}foo"");
		assertThat(keys.size()).isEqualTo(3);
		assertThat(keys.get(""key"")).isEqualTo(""mykey"");
	}
",non-flaky,5
104173,spring-cloud_spring-cloud-config,EnvironmentPrefixHelperTests.testTextWithCurlyBracesNoPrefix,"	@Test
	public void testTextWithCurlyBracesNoPrefix() {
		assertThat(this.helper.stripPrefix(""textwith}brac{es""))
				.isEqualTo(""textwith}brac{es"");
	}
",non-flaky,5
104174,spring-cloud_spring-cloud-config,EnvironmentPrefixHelperTests.testTextWithCurlyBracesPrefix,"	@Test
	public void testTextWithCurlyBracesPrefix() {
		assertThat(
				this.helper.stripPrefix(""{key:foo}{name:bar}textwith}brac{es{and}prefix""))
						.isEqualTo(""textwith}brac{es{and}prefix"");
	}
",non-flaky,5
104175,spring-cloud_spring-cloud-config,EncryptionControllerMultiTextEncryptorTests.shouldEncryptUsingApplicationAndProfiles,"	@Test
	public void shouldEncryptUsingApplicationAndProfiles() {

		this.controller = new EncryptionController(
				new SingleTextEncryptorLocator(Encryptors.text(""application"", ""11"")));

		// when
		String encrypted = this.controller.encrypt(this.application, this.profiles,
				this.data, TEXT_PLAIN);

		// then
		assertThat(this.controller.decrypt(this.application, this.profiles, encrypted,
				TEXT_PLAIN)).isEqualTo(this.data);
	}
",non-flaky,5
104176,spring-cloud_spring-cloud-config,EncryptionControllerMultiTextEncryptorTests.shouldNotEncryptUsingNoOp,"	@Test(expected = EncryptionTooWeakException.class)
	public void shouldNotEncryptUsingNoOp() {
		// given
		String application = ""unknown"";

		// when
		this.controller.encrypt(application, this.profiles, this.data, TEXT_PLAIN);

		// then exception is thrown
	}
",non-flaky,5
104177,spring-cloud_spring-cloud-config,EncryptionControllerMultiTextEncryptorTests.shouldNotDecryptUsingNoOp,"	@Test(expected = EncryptionTooWeakException.class)
	public void shouldNotDecryptUsingNoOp() {
		// given
		String application = ""unknown"";

		// when
		this.controller.decrypt(application, this.profiles, this.data, TEXT_PLAIN);

		// then exception is thrown
	}
",non-flaky,5
104178,spring-cloud_spring-cloud-config,KeyStoreTextEncryptorLocatorTests.testDefaults,"	@Test
	public void testDefaults() {
		TextEncryptor encryptor = this.locator
				.locate(Collections.<String, String>emptyMap());
		assertThat(encryptor.decrypt(encryptor.encrypt(""foo""))).isEqualTo(""foo"");
	}
",non-flaky,5
104179,spring-cloud_spring-cloud-config,KeyStoreTextEncryptorLocatorTests.locate,"	@Test
	public void testDifferentKeyDefaultSecret() {
		this.locator.setSecretLocator(new SecretLocator() {

			@Override
			public char[] locate(String secret) {
				assertThat(secret).isEqualTo(""changeme"");
				// The actual secret for ""mykey"" is the same as the keystore password
				return ""letmein"".toCharArray();
			}
",non-flaky,5
104180,spring-cloud_spring-cloud-config,KeyStoreTextEncryptorLocatorTests.testDifferentKeyAndSecret,"	@Test
	public void testDifferentKeyAndSecret() {
		Map<String, String> map = new HashMap<String, String>();
		map.put(""key"", ""mytestkey"");
		map.put(""secret"", ""changeme"");
		TextEncryptor encryptor = this.locator.locate(map);
		assertThat(encryptor.decrypt(encryptor.encrypt(""foo""))).isEqualTo(""foo"");
	}
",non-flaky,5
104181,spring-cloud_spring-cloud-config,EnvironmentPropertySourceTest.testEscapedPlaceholdersRemoved,"	@Test
	public void testEscapedPlaceholdersRemoved() {
		assertThat(resolvePlaceholders(this.env, ""\\${abc}"")).isEqualTo(""${abc}"");
		// JSON generated from jackson will be double escaped
		assertThat(resolvePlaceholders(this.env, ""\\\\${abc}"")).isEqualTo(""${abc}"");
	}
",non-flaky,5
104182,spring-cloud_spring-cloud-config,GitSkipSslValidationCredentialsProviderTest.testCanHandle,"	@Test
	public void testCanHandle() {
		assertThat(GitSkipSslValidationCredentialsProvider
				.canHandle(""https://github.com/org/repo"")).as(
						""GitSkipSslValidationCredentialsProvider only handles HTTPS uris"")
						.isTrue();
		assertThat(GitSkipSslValidationCredentialsProvider
				.canHandle(""git@github.com:org/repo"")).as(
						""GitSkipSslValidationCredentialsProvider only handles HTTPS uris"")
						.isFalse();
	}
",non-flaky,5
104183,spring-cloud_spring-cloud-config,GitSkipSslValidationCredentialsProviderTest.testIsInteractive,"	@Test
	public void testIsInteractive() {
		assertThat(this.skipSslValidationCredentialsProvider.isInteractive()).as(
				""GitSkipSslValidationCredentialsProvider with no delegate requires no user interaction"")
				.isFalse();
	}
",non-flaky,5
104184,spring-cloud_spring-cloud-config,GitSkipSslValidationCredentialsProviderTest.testIsInteractiveWithDelegate,"	@Test
	public void testIsInteractiveWithDelegate() {
		this.skipSslValidationCredentialsProvider = new GitSkipSslValidationCredentialsProvider(
				this.mockDelegateCredentialsProvider);

		when(this.mockDelegateCredentialsProvider.isInteractive()).thenReturn(true);

		assertThat(this.skipSslValidationCredentialsProvider.isInteractive()).as(
				""With a delegate provider, isInteractive value depends on the delegate"")
				.isTrue();
	}
",non-flaky,5
104185,spring-cloud_spring-cloud-config,GitSkipSslValidationCredentialsProviderTest.testSupportsSslFailureInformationalMessage,"	@Test
	public void testSupportsSslFailureInformationalMessage() {
		CredentialItem informationalMessage = new CredentialItem.InformationalMessage(
				""text "" + JGitText.get().sslFailureTrustExplanation + "" more text"");
		assertThat(this.skipSslValidationCredentialsProvider
				.supports(informationalMessage)).as(
						""GitSkipSslValidationCredentialsProvider should always support SSL failure InformationalMessage"")
						.isTrue();

		informationalMessage = new CredentialItem.InformationalMessage(""unrelated"");
		assertThat(this.skipSslValidationCredentialsProvider
				.supports(informationalMessage)).as(
						""GitSkipSslValidationCredentialsProvider should not support unrelated InformationalMessage items"")
						.isFalse();
	}
",non-flaky,5
104186,spring-cloud_spring-cloud-config,GitSkipSslValidationCredentialsProviderTest.testSupportsSslFailureInformationalMessageWithDelegate,"	@Test
	public void testSupportsSslFailureInformationalMessageWithDelegate() {
		this.skipSslValidationCredentialsProvider = new GitSkipSslValidationCredentialsProvider(
				this.mockDelegateCredentialsProvider);

		testSupportsSslFailureInformationalMessage();
	}
",non-flaky,5
104187,spring-cloud_spring-cloud-config,GitSkipSslValidationCredentialsProviderTest.testSupportsSslValidationYesNoTypes,"	@Test
	public void testSupportsSslValidationYesNoTypes() {
		CredentialItem yesNoType = new CredentialItem.YesNoType(
				JGitText.get().sslTrustNow);
		assertThat(this.skipSslValidationCredentialsProvider.supports(yesNoType)).as(
				""GitSkipSslValidationCredentialsProvider should always support the trust now YesNoType item"")
				.isTrue();

		yesNoType = new CredentialItem.YesNoType(
				MessageFormat.format(JGitText.get().sslTrustForRepo, ""/a/path.git""));
		assertThat(this.skipSslValidationCredentialsProvider.supports(yesNoType)).as(
				""GitSkipSslValidationCredentialsProvider should always support the trust repo YesNoType item"")
				.isTrue();

		yesNoType = new CredentialItem.YesNoType(JGitText.get().sslTrustAlways);
		assertThat(this.skipSslValidationCredentialsProvider.supports(yesNoType)).as(
				""GitSkipSslValidationCredentialsProvider should always support the trust always YesNoType item"")
				.isTrue();

		yesNoType = new CredentialItem.YesNoType(""unrelated"");
		assertThat(this.skipSslValidationCredentialsProvider.supports(yesNoType)).as(
				""GitSkipSslValidationCredentialsProvider should not support unrelated YesNoType items"")
				.isFalse();
	}
",non-flaky,5
104188,spring-cloud_spring-cloud-config,GitSkipSslValidationCredentialsProviderTest.testSupportsYesNoTypeWithDelegate,"	@Test
	public void testSupportsYesNoTypeWithDelegate() {
		this.skipSslValidationCredentialsProvider = new GitSkipSslValidationCredentialsProvider(
				this.mockDelegateCredentialsProvider);

		testSupportsSslValidationYesNoTypes();
	}
",non-flaky,5
104189,spring-cloud_spring-cloud-config,GitSkipSslValidationCredentialsProviderTest.testSupportsUnrelatedCredentialItemTypes,"	@Test
	public void testSupportsUnrelatedCredentialItemTypes() {
		CredentialItem usernameCredentialItem = new CredentialItem.Username();

		boolean supportsItems = this.skipSslValidationCredentialsProvider
				.supports(usernameCredentialItem);

		assertThat(supportsItems).as(
				""Credential item types not related to SSL validation skipping should not be supported"")
				.isFalse();
	}
",non-flaky,5
104190,spring-cloud_spring-cloud-config,GitSkipSslValidationCredentialsProviderTest.testSupportsUnrelatedCredentialItemTypesWithDelegate,"	@Test
	public void testSupportsUnrelatedCredentialItemTypesWithDelegate() {
		this.skipSslValidationCredentialsProvider = new GitSkipSslValidationCredentialsProvider(
				this.mockDelegateCredentialsProvider);
		CredentialItem usernameCredentialItem = new CredentialItem.Username();

		when(this.mockDelegateCredentialsProvider.supports(usernameCredentialItem))
				.thenReturn(true);

		boolean supportsItems = this.skipSslValidationCredentialsProvider
				.supports(usernameCredentialItem);

		assertThat(supportsItems).as(
				""GitSkipSslValidationCredentialsProvider must support the types supported by its delegate CredentialsProvider"")
				.isTrue();
	}
",non-flaky,5
110836,pushtorefresh_storio,GetObjectObserveChangesTest.repeatsOperationWithQueryByChangeOfTable,"    @Test
    public void repeatsOperationWithQueryByChangeOfTable() {
        User user = putUserBlocking();

        PreparedGetObject<User> operation = storIOSQLite
                .get()
                .object(User.class)
                .withQuery(query)
                .prepare();

        verifyChangesReceived(operation, tableChanges, user);
    }
",non-flaky,5
110837,pushtorefresh_storio,GetObjectObserveChangesTest.repeatsOperationWithRawQueryByChangeOfTable,"    @Test
    public void repeatsOperationWithRawQueryByChangeOfTable() {
        User user = putUserBlocking();

        PreparedGetObject<User> operation = storIOSQLite
                .get()
                .object(User.class)
                .withQuery(query)
                .prepare();

        verifyChangesReceived(operation, tableChanges, user);
    }
",non-flaky,5
110838,pushtorefresh_storio,GetObjectObserveChangesTest.repeatsOperationWithQueryByChangeOfTag,"    @Test
    public void repeatsOperationWithQueryByChangeOfTag() {
        User user = putUserBlocking();

        PreparedGetObject<User> operation = storIOSQLite
                .get()
                .object(User.class)
                .withQuery(query)
                .prepare();

        verifyChangesReceived(operation, tagChanges, user);
    }
",non-flaky,5
110839,pushtorefresh_storio,GetObjectObserveChangesTest.repeatsOperationWithRawQueryByChangeOfTag,"    @Test
    public void repeatsOperationWithRawQueryByChangeOfTag() {
        User user = putUserBlocking();

        PreparedGetObject<User> operation = storIOSQLite
                .get()
                .object(User.class)
                .withQuery(query)
                .prepare();

        verifyChangesReceived(operation, tagChanges, user);
    }
",non-flaky,5
110840,pushtorefresh_storio,GetListOfObjectsObserveChangesTest.repeatsOperationWithQueryByChangeOfTable,"    @Test
    public void repeatsOperationWithQueryByChangeOfTable() {
        User user = putUserBlocking();

        PreparedGetListOfObjects<User> operation = storIOSQLite
                .get()
                .listOfObjects(User.class)
                .withQuery(query)
                .prepare();

        verifyChangesReceived(operation, tableChanges, singletonList(user));
    }
",non-flaky,5
110841,pushtorefresh_storio,GetListOfObjectsObserveChangesTest.repeatsOperationWithRawQueryByChangeOfTable,"    @Test
    public void repeatsOperationWithRawQueryByChangeOfTable() {
        User user = putUserBlocking();

        PreparedGetListOfObjects<User> operation = storIOSQLite
                .get()
                .listOfObjects(User.class)
                .withQuery(rawQuery)
                .prepare();

        verifyChangesReceived(operation, tableChanges, singletonList(user));
    }
",non-flaky,5
110842,pushtorefresh_storio,GetListOfObjectsObserveChangesTest.repeatsOperationWithQueryByChangeOfTag,"    @Test
    public void repeatsOperationWithQueryByChangeOfTag() {
        User user = putUserBlocking();

        PreparedGetListOfObjects<User> operation = storIOSQLite
                .get()
                .listOfObjects(User.class)
                .withQuery(query)
                .prepare();

        verifyChangesReceived(operation, tagChanges, singletonList(user));
    }
",non-flaky,5
110843,pushtorefresh_storio,GetListOfObjectsObserveChangesTest.repeatsOperationWithRawQueryByChangeOfTag,"    @Test
    public void repeatsOperationWithRawQueryByChangeOfTag() {
        User user = putUserBlocking();

        PreparedGetListOfObjects<User> operation = storIOSQLite
                .get()
                .listOfObjects(User.class)
                .withQuery(rawQuery)
                .prepare();

        verifyChangesReceived(operation, tagChanges, singletonList(user));
    }
",non-flaky,5
110844,pushtorefresh_storio,ObserveChangesOfTagTest.insertEmission,"    @Test
    public void insertEmission() {
        final List<User> users = TestFactory.newUsers(10);

        final Queue<Changes> expectedChanges = new LinkedList<Changes>();
        expectedChanges.add(Changes.newInstance(UserTableMeta.TABLE, UserTableMeta.NOTIFICATION_TAG));

        final EmissionChecker emissionChecker = new EmissionChecker(expectedChanges);
        final Subscription subscription = emissionChecker.subscribe();

        putUsersBlocking(users);

        // Should receive changes of Users table
        emissionChecker.awaitNextExpectedValue();

        emissionChecker.assertThatNoExpectedValuesLeft();

        subscription.unsubscribe();
    }
",non-flaky,5
110845,pushtorefresh_storio,ObserveChangesOfTagTest.updateEmission,"    @Test
    public void updateEmission() {
        final List<User> users = putUsersBlocking(10);
        final List<User> updated = new ArrayList<User>(users.size());

        for (User user : users) {
            updated.add(User.newInstance(user.id(), user.email()));
        }

        final Queue<Changes> expectedChanges = new LinkedList<Changes>();
        expectedChanges.add(Changes.newInstance(UserTableMeta.TABLE, UserTableMeta.NOTIFICATION_TAG));

        final EmissionChecker emissionChecker = new EmissionChecker(expectedChanges);
        final Subscription subscription = emissionChecker.subscribe();

        storIOSQLite
                .put()
                .objects(updated)
                .prepare()
                .executeAsBlocking();

        // Should receive changes of Users table
        emissionChecker.awaitNextExpectedValue();

        emissionChecker.assertThatNoExpectedValuesLeft();

        subscription.unsubscribe();
    }
",non-flaky,5
110846,pushtorefresh_storio,ObserveChangesOfTagTest.deleteEmission,"    @Test
    public void deleteEmission() {
        final List<User> users = putUsersBlocking(10);

        final Queue<Changes> expectedChanges = new LinkedList<Changes>();
        expectedChanges.add(Changes.newInstance(UserTableMeta.TABLE, UserTableMeta.NOTIFICATION_TAG));

        final EmissionChecker emissionChecker = new EmissionChecker(expectedChanges);
        final Subscription subscription = emissionChecker.subscribe();

        deleteUsersBlocking(users);

        // Should receive changes of Users table
        emissionChecker.awaitNextExpectedValue();

        emissionChecker.assertThatNoExpectedValuesLeft();

        subscription.unsubscribe();
    }
",non-flaky,5
110847,pushtorefresh_storio,QueryTest.queryAll,"    @Test
    public void queryAll() {
        final List<User> users = putUsersBlocking(3);
        final List<User> usersFromQuery = getAllUsersBlocking();
        assertThat(users.equals(usersFromQuery)).isTrue();
    }
",non-flaky,5
110848,pushtorefresh_storio,QueryTest.queryOneByField,"    @Test
    public void queryOneByField() {
        final List<User> users = putUsersBlocking(3);

        for (User user : users) {
            final List<User> usersFromQuery = storIOSQLite
                    .get()
                    .listOfObjects(User.class)
                    .withQuery(Query.builder()
                            .table(UserTableMeta.TABLE)
                            .where(UserTableMeta.COLUMN_EMAIL + ""=?"")
                            .whereArgs(user.email())
                            .build())
                    .prepare()
                    .executeAsBlocking();

            assertThat(usersFromQuery).isNotNull();
            assertThat(usersFromQuery).hasSize(1);
            assertThat(usersFromQuery.get(0)).isEqualTo(user);
        }
    }
",non-flaky,5
110849,pushtorefresh_storio,QueryTest.queryOrdered,"    @Test
    public void queryOrdered() {
        final List<User> users = TestFactory.newUsers(3);

        // Reverse sorting by email before inserting, for the purity of the experiment.
        Collections.reverse(users);

        putUsersBlocking(users);

        final List<User> usersFromQueryOrdered = storIOSQLite
                .get()
                .listOfObjects(User.class)
                .withQuery(Query.builder()
                        .table(UserTableMeta.TABLE)
                        .orderBy(UserTableMeta.COLUMN_EMAIL)
                        .build())
                .prepare()
                .executeAsBlocking();

        assertThat(usersFromQueryOrdered).isNotNull();
        assertThat(usersFromQueryOrdered).hasSize(users.size());

        // Sorting by email for check ordering.
        Collections.sort(users);

        for (int i = 0; i < users.size(); i++) {
            assertThat(usersFromQueryOrdered.get(i)).isEqualTo(users.get(i));
        }
    }
",non-flaky,5
110850,pushtorefresh_storio,QueryTest.queryOrderedDesc,"    @Test
    public void queryOrderedDesc() {
        final List<User> users = TestFactory.newUsers(3);

        // Sorting by email before inserting, for the purity of the experiment.
        Collections.sort(users);

        putUsersBlocking(users);

        final List<User> usersFromQueryOrdered = storIOSQLite
                .get()
                .listOfObjects(User.class)
                .withQuery(Query.builder()
                        .table(UserTableMeta.TABLE)
                        .orderBy(UserTableMeta.COLUMN_EMAIL + "" DESC"")
                        .build())
                .prepare()
                .executeAsBlocking();

        assertThat(usersFromQueryOrdered).isNotNull();
        assertThat(usersFromQueryOrdered).hasSize(users.size());

        // Reverse sorting by email for check ordering.
        Collections.reverse(users);

        for (int i = 0; i < users.size(); i++) {
            assertThat(usersFromQueryOrdered.get(i)).isEqualTo(users.get(i));
        }
    }
",non-flaky,5
110851,pushtorefresh_storio,QueryTest.querySingleLimit,"    @Test
    public void querySingleLimit() {
        putUsersBlocking(10);

        final int limit = 8;
        final List<User> usersFromQuery = storIOSQLite
                .get()
                .listOfObjects(User.class)
                .withQuery(Query.builder()
                        .table(UserTableMeta.TABLE)
                        .limit(String.valueOf(limit))
                        .build())
                .prepare()
                .executeAsBlocking();

        assertThat(usersFromQuery).isNotNull();
        assertThat(usersFromQuery).hasSize(limit);
    }
",non-flaky,5
110852,pushtorefresh_storio,QueryTest.queryLimitOffset,"    @Test
    public void queryLimitOffset() {
        final List<User> users = putUsersBlocking(10);

        final int offset = 5;
        final int limit = 3;
        final List<User> usersFromQuery = storIOSQLite
                .get()
                .listOfObjects(User.class)
                .withQuery(Query.builder()
                        .table(UserTableMeta.TABLE)
                        .orderBy(UserTableMeta.COLUMN_EMAIL)
                        .limit(offset + "", "" + limit)
                        .build())
                .prepare()
                .executeAsBlocking();

        assertThat(usersFromQuery).isNotNull();
        assertThat(usersFromQuery).hasSize(Math.min(limit, users.size() - offset));

        Collections.sort(users);

        int position = 0;
        for (int i = offset; i < offset + limit; i++) {
            assertThat(usersFromQuery.get(position++)).isEqualTo(users.get(i));
        }
    }
",non-flaky,5
110853,pushtorefresh_storio,QueryTest.queryIntegerLimit,"    @Test
    public void queryIntegerLimit() {
        putUsersBlocking(10);

        final int limit = 8;
        final List<User> usersFromQuery = storIOSQLite
                .get()
                .listOfObjects(User.class)
                .withQuery(Query.builder()
                        .table(UserTableMeta.TABLE)
                        .limit(limit)
                        .build())
                .prepare()
                .executeAsBlocking();

        assertThat(usersFromQuery).isNotNull();
        assertThat(usersFromQuery).hasSize(limit);
    }
",non-flaky,5
110854,pushtorefresh_storio,QueryTest.queryLimitOffsetQuantity,"    @Test
    public void queryLimitOffsetQuantity() {
        final List<User> users = putUsersBlocking(10);

        final int offset = 5;
        final int quantity = 3;
        final List<User> usersFromQuery = storIOSQLite
                .get()
                .listOfObjects(User.class)
                .withQuery(Query.builder()
                        .table(UserTableMeta.TABLE)
                        .orderBy(UserTableMeta.COLUMN_EMAIL)
                        .limit(offset, quantity)
                        .build())
                .prepare()
                .executeAsBlocking();

        assertThat(usersFromQuery).isNotNull();
        assertThat(usersFromQuery).hasSize(Math.min(quantity, users.size() - offset));

        Collections.sort(users);

        int position = 0;
        for (int i = offset; i < offset + quantity; i++) {
            assertThat(usersFromQuery.get(position++)).isEqualTo(users.get(i));
        }
    }
",non-flaky,5
110855,pushtorefresh_storio,QueryTest.mapFromCursor,"    @Test
    public void queryGroupBy() {
        final List<User> users = TestFactory.newUsers(10);

        for (int i = 0; i < users.size(); i++) {
            final String commonEmail;
            if (i < 3) {
                commonEmail = ""first_group@gmail.com"";
            } else {
                commonEmail = ""second_group@gmail.com"";
            }

            users.set(i, User.newInstance(null, commonEmail));
        }

        putUsersBlocking(users);

        final List<User> groupsOfUsers = storIOSQLite
                .get()
                .listOfObjects(User.class)
                .withQuery(Query.builder()
                        .table(UserTableMeta.TABLE)
                        .columns(UserTableMeta.COLUMN_EMAIL)
                        .groupBy(UserTableMeta.COLUMN_EMAIL)
                        .build())
                .withGetResolver(new DefaultGetResolver<User>() {
                    @NonNull
                    @Override
                    public User mapFromCursor(@NonNull StorIOSQLite storIOSQLite, @NonNull Cursor cursor) {
                        return User.newInstance(null, cursor.getString(cursor.getColumnIndex(UserTableMeta.COLUMN_EMAIL)));
                    }
",non-flaky,5
110856,pushtorefresh_storio,QueryTest.mapFromCursor,"    @Test
    public void queryHaving() {
        final List<User> users = TestFactory.newUsers(10);

        for (int i = 0; i < users.size(); i++) {
            final String commonEmail;
            if (i < 3) {
                commonEmail = ""first_group@gmail.com"";
            } else {
                commonEmail = ""second_group@gmail.com"";
            }

            users.set(i, User.newInstance(null, commonEmail));
        }

        putUsersBlocking(users);

        final int bigGroupThreshold = 5;

        final List<User> groupsOfUsers = storIOSQLite
                .get()
                .listOfObjects(User.class)
                .withQuery(Query.builder()
                        .table(UserTableMeta.TABLE)
                        .columns(UserTableMeta.COLUMN_EMAIL)
                        .groupBy(UserTableMeta.COLUMN_EMAIL)
                        .having(""COUNT(*) >= "" + bigGroupThreshold)
                        .build())
                .withGetResolver(new DefaultGetResolver<User>() {
                    @NonNull
                    @Override
                    public User mapFromCursor(@NonNull StorIOSQLite storIOSQLite, @NonNull Cursor cursor) {
                        return User.newInstance(null, cursor.getString(cursor.getColumnIndex(UserTableMeta.COLUMN_EMAIL)));
                    }
",non-flaky,5
110857,pushtorefresh_storio,QueryTest.mapFromCursor,"    @Test
    public void queryDistinct() {
        final List<User> users = new ArrayList<User>();

        for (int i = 0; i < 10; i++) {
            users.add(User.newInstance((long) i, ""same@email.com""));
        }

        putUsersBlocking(users);

        final GetResolver<User> customGetResolver = new DefaultGetResolver<User>() {
            @NonNull
            @Override
            public User mapFromCursor(@NonNull StorIOSQLite storIOSQLite, @NonNull Cursor cursor) {
                return User.newInstance(null, cursor.getString(cursor.getColumnIndex(UserTableMeta.COLUMN_EMAIL)));
            }
",non-flaky,5
110858,pushtorefresh_storio,QueryTest.queryWithRawQuery,"    @Test
    public void queryWithRawQuery() {
        final List<User> users = TestFactory.newUsers(20);

        int counter = 1;

        for (int i = 0; i < users.size(); i++) {
            char[] chars = new char[counter++];
            Arrays.fill(chars, '*'); // wtf is going on?
            users.set(i, User.newInstance(null, new String(chars)));
        }

        putUsersBlocking(users);

        final List<User> usersWithLongName = new ArrayList<User>(users.size());

        int lengthSum = 0;
        for (User user : users) {
            lengthSum += user.email().length();
        }

        final int avrLength = lengthSum / users.size();

        for (User user : users) {
            if (user.email().length() > avrLength) {
                usersWithLongName.add(user);
            }
        }

        final String query = ""Select * from "" + UserTableMeta.TABLE
                + "" where length("" + UserTableMeta.COLUMN_EMAIL + "") > ""
                + ""(select avg(length("" + UserTableMeta.COLUMN_EMAIL + "")) from users)"";

        final List<User> usersFromQuery = storIOSQLite
                .get()
                .listOfObjects(User.class)
                .withQuery(RawQuery.builder()
                        .query(query)
                        .build())
                .prepare()
                .executeAsBlocking();

        assertThat(usersFromQuery).isEqualTo(usersWithLongName);
    }
",non-flaky,5
110859,pushtorefresh_storio,QueryTest.queryWithRawQueryAndArguments,"    @Test
    public void queryWithRawQueryAndArguments() {
        final User testUser = User.newInstance(null, ""testUserName"");

        final List<User> users = TestFactory.newUsers(10);
        users.add(testUser);
        putUsersBlocking(users);

        final String query = ""SELECT * FROM "" + UserTableMeta.TABLE
                + "" WHERE "" + UserTableMeta.COLUMN_EMAIL + "" LIKE ?"";

        final List<User> usersFromQuery = storIOSQLite
                .get()
                .listOfObjects(User.class)
                .withQuery(RawQuery.builder()
                        .query(query)
                        .args(testUser.email())
                        .build())
                .prepare()
                .executeAsBlocking();

        assertThat(usersFromQuery).isNotNull();
        assertThat(usersFromQuery).hasSize(1);
        assertThat(usersFromQuery.get(0)).isEqualTo(testUser);
    }
",non-flaky,5
110860,pushtorefresh_storio,QueryTest.queryWithRawQuerySqlInjectionFail,"    @Test
    public void queryWithRawQuerySqlInjectionFail() {
        final List<User> users = putUsersBlocking(10);

        final String query = ""SELECT * FROM "" + UserTableMeta.TABLE
                + "" WHERE "" + UserTableMeta.COLUMN_EMAIL + "" LIKE ?"";

        final String arg = ""(DELETE FROM "" + UserTableMeta.TABLE + "")"";

        storIOSQLite.get()
                .listOfObjects(User.class)
                .withQuery(RawQuery.builder()
                        .query(query)
                        .args(arg)
                        .build())
                .prepare()
                .executeAsBlocking();

        assertThat(getAllUsersBlocking()).isEqualTo(users);
    }
",non-flaky,5
110861,pushtorefresh_storio,QueryTest.getNumberOfResults,"    @Test
    public void getNumberOfResults() {
        putUsersBlocking(8);

        Integer numberOfResults = storIOSQLite
                .get()
                .numberOfResults()
                .withQuery(UserTableMeta.QUERY_ALL)
                .prepare()
                .executeAsBlocking();

        assertThat(numberOfResults).isEqualTo(8);
    }
",non-flaky,5
110862,pushtorefresh_storio,QueryTest.queryOneExistedObject,"    @Test
    public void queryOneExistedObject() {
        final List<User> users = putUsersBlocking(3);
        final User user = users.get(0);

        final User userFromQuery = storIOSQLite
                .get()
                .object(User.class)
                .withQuery(Query.builder()
                        .table(UserTableMeta.TABLE)
                        .where(UserTableMeta.COLUMN_EMAIL + ""=?"")
                        .whereArgs(user.email())
                        .build())
                .prepare()
                .executeAsBlocking();

        assertThat(userFromQuery).isNotNull();
        assertThat(userFromQuery).isEqualTo(user);
    }
",non-flaky,5
110863,pushtorefresh_storio,QueryTest.queryOneNonExistedObject,"    @Test
    public void queryOneNonExistedObject() {
        putUsersBlocking(3);

        final User userFromQuery = storIOSQLite
                .get()
                .object(User.class)
                .withQuery(Query.builder()
                        .table(UserTableMeta.TABLE)
                        .where(UserTableMeta.COLUMN_EMAIL + ""=?"")
                        .whereArgs(""some arg"")
                        .build())
                .prepare()
                .executeAsBlocking();

        assertThat(userFromQuery).isNull();
    }
",non-flaky,5
110864,pushtorefresh_storio,DeleteTest.deleteOne,"    @Test
    public void deleteOne() {
        final User user = putUserBlocking();

        final Cursor cursorAfterInsert = db.query(UserTableMeta.TABLE, null, null, null, null, null, null);
        assertThat(cursorAfterInsert.getCount()).isEqualTo(1);
        cursorAfterInsert.close();

        deleteUserBlocking(user);

        final Cursor cursorAfterDelete = db.query(UserTableMeta.TABLE, null, null, null, null, null, null);
        assertThat(cursorAfterDelete.getCount()).isEqualTo(0);
        cursorAfterDelete.close();
    }
",non-flaky,5
110865,pushtorefresh_storio,DeleteTest.deleteCollection,"    @Test
    public void deleteCollection() {
        final List<User> allUsers = putUsersBlocking(10);

        final List<User> usersToDelete = new ArrayList<User>();

        for (int i = 0; i < allUsers.size(); i += 2) {  // Delete every second user
            usersToDelete.add(allUsers.get(i));
        }

        final DeleteResults<User> deleteResults = storIOSQLite
                .delete()
                .objects(usersToDelete)
                .prepare()
                .executeAsBlocking();

        final List<User> usersAfterDelete = getAllUsersBlocking();

        assertThat(usersAfterDelete).hasSize(allUsers.size() / 2);

        for (User user : allUsers) {
            final boolean shouldBeDeleted = usersToDelete.contains(user);

            // Check that we deleted what we going to.
            assertThat(deleteResults.wasDeleted(user)).isEqualTo(shouldBeDeleted);

            // Check that we didn't delete users that we didn't want to
            assertThat(usersAfterDelete.contains(user)).isEqualTo(!shouldBeDeleted);
        }
    }
",non-flaky,5
110866,pushtorefresh_storio,RxQueryTest.insertEmission,"    @Test
    public void insertEmission() {
        final List<User> initialUsers = putUsersBlocking(10);
        final List<User> usersForInsert = TestFactory.newUsers(10);
        final List<User> allUsers = new ArrayList<User>(initialUsers.size() + usersForInsert.size());

        allUsers.addAll(initialUsers);
        allUsers.addAll(usersForInsert);

        final Queue<List<User>> expectedUsers = new LinkedList<List<User>>();
        expectedUsers.add(initialUsers);
        expectedUsers.add(allUsers);

        final EmissionChecker emissionChecker = new EmissionChecker(expectedUsers);
        final Subscription subscription = emissionChecker.subscribe();

        // Should receive initial users
        emissionChecker.awaitNextExpectedValue();

        putUsersBlocking(usersForInsert);

        // Should receive initial users + inserted users
        emissionChecker.awaitNextExpectedValue();

        emissionChecker.assertThatNoExpectedValuesLeft();

        subscription.unsubscribe();
    }
",non-flaky,5
110867,pushtorefresh_storio,RxQueryTest.updateEmission,"    @Test
    public void updateEmission() {
        final List<User> users = putUsersBlocking(10);

        final Queue<List<User>> expectedUsers = new LinkedList<List<User>>();

        final List<User> updatedList = new ArrayList<User>(users.size());

        int count = 1;
        for (User user : users) {
            updatedList.add(User.newInstance(user.id(), ""new_email"" + count++));
        }
        expectedUsers.add(users);
        expectedUsers.add(updatedList);
        final EmissionChecker emissionChecker = new EmissionChecker(expectedUsers);
        final Subscription subscription = emissionChecker.subscribe();

        // Should receive all users
        emissionChecker.awaitNextExpectedValue();

        storIOSQLite
                .put()
                .objects(updatedList)
                .prepare()
                .executeAsBlocking();

        // Should receive updated users
        emissionChecker.awaitNextExpectedValue();

        emissionChecker.assertThatNoExpectedValuesLeft();

        subscription.unsubscribe();
    }
",non-flaky,5
110868,pushtorefresh_storio,RxQueryTest.deleteEmission,"    @Test
    public void deleteEmission() {
        final List<User> usersThatShouldBeSaved = TestFactory.newUsers(10);
        final List<User> usersThatShouldBeDeleted = TestFactory.newUsers(10);
        final List<User> allUsers = new ArrayList<User>();

        allUsers.addAll(usersThatShouldBeSaved);
        allUsers.addAll(usersThatShouldBeDeleted);

        putUsersBlocking(allUsers);

        final Queue<List<User>> expectedUsers = new LinkedList<List<User>>();

        expectedUsers.add(allUsers);
        expectedUsers.add(usersThatShouldBeSaved);

        final EmissionChecker emissionChecker = new EmissionChecker(expectedUsers);
        final Subscription subscription = emissionChecker.subscribe();

        // Should receive all users
        emissionChecker.awaitNextExpectedValue();

        deleteUsersBlocking(usersThatShouldBeDeleted);

        // Should receive users that should be saved
        emissionChecker.awaitNextExpectedValue();

        emissionChecker.assertThatNoExpectedValuesLeft();

        subscription.unsubscribe();
    }
",non-flaky,5
110869,pushtorefresh_storio,RxQueryTest.run,"    @Test
    public void concurrentPutWithoutGlobalTransaction() throws InterruptedException {
        final int numberOfConcurrentPuts = ConcurrencyTesting.optimalTestThreadsCount();

        TestSubscriber<Changes> testSubscriber = new TestSubscriber<Changes>();

        storIOSQLite
                .observeChangesInTable(TweetTableMeta.TABLE)
                .subscribe(testSubscriber);

        final CountDownLatch concurrentPutLatch = new CountDownLatch(1);
        final CountDownLatch allPutsDoneLatch = new CountDownLatch(numberOfConcurrentPuts);

        for (int i = 0; i < numberOfConcurrentPuts; i++) {
            final int iCopy = i;

            new Thread(new Runnable() {
                @Override
                public void run() {
                    try {
                        concurrentPutLatch.await();
                    } catch (InterruptedException e) {
                        throw new RuntimeException(e);
                    }

                    storIOSQLite
                            .put()
                            .object(Tweet.newInstance(null, 1L, ""Some text: "" + iCopy))
                            .prepare()
                            .executeAsBlocking();

                    allPutsDoneLatch.countDown();
                }
",non-flaky,5
110870,pushtorefresh_storio,RxQueryTest.nestedTransaction,"    @Test
    public void nestedTransaction() {
        storIOSQLite.lowLevel().beginTransaction();

        storIOSQLite.lowLevel().beginTransaction();

        storIOSQLite.lowLevel().setTransactionSuccessful();
        storIOSQLite.lowLevel().endTransaction();

        storIOSQLite.lowLevel().setTransactionSuccessful();
        storIOSQLite.lowLevel().endTransaction();
    }
",non-flaky,5
110871,pushtorefresh_storio,RxQueryTest.queryOneExistedObjectObservable,"    @Test
    public void queryOneExistedObjectObservable() {
        final List<User> users = putUsersBlocking(3);
        final User expectedUser = users.get(0);

        final Observable<User> userObservable = storIOSQLite
                .get()
                .object(User.class)
                .withQuery(Query.builder()
                        .table(UserTableMeta.TABLE)
                        .where(UserTableMeta.COLUMN_EMAIL + ""=?"")
                        .whereArgs(expectedUser.email())
                        .build())
                .prepare()
                .asRxObservable()
                .take(1);

        TestSubscriber<User> testSubscriber = new TestSubscriber<User>();
        userObservable.subscribe(testSubscriber);

        testSubscriber.awaitTerminalEvent(5, SECONDS);
        testSubscriber.assertNoErrors();
        testSubscriber.assertValue(expectedUser);
    }
",non-flaky,5
110872,pushtorefresh_storio,RxQueryTest.queryOneNonExistedObjectObservable,"    @Test
    public void queryOneNonExistedObjectObservable() {
        putUsersBlocking(3);

        final Observable<User> userObservable = storIOSQLite
                .get()
                .object(User.class)
                .withQuery(Query.builder()
                        .table(UserTableMeta.TABLE)
                        .where(UserTableMeta.COLUMN_EMAIL + ""=?"")
                        .whereArgs(""some arg"")
                        .build())
                .prepare()
                .asRxObservable()
                .take(1);

        TestSubscriber<User> testSubscriber = new TestSubscriber<User>();
        userObservable.subscribe(testSubscriber);

        testSubscriber.awaitTerminalEvent(5, SECONDS);
        testSubscriber.assertNoErrors();
        testSubscriber.assertValue(null);
    }
",non-flaky,5
110873,pushtorefresh_storio,RxQueryTest.queryOneExistedObjectTableUpdate,"    @Test
    public void queryOneExistedObjectTableUpdate() {
        User expectedUser = User.newInstance(null, ""such@email.com"");
        putUsersBlocking(3);

        final Observable<User> userObservable = storIOSQLite
                .get()
                .object(User.class)
                .withQuery(Query.builder()
                        .table(UserTableMeta.TABLE)
                        .where(UserTableMeta.COLUMN_EMAIL + ""=?"")
                        .whereArgs(expectedUser.email())
                        .build())
                .prepare()
                .asRxObservable()
                .take(2);

        TestSubscriber<User> testSubscriber = new TestSubscriber<User>();
        userObservable.subscribe(testSubscriber);

        testSubscriber.awaitTerminalEvent(5, SECONDS);
        testSubscriber.assertNoErrors();
        testSubscriber.assertValue(null);

        putUserBlocking(expectedUser);

        testSubscriber.awaitTerminalEvent(5, SECONDS);
        testSubscriber.assertNoErrors();
        testSubscriber.assertValues(null, expectedUser);
    }
",non-flaky,5
110874,pushtorefresh_storio,RxQueryTest.queryOneNonexistedObjectTableUpdate,"    @Test
    public void queryOneNonexistedObjectTableUpdate() {
        final Observable<User> userObservable = storIOSQLite
                .get()
                .object(User.class)
                .withQuery(Query.builder()
                        .table(UserTableMeta.TABLE)
                        .where(UserTableMeta.COLUMN_EMAIL + ""=?"")
                        .whereArgs(""some arg"")
                        .build())
                .prepare()
                .asRxObservable()
                .take(2);

        TestSubscriber<User> testSubscriber = new TestSubscriber<User>();
        userObservable.subscribe(testSubscriber);

        testSubscriber.awaitTerminalEvent(5, SECONDS);
        testSubscriber.assertNoErrors();
        testSubscriber.assertValue(null);

        putUserBlocking();

        testSubscriber.awaitTerminalEvent(5, SECONDS);
        testSubscriber.assertNoErrors();
        testSubscriber.assertValues(null, null);
    }
",non-flaky,5
110875,pushtorefresh_storio,RxQueryTest.queryListOfObjectsAsSingle,"    @Test
    public void queryListOfObjectsAsSingle() {
        final List<User> users = putUsersBlocking(10);

        final Single<List<User>> usersSingle = storIOSQLite
                .get()
                .listOfObjects(User.class)
                .withQuery(UserTableMeta.QUERY_ALL)
                .prepare()
                .asRxSingle();

        TestSubscriber<List<User>> testSubscriber = new TestSubscriber<List<User>>();
        usersSingle.subscribe(testSubscriber);

        testSubscriber.awaitTerminalEvent(5, SECONDS);
        testSubscriber.assertNoErrors();
        testSubscriber.assertValue(users);
        testSubscriber.assertCompleted();
    }
",non-flaky,5
110876,pushtorefresh_storio,RxQueryTest.queryObjectAsSingle,"    @Test
    public void queryObjectAsSingle() {
        final List<User> users = putUsersBlocking(3);

        final Single<User> usersSingle = storIOSQLite
                .get()
                .object(User.class)
                .withQuery(UserTableMeta.QUERY_ALL)
                .prepare()
                .asRxSingle();

        TestSubscriber<User> testSubscriber = new TestSubscriber<User>();
        usersSingle.subscribe(testSubscriber);

        testSubscriber.awaitTerminalEvent(5, SECONDS);
        testSubscriber.assertNoErrors();
        testSubscriber.assertValues(users.get(0));
        testSubscriber.assertCompleted();
    }
",non-flaky,5
110877,pushtorefresh_storio,RxQueryTest.queryNumberOfResultsAsSingle,"    @Test
    public void queryNumberOfResultsAsSingle() {
        final List<User> users = putUsersBlocking(3);

        final Single<Integer> usersSingle = storIOSQLite
                .get()
                .numberOfResults()
                .withQuery(UserTableMeta.QUERY_ALL)
                .prepare()
                .asRxSingle();

        TestSubscriber<Integer> testSubscriber = new TestSubscriber<Integer>();
        usersSingle.subscribe(testSubscriber);

        testSubscriber.awaitTerminalEvent(5, SECONDS);
        testSubscriber.assertNoErrors();
        testSubscriber.assertValue(users.size());
        testSubscriber.assertCompleted();
    }
",non-flaky,5
110878,pushtorefresh_storio,InterceptorTest.deleteByQuery,"    @Test
    public void deleteByQuery() {
        storIOSQLite.delete()
                .byQuery(DeleteQuery.builder()
                        .table(TweetTableMeta.TABLE)
                        .build()
                )
                .prepare()
                .executeAsBlocking();
        checkInterceptorsCalls();
    }
",non-flaky,5
110879,pushtorefresh_storio,InterceptorTest.deleteCollectionOfObjects,"    @Test
    public void deleteCollectionOfObjects() {
        storIOSQLite.delete()
                .objects(Collections.singleton(createTweet()))
                .prepare()
                .executeAsBlocking();
        checkInterceptorsCalls();
    }
",non-flaky,5
110880,pushtorefresh_storio,InterceptorTest.deleteObject,"    @Test
    public void deleteObject() {
        storIOSQLite.delete()
                .object(createTweet())
                .prepare()
                .executeAsBlocking();
        checkInterceptorsCalls();
    }
",non-flaky,5
110881,pushtorefresh_storio,InterceptorTest.execSql,"    @Test
    public void execSql() {
        storIOSQLite.executeSQL()
                .withQuery(RawQuery.builder()
                        .query(""select * from "" + TweetTableMeta.TABLE)
                        .build()
                )
                .prepare()
                .executeAsBlocking();
        checkInterceptorsCalls();
    }
",non-flaky,5
110882,pushtorefresh_storio,InterceptorTest.getCursorWithRawQuery,"    @Test
    public void getCursorWithRawQuery() {
        storIOSQLite.get()
                .cursor()
                .withQuery(RawQuery.builder()
                        .query(""select * from "" + TweetTableMeta.TABLE)
                        .build()
                )
                .prepare()
                .executeAsBlocking();
        checkInterceptorsCalls();
    }
",non-flaky,5
110883,pushtorefresh_storio,InterceptorTest.getCursorWithQuery,"    @Test
    public void getCursorWithQuery() {
        storIOSQLite.get()
                .cursor()
                .withQuery(Query.builder()
                        .table(TweetTableMeta.TABLE)
                        .build()
                )
                .prepare()
                .executeAsBlocking();
        checkInterceptorsCalls();
    }
",non-flaky,5
110884,pushtorefresh_storio,InterceptorTest.getListOfObjectsWithRawQuery,"    @Test
    public void getListOfObjectsWithRawQuery() {
        storIOSQLite.get()
                .listOfObjects(Tweet.class)
                .withQuery(RawQuery.builder()
                        .query(""select * from "" + TweetTableMeta.TABLE)
                        .build()
                )
                .prepare()
                .executeAsBlocking();
        checkInterceptorsCalls();
    }
",non-flaky,5
110885,pushtorefresh_storio,InterceptorTest.getListOfObjectsWithQuery,"    @Test
    public void getListOfObjectsWithQuery() {
        storIOSQLite.get()
                .listOfObjects(Tweet.class)
                .withQuery(Query.builder()
                        .table(TweetTableMeta.TABLE)
                        .build()
                )
                .prepare()
                .executeAsBlocking();
        checkInterceptorsCalls();
    }
",non-flaky,5
110886,pushtorefresh_storio,InterceptorTest.getNumberOfResultsWithRawQuery,"    @Test
    public void getNumberOfResultsWithRawQuery() {
        storIOSQLite.get()
                .numberOfResults()
                .withQuery(RawQuery.builder()
                        .query(""select * from "" + TweetTableMeta.TABLE)
                        .build()
                )
                .prepare()
                .executeAsBlocking();
        checkInterceptorsCalls();
    }
",non-flaky,5
110887,pushtorefresh_storio,InterceptorTest.getNumberOfResultsWithQuery,"    @Test
    public void getNumberOfResultsWithQuery() {
        storIOSQLite.get()
                .numberOfResults()
                .withQuery(Query.builder()
                        .table(TweetTableMeta.TABLE)
                        .build()
                )
                .prepare()
                .executeAsBlocking();
        checkInterceptorsCalls();
    }
",non-flaky,5
110888,pushtorefresh_storio,InterceptorTest.getObjectWithRawQuery,"    @Test
    public void getObjectWithRawQuery() {
        storIOSQLite.get()
                .object(Tweet.class)
                .withQuery(RawQuery.builder()
                        .query(""select * from "" + TweetTableMeta.TABLE)
                        .build()
                )
                .prepare()
                .executeAsBlocking();
        checkInterceptorsCalls();
    }
",non-flaky,5
110889,pushtorefresh_storio,InterceptorTest.getObjectWithQuery,"    @Test
    public void getObjectWithQuery() {
        storIOSQLite.get()
                .object(Tweet.class)
                .withQuery(Query.builder()
                        .table(TweetTableMeta.TABLE)
                        .build()
                )
                .prepare()
                .executeAsBlocking();
        checkInterceptorsCalls();
    }
",non-flaky,5
110890,pushtorefresh_storio,InterceptorTest.putCollection,"    @Test
    public void putCollection() {
        storIOSQLite.put()
                .objects(Collections.singleton(createTweet()))
                .prepare()
                .executeAsBlocking();
        checkInterceptorsCalls();
    }
",non-flaky,5
110891,pushtorefresh_storio,InterceptorTest.putContentValues,"    @Test
    public void putContentValues() {
        storIOSQLite.put()
                .contentValues(createContentValues())
                .withPutResolver(createCVPutResolver())
                .prepare()
                .executeAsBlocking();
        checkInterceptorsCalls();
    }
",non-flaky,5
110892,pushtorefresh_storio,InterceptorTest.putContentValuesIterable,"    @Test
    public void putContentValuesIterable() {
        storIOSQLite.put()
                .contentValues(createContentValues(), createContentValues())
                .withPutResolver(createCVPutResolver())
                .prepare()
                .executeAsBlocking();
        checkInterceptorsCalls();
    }
",non-flaky,5
110893,pushtorefresh_storio,InterceptorTest.putObject,"    @Test
    public void putObject() {
        storIOSQLite.put()
                .object(createTweet())
                .prepare()
                .executeAsBlocking();
        checkInterceptorsCalls();
    }
",non-flaky,5
110894,pushtorefresh_storio,InsertTest.insertOne,"    @Test
    public void insertOne() {
        final User user = putUserBlocking();

        // why we created StorIOSQLite: nobody loves nulls
        final Cursor cursor = db.query(UserTableMeta.TABLE, null, null, null, null, null, null);

        // asserting that values was really inserted to db
        assertThat(cursor.getCount()).isEqualTo(1);
        assertThat(cursor.moveToFirst()).isTrue();

        final User insertedUser = UserTableMeta.GET_RESOLVER.mapFromCursor(storIOSQLite, cursor);

        assertThat(insertedUser.id()).isNotNull();
        assertThat(user.equalsExceptId(insertedUser)).isTrue();

        cursor.close();
    }
",non-flaky,5
110895,pushtorefresh_storio,InsertTest.insertCollection,"    @Test
    public void insertCollection() {
        final List<User> users = putUsersBlocking(3);

        // asserting that values was really inserted to db
        final Cursor cursor = db.query(UserTableMeta.TABLE, null, null, null, null, null, null);

        assertThat(cursor.getCount()).isEqualTo(users.size());

        for (int i = 0; i < users.size(); i++) {
            assertThat(cursor.moveToNext()).isTrue();
            assertThat(UserTableMeta.GET_RESOLVER.mapFromCursor(storIOSQLite, cursor)).isEqualTo(users.get(i));
        }

        cursor.close();
    }
",non-flaky,5
110896,pushtorefresh_storio,InsertTest.insertAndDeleteTwice,"    @Test
    public void insertAndDeleteTwice() {
        final User user = TestFactory.newUser();

        for (int i = 0; i < 2; i++) {
            putUserBlocking(user);

            final List<User> existUsers = getAllUsersBlocking();

            assertThat(existUsers).isNotNull();
            assertThat(existUsers).hasSize(1);

            final Cursor cursorAfterPut = db.query(UserTableMeta.TABLE, null, null, null, null, null, null);
            assertThat(cursorAfterPut.getCount()).isEqualTo(1);
            cursorAfterPut.close();

            deleteUserBlocking(user);

            final Cursor cursorAfterDelete = db.query(UserTableMeta.TABLE, null, null, null, null, null, null);
            assertThat(cursorAfterDelete.getCount()).isEqualTo(0);
            cursorAfterDelete.close();
        }
    }
",non-flaky,5
110897,pushtorefresh_storio,InsertTest.insertOneWithNullField,"    @Test
    public void insertOneWithNullField() {
        User user = User.newInstance(null, ""user@example.com"", null); // phone is null
        putUserBlocking(user);

        final Cursor cursor = db.query(UserTableMeta.TABLE, null, null, null, null, null, null);

        // asserting that values was really inserted to db
        assertThat(cursor.getCount()).isEqualTo(1);
        assertThat(cursor.moveToFirst()).isTrue();

        final User insertedUser = UserTableMeta.GET_RESOLVER.mapFromCursor(storIOSQLite, cursor);

        assertThat(insertedUser.id()).isNotNull();
        assertThat(user.equalsExceptId(insertedUser)).isTrue();

        cursor.close();
    }
",non-flaky,5
110898,pushtorefresh_storio,UpdateTest.updateOne,"    @Test
    public void updateOne() {
        final User userForInsert = putUserBlocking();

        final User userForUpdate = User.newInstance(
                userForInsert.id(), // using id of inserted user
                ""new@email.com"" // new value
        );

        updateUserBlocking(userForUpdate);
        checkOnlyOneItemInStorage(userForUpdate);  // update should not add new rows!
    }
",non-flaky,5
110899,pushtorefresh_storio,UpdateTest.updateNullFieldToNotNull,"    @Test
    public void updateNullFieldToNotNull() {
        final User userForInsert = User.newInstance(null, ""user@email.com"", null); // phone is null

        putUserBlocking(userForInsert);

        final User userForUpdate = User.newInstance(
                userForInsert.id(),
                userForInsert.email(),
                ""1-999-547867""  // phone not null
        );

        updateUserBlocking(userForUpdate);
        checkOnlyOneItemInStorage(userForUpdate);
    }
",non-flaky,5
110900,pushtorefresh_storio,UpdateTest.updateNotNullFieldToNull,"    @Test
    public void updateNotNullFieldToNull() {
        final User userForInsert = User.newInstance(null, ""user@email.com"", ""1-999-547867""); // phone not null

        putUserBlocking(userForInsert);

        final User userForUpdate = User.newInstance(
                userForInsert.id(),
                userForInsert.email(),
                null    // phone is null
        );

        updateUserBlocking(userForUpdate);
        checkOnlyOneItemInStorage(userForUpdate);
    }
",non-flaky,5
110901,pushtorefresh_storio,UpdateTest.updateCollection,"    @Test
    public void updateCollection() {
        final List<User> usersForInsert = TestFactory.newUsers(3);

        final PutResults<User> insertResults = storIOSQLite
                .put()
                .objects(usersForInsert)
                .prepare()
                .executeAsBlocking();

        assertThat(insertResults.numberOfInserts()).isEqualTo(usersForInsert.size());

        final List<User> usersForUpdate = new ArrayList<User>(usersForInsert.size());

        for (int i = 0; i < usersForInsert.size(); i++) {
            usersForUpdate.add(User.newInstance(usersForInsert.get(i).id(), ""new"" + i + ""@email.com"" + i));
        }

        final PutResults<User> updateResults = storIOSQLite
                .put()
                .objects(usersForUpdate)
                .prepare()
                .executeAsBlocking();

        assertThat(updateResults.numberOfUpdates()).isEqualTo(usersForUpdate.size());

        final Cursor cursor = db.query(UserTableMeta.TABLE, null, null, null, null, null, null);

        assertThat(cursor.getCount()).isEqualTo(usersForUpdate.size()); // update should not add new rows!

        for (int i = 0; i < usersForUpdate.size(); i++) {
            assertThat(cursor.moveToNext()).isTrue();
            assertThat(UserTableMeta.GET_RESOLVER.mapFromCursor(storIOSQLite, cursor)).isEqualTo(usersForUpdate.get(i));
        }

        cursor.close();
    }
",non-flaky,5
110902,pushtorefresh_storio,ObserveChangesInTableTest.insertEmission,"    @Test
    public void insertEmission() {
        final List<User> users = TestFactory.newUsers(10);

        final Queue<Changes> expectedChanges = new LinkedList<Changes>();
        expectedChanges.add(Changes.newInstance(UserTableMeta.TABLE, UserTableMeta.NOTIFICATION_TAG));

        final EmissionChecker emissionChecker = new EmissionChecker(expectedChanges);
        final Subscription subscription = emissionChecker.subscribe();

        putUsersBlocking(users);

        // Should receive changes of Users table
        emissionChecker.awaitNextExpectedValue();

        emissionChecker.assertThatNoExpectedValuesLeft();

        subscription.unsubscribe();
    }
",non-flaky,5
110903,pushtorefresh_storio,ObserveChangesInTableTest.updateEmission,"    @Test
    public void updateEmission() {
        final List<User> users = putUsersBlocking(10);
        final List<User> updated = new ArrayList<User>(users.size());

        for (User user : users) {
            updated.add(User.newInstance(user.id(), user.email()));
        }

        final Queue<Changes> expectedChanges = new LinkedList<Changes>();
        expectedChanges.add(Changes.newInstance(UserTableMeta.TABLE, UserTableMeta.NOTIFICATION_TAG));

        final EmissionChecker emissionChecker = new EmissionChecker(expectedChanges);
        final Subscription subscription = emissionChecker.subscribe();

        storIOSQLite
                .put()
                .objects(updated)
                .prepare()
                .executeAsBlocking();

        // Should receive changes of Users table
        emissionChecker.awaitNextExpectedValue();

        emissionChecker.assertThatNoExpectedValuesLeft();

        subscription.unsubscribe();
    }
",non-flaky,5
110904,pushtorefresh_storio,ObserveChangesInTableTest.deleteEmission,"    @Test
    public void deleteEmission() {
        final List<User> users = putUsersBlocking(10);

        final Queue<Changes> expectedChanges = new LinkedList<Changes>();
        expectedChanges.add(Changes.newInstance(UserTableMeta.TABLE, UserTableMeta.NOTIFICATION_TAG));

        final EmissionChecker emissionChecker = new EmissionChecker(expectedChanges);
        final Subscription subscription = emissionChecker.subscribe();

        deleteUsersBlocking(users);

        // Should receive changes of Users table
        emissionChecker.awaitNextExpectedValue();

        emissionChecker.assertThatNoExpectedValuesLeft();

        subscription.unsubscribe();
    }
",non-flaky,5
110905,pushtorefresh_storio,ExecSQLTest.shouldReturnQueryInGetData,"    @Test
    public void shouldReturnQueryInGetData() {
        final RawQuery query = RawQuery.builder()
                .query(""DROP TABLE IF EXISTS no_such_table"") // we don't want to really delete table
                .build();
        final PreparedExecuteSQL operation = storIOSQLite
                .executeSQL()
                .withQuery(query)
                .prepare();

        assertThat(operation.getData()).isEqualTo(query);
    }
",non-flaky,5
110906,pushtorefresh_storio,ExecSQLTest.execSQLWithEmptyArgs,"    @Test
    public void execSQLWithEmptyArgs() {
        // Should not throw exceptions!
        storIOSQLite
                .executeSQL()
                .withQuery(RawQuery.builder()
                        .query(""DROP TABLE IF EXISTS no_such_table"") // we don't want to really delete table
                        .build())
                .prepare()
                .executeAsBlocking();
    }
",non-flaky,5
110907,pushtorefresh_storio,ExecSQLTest.shouldPassArgsAsObjects,"    @Test
    public void shouldPassArgsAsObjects() {
        final User user = putUserBlocking();

        assertThat(user.id()).isNotNull();
        //noinspection ConstantConditions
        final long uid = user.id();

        final String query = ""UPDATE "" + UserTableMeta.TABLE
                + "" SET "" + UserTableMeta.COLUMN_ID + "" = MIN("" + UserTableMeta.COLUMN_ID + "", ?)"";

        storIOSQLite
                .executeSQL()
                .withQuery(
                        RawQuery.builder()
                                .query(query)
                                .args(uid - 1)  // as integer is less, as string is greater
                                .build())
                .prepare()
                .executeAsBlocking();

        List<User> users = getAllUsersBlocking();

        assertThat(users.size()).isEqualTo(1);

        // Was updated, because (uid - 1) passed as object, not string, and (uid - 1) < uid.
        assertThat(users.get(0).id()).isEqualTo(uid - 1);
    }
",non-flaky,5
110908,pushtorefresh_storio,GetCursorObserveChangesTest.repeatsOperationWithQueryByChangeOfTable,"    @Test
    public void repeatsOperationWithQueryByChangeOfTable() {
        putUserBlocking();

        TestSubscriber<Cursor> testSubscriber = new TestSubscriber<Cursor>();
        storIOSQLite
                .get()
                .cursor()
                .withQuery(query)
                .prepare()
                .asRxObservable()
                .subscribe(testSubscriber);

        testSubscriber.assertValueCount(1);

        storIOSQLite.lowLevel().notifyAboutChanges(tableChanges);

        testSubscriber.assertValueCount(2);
    }
",non-flaky,5
110909,pushtorefresh_storio,GetCursorObserveChangesTest.repeatsOperationWithRawQueryByChangeOfTable,"    @Test
    public void repeatsOperationWithRawQueryByChangeOfTable() {
        putUserBlocking();

        TestSubscriber<Cursor> testSubscriber = new TestSubscriber<Cursor>();
        storIOSQLite
                .get()
                .cursor()
                .withQuery(rawQuery)
                .prepare()
                .asRxObservable()
                .subscribe(testSubscriber);

        testSubscriber.assertValueCount(1);

        storIOSQLite.lowLevel().notifyAboutChanges(tableChanges);

        testSubscriber.assertValueCount(2);
    }
",non-flaky,5
110910,pushtorefresh_storio,GetCursorObserveChangesTest.repeatsOperationWithQueryByChangeOfTag,"    @Test
    public void repeatsOperationWithQueryByChangeOfTag() {
        putUserBlocking();

        TestSubscriber<Cursor> testSubscriber = new TestSubscriber<Cursor>();
        storIOSQLite
                .get()
                .cursor()
                .withQuery(query)
                .prepare()
                .asRxObservable()
                .subscribe(testSubscriber);

        testSubscriber.assertValueCount(1);

        storIOSQLite.lowLevel().notifyAboutChanges(tagChanges);

        testSubscriber.assertValueCount(2);
    }
",non-flaky,5
110911,pushtorefresh_storio,GetCursorObserveChangesTest.repeatsOperationWithRawQueryByChangeOfTag,"    @Test
    public void repeatsOperationWithRawQueryByChangeOfTag() {
        putUserBlocking();

        TestSubscriber<Cursor> testSubscriber = new TestSubscriber<Cursor>();
        storIOSQLite
                .get()
                .cursor()
                .withQuery(rawQuery)
                .prepare()
                .asRxObservable()
                .subscribe(testSubscriber);

        testSubscriber.assertValueCount(1);

        storIOSQLite.lowLevel().notifyAboutChanges(tagChanges);

        testSubscriber.assertValueCount(2);
    }
",non-flaky,5
110912,pushtorefresh_storio,GetNumberOfResultsObserveChangesTest.repeatsOperationWithQueryByChangeOfTable,"    @Test
    public void repeatsOperationWithQueryByChangeOfTable() {
        putUserBlocking();

        PreparedGetNumberOfResults operation = storIOSQLite
                .get()
                .numberOfResults()
                .withQuery(query)
                .prepare();

        verifyChangesReceived(operation, tableChanges, 1);
    }
",non-flaky,5
110913,pushtorefresh_storio,GetNumberOfResultsObserveChangesTest.repeatsOperationWithRawQueryByChangeOfTable,"    @Test
    public void repeatsOperationWithRawQueryByChangeOfTable() {
        putUserBlocking();

        PreparedGetNumberOfResults operation = storIOSQLite
                .get()
                .numberOfResults()
                .withQuery(rawQuery)
                .prepare();

        verifyChangesReceived(operation, tableChanges, 1);
    }
",non-flaky,5
110914,pushtorefresh_storio,GetNumberOfResultsObserveChangesTest.repeatsOperationWithQueryByChangeOfTag,"    @Test
    public void repeatsOperationWithQueryByChangeOfTag() {
        putUserBlocking();

        PreparedGetNumberOfResults operation = storIOSQLite
                .get()
                .numberOfResults()
                .withQuery(query)
                .prepare();

        verifyChangesReceived(operation, tagChanges, 1);
    }
",non-flaky,5
110915,pushtorefresh_storio,GetNumberOfResultsObserveChangesTest.repeatsOperationWithRawQueryByChangeOfTag,"    @Test
    public void repeatsOperationWithRawQueryByChangeOfTag() {
        putUserBlocking();

        PreparedGetNumberOfResults operation = storIOSQLite
                .get()
                .numberOfResults()
                .withQuery(rawQuery)
                .prepare();

        verifyChangesReceived(operation, tagChanges, 1);
    }
",non-flaky,5
110916,pushtorefresh_storio,AutoParcelTest.insertObject,"    @Test
    public void insertObject() {
        final Book book = Book.builder()
                .id(1)
                .title(""What a great book"")
                .author(""Somebody"")
                .build();

        final PutResult putResult = storIOSQLite
                .put()
                .object(book)
                .prepare()
                .executeAsBlocking();

        assertThat(putResult.wasInserted()).isTrue();

        final List<Book> storedBooks = storIOSQLite
                .get()
                .listOfObjects(Book.class)
                .withQuery(Query.builder()
                        .table(BookTableMeta.TABLE)
                        .build())
                .prepare()
                .executeAsBlocking();

        assertThat(storedBooks).hasSize(1);

        assertThat(storedBooks.get(0)).isEqualTo(book);
    }
",non-flaky,5
110917,pushtorefresh_storio,AutoParcelTest.updateObject,"    @Test
    public void updateObject() {
        final Book book = Book.builder()
                .id(1)
                .title(""What a great book"")
                .author(""Somebody"")
                .build();

        final PutResult putResult1 = storIOSQLite
                .put()
                .object(book)
                .prepare()
                .executeAsBlocking();

        assertThat(putResult1.wasInserted()).isTrue();

        final Book bookWithUpdatedInfo = Book.builder()
                .id(1) // Same id, should be updated
                .title(""Corrected title"")
                .author(""Corrected author"")
                .build();

        final PutResult putResult2 = storIOSQLite
                .put()
                .object(bookWithUpdatedInfo)
                .prepare()
                .executeAsBlocking();

        assertThat(putResult2.wasUpdated()).isTrue();

        final List<Book> storedBooks = storIOSQLite
                .get()
                .listOfObjects(Book.class)
                .withQuery(Query.builder()
                        .table(BookTableMeta.TABLE)
                        .build())
                .prepare()
                .executeAsBlocking();

        assertThat(storedBooks).hasSize(1);

        assertThat(storedBooks.get(0)).isEqualTo(bookWithUpdatedInfo);
    }
",non-flaky,5
110918,pushtorefresh_storio,AutoParcelTest.deleteObject,"    @Test
    public void deleteObject() {
        final Book book = Book.builder()
                .id(1)
                .title(""What a great book"")
                .author(""Somebody"")
                .build();

        final PutResult putResult = storIOSQLite
                .put()
                .object(book)
                .prepare()
                .executeAsBlocking();

        assertThat(putResult.wasInserted()).isTrue();

        final DeleteResult deleteResult = storIOSQLite
                .delete()
                .object(book)
                .prepare()
                .executeAsBlocking();

        assertThat(deleteResult.numberOfRowsDeleted()).isEqualTo(1);

        final List<Book> storedBooks = storIOSQLite
                .get()
                .listOfObjects(Book.class)
                .withQuery(Query.builder()
                        .table(BookTableMeta.TABLE)
                        .build())
                .prepare()
                .executeAsBlocking();

        assertThat(storedBooks).hasSize(0);
    }
",non-flaky,5
110919,pushtorefresh_storio,SQLiteTypeMappingTest.nullPutResolver,"    @Test(expected = NullPointerException.class)
    public void nullPutResolver() {
        SQLiteTypeMapping.builder()
                .putResolver(null)
                .getResolver(mock(GetResolver.class))
                .deleteResolver(mock(DeleteResolver.class))
                .build();
    }
",non-flaky,5
110920,pushtorefresh_storio,SQLiteTypeMappingTest.nullMapFromCursor,"    @Test(expected = NullPointerException.class)
    public void nullMapFromCursor() {
        SQLiteTypeMapping.builder()
                .putResolver(mock(PutResolver.class))
                .getResolver(null)
                .deleteResolver(mock(DeleteResolver.class))
                .build();
    }
",non-flaky,5
110921,pushtorefresh_storio,SQLiteTypeMappingTest.nullMapToDeleteQuery,"    @Test(expected = NullPointerException.class)
    public void nullMapToDeleteQuery() {
        SQLiteTypeMapping.builder()
                .putResolver(mock(PutResolver.class))
                .getResolver(mock(GetResolver.class))
                .deleteResolver(null)
                .build();
    }
",non-flaky,5
110922,pushtorefresh_storio,SQLiteTypeMappingTest.build,"    @Test
    public void build() {
        class TestItem {

        }

        final PutResolver<TestItem> putResolver = mock(PutResolver.class);
        final GetResolver<TestItem> getResolver = mock(GetResolver.class);
        final DeleteResolver<TestItem> deleteResolver = mock(DeleteResolver.class);

        final SQLiteTypeMapping<TestItem> typeMapping = SQLiteTypeMapping.<TestItem>builder()
                .putResolver(putResolver)
                .getResolver(getResolver)
                .deleteResolver(deleteResolver)
                .build();

        assertThat(typeMapping.putResolver()).isSameAs(putResolver);
        assertThat(typeMapping.getResolver()).isSameAs(getResolver);
        assertThat(typeMapping.deleteResolver()).isSameAs(deleteResolver);
    }
",non-flaky,5
110923,pushtorefresh_storio,InsertQueryTest.shouldNotAllowNullTable,"    @Test
    public void shouldNotAllowNullTable() {
        expectedException.expect(NullPointerException.class);
        expectedException.expectMessage(equalTo(""Table name is null or empty""));
        expectedException.expectCause(nullValue(Throwable.class));

        //noinspection ConstantConditions
        InsertQuery.builder().table(null);
    }
",non-flaky,5
110924,pushtorefresh_storio,InsertQueryTest.shouldNotAllowEmptyTable,"    @Test
    public void shouldNotAllowEmptyTable() {
        expectedException.expect(IllegalStateException.class);
        expectedException.expectMessage(equalTo(""Table name is null or empty""));
        expectedException.expectCause(nullValue(Throwable.class));

        InsertQuery.builder().table("""");
    }
",non-flaky,5
110925,pushtorefresh_storio,InsertQueryTest.nullColumnHackShouldBeNullByDefault,"    @Test
    public void nullColumnHackShouldBeNullByDefault() {
        InsertQuery insertQuery = InsertQuery.builder()
                .table(""test_table"")
                .build();

        assertThat(insertQuery.nullColumnHack()).isNull();
    }
",non-flaky,5
110926,pushtorefresh_storio,InsertQueryTest.completeBuilderShouldNotAllowNullTable,"    @Test
    public void completeBuilderShouldNotAllowNullTable() {
        try {
            //noinspection ConstantConditions
            InsertQuery.builder()
                    .table(""test_table"")
                    .table(null);
            failBecauseExceptionWasNotThrown(NullPointerException.class);
        } catch (NullPointerException expected) {
            assertThat(expected)
                    .hasMessage(""Table name is null or empty"")
                    .hasNoCause();
        }
    }
",non-flaky,5
110927,pushtorefresh_storio,InsertQueryTest.completeBuilderShouldNotAllowEmptyTable,"    @Test
    public void completeBuilderShouldNotAllowEmptyTable() {
        try {
            InsertQuery.builder()
                    .table(""test_table"")
                    .table("""");
            failBecauseExceptionWasNotThrown(IllegalStateException.class);
        } catch (IllegalStateException expected) {
            assertThat(expected)
                    .hasMessage(""Table name is null or empty"")
                    .hasNoCause();
        }
    }
",non-flaky,5
110928,pushtorefresh_storio,InsertQueryTest.completeBuilderShouldUpdateTable,"    @Test
    public void completeBuilderShouldUpdateTable() {
        InsertQuery query = InsertQuery.builder()
                .table(""old_table"")
                .table(""new_table"")
                .build();

        assertThat(query.table()).isEqualTo(""new_table"");
    }
",non-flaky,5
110929,pushtorefresh_storio,InsertQueryTest.createdThroughToBuilderQueryShouldBeEqual,"    @Test
    public void createdThroughToBuilderQueryShouldBeEqual() {
        final String table = ""test_table"";
        final String nullColumnHack = ""test_null_column_hack"";
        final String tag = ""test_tag"";

        final InsertQuery firstQuery = InsertQuery.builder()
                .table(table)
                .nullColumnHack(nullColumnHack)
                .affectsTags(tag)
                .build();

        final InsertQuery secondQuery = firstQuery.toBuilder().build();

        assertThat(secondQuery).isEqualTo(firstQuery);
    }
",non-flaky,5
110930,pushtorefresh_storio,InsertQueryTest.affectsTagsCollectionShouldRewrite,"    @Test
    public void affectsTagsCollectionShouldRewrite() {
        InsertQuery insertQuery = InsertQuery.builder()
                .table(""table"")
                .affectsTags(new HashSet<String>((singletonList(""first_call_collection""))))
                .affectsTags(new HashSet<String>((singletonList(""second_call_collection""))))
                .build();

        assertThat(insertQuery.affectsTags()).isEqualTo(singleton(""second_call_collection""));
    }
",non-flaky,5
110931,pushtorefresh_storio,InsertQueryTest.affectsTagsVarargShouldRewrite,"    @Test
    public void affectsTagsVarargShouldRewrite() {
        InsertQuery insertQuery = InsertQuery.builder()
                .table(""table"")
                .affectsTags(""first_call_vararg"")
                .affectsTags(""second_call_vararg"")
                .build();

        assertThat(insertQuery.affectsTags()).isEqualTo(singleton(""second_call_vararg""));
    }
",non-flaky,5
110932,pushtorefresh_storio,InsertQueryTest.affectsTagsCollectionAllowsNull,"    @Test
    public void affectsTagsCollectionAllowsNull() {
        InsertQuery insertQuery = InsertQuery.builder()
                .table(""table"")
                .affectsTags(new HashSet<String>((singletonList(""first_call_collection""))))
                .affectsTags(null)
                .build();

        assertThat(insertQuery.affectsTags()).isEmpty();
    }
",non-flaky,5
110933,pushtorefresh_storio,InsertQueryTest.buildWithNormalValues,"    @Test
    public void buildWithNormalValues() {
        final String table = ""test_table"";
        final String nullColumnHack = ""test_null_column_hack"";
        final Set<String> tags = singleton(""tag"");

        final InsertQuery insertQuery = InsertQuery.builder()
                .table(table)
                .nullColumnHack(nullColumnHack)
                .affectsTags(tags)
                .build();

        assertThat(insertQuery.table()).isEqualTo(table);
        assertThat(insertQuery.nullColumnHack()).isEqualTo(nullColumnHack);
        assertThat(insertQuery.affectsTags()).isEqualTo(tags);
    }
",non-flaky,5
110934,pushtorefresh_storio,InsertQueryTest.shouldNotAllowNullTag,"    @Test
    public void shouldNotAllowNullTag() {
        expectedException.expect(NullPointerException.class);
        expectedException.expectMessage(startsWith(""affectsTag must not be null or empty, affectsTags = ""));
        expectedException.expectCause(nullValue(Throwable.class));

        //noinspection ConstantConditions
        InsertQuery.builder()
                .table(""table"")
                .affectsTags((String) null)
                .build();
    }
",non-flaky,5
110935,pushtorefresh_storio,InsertQueryTest.shouldNotAllowEmptyTag,"    @Test
    public void shouldNotAllowEmptyTag() {
        expectedException.expect(IllegalStateException.class);
        expectedException.expectMessage(startsWith(""affectsTag must not be null or empty, affectsTags = ""));
        expectedException.expectCause(nullValue(Throwable.class));

        //noinspection ConstantConditions
        InsertQuery.builder()
                .table(""table"")
                .affectsTags("""")
                .build();
    }
",non-flaky,5
113858,spring-projects_spring-data-couchbase,FluxTest.beforeEach,"	@BeforeEach
	public void beforeEach() {
		super.beforeEach();
	}
",non-flaky,5
113859,spring-projects_spring-data-couchbase,FluxTest.concatMapCB,"	@Test
	public void concatMapCB() throws Exception {
		System.out.println(""Start concatMapCB"");
		System.out.println(""\n******** Using concatMap() *********"");
		ParallelFlux<GetResult> concat = Flux.fromIterable(keyList).parallel(2).runOn(Schedulers.parallel())
				.concatMap(item -> cbGet(item)
						/* rCollection.get(item) */.doOnSubscribe((x) -> System.out.println("" +"" + rCat.incrementAndGet()))
						.doOnTerminate(() -> System.out.println("" -"" + rCat.decrementAndGet())));
		System.out.println(concat.sequential().collectList().block());
	}
",non-flaky,5
113860,spring-projects_spring-data-couchbase,FluxTest.cbse,"	@Test
	public void cbse() {
		LinkedList<LinkedList<Airport>> listOfLists = new LinkedList<>();
		Airport a = new Airport(UUID.randomUUID().toString(), ""iata"", ""lowp"");
		String last = null;
		for (int i = 0; i < 5; i++) {
			LinkedList<Airport> list = new LinkedList<>();
			for (int j = 0; j < 10; j++) {
				list.add(a.withId(UUID.randomUUID().toString()));
				last = a.getId();
			}
			listOfLists.add(list);
		}
		Flux<Object> af = Flux.fromIterable(listOfLists).concatMap(catalogToStore -> Flux.fromIterable(catalogToStore)
				.parallel(4).runOn(Schedulers.parallel()).concatMap((entity) -> airportRepository.save(entity)));
		List<Object> saved = af.collectList().block();
		System.out.println(""results.size() : "" + saved.size());

		String statement = ""select * from `"" + /*config().bucketname()*/ ""_default"" + ""` where META().id >= '"" + last + ""'"";
		System.out.println(""statement: "" + statement);
		try {
			QueryResult qr = couchbaseTemplate.getCouchbaseClientFactory().getScope().query(statement,
					QueryOptions.queryOptions().profile(QueryProfile.PHASES));
			List<RemoveResult> rr = couchbaseTemplate.removeByQuery(Airport.class)
					.withOptions(QueryOptions.queryOptions().scanConsistency(QueryScanConsistency.REQUEST_PLUS)).all();
			System.out.println(qr.metaData().profile().get());
		} catch (Exception e) {
			e.printStackTrace();
			throw e;
		}
		List<Airport> airports = airportRepository.findAll().collectList().block();
		assertEquals(0, airports.size(), ""should have been all deleted"");
	}
",non-flaky,5
113861,spring-projects_spring-data-couchbase,FluxTest.pairIdAndResult,"	@Test
	public void pairIdAndResult() {
		LinkedList<Airport> list = new LinkedList<>();
		Airport a = new Airport(UUID.randomUUID().toString(), ""iata"", ""lowp"");
		for (int i = 0; i < 5; i++) {
			list.add(a.withId(UUID.randomUUID().toString()));
		}
		Flux<Object> af = Flux.fromIterable(list).concatMap((entity) -> airportRepository.save(entity));
		List<Object> saved = af.collectList().block();
		System.out.println(""results.size() : "" + saved.size());
		Flux<Pair<String, Mono<Airport>>> pairFlux = Flux.fromIterable(list)
				.map((airport) -> Pair.of(airport.getId(), airportRepository.findById(airport.getId())));
		List<Pair<String, Mono<Airport>>> airportPairs = pairFlux.collectList().block();
		for (Pair<String, Mono<Airport>> airportPair : airportPairs) {
			System.out.println(""id: "" + airportPair.getFirst() + "" airport: "" + airportPair.getSecond().block());
		}

	}
",non-flaky,5
113862,spring-projects_spring-data-couchbase,FluxTest.flatMapCB,"	@Test
	public void flatMapCB() throws Exception {
		System.out.println(""Start flatMapCB"");
		ParallelFlux<GetResult> concat = Flux.fromIterable(keyList).parallel(2).runOn(Schedulers.parallel())
				.flatMap(item -> cbGet(item) /* rCollection.get(item) */
						.doOnSubscribe((x) -> System.out.println("" +"" + rCat.incrementAndGet()))
						.doOnTerminate(() -> System.out.println("" -"" + rCat.decrementAndGet())));
		System.out.println(concat.sequential().collectList().block());
	}
",non-flaky,5
113863,spring-projects_spring-data-couchbase,FluxTest.flatMapSyncCB,"	@Test
	public void flatMapSyncCB() throws Exception {
		System.out.println(""Start flatMapSyncCB"");
		System.out.println(""\n******** Using flatSyncMap() *********"");
		ParallelFlux<GetResult> concat = Flux.fromIterable(keyList).parallel(2).runOn(Schedulers.parallel())
				.flatMap(item -> Flux.just(cbGetSync(item) /* collection.get(item) */));
		System.out.println(concat.sequential().collectList().block());
		;
	}
",non-flaky,5
113864,spring-projects_spring-data-couchbase,FluxTest.flatMapVsConcatMapCB2,"	@Test
	public void flatMapVsConcatMapCB2() throws Exception {
		System.out.println(""Start flatMapCB2"");
		System.out.println(""\n******** Using flatMap() *********"");
		ParallelFlux<GetResult> flat = Flux.fromIterable(keyList).parallel(1).runOn(Schedulers.parallel())
				.flatMap(item -> rCollection.get(item).doOnSubscribe((x) -> System.out.println("" +"" + rCat.incrementAndGet()))
						.doOnTerminate(() -> System.out.println("" -"" + rCat.getAndDecrement())));
		System.out.println(flat.sequential().collectList().block());
		System.out.println(""Start concatMapCB"");
		System.out.println(""\n******** Using concatMap() *********"");
		ParallelFlux<GetResult> concat = Flux.fromIterable(keyList).parallel(2).runOn(Schedulers.parallel())
				.concatMap(item -> cbGet(item).doOnSubscribe((x) -> System.out.println("" +"" + rCat.incrementAndGet()))
						.doOnTerminate(() -> System.out.println("" -"" + rCat.getAndDecrement())));
		System.out.println(concat.sequential().collectList().block());
		;
	}
",non-flaky,5
113865,spring-projects_spring-data-couchbase,CouchbaseTemplateQueryIntegrationTests.beforeEach,"	@BeforeEach
	public void beforeEach() {
		super.beforeEach();
		// already setup by JavaIntegrationTests.beforeAll()
		// ApplicationContext ac = new AnnotationConfigApplicationContext(Config.class);
		// couchbaseTemplate = (CouchbaseTemplate) ac.getBean(COUCHBASE_TEMPLATE);
		// reactiveCouchbaseTemplate = (ReactiveCouchbaseTemplate) ac.getBean(REACTIVE_COUCHBASE_TEMPLATE);
		// ensure each test starts with clean state

		couchbaseTemplate.removeByQuery(User.class).all();
		couchbaseTemplate.findByQuery(User.class).withConsistency(QueryScanConsistency.REQUEST_PLUS).all();
	}
",non-flaky,5
113866,spring-projects_spring-data-couchbase,CouchbaseTemplateKeyValueIntegrationTests.beforeEach,"	@BeforeEach
	public void beforeEach() {
		super.beforeEach();
		couchbaseTemplate.removeByQuery(User.class).all();
		couchbaseTemplate.removeByQuery(UserAnnotated.class).all();
		couchbaseTemplate.removeByQuery(UserAnnotated2.class).all();
		couchbaseTemplate.removeByQuery(UserAnnotated3.class).all();
		couchbaseTemplate.removeByQuery(User.class).withConsistency(QueryScanConsistency.REQUEST_PLUS).all();
	}
",non-flaky,5
113867,spring-projects_spring-data-couchbase,ReactiveCouchbaseTemplateKeyValueIntegrationTests.beforeEach,"	@BeforeEach
	public void beforeEach() {
		super.beforeEach();
		List<RemoveResult> r1 = reactiveCouchbaseTemplate.removeByQuery(User.class).all().collectList().block();
		List<RemoveResult> r2 = reactiveCouchbaseTemplate.removeByQuery(UserAnnotated.class).all().collectList().block();
		List<RemoveResult> r3 = reactiveCouchbaseTemplate.removeByQuery(UserAnnotated2.class).all().collectList().block();
	}
",non-flaky,5
113868,spring-projects_spring-data-couchbase,ReactiveCouchbaseTemplateQueryCollectionIntegrationTests.beforeEach,"	@BeforeEach
	public void beforeEach() {
		// first call the super method
		super.beforeEach();
		// then do processing for this class
		couchbaseTemplate.removeByQuery(User.class).inCollection(collectionName).all();
		couchbaseTemplate.findByQuery(User.class).withConsistency(QueryScanConsistency.REQUEST_PLUS)
				.inCollection(collectionName).all();
		couchbaseTemplate.removeByQuery(Airport.class).inScope(scopeName).inCollection(collectionName).all();
		couchbaseTemplate.findByQuery(Airport.class).withConsistency(QueryScanConsistency.REQUEST_PLUS).inScope(scopeName)
				.inCollection(collectionName).all();
		couchbaseTemplate.removeByQuery(Airport.class).inScope(otherScope).inCollection(otherCollection).all();
		couchbaseTemplate.findByQuery(Airport.class).withConsistency(QueryScanConsistency.REQUEST_PLUS).inScope(otherScope)
				.inCollection(otherCollection).all();
	}
",non-flaky,5
113869,spring-projects_spring-data-couchbase,ReactiveCouchbaseTemplateQueryCollectionIntegrationTests.afterEach,"	@AfterEach
	public void afterEach() {
		// first do processing for this class
		couchbaseTemplate.removeByQuery(User.class).inCollection(collectionName).all();
		// query with REQUEST_PLUS to ensure that the remove has completed.
		couchbaseTemplate.findByQuery(User.class).withConsistency(QueryScanConsistency.REQUEST_PLUS)
				.inCollection(collectionName).all();
		// then call the super method
		super.afterEach();
	}
",non-flaky,5
113870,spring-projects_spring-data-couchbase,ReactiveCouchbaseTemplateQueryCollectionIntegrationTests.existsById,"	@Test
	public void existsById() { // 1
		GetOptions options = GetOptions.getOptions().timeout(Duration.ofSeconds(10));
		ExistsOptions existsOptions = ExistsOptions.existsOptions().timeout(Duration.ofSeconds(10));
		Airport saved = template.insertById(Airport.class).inScope(scopeName).inCollection(collectionName).one(vie.withIcao(""low7"")).block();
		try {
			Boolean exists = template.existsById().inScope(scopeName).inCollection(collectionName).withOptions(existsOptions)
					.one(saved.getId()).block();
			assertTrue(exists, ""Airport should exist: "" + saved.getId());
		} finally {
			template.removeById().inScope(scopeName).inCollection(collectionName).one(saved.getId()).block();
		}
	}
",non-flaky,5
113871,spring-projects_spring-data-couchbase,ReactiveCouchbaseTemplateQueryCollectionIntegrationTests.findByAnalytics,"	@Test
	public void findByAnalytics() { // 2
		AnalyticsOptions options = AnalyticsOptions.analyticsOptions().timeout(Duration.ofSeconds(10));
		Airport saved = template.insertById(Airport.class).inScope(scopeName).inCollection(collectionName).one(vie.withIcao(""low8"")).block();
		try {
			List<Airport> found = template.findByAnalytics(Airport.class).inScope(scopeName).inCollection(collectionName)
					.withOptions(options).all().collectList().block();
			assertEquals(saved, found);
		} finally {
			template.removeById().inScope(scopeName).inCollection(collectionName).one(saved.getId()).block();
		}
	}
",non-flaky,5
113872,spring-projects_spring-data-couchbase,ReactiveCouchbaseTemplateQueryCollectionIntegrationTests.findById,"	@Test
	public void findById() { // 3
		GetOptions options = GetOptions.getOptions().timeout(Duration.ofSeconds(10));
		Airport saved = template.insertById(Airport.class).inScope(scopeName).inCollection(collectionName).one(vie.withIcao(""low9"")).block();
		try {
			Airport found = template.findById(Airport.class).inScope(scopeName).inCollection(collectionName)
					.withOptions(options).one(saved.getId()).block();
			assertEquals(saved, found);
		} finally {
			template.removeById().inScope(scopeName).inCollection(collectionName).one(saved.getId()).block();
		}
	}
",non-flaky,5
113873,spring-projects_spring-data-couchbase,ReactiveCouchbaseTemplateQueryCollectionIntegrationTests.findByQuery,"	@Test
	public void findByQuery() { // 4
		QueryOptions options = QueryOptions.queryOptions().timeout(Duration.ofSeconds(10));
		Airport saved = template.insertById(Airport.class).inScope(scopeName).inCollection(collectionName).one(vie.withIcao(""lowa"")).block();
		try {
			List<Airport> found = template.findByQuery(Airport.class).withConsistency(QueryScanConsistency.REQUEST_PLUS)
					.inScope(scopeName).inCollection(collectionName).withOptions(options).all().collectList().block();
			assertEquals(saved.getId(), found.get(0).getId());
		} finally {
			template.removeById().inScope(scopeName).inCollection(collectionName).one(saved.getId()).block();
		}
	}
",non-flaky,5
113874,spring-projects_spring-data-couchbase,ReactiveCouchbaseTemplateQueryCollectionIntegrationTests.findFromReplicasById,"	@Test
	public void findFromReplicasById() { // 5
		GetAnyReplicaOptions options = GetAnyReplicaOptions.getAnyReplicaOptions().timeout(Duration.ofSeconds(10));
		Airport saved = template.insertById(Airport.class).inScope(scopeName).inCollection(collectionName).one(vie.withIcao(""lowb"")).block();
		try {
			Airport found = template.findFromReplicasById(Airport.class).inScope(scopeName).inCollection(collectionName)
					.withOptions(options).any(saved.getId()).block();
			assertEquals(saved, found);
		} finally {
			template.removeById().inScope(scopeName).inCollection(collectionName).one(saved.getId()).block();
		}
	}
",non-flaky,5
113875,spring-projects_spring-data-couchbase,ReactiveCouchbaseTemplateQueryCollectionIntegrationTests.insertById,"	@Test
	public void insertById() { // 6
		InsertOptions options = InsertOptions.insertOptions().timeout(Duration.ofSeconds(10));
		GetOptions getOptions = GetOptions.getOptions().timeout(Duration.ofSeconds(10));
		Airport saved = template.insertById(Airport.class).inScope(scopeName).inCollection(collectionName)
				.withOptions(options).one(vie.withIcao(""lowc"").withId(UUID.randomUUID().toString())).block();
		try {
			Airport found = template.findById(Airport.class).inScope(scopeName).inCollection(collectionName)
					.withOptions(getOptions).one(saved.getId()).block();
			assertEquals(saved, found);
		} finally {
			template.removeById().inScope(scopeName).inCollection(collectionName).one(saved.getId()).block();
		}
	}
",non-flaky,5
113876,spring-projects_spring-data-couchbase,ReactiveCouchbaseTemplateQueryCollectionIntegrationTests.removeById,"	@Test
	public void removeById() { // 7
		RemoveOptions options = RemoveOptions.removeOptions().timeout(Duration.ofSeconds(10));
		Airport saved = template.insertById(Airport.class).inScope(scopeName).inCollection(collectionName).one(vie.withIcao(""lowd"")).block();
		RemoveResult removeResult = template.removeById().inScope(scopeName).inCollection(collectionName)
				.withOptions(options).one(saved.getId()).block();
		assertEquals(saved.getId(), removeResult.getId());
	}
",non-flaky,5
113877,spring-projects_spring-data-couchbase,ReactiveCouchbaseTemplateQueryCollectionIntegrationTests.removeByQuery,"	@Test
	public void removeByQuery() { // 8
		QueryOptions options = QueryOptions.queryOptions().timeout(Duration.ofSeconds(10));
		Airport saved = template.insertById(Airport.class).inScope(scopeName).inCollection(collectionName).one(vie.withIcao(""lowe"")).block();
		List<RemoveResult> removeResults = template.removeByQuery(Airport.class)
				.withConsistency(QueryScanConsistency.REQUEST_PLUS).inScope(scopeName).inCollection(collectionName)
				.withOptions(options).matching(Query.query(QueryCriteria.where(""iata"").is(vie.getIata()))).all().collectList()
				.block();
		assertEquals(saved.getId(), removeResults.get(0).getId());
	}
",non-flaky,5
113878,spring-projects_spring-data-couchbase,ReactiveCouchbaseTemplateQueryCollectionIntegrationTests.replaceById,"	@Test
	public void replaceById() { // 9
		InsertOptions insertOptions = InsertOptions.insertOptions().timeout(Duration.ofSeconds(10));
		ReplaceOptions options = ReplaceOptions.replaceOptions().timeout(Duration.ofSeconds(10));
		GetOptions getOptions = GetOptions.getOptions().timeout(Duration.ofSeconds(10));
		Airport saved = template.insertById(Airport.class).inScope(scopeName).inCollection(collectionName)
				.withOptions(insertOptions).one(vie.withIcao(""lowe"")).block();
		Airport replaced = template.replaceById(Airport.class).inScope(scopeName).inCollection(collectionName)
				.withOptions(options).one(vie.withIcao(""newIcao"")).block();
		try {
			Airport found = template.findById(Airport.class).inScope(scopeName).inCollection(collectionName)
					.withOptions(getOptions).one(saved.getId()).block();
			assertEquals(replaced, found);
		} finally {
			template.removeById().inScope(scopeName).inCollection(collectionName).one(saved.getId()).block();
		}
	}
",non-flaky,5
113879,spring-projects_spring-data-couchbase,ReactiveCouchbaseTemplateQueryCollectionIntegrationTests.upsertById,"	@Test
	public void upsertById() { // 10
		UpsertOptions options = UpsertOptions.upsertOptions().timeout(Duration.ofSeconds(10));
		GetOptions getOptions = GetOptions.getOptions().timeout(Duration.ofSeconds(10));

		Airport saved = template.upsertById(Airport.class).inScope(scopeName).inCollection(collectionName)
				.withOptions(options).one(vie.withIcao(""lowf"")).block();
		try {
			Airport found = template.findById(Airport.class).inScope(scopeName).inCollection(collectionName)
					.withOptions(getOptions).one(saved.getId()).block();
			assertEquals(saved, found);
		} finally {
			template.removeById().inScope(scopeName).inCollection(collectionName).one(saved.getId()).block();
		}
	}
",non-flaky,5
113880,spring-projects_spring-data-couchbase,ReactiveCouchbaseTemplateQueryCollectionIntegrationTests.existsByIdOther,"	@Test
	public void existsByIdOther() { // 1
		GetOptions options = GetOptions.getOptions().timeout(Duration.ofSeconds(10));
		ExistsOptions existsOptions = ExistsOptions.existsOptions().timeout(Duration.ofSeconds(10));
		Airport saved = template.insertById(Airport.class).inScope(otherScope).inCollection(otherCollection).one(vie.withIcao(""lowg""))
				.block();
		try {
			Boolean exists = template.existsById().inScope(otherScope).inCollection(otherCollection)
					.withOptions(existsOptions).one(saved.getId()).block();
			assertTrue(exists, ""Airport should exist: "" + saved.getId());
		} finally {
			template.removeById().inScope(otherScope).inCollection(otherCollection).one(saved.getId()).block();
		}
	}
",non-flaky,5
113881,spring-projects_spring-data-couchbase,ReactiveCouchbaseTemplateQueryCollectionIntegrationTests.findByAnalyticsOther,"	@Test
	public void findByAnalyticsOther() { // 2
		AnalyticsOptions options = AnalyticsOptions.analyticsOptions().timeout(Duration.ofSeconds(10));
		Airport saved = template.insertById(Airport.class).inScope(otherScope).inCollection(otherCollection).one(vie.withIcao(""lowh""))
				.block();
		try {
			List<Airport> found = template.findByAnalytics(Airport.class).inScope(otherScope).inCollection(otherCollection)
					.withOptions(options).all().collectList().block();
			assertEquals(saved, found);
		} finally {
			template.removeById().inScope(otherScope).inCollection(otherCollection).one(saved.getId()).block();
		}
	}
",non-flaky,5
113882,spring-projects_spring-data-couchbase,ReactiveCouchbaseTemplateQueryCollectionIntegrationTests.findByIdOther,"	@Test
	public void findByIdOther() { // 3
		GetOptions options = GetOptions.getOptions().timeout(Duration.ofSeconds(10));
		Airport saved = template.insertById(Airport.class).inScope(otherScope).inCollection(otherCollection).one(vie.withIcao(""lowi""))
				.block();
		try {
			Airport found = template.findById(Airport.class).inScope(otherScope).inCollection(otherCollection)
					.withOptions(options).one(saved.getId()).block();
			assertEquals(saved, found);
		} finally {
			template.removeById().inScope(otherScope).inCollection(otherCollection).one(saved.getId()).block();
		}
	}
",non-flaky,5
113883,spring-projects_spring-data-couchbase,ReactiveCouchbaseTemplateQueryCollectionIntegrationTests.findByQueryOther,"	@Test
	public void findByQueryOther() { // 4
		QueryOptions options = QueryOptions.queryOptions().timeout(Duration.ofSeconds(10));
		Airport saved = template.insertById(Airport.class).inScope(otherScope).inCollection(otherCollection).one(vie.withIcao(""lowj""))
				.block();
		try {
			List<Airport> found = template.findByQuery(Airport.class).withConsistency(QueryScanConsistency.REQUEST_PLUS)
					.inScope(otherScope).inCollection(otherCollection).withOptions(options).all().collectList().block();
			assertEquals(saved.getId(), found.get(0).getId());
		} finally {
			template.removeById().inScope(otherScope).inCollection(otherCollection).one(saved.getId()).block();
		}
	}
",non-flaky,5
113884,spring-projects_spring-data-couchbase,ReactiveCouchbaseTemplateQueryCollectionIntegrationTests.findFromReplicasByIdOther,"	@Test
	public void findFromReplicasByIdOther() { // 5
		GetAnyReplicaOptions options = GetAnyReplicaOptions.getAnyReplicaOptions().timeout(Duration.ofSeconds(10));
		Airport saved = template.insertById(Airport.class).inScope(otherScope).inCollection(otherCollection).one(vie.withIcao(""lowk""))
				.block();
		try {
			Airport found = template.findFromReplicasById(Airport.class).inScope(otherScope).inCollection(otherCollection)
					.withOptions(options).any(saved.getId()).block();
			assertEquals(saved, found);
		} finally {
			template.removeById().inScope(otherScope).inCollection(otherCollection).one(saved.getId()).block();
		}
	}
",non-flaky,5
113885,spring-projects_spring-data-couchbase,ReactiveCouchbaseTemplateQueryCollectionIntegrationTests.insertByIdOther,"	@Test
	public void insertByIdOther() { // 6
		InsertOptions options = InsertOptions.insertOptions().timeout(Duration.ofSeconds(10));
		GetOptions getOptions = GetOptions.getOptions().timeout(Duration.ofSeconds(10));
		Airport saved = template.insertById(Airport.class).inScope(otherScope).inCollection(otherCollection)
				.withOptions(options).one(vie.withIcao(""lowl"").withId(UUID.randomUUID().toString())).block();
		try {
			Airport found = template.findById(Airport.class).inScope(otherScope).inCollection(otherCollection)
					.withOptions(getOptions).one(saved.getId()).block();
			assertEquals(saved, found);
		} finally {
			template.removeById().inScope(otherScope).inCollection(otherCollection).one(saved.getId()).block();
		}
	}
",non-flaky,5
113886,spring-projects_spring-data-couchbase,ReactiveCouchbaseTemplateQueryCollectionIntegrationTests.removeByIdOther,"	@Test
	public void removeByIdOther() { // 7
		RemoveOptions options = RemoveOptions.removeOptions().timeout(Duration.ofSeconds(10));
		Airport saved = template.insertById(Airport.class).inScope(otherScope).inCollection(otherCollection).one(vie.withIcao(""lowm""))
				.block();
		RemoveResult removeResult = template.removeById().inScope(otherScope).inCollection(otherCollection)
				.withOptions(options).one(saved.getId()).block();
		assertEquals(saved.getId(), removeResult.getId());
	}
",non-flaky,5
113887,spring-projects_spring-data-couchbase,ReactiveCouchbaseTemplateQueryCollectionIntegrationTests.removeByQueryOther,"	@Test
	public void removeByQueryOther() { // 8
		QueryOptions options = QueryOptions.queryOptions().timeout(Duration.ofSeconds(10));
		Airport saved = template.insertById(Airport.class).inScope(otherScope).inCollection(otherCollection).one(vie.withIcao(""lown""))
				.block();
		List<RemoveResult> removeResults = template.removeByQuery(Airport.class)
				.withConsistency(QueryScanConsistency.REQUEST_PLUS).inScope(otherScope).inCollection(otherCollection)
				.withOptions(options).matching(Query.query(QueryCriteria.where(""iata"").is(vie.getIata()))).all().collectList()
				.block();
		assertEquals(saved.getId(), removeResults.get(0).getId());
	}
",non-flaky,5
113888,spring-projects_spring-data-couchbase,ReactiveCouchbaseTemplateQueryCollectionIntegrationTests.replaceByIdOther,"	@Test
	public void replaceByIdOther() { // 9
		InsertOptions insertOptions = InsertOptions.insertOptions().timeout(Duration.ofSeconds(10));
		ReplaceOptions options = ReplaceOptions.replaceOptions().timeout(Duration.ofSeconds(10));
		GetOptions getOptions = GetOptions.getOptions().timeout(Duration.ofSeconds(10));
		Airport saved = template.insertById(Airport.class).inScope(otherScope).inCollection(otherCollection)
				.withOptions(insertOptions).one(vie.withIcao(""lown"")).block();
		Airport replaced = template.replaceById(Airport.class).inScope(otherScope).inCollection(otherCollection)
				.withOptions(options).one(vie.withIcao(""newIcao"")).block();
		try {
			Airport found = template.findById(Airport.class).inScope(otherScope).inCollection(otherCollection)
					.withOptions(getOptions).one(saved.getId()).block();
			assertEquals(replaced, found);
		} finally {
			template.removeById().inScope(otherScope).inCollection(otherCollection).one(saved.getId()).block();
		}
	}
",non-flaky,5
113889,spring-projects_spring-data-couchbase,ReactiveCouchbaseTemplateQueryCollectionIntegrationTests.upsertByIdOther,"	@Test
	public void upsertByIdOther() { // 10
		UpsertOptions options = UpsertOptions.upsertOptions().timeout(Duration.ofSeconds(10));
		GetOptions getOptions = GetOptions.getOptions().timeout(Duration.ofSeconds(10));

		Airport saved = template.upsertById(Airport.class).inScope(otherScope).inCollection(otherCollection)
				.withOptions(options).one(vie.withIcao(""lowo"")).block();
		try {
			Airport found = template.findById(Airport.class).inScope(otherScope).inCollection(otherCollection)
					.withOptions(getOptions).one(saved.getId()).block();
			assertEquals(saved, found);
		} finally {
			template.removeById().inScope(otherScope).inCollection(otherCollection).one(saved.getId()).block();
		}
	}
",non-flaky,5
113890,spring-projects_spring-data-couchbase,ReactiveCouchbaseTemplateQueryCollectionIntegrationTests.existsByIdOptions,"	@Test
	public void existsByIdOptions() { // 1 - Options
		ExistsOptions options = ExistsOptions.existsOptions().timeout(Duration.ofNanos(10));
		assertThrows(UnambiguousTimeoutException.class, () -> template.existsById().inScope(otherScope)
				.inCollection(otherCollection).withOptions(options).one(vie.getId()).block());
	}
",non-flaky,5
113891,spring-projects_spring-data-couchbase,ReactiveCouchbaseTemplateQueryCollectionIntegrationTests.findByAnalyticsOptions,"	@Test
	public void findByAnalyticsOptions() { // 2
		AnalyticsOptions options = AnalyticsOptions.analyticsOptions().timeout(Duration.ofNanos(10));
		assertThrows(AmbiguousTimeoutException.class, () -> template.findByAnalytics(Airport.class).inScope(otherScope)
				.inCollection(otherCollection).withOptions(options).all().collectList().block());
	}
",non-flaky,5
113892,spring-projects_spring-data-couchbase,ReactiveCouchbaseTemplateQueryCollectionIntegrationTests.findByIdOptions,"	@Test
	public void findByIdOptions() { // 3
		GetOptions options = GetOptions.getOptions().timeout(Duration.ofNanos(10));
		assertThrows(UnambiguousTimeoutException.class, () -> template.findById(Airport.class).inScope(otherScope)
				.inCollection(otherCollection).withOptions(options).one(vie.getId()).block());
	}
",non-flaky,5
113893,spring-projects_spring-data-couchbase,ReactiveCouchbaseTemplateQueryCollectionIntegrationTests.findByQueryOptions,"	@Test
	public void findByQueryOptions() { // 4
		QueryOptions options = QueryOptions.queryOptions().timeout(Duration.ofNanos(10));
		assertThrows(AmbiguousTimeoutException.class,
				() -> template.findByQuery(Airport.class).withConsistency(QueryScanConsistency.REQUEST_PLUS).inScope(otherScope)
						.inCollection(otherCollection).withOptions(options).all().collectList().block());
	}
",non-flaky,5
113894,spring-projects_spring-data-couchbase,ReactiveCouchbaseTemplateQueryCollectionIntegrationTests.findFromReplicasByIdOptions,"	@Test
	public void findFromReplicasByIdOptions() { // 5
		GetAnyReplicaOptions options = GetAnyReplicaOptions.getAnyReplicaOptions().timeout(Duration.ofNanos(1000));
		Airport saved = template.insertById(Airport.class).inScope(otherScope).inCollection(otherCollection).one(vie)
				.block();
		try {
			Airport found = template.findFromReplicasById(Airport.class).inScope(otherScope).inCollection(otherCollection)
					.withOptions(options).any(saved.getId()).block();
			assertNull(found, ""should not have found document in short timeout"");
		} finally {
			template.removeById().inScope(otherScope).inCollection(otherCollection).one(saved.getId()).block();
		}
	}
",non-flaky,5
113895,spring-projects_spring-data-couchbase,ReactiveCouchbaseTemplateQueryCollectionIntegrationTests.insertByIdOptions,"	@Test
	public void insertByIdOptions() { // 6
		InsertOptions options = InsertOptions.insertOptions().timeout(Duration.ofNanos(10));
		assertThrows(AmbiguousTimeoutException.class, () -> template.insertById(Airport.class).inScope(otherScope)
				.inCollection(otherCollection).withOptions(options).one(vie.withId(UUID.randomUUID().toString())).block());
	}
",non-flaky,5
113896,spring-projects_spring-data-couchbase,ReactiveCouchbaseTemplateQueryCollectionIntegrationTests.removeByIdOptions,"	@Test
	public void removeByIdOptions() { // 7 - options
		Airport saved = template.insertById(Airport.class).inScope(otherScope).inCollection(otherCollection).one(vie)
				.block();
		RemoveOptions options = RemoveOptions.removeOptions().timeout(Duration.ofNanos(10));
		assertThrows(AmbiguousTimeoutException.class, () -> template.removeById().inScope(otherScope)
				.inCollection(otherCollection).withOptions(options).one(vie.getId()).block());

	}
",non-flaky,5
113897,spring-projects_spring-data-couchbase,ReactiveCouchbaseTemplateQueryCollectionIntegrationTests.removeByQueryOptions,"	@Test
	public void removeByQueryOptions() { // 8 - options
		QueryOptions options = QueryOptions.queryOptions().timeout(Duration.ofNanos(10));
		assertThrows(AmbiguousTimeoutException.class,
				() -> template.removeByQuery(Airport.class).withConsistency(QueryScanConsistency.REQUEST_PLUS)
						.inScope(otherScope).inCollection(otherCollection).withOptions(options)
						.matching(Query.query(QueryCriteria.where(""iata"").is(vie.getIata()))).all().collectList().block());
	}
",non-flaky,5
113898,spring-projects_spring-data-couchbase,ReactiveCouchbaseTemplateQueryCollectionIntegrationTests.replaceByIdOptions,"	@Test
	public void replaceByIdOptions() { // 9 - options
		ReplaceOptions options = ReplaceOptions.replaceOptions().timeout(Duration.ofNanos(10));
		assertThrows(AmbiguousTimeoutException.class, () -> template.replaceById(Airport.class).inScope(otherScope)
				.inCollection(otherCollection).withOptions(options).one(vie.withIcao(""newIcao"")).block());
	}
",non-flaky,5
113899,spring-projects_spring-data-couchbase,ReactiveCouchbaseTemplateQueryCollectionIntegrationTests.upsertByIdOptions,"	@Test
	public void upsertByIdOptions() { // 10 - options
		UpsertOptions options = UpsertOptions.upsertOptions().timeout(Duration.ofNanos(10));
		assertThrows(AmbiguousTimeoutException.class, () -> template.upsertById(Airport.class).inScope(otherScope)
				.inCollection(otherCollection).withOptions(options).one(vie).block());
	}
",non-flaky,5
113900,spring-projects_spring-data-couchbase,QueryCriteriaTests.testNullValue,"	@Test
	public void testNullValue() {
		QueryCriteria c = where(i(""name"")).is(null);
		assertEquals(""`name` = null"", c.export());
	}
",non-flaky,5
113901,spring-projects_spring-data-couchbase,CouchbaseTemplateQueryCollectionIntegrationTests.beforeEach,"	@BeforeEach
	public void beforeEach() {
		// first call the super method
		super.beforeEach();
		// then do processing for this class
		couchbaseTemplate.removeByQuery(User.class).inCollection(collectionName).all();
		couchbaseTemplate.findByQuery(User.class).withConsistency(QueryScanConsistency.REQUEST_PLUS)
				.inCollection(collectionName).all();
		couchbaseTemplate.removeByQuery(Airport.class).inScope(scopeName).inCollection(collectionName).all();
		couchbaseTemplate.findByQuery(Airport.class).withConsistency(QueryScanConsistency.REQUEST_PLUS).inScope(scopeName)
				.inCollection(collectionName).all();
		couchbaseTemplate.removeByQuery(Airport.class).inScope(otherScope).inCollection(otherCollection).all();
		couchbaseTemplate.findByQuery(Airport.class).withConsistency(QueryScanConsistency.REQUEST_PLUS).inScope(otherScope)
				.inCollection(otherCollection).all();
	}
",non-flaky,5
113902,spring-projects_spring-data-couchbase,CouchbaseTemplateQueryCollectionIntegrationTests.afterEach,"	@AfterEach
	public void afterEach() {
		// first do processing for this class
		couchbaseTemplate.removeByQuery(User.class).inCollection(collectionName).all();
		// query with REQUEST_PLUS to ensure that the remove has completed.
		couchbaseTemplate.findByQuery(User.class).withConsistency(QueryScanConsistency.REQUEST_PLUS)
				.inCollection(collectionName).all();
		// then call the super method
		super.afterEach();
	}
",non-flaky,5
113903,spring-projects_spring-data-couchbase,CouchbaseTemplateQueryCollectionIntegrationTests.existsById,"	@Test
	public void existsById() { // 1
		GetOptions options = GetOptions.getOptions().timeout(Duration.ofSeconds(10));
		ExistsOptions existsOptions = ExistsOptions.existsOptions().timeout(Duration.ofSeconds(10));
		Airport saved = couchbaseTemplate.insertById(Airport.class).inScope(scopeName).inCollection(collectionName)
				.one(vie);
		try {
			Boolean exists = couchbaseTemplate.existsById().inScope(scopeName).inCollection(collectionName)
					.withOptions(existsOptions).one(saved.getId());
			assertTrue(exists, ""Airport should exist: "" + saved.getId());
		} finally {
			couchbaseTemplate.removeById().inScope(scopeName).inCollection(collectionName).one(saved.getId());
		}
	}
",non-flaky,5
113904,spring-projects_spring-data-couchbase,CouchbaseTemplateQueryCollectionIntegrationTests.findByAnalytics,"	@Test
	public void findByAnalytics() { // 2
		AnalyticsOptions options = AnalyticsOptions.analyticsOptions().timeout(Duration.ofSeconds(10));
		Airport saved = couchbaseTemplate.insertById(Airport.class).inScope(scopeName).inCollection(collectionName)
				.one(vie);
		try {
			List<Airport> found = couchbaseTemplate.findByAnalytics(Airport.class).inScope(scopeName)
					.inCollection(collectionName).withOptions(options).all();
			assertEquals(saved, found);
		} finally {
			couchbaseTemplate.removeById().inScope(scopeName).inCollection(collectionName).one(saved.getId());
		}
	}
",non-flaky,5
113905,spring-projects_spring-data-couchbase,CouchbaseTemplateQueryCollectionIntegrationTests.findById,"	@Test
	public void findById() { // 3
		GetOptions options = GetOptions.getOptions().timeout(Duration.ofSeconds(10));
		Airport saved = couchbaseTemplate.insertById(Airport.class).inScope(scopeName).inCollection(collectionName)
				.one(vie);
		try {
			Airport found = couchbaseTemplate.findById(Airport.class).inScope(scopeName).inCollection(collectionName)
					.withOptions(options).one(saved.getId());
			assertEquals(saved, found);
		} finally {
			couchbaseTemplate.removeById().inScope(scopeName).inCollection(collectionName).one(saved.getId());
		}
	}
",non-flaky,5
113906,spring-projects_spring-data-couchbase,CouchbaseTemplateQueryCollectionIntegrationTests.findByQuery,"	@Test
	public void findByQuery() { // 4
		QueryOptions options = QueryOptions.queryOptions().timeout(Duration.ofSeconds(10));
		Airport saved = couchbaseTemplate.insertById(Airport.class).inScope(scopeName).inCollection(collectionName)
				.one(vie);
		try {
			List<Airport> found = couchbaseTemplate.findByQuery(Airport.class)
					.withConsistency(QueryScanConsistency.REQUEST_PLUS).inScope(scopeName).inCollection(collectionName)
					.withOptions(options).all();
			assertEquals(saved.getId(), found.get(0).getId());
		} finally {
			couchbaseTemplate.removeById().inScope(scopeName).inCollection(collectionName).one(saved.getId());
		}
	}
",non-flaky,5
113907,spring-projects_spring-data-couchbase,CouchbaseTemplateQueryCollectionIntegrationTests.findFromReplicasById,"	@Test
	public void findFromReplicasById() { // 5
		GetAnyReplicaOptions options = GetAnyReplicaOptions.getAnyReplicaOptions().timeout(Duration.ofSeconds(10));
		Airport saved = couchbaseTemplate.insertById(Airport.class).inScope(scopeName).inCollection(collectionName)
				.one(vie);
		try {
			Airport found = couchbaseTemplate.findFromReplicasById(Airport.class).inScope(scopeName)
					.inCollection(collectionName).withOptions(options).any(saved.getId());
			assertEquals(saved, found);
		} finally {
			couchbaseTemplate.removeById().inScope(scopeName).inCollection(collectionName).one(saved.getId());
		}
	}
",non-flaky,5
113908,spring-projects_spring-data-couchbase,CouchbaseTemplateQueryCollectionIntegrationTests.insertById,"	@Test
	public void insertById() { // 6
		InsertOptions options = InsertOptions.insertOptions().timeout(Duration.ofSeconds(10));
		GetOptions getOptions = GetOptions.getOptions().timeout(Duration.ofSeconds(10));
		Airport saved = couchbaseTemplate.insertById(Airport.class).inScope(scopeName).inCollection(collectionName)
				.withOptions(options).one(vie.withId(UUID.randomUUID().toString()));
		try {
			Airport found = couchbaseTemplate.findById(Airport.class).inScope(scopeName).inCollection(collectionName)
					.withOptions(getOptions).one(saved.getId());
			assertEquals(saved, found);
		} finally {
			couchbaseTemplate.removeById().inScope(scopeName).inCollection(collectionName).one(saved.getId());
		}
	}
",non-flaky,5
113909,spring-projects_spring-data-couchbase,CouchbaseTemplateQueryCollectionIntegrationTests.removeById,"	@Test
	public void removeById() { // 7
		RemoveOptions options = RemoveOptions.removeOptions().timeout(Duration.ofSeconds(10));
		Airport saved = couchbaseTemplate.insertById(Airport.class).inScope(scopeName).inCollection(collectionName)
				.one(vie);
		RemoveResult removeResult = couchbaseTemplate.removeById().inScope(scopeName).inCollection(collectionName)
				.withOptions(options).one(saved.getId());
		assertEquals(saved.getId(), removeResult.getId());
	}
",non-flaky,5
113910,spring-projects_spring-data-couchbase,CouchbaseTemplateQueryCollectionIntegrationTests.removeByQuery,"	@Test
	public void removeByQuery() { // 8
		QueryOptions options = QueryOptions.queryOptions().timeout(Duration.ofSeconds(10));
		Airport saved = couchbaseTemplate.insertById(Airport.class).inScope(scopeName).inCollection(collectionName)
				.one(vie);
		List<RemoveResult> removeResults = couchbaseTemplate.removeByQuery(Airport.class)
				.withConsistency(QueryScanConsistency.REQUEST_PLUS).inScope(scopeName).inCollection(collectionName)
				.withOptions(options).matching(Query.query(QueryCriteria.where(""iata"").is(vie.getIata()))).all();
		assertEquals(saved.getId(), removeResults.get(0).getId());
	}
",non-flaky,5
113911,spring-projects_spring-data-couchbase,CouchbaseTemplateQueryCollectionIntegrationTests.replaceById,"	@Test
	public void replaceById() { // 9
		InsertOptions insertOptions = InsertOptions.insertOptions().timeout(Duration.ofSeconds(10));
		ReplaceOptions options = ReplaceOptions.replaceOptions().timeout(Duration.ofSeconds(10));
		GetOptions getOptions = GetOptions.getOptions().timeout(Duration.ofSeconds(10));
		Airport saved = couchbaseTemplate.insertById(Airport.class).inScope(scopeName).inCollection(collectionName)
				.withOptions(insertOptions).one(vie);
		Airport replaced = couchbaseTemplate.replaceById(Airport.class).inScope(scopeName).inCollection(collectionName)
				.withOptions(options).one(vie.withIcao(""newIcao""));
		try {
			Airport found = couchbaseTemplate.findById(Airport.class).inScope(scopeName).inCollection(collectionName)
					.withOptions(getOptions).one(saved.getId());
			assertEquals(replaced, found);
		} finally {
			couchbaseTemplate.removeById().inScope(scopeName).inCollection(collectionName).one(saved.getId());
		}
	}
",non-flaky,5
113912,spring-projects_spring-data-couchbase,CouchbaseTemplateQueryCollectionIntegrationTests.upsertById,"	@Test
	public void upsertById() { // 10
		UpsertOptions options = UpsertOptions.upsertOptions().timeout(Duration.ofSeconds(10));
		GetOptions getOptions = GetOptions.getOptions().timeout(Duration.ofSeconds(10));

		Airport saved = couchbaseTemplate.upsertById(Airport.class).inScope(scopeName).inCollection(collectionName)
				.withOptions(options).one(vie);
		try {
			Airport found = couchbaseTemplate.findById(Airport.class).inScope(scopeName).inCollection(collectionName)
					.withOptions(getOptions).one(saved.getId());
			assertEquals(saved, found);
		} finally {
			couchbaseTemplate.removeById().inScope(scopeName).inCollection(collectionName).one(saved.getId());
		}
	}
",non-flaky,5
113913,spring-projects_spring-data-couchbase,CouchbaseTemplateQueryCollectionIntegrationTests.existsByIdOther,"	@Test
	public void existsByIdOther() { // 1
		GetOptions options = GetOptions.getOptions().timeout(Duration.ofSeconds(10));
		ExistsOptions existsOptions = ExistsOptions.existsOptions().timeout(Duration.ofSeconds(10));
		Airport saved = couchbaseTemplate.insertById(Airport.class).inScope(otherScope).inCollection(otherCollection)
				.one(vie);
		try {
			Boolean exists = couchbaseTemplate.existsById().inScope(otherScope).inCollection(otherCollection)
					.withOptions(existsOptions).one(saved.getId());
			assertTrue(exists, ""Airport should exist: "" + saved.getId());
		} finally {
			couchbaseTemplate.removeById().inScope(otherScope).inCollection(otherCollection).one(saved.getId());
		}
	}
",non-flaky,5
113914,spring-projects_spring-data-couchbase,CouchbaseTemplateQueryCollectionIntegrationTests.findByAnalyticsOther,"	@Test
	public void findByAnalyticsOther() { // 2
		AnalyticsOptions options = AnalyticsOptions.analyticsOptions().timeout(Duration.ofSeconds(10));
		Airport saved = couchbaseTemplate.insertById(Airport.class).inScope(otherScope).inCollection(otherCollection)
				.one(vie);
		try {
			List<Airport> found = couchbaseTemplate.findByAnalytics(Airport.class).inScope(otherScope)
					.inCollection(otherCollection).withOptions(options).all();
			assertEquals(saved, found);
		} finally {
			couchbaseTemplate.removeById().inScope(otherScope).inCollection(otherCollection).one(saved.getId());
		}
	}
",non-flaky,5
113915,spring-projects_spring-data-couchbase,CouchbaseTemplateQueryCollectionIntegrationTests.findByIdOther,"	@Test
	public void findByIdOther() { // 3
		GetOptions options = GetOptions.getOptions().timeout(Duration.ofSeconds(10));
		Airport saved = couchbaseTemplate.insertById(Airport.class).inScope(otherScope).inCollection(otherCollection)
				.one(vie);
		try {
			Airport found = couchbaseTemplate.findById(Airport.class).inScope(otherScope).inCollection(otherCollection)
					.withOptions(options).one(saved.getId());
			assertEquals(saved, found);
		} finally {
			couchbaseTemplate.removeById().inScope(otherScope).inCollection(otherCollection).one(saved.getId());
		}
	}
",non-flaky,5
113916,spring-projects_spring-data-couchbase,CouchbaseTemplateQueryCollectionIntegrationTests.findByQueryOther,"	@Test
	public void findByQueryOther() { // 4
		QueryOptions options = QueryOptions.queryOptions().timeout(Duration.ofSeconds(10));
		Airport saved = couchbaseTemplate.insertById(Airport.class).inScope(otherScope).inCollection(otherCollection)
				.one(vie);
		try {
			List<Airport> found = couchbaseTemplate.findByQuery(Airport.class)
					.withConsistency(QueryScanConsistency.REQUEST_PLUS).inScope(otherScope).inCollection(otherCollection)
					.withOptions(options).all();
			assertEquals(saved.getId(), found.get(0).getId());
		} finally {
			couchbaseTemplate.removeById().inScope(otherScope).inCollection(otherCollection).one(saved.getId());
		}
	}
",non-flaky,5
113917,spring-projects_spring-data-couchbase,CouchbaseTemplateQueryCollectionIntegrationTests.findFromReplicasByIdOther,"	@Test
	public void findFromReplicasByIdOther() { // 5
		GetAnyReplicaOptions options = GetAnyReplicaOptions.getAnyReplicaOptions().timeout(Duration.ofSeconds(10));
		Airport saved = couchbaseTemplate.insertById(Airport.class).inScope(otherScope).inCollection(otherCollection)
				.one(vie);
		try {
			Airport found = couchbaseTemplate.findFromReplicasById(Airport.class).inScope(otherScope)
					.inCollection(otherCollection).withOptions(options).any(saved.getId());
			assertEquals(saved, found);
		} finally {
			couchbaseTemplate.removeById().inScope(otherScope).inCollection(otherCollection).one(saved.getId());
		}
	}
",non-flaky,5
113918,spring-projects_spring-data-couchbase,CouchbaseTemplateQueryCollectionIntegrationTests.insertByIdOther,"	@Test
	public void insertByIdOther() { // 6
		InsertOptions options = InsertOptions.insertOptions().timeout(Duration.ofSeconds(10));
		GetOptions getOptions = GetOptions.getOptions().timeout(Duration.ofSeconds(10));
		Airport saved = couchbaseTemplate.insertById(Airport.class).inScope(otherScope).inCollection(otherCollection)
				.withOptions(options).one(vie.withId(UUID.randomUUID().toString()));
		try {
			Airport found = couchbaseTemplate.findById(Airport.class).inScope(otherScope).inCollection(otherCollection)
					.withOptions(getOptions).one(saved.getId());
			assertEquals(saved, found);
		} finally {
			couchbaseTemplate.removeById().inScope(otherScope).inCollection(otherCollection).one(saved.getId());
		}
	}
",non-flaky,5
113919,spring-projects_spring-data-couchbase,CouchbaseTemplateQueryCollectionIntegrationTests.removeByIdOther,"	@Test
	public void removeByIdOther() { // 7
		RemoveOptions options = RemoveOptions.removeOptions().timeout(Duration.ofSeconds(10));
		Airport saved = couchbaseTemplate.insertById(Airport.class).inScope(otherScope).inCollection(otherCollection)
				.one(vie);
		RemoveResult removeResult = couchbaseTemplate.removeById().inScope(otherScope).inCollection(otherCollection)
				.withOptions(options).one(saved.getId());
		assertEquals(saved.getId(), removeResult.getId());
	}
",non-flaky,5
113920,spring-projects_spring-data-couchbase,CouchbaseTemplateQueryCollectionIntegrationTests.removeByQueryOther,"	@Test
	public void removeByQueryOther() { // 8
		QueryOptions options = QueryOptions.queryOptions().timeout(Duration.ofSeconds(10));
		Airport saved = couchbaseTemplate.insertById(Airport.class).inScope(otherScope).inCollection(otherCollection)
				.one(vie);
		List<RemoveResult> removeResults = couchbaseTemplate.removeByQuery(Airport.class)
				.withConsistency(QueryScanConsistency.REQUEST_PLUS).inScope(otherScope).inCollection(otherCollection)
				.withOptions(options).matching(Query.query(QueryCriteria.where(""iata"").is(vie.getIata()))).all();
		assertEquals(saved.getId(), removeResults.get(0).getId());
	}
",non-flaky,5
113921,spring-projects_spring-data-couchbase,CouchbaseTemplateQueryCollectionIntegrationTests.replaceByIdOther,"	@Test
	public void replaceByIdOther() { // 9
		InsertOptions insertOptions = InsertOptions.insertOptions().timeout(Duration.ofSeconds(10));
		ReplaceOptions options = ReplaceOptions.replaceOptions().timeout(Duration.ofSeconds(10));
		GetOptions getOptions = GetOptions.getOptions().timeout(Duration.ofSeconds(10));
		Airport saved = couchbaseTemplate.insertById(Airport.class).inScope(otherScope).inCollection(otherCollection)
				.withOptions(insertOptions).one(vie);
		Airport replaced = couchbaseTemplate.replaceById(Airport.class).inScope(otherScope).inCollection(otherCollection)
				.withOptions(options).one(vie.withIcao(""newIcao""));
		try {
			Airport found = couchbaseTemplate.findById(Airport.class).inScope(otherScope).inCollection(otherCollection)
					.withOptions(getOptions).one(saved.getId());
			assertEquals(replaced, found);
		} finally {
			couchbaseTemplate.removeById().inScope(otherScope).inCollection(otherCollection).one(saved.getId());
		}
	}
",non-flaky,5
113922,spring-projects_spring-data-couchbase,CouchbaseTemplateQueryCollectionIntegrationTests.upsertByIdOther,"	@Test
	public void upsertByIdOther() { // 10
		UpsertOptions options = UpsertOptions.upsertOptions().timeout(Duration.ofSeconds(10));
		GetOptions getOptions = GetOptions.getOptions().timeout(Duration.ofSeconds(10));

		Airport saved = couchbaseTemplate.upsertById(Airport.class).inScope(otherScope).inCollection(otherCollection)
				.withOptions(options).one(vie);
		try {
			Airport found = couchbaseTemplate.findById(Airport.class).inScope(otherScope).inCollection(otherCollection)
					.withOptions(getOptions).one(saved.getId());
			assertEquals(saved, found);
		} finally {
			couchbaseTemplate.removeById().inScope(otherScope).inCollection(otherCollection).one(saved.getId());
		}
	}
",non-flaky,5
113923,spring-projects_spring-data-couchbase,CouchbaseTemplateQueryCollectionIntegrationTests.existsByIdOptions,"	@Test
	public void existsByIdOptions() { // 1 - Options
		ExistsOptions options = ExistsOptions.existsOptions().timeout(Duration.ofNanos(10));
		assertThrows(UnambiguousTimeoutException.class, () -> couchbaseTemplate.existsById().inScope(otherScope)
				.inCollection(otherCollection).withOptions(options).one(vie.getId()));
	}
",non-flaky,5
113924,spring-projects_spring-data-couchbase,CouchbaseTemplateQueryCollectionIntegrationTests.findByAnalyticsOptions,"	@Test
	public void findByAnalyticsOptions() { // 2
		AnalyticsOptions options = AnalyticsOptions.analyticsOptions().timeout(Duration.ofNanos(10));
		assertThrows(AmbiguousTimeoutException.class, () -> couchbaseTemplate.findByAnalytics(Airport.class)
				.inScope(otherScope).inCollection(otherCollection).withOptions(options).all());
	}
",non-flaky,5
113925,spring-projects_spring-data-couchbase,CouchbaseTemplateQueryCollectionIntegrationTests.findByIdOptions,"	@Test
	public void findByIdOptions() { // 3
		GetOptions options = GetOptions.getOptions().timeout(Duration.ofNanos(10));
		assertThrows(UnambiguousTimeoutException.class, () -> couchbaseTemplate.findById(Airport.class).inScope(otherScope)
				.inCollection(otherCollection).withOptions(options).one(vie.getId()));
	}
",non-flaky,5
113926,spring-projects_spring-data-couchbase,CouchbaseTemplateQueryCollectionIntegrationTests.findByQueryOptions,"	@Test
	public void findByQueryOptions() { // 4
		QueryOptions options = QueryOptions.queryOptions().timeout(Duration.ofNanos(10));
		assertThrows(AmbiguousTimeoutException.class,
				() -> couchbaseTemplate.findByQuery(Airport.class).withConsistency(QueryScanConsistency.REQUEST_PLUS)
						.inScope(otherScope).inCollection(otherCollection).withOptions(options).all());
	}
",non-flaky,5
113927,spring-projects_spring-data-couchbase,CouchbaseTemplateQueryCollectionIntegrationTests.findFromReplicasByIdOptions,"	@Test
	public void findFromReplicasByIdOptions() { // 5
		GetAnyReplicaOptions options = GetAnyReplicaOptions.getAnyReplicaOptions().timeout(Duration.ofNanos(1000));
		Airport saved = couchbaseTemplate.insertById(Airport.class).inScope(otherScope).inCollection(otherCollection)
				.one(vie);
		try {
			Airport found = couchbaseTemplate.findFromReplicasById(Airport.class).inScope(otherScope)
					.inCollection(otherCollection).withOptions(options).any(saved.getId());
			assertNull(found, ""should not have found document in short timeout"");
		} finally {
			couchbaseTemplate.removeById().inScope(otherScope).inCollection(otherCollection).one(saved.getId());
		}
	}
",non-flaky,5
113928,spring-projects_spring-data-couchbase,CouchbaseTemplateQueryCollectionIntegrationTests.insertByIdOptions,"	@Test
	public void insertByIdOptions() { // 6
		InsertOptions options = InsertOptions.insertOptions().timeout(Duration.ofNanos(10));
		assertThrows(AmbiguousTimeoutException.class, () -> couchbaseTemplate.insertById(Airport.class).inScope(otherScope)
				.inCollection(otherCollection).withOptions(options).one(vie.withId(UUID.randomUUID().toString())));
	}
",non-flaky,5
113929,spring-projects_spring-data-couchbase,CouchbaseTemplateQueryCollectionIntegrationTests.removeByIdOptions,"	@Test
	public void removeByIdOptions() { // 7 - options
		Airport saved = couchbaseTemplate.insertById(Airport.class).inScope(otherScope).inCollection(otherCollection)
				.one(vie);
		RemoveOptions options = RemoveOptions.removeOptions().timeout(Duration.ofNanos(10));
		assertThrows(AmbiguousTimeoutException.class, () -> couchbaseTemplate.removeById().inScope(otherScope)
				.inCollection(otherCollection).withOptions(options).one(vie.getId()));

	}
",non-flaky,5
113930,spring-projects_spring-data-couchbase,CouchbaseTemplateQueryCollectionIntegrationTests.removeByQueryOptions,"	@Test
	public void removeByQueryOptions() { // 8 - options
		QueryOptions options = QueryOptions.queryOptions().timeout(Duration.ofNanos(10));
		assertThrows(AmbiguousTimeoutException.class,
				() -> couchbaseTemplate.removeByQuery(Airport.class).withConsistency(QueryScanConsistency.REQUEST_PLUS)
						.inScope(otherScope).inCollection(otherCollection).withOptions(options)
						.matching(Query.query(QueryCriteria.where(""iata"").is(vie.getIata()))).all());
	}
",non-flaky,5
113931,spring-projects_spring-data-couchbase,CouchbaseTemplateQueryCollectionIntegrationTests.replaceByIdOptions,"	@Test
	public void replaceByIdOptions() { // 9 - options
		ReplaceOptions options = ReplaceOptions.replaceOptions().timeout(Duration.ofNanos(10));
		assertThrows(AmbiguousTimeoutException.class, () -> couchbaseTemplate.replaceById(Airport.class).inScope(otherScope)
				.inCollection(otherCollection).withOptions(options).one(vie.withIcao(""newIcao"")));
	}
",non-flaky,5
113932,spring-projects_spring-data-couchbase,CouchbaseTemplateQueryCollectionIntegrationTests.upsertByIdOptions,"	@Test
	public void upsertByIdOptions() { // 10 - options
		UpsertOptions options = UpsertOptions.upsertOptions().timeout(Duration.ofNanos(10));
		assertThrows(AmbiguousTimeoutException.class, () -> couchbaseTemplate.upsertById(Airport.class).inScope(otherScope)
				.inCollection(otherCollection).withOptions(options).one(vie));
	}
",non-flaky,5
113933,spring-projects_spring-data-couchbase,CouchbaseTemplateQueryCollectionIntegrationTests.testScopeCollectionAnnotation,"	@Test
	public void testScopeCollectionAnnotation() {
		UserCol user = new UserCol(""1"", ""Dave"", ""Wilson"");
		Query query = Query.query(QueryCriteria.where(""firstname"").is(user.getFirstname()));
		try {
			UserCol saved = couchbaseTemplate.insertById(UserCol.class).inScope(scopeName).inCollection(collectionName)
					.one(user);
			List<UserCol> found = couchbaseTemplate.findByQuery(UserCol.class)
					.withConsistency(QueryScanConsistency.REQUEST_PLUS).inScope(scopeName).inCollection(collectionName)
					.matching(query).all();
			assertEquals(saved, found.get(0), ""should have found what was saved"");
			List<UserCol> notfound = couchbaseTemplate.findByQuery(UserCol.class).inScope(CollectionIdentifier.DEFAULT_SCOPE)
					.inCollection(CollectionIdentifier.DEFAULT_COLLECTION).matching(query).all();
			assertEquals(0, notfound.size(), ""should not have found what was saved"");
			couchbaseTemplate.removeByQuery(UserCol.class).inScope(scopeName).inCollection(collectionName).matching(query)
					.all();
		} finally {
			try {
				couchbaseTemplate.removeByQuery(UserCol.class).inScope(scopeName).inCollection(collectionName).matching(query)
						.all();
			} catch (DataRetrievalFailureException drfe) {}
		}
	}
",non-flaky,5
113934,spring-projects_spring-data-couchbase,CouchbaseTemplateQueryCollectionIntegrationTests.testScopeCollectionRepoWith,"	@Test
	public void testScopeCollectionRepoWith() {
		UserCol user = new UserCol(""1"", ""Dave"", ""Wilson"");
		Query query = Query.query(QueryCriteria.where(""firstname"").is(user.getFirstname()));
		try {
			UserCol saved = couchbaseTemplate.insertById(UserCol.class).inScope(scopeName).inCollection(collectionName)
					.one(user);
			List<UserCol> found = couchbaseTemplate.findByQuery(UserCol.class)
					.withConsistency(QueryScanConsistency.REQUEST_PLUS).inScope(scopeName).inCollection(collectionName)
					.matching(query).all();
			assertEquals(saved, found.get(0), ""should have found what was saved"");
			List<UserCol> notfound = couchbaseTemplate.findByQuery(UserCol.class).inScope(CollectionIdentifier.DEFAULT_SCOPE)
					.inCollection(CollectionIdentifier.DEFAULT_COLLECTION).matching(query).all();
			assertEquals(0, notfound.size(), ""should not have found what was saved"");
			couchbaseTemplate.removeByQuery(UserCol.class).inScope(scopeName).inCollection(collectionName).matching(query)
					.all();
		} finally {
			try {
				couchbaseTemplate.removeByQuery(UserCol.class).inScope(scopeName).inCollection(collectionName).matching(query)
						.all();
			} catch (DataRetrievalFailureException drfe) {}
		}
	}
",non-flaky,5
113935,spring-projects_spring-data-couchbase,CustomTypeKeyIntegrationTests.getConnectionString,"	@Test
		public String getConnectionString() {
			return connectionString();
		}
",non-flaky,5
113936,spring-projects_spring-data-couchbase,BasicCouchbasePersistentPropertyTests.getId,"	@Test
		public String getId() {
			return springId;
		}
",non-flaky,5
113937,spring-projects_spring-data-couchbase,CustomConvertersTests.convert,"	@Test
		public String convert(Integer source) {
			return source % 2 == 0 ? ""even"" : ""odd"";
		}
",non-flaky,5
113938,spring-projects_spring-data-couchbase,N1qlQueryCreatorTests.beforeEach,"	@BeforeEach
	public void beforeEach() {
		context = new CouchbaseMappingContext();
		converter = new MappingCouchbaseConverter(context);
		bucketName = ""sample-bucket"";
	}
",non-flaky,5
113939,spring-projects_spring-data-couchbase,StringN1qlQueryCreatorMockedTests.beforeEach,"	@BeforeEach
	public void beforeEach() {
		context = new CouchbaseMappingContext();
		converter = new MappingCouchbaseConverter(context);
		ApplicationContext ac = new AnnotationConfigApplicationContext(Config.class);
		couchbaseTemplate = (CouchbaseTemplate) ac.getBean(COUCHBASE_TEMPLATE);
	}
",non-flaky,5
113940,spring-projects_spring-data-couchbase,StringN1qlQueryCreatorMockedTests.getConnectionString,"	@Test
		public String getConnectionString() {
			return connectionString();
		}
",non-flaky,5
113941,spring-projects_spring-data-couchbase,StringN1qlQueryCreatorTests.beforeEach,"	@BeforeEach
	public void beforeEach() {
		context = new CouchbaseMappingContext();
		converter = new MappingCouchbaseConverter(context);
		ApplicationContext ac = new AnnotationConfigApplicationContext(Config.class);
		couchbaseTemplate = (CouchbaseTemplate) ac.getBean(COUCHBASE_TEMPLATE);
	}
",non-flaky,5
113942,spring-projects_spring-data-couchbase,StringN1qlQueryCreatorTests.getConnectionString,"	@Test
		public String getConnectionString() {
			return connectionString();
		}
",non-flaky,5
113943,spring-projects_spring-data-couchbase,CouchbaseRepositoryQueryCollectionIntegrationTests.beforeEach,"	@BeforeEach
	public void beforeEach() {
		// first call the super method
		super.beforeEach();
		// then do processing for this class
		couchbaseTemplate.removeByQuery(User.class).inCollection(collectionName).all();
		couchbaseTemplate.removeByQuery(UserCol.class).inScope(otherScope).inCollection(otherCollection).all();
		ApplicationContext ac = new AnnotationConfigApplicationContext(Config.class);
		// seems that @Autowired is not adequate, so ...
		airportRepository = (AirportRepository) ac.getBean(""airportRepository"");
		userColRepository = (UserColRepository) ac.getBean(""userColRepository"");
	}
",non-flaky,5
113944,spring-projects_spring-data-couchbase,CouchbaseRepositoryQueryCollectionIntegrationTests.afterEach,"	@AfterEach
	public void afterEach() {
		// first do processing for this class
		// no-op
		// then call the super method
		super.afterEach();
	}
",non-flaky,5
113945,spring-projects_spring-data-couchbase,CouchbaseRepositoryQueryCollectionIntegrationTests.myTest,"	@Test
	public void myTest() {

		AirportRepository ar = airportRepository.withScope(scopeName).withCollection(collectionName);
		Airport vie = new Airport(""airports::vie"", ""vie"", ""loww"");
		try {
			Airport saved = ar.save(vie);
			Airport airport2 = ar.save(saved);
		} catch (Exception e) {
			e.printStackTrace();
			throw e;
		} finally {
			ar.delete(vie);
		}

	}
",non-flaky,5
113946,spring-projects_spring-data-couchbase,CouchbaseRepositoryQueryCollectionIntegrationTests.testScopeCollectionAnnotation,"	@Test
	public void testScopeCollectionAnnotation() {
		// template default scope is my_scope
		// UserCol annotation scope is other_scope
		UserCol user = new UserCol(""1"", ""Dave"", ""Wilson"");
		try {
			UserCol saved = userColRepository.withCollection(otherCollection).save(user); // should use UserCol annotation
																																										// scope
			List<UserCol> found = userColRepository.withCollection(otherCollection).findByFirstname(user.getFirstname());
			assertEquals(saved, found.get(0), ""should have found what was saved"");
			List<UserCol> notfound = userColRepository.withScope(DEFAULT_SCOPE)
					.withCollection(CollectionIdentifier.DEFAULT_COLLECTION).findByFirstname(user.getFirstname());
			assertEquals(0, notfound.size(), ""should not have found what was saved"");
		} finally {
			try {
				userColRepository.withScope(otherScope).withCollection(otherCollection).delete(user);
			} catch (DataRetrievalFailureException drfe) {}
		}
	}
",non-flaky,5
113947,spring-projects_spring-data-couchbase,CouchbaseRepositoryQueryCollectionIntegrationTests.testScopeCollectionAnnotationSwap,"	@Test
	public void testScopeCollectionAnnotationSwap() {
		// UserCol annotation scope is other_scope, collection is other_collection
		// airportRepository relies on Config.setScopeName(scopeName) (""my_scope"") from CollectionAwareIntegrationTests.
		// using airportRepository without specified a collection should fail.
		// This test ensures that airportRepository.save(airport) doesn't get the
		// collection from CrudMethodMetadata of UserCol.save()
		UserCol userCol = new UserCol(""1"", ""Dave"", ""Wilson"");
		Airport airport = new Airport(""3"", ""myIata"", ""myIcao"");
		UserCol savedCol = userColRepository.save(userCol); // uses UserCol annotation scope, populates CrudMethodMetadata
		userColRepository.delete(userCol); // uses UserCol annotation scope, populates CrudMethodMetadata
		assertThrows(IllegalStateException.class, () -> airportRepository.save(airport));
	}
",non-flaky,5
113948,spring-projects_spring-data-couchbase,CouchbaseRepositoryQueryCollectionIntegrationTests.testScopeCollectionRepoWith,"	@Test
	public void testScopeCollectionRepoWith() {
		UserCol user = new UserCol(""1"", ""Dave"", ""Wilson"");
		try {
			UserCol saved = userColRepository.withScope(scopeName).withCollection(collectionName).save(user);
			List<UserCol> found = userColRepository.withScope(scopeName).withCollection(collectionName)
					.findByFirstname(user.getFirstname());
			assertEquals(saved, found.get(0), ""should have found what was saved"");
			List<UserCol> notfound = userColRepository.withScope(DEFAULT_SCOPE)
					.withCollection(CollectionIdentifier.DEFAULT_COLLECTION).findByFirstname(user.getFirstname());
			assertEquals(0, notfound.size(), ""should not have found what was saved"");
			userColRepository.withScope(scopeName).withCollection(collectionName).delete(user);
		} finally {
			try {
				userColRepository.withScope(scopeName).withCollection(collectionName).delete(user);
			} catch (DataRetrievalFailureException drfe) {}
		}
	}
",non-flaky,5
113949,spring-projects_spring-data-couchbase,ReactiveCouchbaseRepositoryQueryCollectionIntegrationTests.beforeEach,"	@BeforeEach
	public void beforeEach() {
		// first call the super method
		super.beforeEach();
		// then do processing for this class
		couchbaseTemplate.removeByQuery(User.class).inCollection(collectionName).all();
		couchbaseTemplate.removeByQuery(UserCol.class).inScope(otherScope).inCollection(otherCollection).all();

		ApplicationContext ac = new AnnotationConfigApplicationContext(Config.class);
		// seems that @Autowired is not adequate, so ...
		airportRepository = (ReactiveAirportRepository) ac.getBean(""reactiveAirportRepository"");
		userColRepository = (ReactiveUserColRepository) ac.getBean(""reactiveUserColRepository"");
	}
",non-flaky,5
113950,spring-projects_spring-data-couchbase,ReactiveCouchbaseRepositoryQueryCollectionIntegrationTests.afterEach,"	@AfterEach
	public void afterEach() {
		// first do processing for this class
		// no-op
		// then call the super method
		super.afterEach();
	}
",non-flaky,5
113951,spring-projects_spring-data-couchbase,ReactiveCouchbaseRepositoryQueryCollectionIntegrationTests.myTest,"	@Test
	public void myTest() {

		ReactiveAirportRepository ar = airportRepository.withScope(scopeName).withCollection(collectionName);
		Airport vie = new Airport(""airports::vie"", ""vie"", ""loww"");
		try {
			Airport saved = ar.save(vie).block();
			Airport airport2 = ar.save(saved).block();
		} catch (Exception e) {
			e.printStackTrace();
			throw e;
		} finally {
			ar.delete(vie).block();
		}

	}
",non-flaky,5
113952,spring-projects_spring-data-couchbase,ReactiveCouchbaseRepositoryQueryCollectionIntegrationTests.testScopeCollectionAnnotation,"	@Test
	public void testScopeCollectionAnnotation() {
		// template default scope is my_scope
		// UserCol annotation scope is other_scope
		UserCol user = new UserCol(""1"", ""Dave"", ""Wilson"");
		try {
			UserCol saved = userColRepository.withCollection(otherCollection).save(user).block(); // should use UserCol
																																														// annotation
			// scope
			List<UserCol> found = userColRepository.withCollection(otherCollection).findByFirstname(user.getFirstname())
					.collectList().block();
			assertEquals(saved, found.get(0), ""should have found what was saved"");
			List<UserCol> notfound = userColRepository.withScope(CollectionIdentifier.DEFAULT_SCOPE)
					.withCollection(CollectionIdentifier.DEFAULT_COLLECTION).findByFirstname(user.getFirstname()).collectList()
					.block();
			assertEquals(0, notfound.size(), ""should not have found what was saved"");
		} finally {
			try {
				userColRepository.withScope(otherScope).withCollection(otherCollection).delete(user);
			} catch (DataRetrievalFailureException drfe) {}
		}
	}
",non-flaky,5
113953,spring-projects_spring-data-couchbase,ReactiveCouchbaseRepositoryQueryCollectionIntegrationTests.testScopeCollectionRepoWith,"	@Test
	public void testScopeCollectionRepoWith() {
		UserCol user = new UserCol(""1"", ""Dave"", ""Wilson"");
		try {
			UserCol saved = userColRepository.withScope(scopeName).withCollection(collectionName).save(user).block();
			List<UserCol> found = userColRepository.withScope(scopeName).withCollection(collectionName)
					.findByFirstname(user.getFirstname()).collectList().block();
			assertEquals(saved, found.get(0), ""should have found what was saved"");
			List<UserCol> notfound = userColRepository.withScope(CollectionIdentifier.DEFAULT_SCOPE)
					.withCollection(CollectionIdentifier.DEFAULT_COLLECTION).findByFirstname(user.getFirstname()).collectList()
					.block();
			assertEquals(0, notfound.size(), ""should not have found what was saved"");
			userColRepository.withScope(scopeName).withCollection(collectionName).delete(user).block();
		} finally {
			try {
				userColRepository.withScope(scopeName).withCollection(collectionName).delete(user).block();
			} catch (DataRetrievalFailureException drfe) {}
		}
	}
",non-flaky,5
113954,spring-projects_spring-data-couchbase,CouchbaseRepositoryKeyValueIntegrationTests.getConnectionString,"	@Test
		public String getConnectionString() {
			return connectionString();
		}
",non-flaky,5
113955,spring-projects_spring-data-couchbase,ReactiveCouchbaseRepositoryQueryIntegrationTests.testCas,"	@Test
	public void testCas() {
		User user = new User(""1"", ""Dave"", ""Wilson"");
		userRepository.save(user).block();
		user.setVersion(user.getVersion() - 1);
		assertThrows(DataIntegrityViolationException.class, () -> userRepository.save(user).block());
		user.setVersion(0);
		userRepository.save(user).block();
		userRepository.delete(user).block();
	}
",non-flaky,5
113956,spring-projects_spring-data-couchbase,ReactiveCouchbaseRepositoryQueryIntegrationTests.getPolicyByIdAndEffectiveDateTime,"	@Test
	public Mono<Airport> getPolicyByIdAndEffectiveDateTime(String policyId, Instant effectiveDateTime) {
		return airportRepository
				.findPolicySnapshotByPolicyIdAndEffectiveDateTime(policyId, effectiveDateTime.toEpochMilli())
				// .map(Airport::getEntity)
				.doOnError(
						error -> System.out.println(""MSG='Exception happened while retrieving Policy by Id and effectiveDateTime', ""
								+ ""policyId={}, effectiveDateTime={}""));
	}
",non-flaky,5
113957,spring-projects_spring-data-couchbase,ReactiveCouchbaseRepositoryQueryIntegrationTests.getConnectionString,"	@Test
		public String getConnectionString() {
			return connectionString();
		}
",non-flaky,5
136461,doanduyhai_Achilles,TestEntityWithDSESearch.should_search_text_using_prefix,"    @Test
    public void should_search_text_using_prefix() throws Exception {
        //Given

        //When
        final List<EntityWithDSESearch> actual = manager
                .indexed()
                .select()
                .allColumns_FromBaseTable()
                .where()
                .string().StartWith(""speed"")
                .getList();

        //Then
        assertThat(actual).hasSize(2);
        assertThat(actual.stream().map(EntityWithDSESearch::getString).collect(toList()))
                .contains(""speedster"", ""speedrun"");
    }
",non-flaky,5
136462,doanduyhai_Achilles,TestEntityWithDSESearch.should_search_text_using_suffix,"    @Test
    public void should_search_text_using_suffix() throws Exception {
        //Given

        //When
        final List<EntityWithDSESearch> actual = manager
                .indexed()
                .select()
                .allColumns_FromBaseTable()
                .where()
                .string().EndWith(""run"")
                .getList();

        //Then
        assertThat(actual).hasSize(2);
        assertThat(actual.stream().map(EntityWithDSESearch::getString).collect(toList()))
                .contains(""long run"", ""speedrun"");
    }
",non-flaky,5
136463,doanduyhai_Achilles,TestEntityWithDSESearch.should_search_text_using_substring,"    @Test
    public void should_search_text_using_substring() throws Exception {
        //Given

        //When
        final List<EntityWithDSESearch> actual = manager
                .indexed()
                .select()
                .allColumns_FromBaseTable()
                .where()
                .string().Contains(""eds"")
                .getList();

        //Then
        assertThat(actual).hasSize(1);
        assertThat(actual.stream().map(EntityWithDSESearch::getString).collect(toList()))
                .contains(""speedster"");
    }
",non-flaky,5
136464,doanduyhai_Achilles,TestEntityWithDSESearch.should_search_numeric_eq,"    @Test
    public void should_search_numeric_eq() throws Exception {
        //Given

        //When
        final List<EntityWithDSESearch> actual = manager
                .indexed()
                .select()
                .allColumns_FromBaseTable()
                .where()
                .numeric().Eq(100.03f)
                .getList();

        //Then
        assertThat(actual).hasSize(1);
        assertThat(actual.get(0).getNumeric()).isEqualTo(100.03f);
    }
",non-flaky,5
136465,doanduyhai_Achilles,TestEntityWithDSESearch.should_search_numeric_gt,"    @Test
    public void should_search_numeric_gt() throws Exception {
        //Given

        //When
        final List<EntityWithDSESearch> actual = manager
                .indexed()
                .select()
                .allColumns_FromBaseTable()
                .where()
                .numeric().Gt(100.03f)
                .getList();

        //Then
        assertThat(actual).hasSize(1);
        assertThat(actual.get(0).getNumeric()).isEqualTo(138.47f);
    }
",non-flaky,5
136466,doanduyhai_Achilles,TestEntityWithDSESearch.should_search_numeric_gte,"    @Test
    public void should_search_numeric_gte() throws Exception {
        //Given

        //When
        final List<EntityWithDSESearch> actual = manager
                .indexed()
                .select()
                .allColumns_FromBaseTable()
                .where()
                .numeric().Gte(138.47f)
                .getList();

        //Then
        assertThat(actual).hasSize(1);
        assertThat(actual.get(0).getNumeric()).isEqualTo(138.47f);
    }
",non-flaky,5
136467,doanduyhai_Achilles,TestEntityWithDSESearch.should_search_numeric_lt,"    @Test
    public void should_search_numeric_lt() throws Exception {
        //Given

        //When
        final List<EntityWithDSESearch> actual = manager
                .indexed()
                .select()
                .allColumns_FromBaseTable()
                .where()
                .numeric().Lt(100.03f)
                .getList();

        //Then
        assertThat(actual).hasSize(1);
        assertThat(actual.get(0).getNumeric()).isEqualTo(87.39f);
    }
",non-flaky,5
136468,doanduyhai_Achilles,TestEntityWithDSESearch.should_search_numeric_lte,"    @Test
    public void should_search_numeric_lte() throws Exception {
        //Given

        //When
        final List<EntityWithDSESearch> actual = manager
                .indexed()
                .select()
                .allColumns_FromBaseTable()
                .where()
                .numeric().Lte(87.39f)
                .getList();

        //Then
        assertThat(actual).hasSize(1);
        assertThat(actual.get(0).getNumeric()).isEqualTo(87.39f);
    }
",non-flaky,5
136469,doanduyhai_Achilles,TestEntityWithDSESearch.should_search_numeric_gt_and_lt,"    @Test
    public void should_search_numeric_gt_and_lt() throws Exception {
        //Given

        //When
        final List<EntityWithDSESearch> actual = manager
                .indexed()
                .select()
                .allColumns_FromBaseTable()
                .where()
                .numeric().Gt_And_Lt(87.39f, 138.47f)
                .getList();

        //Then
        assertThat(actual).hasSize(1);
        assertThat(actual.get(0).getNumeric()).isEqualTo(100.03f);
    }
",non-flaky,5
136470,doanduyhai_Achilles,TestEntityWithDSESearch.should_search_numeric_gt_and_lte,"    @Test
    public void should_search_numeric_gt_and_lte() throws Exception {
        //Given

        //When
        final List<EntityWithDSESearch> actual = manager
                .indexed()
                .select()
                .allColumns_FromBaseTable()
                .where()
                .numeric().Gt_And_Lte(87.39f, 138.47f)
                .getList();

        //Then
        assertThat(actual).hasSize(2);
        assertThat(actual.stream().map(EntityWithDSESearch::getNumeric).collect(toList()))
                .contains(100.03f, 138.47f);
    }
",non-flaky,5
136471,doanduyhai_Achilles,TestEntityWithDSESearch.should_search_numeric_gte_and_lt,"    @Test
    public void should_search_numeric_gte_and_lt() throws Exception {
        //Given

        //When
        final List<EntityWithDSESearch> actual = manager
                .indexed()
                .select()
                .allColumns_FromBaseTable()
                .where()
                .numeric().Gte_And_Lt(87.39f, 138.47f)
                .getList();

        //Then
        assertThat(actual).hasSize(2);
        assertThat(actual.stream().map(EntityWithDSESearch::getNumeric).collect(toList()))
                .contains(87.39f, 100.03f);
    }
",non-flaky,5
136472,doanduyhai_Achilles,TestEntityWithDSESearch.should_search_numeric_gte_and_lte,"    @Test
    public void should_search_numeric_gte_and_lte() throws Exception {
        //Given

        //When
        final List<EntityWithDSESearch> actual = manager
                .indexed()
                .select()
                .allColumns_FromBaseTable()
                .where()
                .numeric().Gte_And_Lte(87.39f, 138.47f)
                .getList();

        //Then
        assertThat(actual).hasSize(3);
        assertThat(actual.stream().map(EntityWithDSESearch::getNumeric).collect(toList()))
                .contains(87.39f, 100.03f, 138.47f);
    }
",non-flaky,5
136473,doanduyhai_Achilles,TestEntityWithDSESearch.should_search_date_eq,"    @Test
    public void should_search_date_eq() throws Exception {
        //Given
        final Date searchedDate = toDate(""2016-09-26 08:00:00.000Z"");

        //When
        final List<EntityWithDSESearch> actual = manager
                .indexed()
                .select()
                .allColumns_FromBaseTable()
                .where()
                .date().Eq(searchedDate)
                .getList();

        //Then
        assertThat(actual).hasSize(1);
        assertThat(toString(actual.get(0).getDate())).isEqualTo(""2016-09-26 08:00:00.000Z"");
    }
",non-flaky,5
136474,doanduyhai_Achilles,TestEntityWithDSESearch.should_search_date_gt,"    @Test
    public void should_search_date_gt() throws Exception {
        //Given
        final Date searchedDate = toDate(""2016-09-26 08:00:00.000Z"");

        //When
        final List<EntityWithDSESearch> actual = manager
                .indexed()
                .select()
                .allColumns_FromBaseTable()
                .where()
                .date().Gt(searchedDate)
                .getList();

        //Then
        assertThat(actual).hasSize(1);
        assertThat(toString(actual.get(0).getDate())).isEqualTo(""2016-09-26 09:00:00.000Z"");
    }
",non-flaky,5
136475,doanduyhai_Achilles,TestEntityWithDSESearch.should_search_date_gte,"    @Test
    public void should_search_date_gte() throws Exception {
        //Given
        final Date searchedDate = toDate(""2016-09-26 08:00:00.000Z"");

        //When
        final List<EntityWithDSESearch> actual = manager
                .indexed()
                .select()
                .allColumns_FromBaseTable()
                .where()
                .date().Gte(searchedDate)
                .getList();

        //Then
        assertThat(actual).hasSize(2);
        assertThat(actual.stream()
                .map(EntityWithDSESearch::getDate)
                .map(this::toString)
                .collect(toList()))
            .contains(""2016-09-26 08:00:00.000Z"", ""2016-09-26 09:00:00.000Z"");
    }
",non-flaky,5
136476,doanduyhai_Achilles,TestEntityWithDSESearch.should_search_date_lt,"    @Test
    public void should_search_date_lt() throws Exception {
        //Given
        final Date searchedDate = toDate(""2016-09-26 08:00:00.000Z"");

        //When
        final List<EntityWithDSESearch> actual = manager
                .indexed()
                .select()
                .allColumns_FromBaseTable()
                .where()
                .date().Lt(searchedDate)
                .getList();

        //Then
        assertThat(actual).hasSize(1);
        assertThat(toString(actual.get(0).getDate())).isEqualTo(""2016-09-25 13:00:00.000Z"");
    }
",non-flaky,5
136477,doanduyhai_Achilles,TestEntityWithDSESearch.should_search_date_lte,"    @Test
    public void should_search_date_lte() throws Exception {
        //Given
        final Date searchedDate = toDate(""2016-09-26 08:00:00.000Z"");

        //When
        final List<EntityWithDSESearch> actual = manager
                .indexed()
                .select()
                .allColumns_FromBaseTable()
                .where()
                .date().Lte(searchedDate)
                .getList();

        //Then
        assertThat(actual).hasSize(2);
        assertThat(actual.stream()
                .map(EntityWithDSESearch::getDate)
                .map(this::toString)
                .collect(toList()))
                .contains(""2016-09-26 08:00:00.000Z"", ""2016-09-25 13:00:00.000Z"");
    }
",non-flaky,5
136478,doanduyhai_Achilles,TestEntityWithDSESearch.should_search_date_gt_and_lt,"    @Test
    public void should_search_date_gt_and_lt() throws Exception {
        //Given
        final Date searchedDate1 = toDate(""2016-09-25 13:00:00.000Z"");
        final Date searchedDate2 = toDate(""2016-09-26 09:00:00.000Z"");

        //When
        final List<EntityWithDSESearch> actual = manager
                .indexed()
                .select()
                .allColumns_FromBaseTable()
                .where()
                .date().Gt_And_Lt(searchedDate1, searchedDate2)
                .getList();

        //Then
        assertThat(actual).hasSize(1);
        assertThat(actual.stream()
                .map(EntityWithDSESearch::getDate)
                .map(this::toString)
                .collect(toList()))
                .contains(""2016-09-26 08:00:00.000Z"");
    }
",non-flaky,5
136479,doanduyhai_Achilles,TestEntityWithDSESearch.should_search_date_gt_and_lte,"    @Test
    public void should_search_date_gt_and_lte() throws Exception {
        //Given
        final Date searchedDate1 = toDate(""2016-09-25 13:00:00.000Z"");
        final Date searchedDate2 = toDate(""2016-09-26 09:00:00.000Z"");

        //When
        final List<EntityWithDSESearch> actual = manager
                .indexed()
                .select()
                .allColumns_FromBaseTable()
                .where()
                .date().Gt_And_Lte(searchedDate1, searchedDate2)
                .getList();

        //Then
        assertThat(actual).hasSize(2);
        assertThat(actual.stream()
                .map(EntityWithDSESearch::getDate)
                .map(this::toString)
                .collect(toList()))
                .contains(""2016-09-26 08:00:00.000Z"", ""2016-09-26 09:00:00.000Z"");
    }
",non-flaky,5
136480,doanduyhai_Achilles,TestEntityWithDSESearch.should_search_date_gte_and_lt,"    @Test
    public void should_search_date_gte_and_lt() throws Exception {
        //Given
        final Date searchedDate1 = toDate(""2016-09-25 13:00:00.000Z"");
        final Date searchedDate2 = toDate(""2016-09-26 09:00:00.000Z"");

        //When
        final List<EntityWithDSESearch> actual = manager
                .indexed()
                .select()
                .allColumns_FromBaseTable()
                .where()
                .date().Gte_And_Lt(searchedDate1, searchedDate2)
                .getList();

        //Then
        assertThat(actual).hasSize(2);
        assertThat(actual.stream()
                .map(EntityWithDSESearch::getDate)
                .map(this::toString)
                .collect(toList()))
                .contains(""2016-09-26 08:00:00.000Z"", ""2016-09-25 13:00:00.000Z"");
    }
",non-flaky,5
136481,doanduyhai_Achilles,TestEntityWithDSESearch.should_search_date_gte_and_lte,"    @Test
    public void should_search_date_gte_and_lte() throws Exception {
        //Given
        final Date searchedDate1 = toDate(""2016-09-25 13:00:00.000Z"");
        final Date searchedDate2 = toDate(""2016-09-26 09:00:00.000Z"");

        //When
        final List<EntityWithDSESearch> actual = manager
                .indexed()
                .select()
                .allColumns_FromBaseTable()
                .where()
                .date().Gte_And_Lte(searchedDate1, searchedDate2)
                .getList();

        //Then
        assertThat(actual).hasSize(3);
        assertThat(actual.stream()
                .map(EntityWithDSESearch::getDate)
                .map(this::toString)
                .collect(toList()))
                .contains(""2016-09-26 09:00:00.000Z"",
                        ""2016-09-26 08:00:00.000Z"",
                        ""2016-09-25 13:00:00.000Z"");
    }
",non-flaky,5
136482,doanduyhai_Achilles,TestEntityWithDSESearch.should_search_using_multiple_predicates,"    @Test
    public void should_search_using_multiple_predicates() throws Exception {
        //Given
        final Date searchedDate1 = toDate(""2016-09-25 13:00:00.000Z"");
        final Date searchedDate2 = toDate(""2016-09-26 09:00:00.000Z"");

        //When
        final List<EntityWithDSESearch> actual = manager
                .indexed()
                .select()
                .allColumns_FromBaseTable()
                .where()
                .date().Gte_And_Lte(searchedDate1, searchedDate2)
                .string().Contains(""run"")
                .numeric().Gt_And_Lte(80f, 110f)
                .getList();

        //Then
        assertThat(actual).hasSize(1);
        assertThat(actual.get(0).getNumeric()).isEqualTo(87.39f);
    }
",non-flaky,5
136483,doanduyhai_Achilles,TestEntityWithDSESearch.should_search_using_raw_predicates,"    @Test
    public void should_search_using_raw_predicates() throws Exception {
        //Given

        //When
        final List<EntityWithDSESearch> actual = manager
                .indexed()
                .select()
                .allColumns_FromBaseTable()
                .where()
                .string().RawPredicate(""*eed?u*"")
                .numeric().RawPredicate(""{100 TO 150}"")
                .getList();

        //Then
        assertThat(actual).hasSize(1);
        assertThat(actual.get(0).getString()).isEqualTo(""speedrun"");
    }
",non-flaky,5
136484,doanduyhai_Achilles,TestEntityWithDSESearch.should_search_using_raw_solr_query,"    @Test
    public void should_search_using_raw_solr_query() throws Exception {
        //Given

        //When
        final List<EntityWithDSESearch> actual = manager
                .indexed()
                .select()
                .allColumns_FromBaseTable()
                .where()
                .rawSolrQuery(""string:*eed?u* OR numeric:[100 TO *]"")
                .getList();

        //Then
        assertThat(actual).hasSize(2);
        assertThat(actual.stream()
                .map(EntityWithDSESearch::getString)
                .collect(toList()))
                .contains(""speedrun"",""speedster"");
    }
",non-flaky,5
136485,doanduyhai_Achilles,TestEntityWithDSESearch.should_search_using_solr_and_partition,"    @Test
    public void should_search_using_solr_and_partition() throws Exception {
        //Given
        final Date searchedDate1 = toDate(""2016-09-25 13:00:00.000Z"");
        final Date searchedDate2 = toDate(""2016-09-26 09:00:00.000Z"");

        //When
        final List<EntityWithDSESearch> actual = manager
                .indexed()
                .select()
                .allColumns_FromBaseTable()
                .where()
                .date().Gte_And_Lte(searchedDate1, searchedDate2)
                .string().Contains(""run"")
                .id().Eq(3L)
                .getList();

        //Then
        assertThat(actual).hasSize(1);
        assertThat(actual.get(0).getNumeric()).isEqualTo(87.39f);
    }
",non-flaky,5
136486,doanduyhai_Achilles,TestEntityWithClusteringIT.should_select_with_allow_per_partition_limit,"    @Test
    public void should_select_with_allow_per_partition_limit() throws Exception {
        //Given
        Long id1 = RandomUtils.nextLong(0L, Long.MAX_VALUE);
        Long id2 = RandomUtils.nextLong(0L, Long.MAX_VALUE);
        Long id3 = RandomUtils.nextLong(0L, Long.MAX_VALUE);

        scriptExecutor.executeScriptTemplate(""EntityWithClustering/insertRows.cql"", ImmutableMap.of(""id1"", id1, ""id2"", id2, ""id3"", id3));

        //When
        final List<EntityWithClustering> list = manager
                .dsl()
                .select()
                .allColumns_FromBaseTable()
                .without_WHERE_Clause()
                .perPartitionLimit(2)
                .getList();

        //Then
        assertThat(list).hasSize(6);
        assertThat(list.stream().map(x -> x.getClust()).collect(Collectors.toList())).containsExactly(1L, 2L, 1L, 2L, 1L, 2L);
    }
",non-flaky,5
136487,doanduyhai_Achilles,TestEntityWithNonFrozenUDTIT.should_update_udt_fields,"    @Test
    public void should_update_udt_fields() throws Exception {
        //Given
        Long id = RandomUtils.nextLong(0L, Long.MAX_VALUE);
        scriptExecutor.executeScriptTemplate(""EntityWithNonFrozenUDT/insertRow.cql"", ImmutableMap.of(""id"", id));

        //When
        manager
                .dsl()
                .update()
                .fromBaseTable()
                .nonFrozen().val().Set(""new_value"")
                .nonFrozen().li().Set(Arrays.asList(""3""))
                .nonFrozen().se().Set_FromJSON(""[\""3\""]"")
                .nonFrozen().ma().Set(ImmutableMap.of(3, ""3""))
                .nonFrozen().address().Set_FromJSON(""{\""street\"": \""new_street\"", \""number\"": 100}"")
                .where()
                .id().Eq(id)
                .execute();

        //Then
        final Row actual = session.execute(""SELECT * FROM it_3_6.non_frozen_udt WHERE id = "" + id).one();

        assertThat(actual).isNotNull();

        final UDTValue nonFrozen = actual.getUDTValue(""nonFrozen"");

        assertThat(nonFrozen).isNotNull();
        assertThat(nonFrozen.getList(""li"", String.class)).containsExactly(""3"");
        assertThat(nonFrozen.getSet(""se"", String.class)).containsExactly(""3"");
        assertThat(nonFrozen.getMap(""ma"", Integer.class, String.class)).hasSize(1);
        assertThat(nonFrozen.getMap(""ma"", Integer.class, String.class)).containsEntry(3, ""3"");

        final UDTValue address = nonFrozen.getUDTValue(""address"");

        assertThat(address).isNotNull();
        assertThat(address.getString(""street"")).isEqualTo(""new_street"");
        assertThat(address.getInt(""number"")).isEqualTo(100);
    }
",non-flaky,5
136488,doanduyhai_Achilles,TypedMapTest.should_build_typed_map_from_source,"    @Test
    public void should_build_typed_map_from_source() throws Exception {
        Map<String, Object> source = new HashMap<>();
        source.put(""string"", ""value"");
        source.put(""int"", 10);

        TypedMap typedMap = TypedMap.fromMap(source);

        assertThat(typedMap.<String>getTyped(""string"")).isInstanceOf(String.class);
        assertThat(typedMap.<Integer>getTyped(""int"")).isInstanceOf(Integer.class);

    }
",non-flaky,5
136489,doanduyhai_Achilles,CaseSensitiveNamingTest.should_return_blank_name,"    @Test
    public void should_return_blank_name() throws Exception {
        //Given
        String name = ""     "";

        //When
        final String actual = strategy.apply(name);

        //Then
        assertThat(actual).isEmpty();
    }
",non-flaky,5
136490,doanduyhai_Achilles,CaseSensitiveNamingTest.should_return_blank_name_on_null,"    @Test
    public void should_return_blank_name_on_null() throws Exception {
        //Given
        String name = null;

        //When
        final String actual = strategy.apply(name);

        //Then
        assertThat(actual).isEmpty();
    }
",non-flaky,5
136491,doanduyhai_Achilles,CaseSensitiveNamingTest.should_return_case_sensitive,"    @Test
    public void should_return_case_sensitive() throws Exception {
        //Given
        String name = ""dcSdIfk_sdf"";

        //When
        final String actual = strategy.apply(name);

        //Then
        assertThat(actual).isEqualTo(""\""dcSdIfk_sdf\"""");
    }
",non-flaky,5
136492,doanduyhai_Achilles,SnakeCaseNamingTest.should_return_blank_name,"    @Test
    public void should_return_blank_name() throws Exception {
        //Given
        String name = ""     "";

        //When
        final String actual = strategy.apply(name);

        //Then
        assertThat(actual).isEmpty();
    }
",non-flaky,5
136493,doanduyhai_Achilles,SnakeCaseNamingTest.should_return_blank_name_on_null,"    @Test
    public void should_return_blank_name_on_null() throws Exception {
        //Given
        String name = null;

        //When
        final String actual = strategy.apply(name);

        //Then
        assertThat(actual).isEmpty();
    }
",non-flaky,5
136494,doanduyhai_Achilles,SnakeCaseNamingTest.should_return_snake_case,"    @Test
    public void should_return_snake_case() throws Exception {
        //Given
        String name = ""theBigOne__andSmaller_One"";

        //When
        final String actual = strategy.apply(name);

        //Then
        assertThat(actual).isEqualTo(""the_big_one_and_smaller_one"");
    }
",non-flaky,5
136495,doanduyhai_Achilles,LowerCaseNamingTest.should_return_blank_name,"    @Test
    public void should_return_blank_name() throws Exception {
        //Given
        String name = ""     "";

        //When
        final String actual = strategy.apply(name);

        //Then
        assertThat(actual).isEmpty();
    }
",non-flaky,5
136496,doanduyhai_Achilles,LowerCaseNamingTest.should_return_blank_name_on_null,"    @Test
    public void should_return_blank_name_on_null() throws Exception {
        //Given
        String name = null;

        //When
        final String actual = strategy.apply(name);

        //Then
        assertThat(actual).isEmpty();
    }
",non-flaky,5
136497,doanduyhai_Achilles,LowerCaseNamingTest.should_return_lower_case,"    @Test
    public void should_return_lower_case() throws Exception {
        //Given
        String name = ""ZfrEr_rfR"";

        //When
        final String actual = strategy.apply(name);

        //Then
        assertThat(actual).isEqualTo(""zfrer_rfr"");
    }
",non-flaky,5
136498,doanduyhai_Achilles,FrozenNestedTypeStrategyTest.should_fail_for_non_frozen_udt,"    @Test
    public void should_fail_for_non_frozen_udt() throws Exception {
        setExec(aptUtils -> {
            final NestedTypeValidator2_1 strategy = new NestedTypeValidator2_1();
            final String className = TestEntityWithNestedTypes.class.getCanonicalName();
            final TypeName rawClass = ClassName.get(TestEntityWithNestedTypes.class);
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            // private TestUDT testUDT;
            VariableElement elm = findFieldInType(typeElement, ""testUDT"");
            final AnnotationTree annotationTree = AnnotationTree.buildFrom(aptUtils, globalParsingContext, elm);
            strategy.validate(aptUtils, annotationTree, ""testUDT"", rawClass);
        });
        failTestWithMessage(""UDT class "" +
                ""'info.archinnov.achilles.internals.sample_classes.parser.field.TestUDT' "" +
                ""in field 'testUDT' "" +
                ""of class 'info.archinnov.achilles.internals.sample_classes.parser.strategy.TestEntityWithNestedTypes' "" +
                ""should be annotated with @Frozen"", TestEntityWithNestedTypes.class);
    }
",non-flaky,5
136499,doanduyhai_Achilles,FrozenNestedTypeStrategyTest.should_fail_for_non_frozen_udtValue,"    @Test
    public void should_fail_for_non_frozen_udtValue() throws Exception {
        setExec(aptUtils -> {
            final NestedTypeValidator2_1 strategy = new NestedTypeValidator2_1();
            final String className = TestEntityWithNestedTypes.class.getCanonicalName();
            final TypeName rawClass = ClassName.get(TestEntityWithNestedTypes.class);
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            // private UDTValue udtValue;
            VariableElement elm = findFieldInType(typeElement, ""udtValue"");
            final AnnotationTree annotationTree = AnnotationTree.buildFrom(aptUtils, globalParsingContext, elm);
            strategy.validate(aptUtils, annotationTree, ""udtValue"", rawClass);
        });
        failTestWithMessage(""UDTValue "" +
                ""in field 'udtValue' "" +
                ""of class 'info.archinnov.achilles.internals.sample_classes.parser.strategy.TestEntityWithNestedTypes' "" +
                ""should be annotated with @Frozen"", TestEntityWithNestedTypes.class);
    }
",non-flaky,5
136500,doanduyhai_Achilles,FrozenNestedTypeStrategyTest.should_not_fail_for_non_frozen_tupleValue,"    @Test
    public void should_not_fail_for_non_frozen_tupleValue() throws Exception {
        setExec(aptUtils -> {
            final NestedTypeValidator2_1 strategy = new NestedTypeValidator2_1();
            final String className = TestEntityWithNestedTypes.class.getCanonicalName();
            final TypeName rawClass = ClassName.get(TestEntityWithNestedTypes.class);
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            // private TupleValue tupleValue;
            VariableElement elm = findFieldInType(typeElement, ""tupleValue"");
            final AnnotationTree annotationTree = AnnotationTree.buildFrom(aptUtils, globalParsingContext, elm);
            strategy.validate(aptUtils, annotationTree, ""tupleValue"", rawClass);
        });
        launchTest(TestEntityWithNestedTypes.class);
    }
",non-flaky,5
136501,doanduyhai_Achilles,FrozenNestedTypeStrategyTest.should_fail_for_non_frozen_list_list,"    @Test
    public void should_fail_for_non_frozen_list_list() throws Exception {
        setExec(aptUtils -> {
            final NestedTypeValidator2_1 strategy = new NestedTypeValidator2_1();
            final String className = TestEntityWithNestedTypes.class.getCanonicalName();
            final TypeName rawClass = ClassName.get(TestEntityWithNestedTypes.class);
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            // private List<List<String>> listList;
            VariableElement elm = findFieldInType(typeElement, ""listList"");
            final AnnotationTree annotationTree = AnnotationTree.buildFrom(aptUtils, globalParsingContext, elm);
            strategy.validate(aptUtils, annotationTree, ""listList"", rawClass);
        });
        failTestWithMessage(""collections/array type/UDT "" +
                ""'java.util.List<java.lang.String>' "" +
                ""in 'listList' "" +
                ""of class 'info.archinnov.achilles.internals.sample_classes.parser.strategy.TestEntityWithNestedTypes' "" +
                ""should be annotated with @Frozen"", TestEntityWithNestedTypes.class);
    }
",non-flaky,5
136502,doanduyhai_Achilles,FrozenNestedTypeStrategyTest.should_fail_for_non_frozen_list_udt,"    @Test
    public void should_fail_for_non_frozen_list_udt() throws Exception {
        setExec(aptUtils -> {
            final NestedTypeValidator2_1 strategy = new NestedTypeValidator2_1();
            final String className = TestEntityWithNestedTypes.class.getCanonicalName();
            final TypeName rawClass = ClassName.get(TestEntityWithNestedTypes.class);
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            // private List<TestUDT> listUdt;
            VariableElement elm = findFieldInType(typeElement, ""listUdt"");
            final AnnotationTree annotationTree = AnnotationTree.buildFrom(aptUtils, globalParsingContext, elm);
            strategy.validate(aptUtils, annotationTree, ""listUdt"", rawClass);
        });
        failTestWithMessage(""collections/array type/UDT "" +
                ""'info.archinnov.achilles.internals.sample_classes.parser.field.TestUDT' "" +
                ""in 'listUdt' "" +
                ""of class 'info.archinnov.achilles.internals.sample_classes.parser.strategy.TestEntityWithNestedTypes' "" +
                ""should be annotated with @Frozen"", TestEntityWithNestedTypes.class);
    }
",non-flaky,5
136503,doanduyhai_Achilles,FrozenNestedTypeStrategyTest.should_not_fail_for_non_frozen_list_tuple,"    @Test
    public void should_not_fail_for_non_frozen_list_tuple() throws Exception {
        setExec(aptUtils -> {
            final NestedTypeValidator2_1 strategy = new NestedTypeValidator2_1();
            final String className = TestEntityWithNestedTypes.class.getCanonicalName();
            final TypeName rawClass = ClassName.get(TestEntityWithNestedTypes.class);
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            // private List<Tuple3<Integer, String, String>> listTuple;
            VariableElement elm = findFieldInType(typeElement, ""listTuple"");
            final AnnotationTree annotationTree = AnnotationTree.buildFrom(aptUtils, globalParsingContext, elm);
            strategy.validate(aptUtils, annotationTree, ""listTuple"", rawClass);
        });
        launchTest(TestEntityWithNestedTypes.class);
    }
",non-flaky,5
136504,doanduyhai_Achilles,FrozenNestedTypeStrategyTest.should_not_fail_for_non_frozen_list_tupleValue,"    @Test
    public void should_not_fail_for_non_frozen_list_tupleValue() throws Exception {
        setExec(aptUtils -> {
            final NestedTypeValidator2_1 strategy = new NestedTypeValidator2_1();
            final String className = TestEntityWithNestedTypes.class.getCanonicalName();
            final TypeName rawClass = ClassName.get(TestEntityWithNestedTypes.class);
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            // private List<TupleValue> listTupleValue;
            VariableElement elm = findFieldInType(typeElement, ""listTupleValue"");
            final AnnotationTree annotationTree = AnnotationTree.buildFrom(aptUtils, globalParsingContext, elm);
            strategy.validate(aptUtils, annotationTree, ""listTupleValue"", rawClass);
        });
        launchTest(TestEntityWithNestedTypes.class);
    }
",non-flaky,5
136505,doanduyhai_Achilles,FrozenNestedTypeStrategyTest.should_fail_for_non_frozen_list_udtValue,"    @Test
    public void should_fail_for_non_frozen_list_udtValue() throws Exception {
        setExec(aptUtils -> {
            final NestedTypeValidator2_1 strategy = new NestedTypeValidator2_1();
            final String className = TestEntityWithNestedTypes.class.getCanonicalName();
            final TypeName rawClass = ClassName.get(TestEntityWithNestedTypes.class);
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            // private List<UDTValue> listUdtValue;
            VariableElement elm = findFieldInType(typeElement, ""listUdtValue"");
            final AnnotationTree annotationTree = AnnotationTree.buildFrom(aptUtils, globalParsingContext, elm);
            strategy.validate(aptUtils, annotationTree, ""listUdtValue"", rawClass);
        });
        failTestWithMessage(""collections/array type/UDT "" +
                ""'com.datastax.driver.core.UDTValue' "" +
                ""in 'listUdtValue' "" +
                ""of class 'info.archinnov.achilles.internals.sample_classes.parser.strategy.TestEntityWithNestedTypes' "" +
                ""should be annotated with @Frozen"", TestEntityWithNestedTypes.class);
    }
",non-flaky,5
136506,doanduyhai_Achilles,FrozenNestedTypeStrategyTest.should_fail_for_non_frozen_map_list,"    @Test
    public void should_fail_for_non_frozen_map_list() throws Exception {
        setExec(aptUtils -> {
            final NestedTypeValidator2_1 strategy = new NestedTypeValidator2_1();
            final String className = TestEntityWithNestedTypes.class.getCanonicalName();
            final TypeName rawClass = ClassName.get(TestEntityWithNestedTypes.class);
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            // private Map<Integer, List<String>> mapList;
            VariableElement elm = findFieldInType(typeElement, ""mapList"");
            final AnnotationTree annotationTree = AnnotationTree.buildFrom(aptUtils, globalParsingContext, elm);
            strategy.validate(aptUtils, annotationTree, ""mapList"", rawClass);
        });
        failTestWithMessage(""collections/array type/UDT "" +
                ""'java.util.List<java.lang.String>' "" +
                ""in 'mapList' "" +
                ""of class 'info.archinnov.achilles.internals.sample_classes.parser.strategy.TestEntityWithNestedTypes' "" +
                ""should be annotated with @Frozen"", TestEntityWithNestedTypes.class);
    }
",non-flaky,5
136507,doanduyhai_Achilles,FrozenNestedTypeStrategyTest.should_fail_for_non_frozen_map_udt,"    @Test
    public void should_fail_for_non_frozen_map_udt() throws Exception {
        setExec(aptUtils -> {
            final NestedTypeValidator2_1 strategy = new NestedTypeValidator2_1();
            final String className = TestEntityWithNestedTypes.class.getCanonicalName();
            final TypeName rawClass = ClassName.get(TestEntityWithNestedTypes.class);
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            // private Map<Integer, TestUDT> mapUdt;
            VariableElement elm = findFieldInType(typeElement, ""mapUdt"");
            final AnnotationTree annotationTree = AnnotationTree.buildFrom(aptUtils, globalParsingContext, elm);
            strategy.validate(aptUtils, annotationTree, ""mapUdt"", rawClass);
        });
        failTestWithMessage(""collections/array type/UDT "" +
                ""'info.archinnov.achilles.internals.sample_classes.parser.field.TestUDT' "" +
                ""in 'mapUdt' "" +
                ""of class 'info.archinnov.achilles.internals.sample_classes.parser.strategy.TestEntityWithNestedTypes' "" +
                ""should be annotated with @Frozen"", TestEntityWithNestedTypes.class);
    }
",non-flaky,5
136508,doanduyhai_Achilles,FrozenNestedTypeStrategyTest.should_not_fail_for_non_frozen_map_tuple,"    @Test
    public void should_not_fail_for_non_frozen_map_tuple() throws Exception {
        setExec(aptUtils -> {
            final NestedTypeValidator2_1 strategy = new NestedTypeValidator2_1();
            final String className = TestEntityWithNestedTypes.class.getCanonicalName();
            final TypeName rawClass = ClassName.get(TestEntityWithNestedTypes.class);
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            // private Map<Integer, Tuple2<Integer, String>> mapTuple;
            VariableElement elm = findFieldInType(typeElement, ""mapTuple"");
            final AnnotationTree annotationTree = AnnotationTree.buildFrom(aptUtils, globalParsingContext, elm);
            strategy.validate(aptUtils, annotationTree, ""mapTuple"", rawClass);
        });
        launchTest(TestEntityWithNestedTypes.class);
    }
",non-flaky,5
136509,doanduyhai_Achilles,FrozenNestedTypeStrategyTest.should_not_fail_for_non_frozen_map_tupleValue,"    @Test
    public void should_not_fail_for_non_frozen_map_tupleValue() throws Exception {
        setExec(aptUtils -> {
            final NestedTypeValidator2_1 strategy = new NestedTypeValidator2_1();
            final String className = TestEntityWithNestedTypes.class.getCanonicalName();
            final TypeName rawClass = ClassName.get(TestEntityWithNestedTypes.class);
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            // private Map<Integer, TupleValue> mapTupleValue;
            VariableElement elm = findFieldInType(typeElement, ""mapTupleValue"");
            final AnnotationTree annotationTree = AnnotationTree.buildFrom(aptUtils, globalParsingContext, elm);
            strategy.validate(aptUtils, annotationTree, ""mapTupleValue"", rawClass);
        });
        launchTest(TestEntityWithNestedTypes.class);
    }
",non-flaky,5
136510,doanduyhai_Achilles,FrozenNestedTypeStrategyTest.should_fail_for_non_frozen_map_udtValue,"    @Test
    public void should_fail_for_non_frozen_map_udtValue() throws Exception {
        setExec(aptUtils -> {
            final NestedTypeValidator2_1 strategy = new NestedTypeValidator2_1();
            final String className = TestEntityWithNestedTypes.class.getCanonicalName();
            final TypeName rawClass = ClassName.get(TestEntityWithNestedTypes.class);
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            // private Map<Integer, UDTValue> mapUDTValue;
            VariableElement elm = findFieldInType(typeElement, ""mapUDTValue"");
            final AnnotationTree annotationTree = AnnotationTree.buildFrom(aptUtils, globalParsingContext, elm);
            strategy.validate(aptUtils, annotationTree, ""mapUDTValue"", rawClass);
        });
        failTestWithMessage(""collections/array type/UDT "" +
                ""'com.datastax.driver.core.UDTValue' "" +
                ""in 'mapUDTValue' "" +
                ""of class 'info.archinnov.achilles.internals.sample_classes.parser.strategy.TestEntityWithNestedTypes' "" +
                ""should be annotated with @Frozen"", TestEntityWithNestedTypes.class);
    }
",non-flaky,5
136511,doanduyhai_Achilles,FrozenNestedTypeStrategyTest.should_fail_for_non_frozen_map_listKey,"    @Test
    public void should_fail_for_non_frozen_map_listKey() throws Exception {
        setExec(aptUtils -> {
            final NestedTypeValidator2_1 strategy = new NestedTypeValidator2_1();
            final String className = TestEntityWithNestedTypes.class.getCanonicalName();
            final TypeName rawClass = ClassName.get(TestEntityWithNestedTypes.class);
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            // private Map<List<Integer>, String> mapListKey;
            VariableElement elm = findFieldInType(typeElement, ""mapListKey"");
            final AnnotationTree annotationTree = AnnotationTree.buildFrom(aptUtils, globalParsingContext, elm);
            strategy.validate(aptUtils, annotationTree, ""mapListKey"", rawClass);
        });
        failTestWithMessage(""Map key of type collection/UDT "" +
                ""'java.util.List<java.lang.Integer>' "" +
                ""in 'mapListKey' "" +
                ""of class 'info.archinnov.achilles.internals.sample_classes.parser.strategy.TestEntityWithNestedTypes' "" +
                ""should be annotated with @Frozen"", TestEntityWithNestedTypes.class);
    }
",non-flaky,5
136512,doanduyhai_Achilles,FrozenNestedTypeStrategyTest.should_fail_for_non_frozen_map_udtKey,"    @Test
    public void should_fail_for_non_frozen_map_udtKey() throws Exception {
        setExec(aptUtils -> {
            final NestedTypeValidator2_1 strategy = new NestedTypeValidator2_1();
            final String className = TestEntityWithNestedTypes.class.getCanonicalName();
            final TypeName rawClass = ClassName.get(TestEntityWithNestedTypes.class);
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            // private Map<TestUDT, String> mapUdtKey;
            VariableElement elm = findFieldInType(typeElement, ""mapUdtKey"");
            final AnnotationTree annotationTree = AnnotationTree.buildFrom(aptUtils, globalParsingContext, elm);
            strategy.validate(aptUtils, annotationTree, ""mapUdtKey"", rawClass);
        });
        failTestWithMessage(""Map key of type collection/UDT "" +
                ""'info.archinnov.achilles.internals.sample_classes.parser.field.TestUDT' "" +
                ""in 'mapUdtKey' "" +
                ""of class 'info.archinnov.achilles.internals.sample_classes.parser.strategy.TestEntityWithNestedTypes' "" +
                ""should be annotated with @Frozen"", TestEntityWithNestedTypes.class);
    }
",non-flaky,5
136513,doanduyhai_Achilles,FrozenNestedTypeStrategyTest.should_not_fail_for_non_frozen_map_tupleKey,"    @Test
    public void should_not_fail_for_non_frozen_map_tupleKey() throws Exception {
        setExec(aptUtils -> {
            final NestedTypeValidator2_1 strategy = new NestedTypeValidator2_1();
            final String className = TestEntityWithNestedTypes.class.getCanonicalName();
            final TypeName rawClass = ClassName.get(TestEntityWithNestedTypes.class);
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            // private Map<Tuple4<Integer, Integer, String, String>, String> mapTupleKey;
            VariableElement elm = findFieldInType(typeElement, ""mapTupleKey"");
            final AnnotationTree annotationTree = AnnotationTree.buildFrom(aptUtils, globalParsingContext, elm);
            strategy.validate(aptUtils, annotationTree, ""mapTupleKey"", rawClass);
        });
        launchTest(TestEntityWithNestedTypes.class);
    }
",non-flaky,5
136514,doanduyhai_Achilles,FrozenNestedTypeStrategyTest.should_not_fail_for_non_frozen_map_tupleValueKey,"    @Test
    public void should_not_fail_for_non_frozen_map_tupleValueKey() throws Exception {
        setExec(aptUtils -> {
            final NestedTypeValidator2_1 strategy = new NestedTypeValidator2_1();
            final String className = TestEntityWithNestedTypes.class.getCanonicalName();
            final TypeName rawClass = ClassName.get(TestEntityWithNestedTypes.class);
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            // private Map<TupleValue, String> mapTupleValueKey;
            VariableElement elm = findFieldInType(typeElement, ""mapTupleValueKey"");
            final AnnotationTree annotationTree = AnnotationTree.buildFrom(aptUtils, globalParsingContext, elm);
            strategy.validate(aptUtils, annotationTree, ""mapTupleValueKey"", rawClass);
        });
        launchTest(TestEntityWithNestedTypes.class);
    }
",non-flaky,5
136515,doanduyhai_Achilles,FrozenNestedTypeStrategyTest.should_fail_for_non_frozen_map_udtValueKey,"    @Test
    public void should_fail_for_non_frozen_map_udtValueKey() throws Exception {
        setExec(aptUtils -> {
            final NestedTypeValidator2_1 strategy = new NestedTypeValidator2_1();
            final String className = TestEntityWithNestedTypes.class.getCanonicalName();
            final TypeName rawClass = ClassName.get(TestEntityWithNestedTypes.class);
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            // private Map<UDTValue, String> mapUDTValueKey;
            VariableElement elm = findFieldInType(typeElement, ""mapUDTValueKey"");
            final AnnotationTree annotationTree = AnnotationTree.buildFrom(aptUtils, globalParsingContext, elm);
            strategy.validate(aptUtils, annotationTree, ""mapUDTValueKey"", rawClass);
        });
        failTestWithMessage(""Map key of type collection/UDT "" +
                ""'com.datastax.driver.core.UDTValue' "" +
                ""in 'mapUDTValueKey' "" +
                ""of class 'info.archinnov.achilles.internals.sample_classes.parser.strategy.TestEntityWithNestedTypes' "" +
                ""should be annotated with @Frozen"", TestEntityWithNestedTypes.class);
    }
",non-flaky,5
136516,doanduyhai_Achilles,FrozenNestedTypeStrategyTest.should_not_fail_for_non_frozen_tuple_list,"    @Test
    public void should_not_fail_for_non_frozen_tuple_list() throws Exception {
        setExec(aptUtils -> {
            final NestedTypeValidator2_1 strategy = new NestedTypeValidator2_1();
            final String className = TestEntityWithNestedTypes.class.getCanonicalName();
            final TypeName rawClass = ClassName.get(TestEntityWithNestedTypes.class);
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            // private Tuple5<Integer, List<String>, Integer, Integer, String> tupleList;
            VariableElement elm = findFieldInType(typeElement, ""tupleList"");
            final AnnotationTree annotationTree = AnnotationTree.buildFrom(aptUtils, globalParsingContext, elm);
            strategy.validate(aptUtils, annotationTree, ""tupleList"", rawClass);
        });
        launchTest(TestEntityWithNestedTypes.class);
    }
",non-flaky,5
136517,doanduyhai_Achilles,FrozenNestedTypeStrategyTest.should_not_fail_for_non_frozen_tuple_udt,"    @Test
    public void should_not_fail_for_non_frozen_tuple_udt() throws Exception {
        setExec(aptUtils -> {
            final NestedTypeValidator2_1 strategy = new NestedTypeValidator2_1();
            final String className = TestEntityWithNestedTypes.class.getCanonicalName();
            final TypeName rawClass = ClassName.get(TestEntityWithNestedTypes.class);
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            // private Tuple6<Integer, Integer, Integer, Integer, String, TestUDT> tupleUDT;
            VariableElement elm = findFieldInType(typeElement, ""tupleUDT"");
            final AnnotationTree annotationTree = AnnotationTree.buildFrom(aptUtils, globalParsingContext, elm);
            strategy.validate(aptUtils, annotationTree, ""tupleUDT"", rawClass);
        });
        launchTest(TestEntityWithNestedTypes.class);
    }
",non-flaky,5
136518,doanduyhai_Achilles,FrozenNestedTypeStrategyTest.should_not_fail_for_non_frozen_tuple_udtValue,"    @Test
    public void should_not_fail_for_non_frozen_tuple_udtValue() throws Exception {
        setExec(aptUtils -> {
            final NestedTypeValidator2_1 strategy = new NestedTypeValidator2_1();
            final String className = TestEntityWithNestedTypes.class.getCanonicalName();
            final TypeName rawClass = ClassName.get(TestEntityWithNestedTypes.class);
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            // private Tuple7<Integer, Integer, Integer, Integer, String, String, UDTValue> tupleUDTValue;
            VariableElement elm = findFieldInType(typeElement, ""tupleUDTValue"");
            final AnnotationTree annotationTree = AnnotationTree.buildFrom(aptUtils, globalParsingContext, elm);
            strategy.validate(aptUtils, annotationTree, ""tupleUDTValue"", rawClass);
        });
        launchTest(TestEntityWithNestedTypes.class);
    }
",non-flaky,5
136519,doanduyhai_Achilles,FrozenNestedTypeStrategyTest.should_not_fail_for_non_frozen_tuple_tupleValue,"    @Test
    public void should_not_fail_for_non_frozen_tuple_tupleValue() throws Exception {
        setExec(aptUtils -> {
            final NestedTypeValidator2_1 strategy = new NestedTypeValidator2_1();
            final String className = TestEntityWithNestedTypes.class.getCanonicalName();
            final TypeName rawClass = ClassName.get(TestEntityWithNestedTypes.class);
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            // private Tuple8<Integer, Integer, Integer, Integer, String, String, String, TupleValue> tupleTupleValue;
            VariableElement elm = findFieldInType(typeElement, ""tupleTupleValue"");
            final AnnotationTree annotationTree = AnnotationTree.buildFrom(aptUtils, globalParsingContext, elm);
            strategy.validate(aptUtils, annotationTree, ""tupleTupleValue"", rawClass);
        });
        launchTest(TestEntityWithNestedTypes.class);
    }
",non-flaky,5
136520,doanduyhai_Achilles,FrozenNestedTypeStrategyTest.should_not_fail_for_non_frozen_tuple_map,"    @Test
    public void should_not_fail_for_non_frozen_tuple_map() throws Exception {
        setExec(aptUtils -> {
            final NestedTypeValidator2_1 strategy = new NestedTypeValidator2_1();
            final String className = TestEntityWithNestedTypes.class.getCanonicalName();
            final TypeName rawClass = ClassName.get(TestEntityWithNestedTypes.class);
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            // private Tuple9<Integer, Integer, Integer, Integer, String, String, String, String, Map<Integer, String>> tupleMap;
            VariableElement elm = findFieldInType(typeElement, ""tupleMap"");
            final AnnotationTree annotationTree = AnnotationTree.buildFrom(aptUtils, globalParsingContext, elm);
            strategy.validate(aptUtils, annotationTree, ""tupleMap"", rawClass);
        });
        launchTest(TestEntityWithNestedTypes.class);
    }
",non-flaky,5
136521,doanduyhai_Achilles,FrozenNestedTypeStrategyTest.should_validate_index_depth_1,"    @Test
    public void should_validate_index_depth_1() throws Exception {
        setExec(aptUtils -> {
            final NestedTypeValidator2_1 strategy = new NestedTypeValidator2_1();
            final String className = TestEntityWithNestedTypes.class.getCanonicalName();
            final TypeName rawClass = ClassName.get(TestEntityWithNestedTypes.class);
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            // @Index private String indexedString;
            VariableElement elm = findFieldInType(typeElement, ""indexedString"");
            final AnnotationTree annotationTree = AnnotationTree.buildFrom(aptUtils, globalParsingContext, elm);
            strategy.validate(aptUtils, annotationTree, ""indexedString"", rawClass);
        });
        launchTest(TestEntityWithNestedTypes.class);
    }
",non-flaky,5
136522,doanduyhai_Achilles,FrozenNestedTypeStrategyTest.should_validate_index_depth_2,"    @Test
    public void should_validate_index_depth_2() throws Exception {
        setExec(aptUtils -> {
            final NestedTypeValidator2_1 strategy = new NestedTypeValidator2_1();
            final String className = TestEntityWithNestedTypes.class.getCanonicalName();
            final TypeName rawClass = ClassName.get(TestEntityWithNestedTypes.class);
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            // private Map<Integer, @Frozen @Index List<String>> indexedMapList;
            VariableElement elm = findFieldInType(typeElement, ""indexedMapList"");
            final AnnotationTree annotationTree = AnnotationTree.buildFrom(aptUtils, globalParsingContext, elm);
            strategy.validate(aptUtils, annotationTree, ""indexedMapList"", rawClass);
        });
        launchTest(TestEntityWithNestedTypes.class);
    }
",non-flaky,5
136523,doanduyhai_Achilles,FrozenNestedTypeStrategyTest.should_validate_index_depth_2_as_map_key,"    @Test
    public void should_validate_index_depth_2_as_map_key() throws Exception {
        setExec(aptUtils -> {
            final NestedTypeValidator2_1 strategy = new NestedTypeValidator2_1();
            final String className = TestEntityWithNestedTypes.class.getCanonicalName();
            final TypeName rawClass = ClassName.get(TestEntityWithNestedTypes.class);
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            // private Map<@Index Integer, @Frozen List<String>> indexedMapKey;
            VariableElement elm = findFieldInType(typeElement, ""indexedMapKey"");
            final AnnotationTree annotationTree = AnnotationTree.buildFrom(aptUtils, globalParsingContext, elm);
            strategy.validate(aptUtils, annotationTree, ""indexedMapKey"", rawClass);
        });
        launchTest(TestEntityWithNestedTypes.class);
    }
",non-flaky,5
136524,doanduyhai_Achilles,FrozenNestedTypeStrategyTest.should_fail_validating_index_depth_gt_2,"    @Test
    public void should_fail_validating_index_depth_gt_2() throws Exception {
        setExec(aptUtils -> {
            final NestedTypeValidator2_1 strategy = new NestedTypeValidator2_1();
            final String className = TestEntityWithNestedTypes.class.getCanonicalName();
            final TypeName rawClass = ClassName.get(TestEntityWithNestedTypes.class);
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            // private Map<Integer, @Frozen Map<Integer, @Index List<String>>> nestedIndex;
            VariableElement elm = findFieldInType(typeElement, ""nestedIndex"");
            final AnnotationTree annotationTree = AnnotationTree.buildFrom(aptUtils, globalParsingContext, elm);
            strategy.validate(aptUtils, annotationTree, ""nestedIndex"", rawClass);
        });
        failTestWithMessage(""@Index annotation cannot be nested for depth > 2 for field 'nestedIndex' of class 'info.archinnov.achilles.internals.sample_classes.parser.strategy.TestEntityWithNestedTypes'"",
                TestEntityWithNestedTypes.class);
    }
",non-flaky,5
136525,doanduyhai_Achilles,IndexSelectDSLCodeGenTest.should_generate_indexed_select_dsl_class_with_udt,"  @Test
  public void should_generate_indexed_select_dsl_class_with_udt() throws Exception {
    setExec(aptUtils -> {

      final GlobalParsingContext globalContext = new GlobalParsingContext(
          V2_1.INSTANCE,
          InsertStrategy.ALL_FIELDS,
          new LowerCaseNaming(),
          FieldFilter.EXPLICIT_ENTITY_FIELD_FILTER,
          FieldFilter.EXPLICIT_UDT_FIELD_FILTER,
          Optional.empty());

      final String className = TestEntityWithIndexAndUDT.class.getCanonicalName();
      final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

      final EntityParser entityParser = new EntityParser(aptUtils);

      final EntityMetaCodeGen.EntityMetaSignature entityMetaSignature = entityParser.parseEntity(typeElement, globalContext);

      final IndexSelectDSLCodeGen2_1 indexSelectDSLCodeGen2_1 = new IndexSelectDSLCodeGen2_1();

      final StringBuilder builder = new StringBuilder();
      try {
        JavaFile.builder(TypeUtils.GENERATED_PACKAGE, indexSelectDSLCodeGen2_1.buildSelectClass(globalContext, entityMetaSignature))
            .build()
            .writeTo(builder);
      } catch (IOException e) {
        Assertions.assertThat(false).as(""IOException when generating class : %s"", e.getMessage()).isTrue();
      }

      assertThat(builder.toString().trim()).isEqualTo(
          readCodeBlockFromFile(""expected_code/dsl/index/should_generate_indexed_select_dsl_class_with_udt.txt"", false));
    });
    launchTest();
  }
",non-flaky,5
136526,doanduyhai_Achilles,ManagerCodeGenTest.should_generate_manager_class_for_index_dsl,"    @Test
    public void should_generate_manager_class_for_index_dsl() throws Exception {
        setExec(aptUtils -> {

            final GlobalParsingContext globalContext = new GlobalParsingContext(
                    V3_7.INSTANCE,
                    InsertStrategy.ALL_FIELDS,
                    new LowerCaseNaming(),
                    FieldFilter.EXPLICIT_ENTITY_FIELD_FILTER,
                    FieldFilter.EXPLICIT_UDT_FIELD_FILTER,
                    Optional.empty());

            final String className = TestEntityWithSASI.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            final EntityParser entityParser = new EntityParser(aptUtils);

            final EntityMetaSignature entityMetaSignature = entityParser.parseEntity(typeElement, globalContext);

            final ManagerAndDSLClasses managerAndDSLClasses = ManagerCodeGen.buildManager(globalContext, aptUtils, entityMetaSignature);

            final StringBuilder builder = new StringBuilder();
            try {
                JavaFile.builder(TypeUtils.GENERATED_PACKAGE, managerAndDSLClasses.managerClass)
                        .build()
                        .writeTo(builder);
            } catch (IOException e) {
                Assertions.assertThat(false).as(""IOException when generating class : %s"", e.getMessage()).isTrue();
            }

            assertThat(builder.toString().trim())
                    .isEqualTo(readCodeBlockFromFile(""expected_code/manager/should_generate_manager_class_for_index_dsl.txt""));
        });
        launchTest();
    }
",non-flaky,5
136527,doanduyhai_Achilles,SelectDSLCodeGenTest.should_generate_select_dsl_class_for_udt_as_clustering,"    @Test
    public void should_generate_select_dsl_class_for_udt_as_clustering() throws Exception {
        setExec(aptUtils -> {

            final GlobalParsingContext globalContext = new GlobalParsingContext(
                    V2_1.INSTANCE,
                    InsertStrategy.ALL_FIELDS,
                    new LowerCaseNaming(),
                    FieldFilter.EXPLICIT_ENTITY_FIELD_FILTER,
                    FieldFilter.EXPLICIT_UDT_FIELD_FILTER,
                    Optional.empty());

            final String className = TestEntityWithUDTAsClustering.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            final EntityParser entityParser = new EntityParser(aptUtils);

            final EntityMetaCodeGen.EntityMetaSignature entityMetaSignature = entityParser.parseEntity(typeElement, globalContext);

            final SelectDSLCodeGen2_1 selectDSLCodeGen2_1 = new SelectDSLCodeGen2_1();

            final StringBuilder builder = new StringBuilder();
            try {
                JavaFile.builder(TypeUtils.GENERATED_PACKAGE, selectDSLCodeGen2_1.buildSelectClass(globalContext, entityMetaSignature))
                        .build()
                        .writeTo(builder);
            } catch (IOException e) {
                Assertions.assertThat(false).as(""IOException when generating class : %s"", e.getMessage()).isTrue();
            }

            assertThat(builder.toString().trim()).isEqualTo(
                    readCodeBlockFromFile(""expected_code/dsl/select/should_generate_manager_class_for_index_dsl.txt"", false));
        });
        launchTest();
    }
",non-flaky,5
136528,doanduyhai_Achilles,EntityMetaCodeGenTest.should_build_entity_with_simple_partition_key,"    @Test
    public void should_build_entity_with_simple_partition_key() throws Exception {
        setExec(aptUtils -> {
            final String className = TestEntityWithSimplePartitionKey.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            final EntityMetaCodeGen builder = new EntityMetaCodeGen(aptUtils);
            final List<FieldParser.FieldMetaSignature> parsingResults = getTypeParsingResults(aptUtils, typeElement, context);
            final TypeSpec typeSpec = builder.buildEntityMeta(EntityType.TABLE, typeElement, context, parsingResults, emptyList()).sourceCode;

            assertThat(buildSource(typeSpec)).isEqualTo(
                    readCodeBlockFromFile(""expected_code/entity_meta_builder/should_build_entity_with_simple_partition_key.txt""));
        });
        launchTest(TestEntityWithSimplePartitionKey.class);
    }
",non-flaky,5
136529,doanduyhai_Achilles,EntityMetaCodeGenTest.should_build_entity_with_composite_partition_key,"    @Test
    public void should_build_entity_with_composite_partition_key() throws Exception {
        setExec(aptUtils -> {
            final String className = TestEntityWithCompositePartitionKey.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            final EntityMetaCodeGen builder = new EntityMetaCodeGen(aptUtils);
            final List<FieldParser.FieldMetaSignature> parsingResults = getTypeParsingResults(aptUtils, typeElement, context);
            final TypeSpec typeSpec = builder.buildEntityMeta(EntityType.TABLE, typeElement, context, parsingResults, emptyList()).sourceCode;

            assertThat(buildSource(typeSpec)).isEqualTo(
                    readCodeBlockFromFile(""expected_code/entity_meta_builder/should_build_entity_with_composite_partition_key.txt""));
        });
        launchTest(TestEntityWithCompositePartitionKey.class);
    }
",non-flaky,5
136530,doanduyhai_Achilles,EntityMetaCodeGenTest.should_build_entity_with_clustering_column,"    @Test
    public void should_build_entity_with_clustering_column() throws Exception {
        setExec(aptUtils -> {
            final String className = TestEntityWithClusteringColumns.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            final EntityMetaCodeGen builder = new EntityMetaCodeGen(aptUtils);
            final List<FieldParser.FieldMetaSignature> parsingResults = getTypeParsingResults(aptUtils, typeElement, context);
            final TypeSpec typeSpec = builder.buildEntityMeta(EntityType.TABLE, typeElement, context, parsingResults, emptyList()).sourceCode;

            assertThat(buildSource(typeSpec)).isEqualTo(
                    readCodeBlockFromFile(""expected_code/entity_meta_builder/should_build_entity_with_clustering_column.txt""));
        });
        launchTest(TestEntityWithClusteringColumns.class);
    }
",non-flaky,5
136531,doanduyhai_Achilles,EntityMetaCodeGenTest.should_build_entity_with_counter_column,"    @Test
    public void should_build_entity_with_counter_column() throws Exception {
        setExec(aptUtils -> {
            final String className = TestEntityWithCounterColumn.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            final EntityMetaCodeGen builder = new EntityMetaCodeGen(aptUtils);
            final List<FieldParser.FieldMetaSignature> parsingResults = getTypeParsingResults(aptUtils, typeElement, context);
            final TypeSpec typeSpec = builder.buildEntityMeta(EntityType.TABLE, typeElement, context, parsingResults, emptyList()).sourceCode;

            assertThat(buildSource(typeSpec)).isEqualTo(
                    readCodeBlockFromFile(""expected_code/entity_meta_builder/should_build_entity_with_counter_column.txt""));
        });
        launchTest(TestEntityWithCounterColumn.class);
    }
",non-flaky,5
136532,doanduyhai_Achilles,EntityMetaCodeGenTest.should_build_entity_with_static_counter_column,"    @Test
    public void should_build_entity_with_static_counter_column() throws Exception {
        setExec(aptUtils -> {
            final String className = TestEntityWithStaticCounterColumn.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            final EntityMetaCodeGen builder = new EntityMetaCodeGen(aptUtils);
            final List<FieldParser.FieldMetaSignature> parsingResults = getTypeParsingResults(aptUtils, typeElement, context);
            final TypeSpec typeSpec = builder.buildEntityMeta(EntityType.TABLE, typeElement, context, parsingResults, emptyList()).sourceCode;

            assertThat(buildSource(typeSpec)).isEqualTo(
                    readCodeBlockFromFile(""expected_code/entity_meta_builder/should_build_entity_with_static_counter_column.txt""));
        });
        launchTest(TestEntityWithStaticCounterColumn.class);
    }
",non-flaky,5
136533,doanduyhai_Achilles,EntityMetaCodeGenTest.should_build_entity_with_static_column,"    @Test
    public void should_build_entity_with_static_column() throws Exception {
        setExec(aptUtils -> {
            final String className = TestEntityWithStaticColumn.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            final EntityMetaCodeGen builder = new EntityMetaCodeGen(aptUtils);
            final List<FieldParser.FieldMetaSignature> parsingResults = getTypeParsingResults(aptUtils, typeElement, context);
            final TypeSpec typeSpec = builder.buildEntityMeta(EntityType.TABLE, typeElement, context, parsingResults, emptyList()).sourceCode;

            assertThat(buildSource(typeSpec)).isEqualTo(
                    readCodeBlockFromFile(""expected_code/entity_meta_builder/should_build_entity_with_static_column.txt""));
        });
        launchTest(TestEntityWithStaticColumn.class);
    }
",non-flaky,5
136534,doanduyhai_Achilles,EntityMetaCodeGenTest.should_build_entity_with_computed_column,"    @Test
    public void should_build_entity_with_computed_column() throws Exception {
        setExec(aptUtils -> {
            final String className = TestEntityWithComputedColumn.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            final EntityMetaCodeGen builder = new EntityMetaCodeGen(aptUtils);
            final List<FieldParser.FieldMetaSignature> parsingResults = getTypeParsingResults(aptUtils, typeElement, context);
            final TypeSpec typeSpec = builder.buildEntityMeta(EntityType.TABLE, typeElement, context, parsingResults, emptyList()).sourceCode;

            assertThat(buildSource(typeSpec)).isEqualTo(
                    readCodeBlockFromFile(""expected_code/entity_meta_builder/should_build_entity_with_computed_column.txt""));
        });
        launchTest(TestEntityWithComputedColumn.class);
    }
",non-flaky,5
136535,doanduyhai_Achilles,EntityMetaCodeGenTest.should_build_inherited_entity,"    @Test
    public void should_build_inherited_entity() throws Exception {
        setExec(aptUtils -> {
            final String className = TestEntityAsChild.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            final EntityMetaCodeGen builder = new EntityMetaCodeGen(aptUtils);
            final List<FieldParser.FieldMetaSignature> parsingResults = getTypeParsingResults(aptUtils, typeElement, context);
            final TypeSpec typeSpec = builder.buildEntityMeta(EntityType.TABLE, typeElement, context, parsingResults, emptyList()).sourceCode;

            assertThat(buildSource(typeSpec)).isEqualTo(
                    readCodeBlockFromFile(""expected_code/entity_meta_builder/should_build_inherited_entity.txt""));
        });
        launchTest(TestEntityAsChild.class);
    }
",non-flaky,5
136536,doanduyhai_Achilles,EntityMetaCodeGenTest.should_build_entity_with_static_annotations,"    @Test
    public void should_build_entity_with_static_annotations() throws Exception {
        setExec(aptUtils -> {
            final String className = TestEntityWithStaticAnnotations.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            final EntityMetaCodeGen builder = new EntityMetaCodeGen(aptUtils);
            final List<FieldParser.FieldMetaSignature> parsingResults = getTypeParsingResults(aptUtils, typeElement, context);
            final TypeSpec typeSpec = builder.buildEntityMeta(EntityType.TABLE, typeElement, context, parsingResults, emptyList()).sourceCode;

            assertThat(buildSource(typeSpec)).isEqualTo(
                    readCodeBlockFromFile(""expected_code/entity_meta_builder/should_build_entity_with_static_annotations.txt""));
        });
        launchTest(TestEntityWithStaticAnnotations.class);
    }
",non-flaky,5
136537,doanduyhai_Achilles,EntityMetaCodeGenTest.should_build_entity_with_complex_types,"    @Test
    public void should_build_entity_with_complex_types() throws Exception {
        setExec(aptUtils -> {
            final String className = TestEntityWithComplexTypes.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            final EntityMetaCodeGen builder = new EntityMetaCodeGen(aptUtils);
            final List<FieldParser.FieldMetaSignature> parsingResults = getTypeParsingResults(aptUtils, typeElement, context);
            final TypeSpec typeSpec = builder.buildEntityMeta(EntityType.TABLE, typeElement, context, parsingResults, emptyList()).sourceCode;

            assertThat(buildSource(typeSpec)).isEqualTo(
                    readCodeBlockFromFile(""expected_code/entity_meta_builder/should_build_entity_with_complex_types.txt""));
        });
        launchTest(TestEntityWithComplexTypes.class);
    }
",non-flaky,5
136538,doanduyhai_Achilles,EntityMetaCodeGenTest.should_build_entity_with_complex_counter_types,"    @Test
    public void should_build_entity_with_complex_counter_types() throws Exception {
        setExec(aptUtils -> {
            final String className = TestEntityWithComplexCounters.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            final EntityMetaCodeGen builder = new EntityMetaCodeGen(aptUtils);
            final List<FieldParser.FieldMetaSignature> parsingResults = getTypeParsingResults(aptUtils, typeElement, context);
            final TypeSpec typeSpec = builder.buildEntityMeta(EntityType.TABLE, typeElement, context, parsingResults, emptyList()).sourceCode;

            assertThat(buildSource(typeSpec)).isEqualTo(
                    readCodeBlockFromFile(""expected_code/entity_meta_builder/should_build_entity_with_complex_counter_types.txt""));
        });
        launchTest(TestEntityWithComplexCounters.class);
    }
",non-flaky,5
136539,doanduyhai_Achilles,EntityMetaCodeGenTest.should_build_entity_with_complex_indices,"    @Test
    public void should_build_entity_with_complex_indices() throws Exception {
        setExec(aptUtils -> {
            final String className = TestEntityWithComplexIndices.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            final EntityMetaCodeGen builder = new EntityMetaCodeGen(aptUtils);
            final List<FieldParser.FieldMetaSignature> parsingResults = getTypeParsingResults(aptUtils, typeElement, context);
            final TypeSpec typeSpec = builder.buildEntityMeta(EntityType.TABLE, typeElement, context, parsingResults, emptyList()).sourceCode;

            assertThat(buildSource(typeSpec)).isEqualTo(
                    readCodeBlockFromFile(""expected_code/entity_meta_builder/should_build_entity_with_complex_indices.txt""));
        });
        launchTest(TestEntityWithComplexIndices.class);
    }
",non-flaky,5
136540,doanduyhai_Achilles,EntityMetaCodeGenTest.should_build_entity_with_implicit_field_parsing,"    @Test
    public void should_build_entity_with_implicit_field_parsing() throws Exception {
        setExec(aptUtils -> {
            final String className = TestEntityWithImplicitFieldParsing.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            GlobalParsingContext globalParsingContext = new GlobalParsingContext(V3_0.INSTANCE,
                    InsertStrategy.ALL_FIELDS, new SnakeCaseNaming(), FieldFilter.IMPLICIT_ENTITY_FIELD_FILTER,
                    FieldFilter.IMPLICIT_UDT_FIELD_FILTER, Optional.empty());

            final EntityMetaCodeGen builder = new EntityMetaCodeGen(aptUtils);
            final List<FieldParser.FieldMetaSignature> parsingResults = getTypeParsingResults(aptUtils, typeElement, globalParsingContext);
            final TypeSpec typeSpec = builder.buildEntityMeta(EntityType.TABLE, typeElement, globalParsingContext, parsingResults, emptyList()).sourceCode;

            assertThat(buildSource(typeSpec)).isEqualTo(
                    readCodeBlockFromFile(""expected_code/entity_meta_builder/should_build_entity_with_implicit_field_parsing.txt""));
        });
        launchTest(TestEntityWithImplicitFieldParsing.class);
    }
",non-flaky,5
136541,doanduyhai_Achilles,EntityMetaCodeGenTest.should_build_entity_with_custom_constructor,"    @Test
    public void should_build_entity_with_custom_constructor() throws Exception {
        final AptAssertOK aptAssertOK = aptUtils -> {
            final String className = TestEntityWithCustomConstructor.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            final EntityMetaCodeGen builder = new EntityMetaCodeGen(aptUtils);
            final List<AccessorsExclusionContext> exclusionContexts = Arrays.asList(
                    new AccessorsExclusionContext(""id"", false, true),
                    new AccessorsExclusionContext(""date"", false, true),
                    new AccessorsExclusionContext(""value"", false, true));
            final List<FieldParser.FieldMetaSignature> fieldMetaSignatures = getTypeParsingResults(aptUtils, typeElement, exclusionContexts, context);
            final TypeSpec typeSpec = builder.buildEntityMeta(EntityType.TABLE, typeElement, context, fieldMetaSignatures, fieldMetaSignatures).sourceCode;

            assertThat(buildSource(typeSpec)).isEqualTo(
                    readCodeBlockFromFile(""expected_code/entity_meta_builder/should_build_entity_with_custom_constructor.txt""));
        }; setExec(aptAssertOK);
        launchTest(TestEntityWithCustomConstructor.class);
    }
",non-flaky,5
136542,doanduyhai_Achilles,EntityMetaCodeGenTest.should_build_entity_with_custom_constructor_with_declared_fields,"    @Test
    public void should_build_entity_with_custom_constructor_with_declared_fields() throws Exception {
        final AptAssertOK aptAssertOK = aptUtils -> {
            final String className = TestEntityWithCustomConstructorAndDeclaredFields.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            final EntityMetaCodeGen builder = new EntityMetaCodeGen(aptUtils);
            final List<AccessorsExclusionContext> exclusionContexts = Arrays.asList(
                    new AccessorsExclusionContext(""id"", false, true),
                    new AccessorsExclusionContext(""date"", false, true),
                    new AccessorsExclusionContext(""value"", false, true));
            final List<FieldParser.FieldMetaSignature> fieldMetaSignatures = getTypeParsingResults(aptUtils, typeElement, exclusionContexts, context);
            final TypeSpec typeSpec = builder.buildEntityMeta(EntityType.TABLE, typeElement, context, fieldMetaSignatures, fieldMetaSignatures).sourceCode;

            assertThat(buildSource(typeSpec)).isEqualTo(
                    readCodeBlockFromFile(""expected_code/entity_meta_builder/should_build_entity_with_custom_constructor_with_declared_fields.txt""));
        }; setExec(aptAssertOK);
        launchTest(TestEntityWithCustomConstructorAndDeclaredFields.class);
    }
",non-flaky,5
136543,doanduyhai_Achilles,EntityMetaCodeGenTest.should_build_view_meta,"    @Test
    public void should_build_view_meta() throws Exception {
        setExec(aptUtils -> {
            final String className = TestViewSensorByType.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            final EntityMetaCodeGen builder = new EntityMetaCodeGen(aptUtils);
            final List<FieldParser.FieldMetaSignature> parsingResults = getTypeParsingResults(aptUtils, typeElement, context);
            final TypeSpec typeSpec = builder.buildEntityMeta(EntityType.VIEW, typeElement, context, parsingResults, emptyList()).sourceCode;

            assertThat(buildSource(typeSpec)).isEqualTo(
                    readCodeBlockFromFile(""expected_code/entity_meta_builder/should_build_view_meta.txt""));
        });
        launchTest(TestViewSensorByType.class);
    }
",non-flaky,5
136544,doanduyhai_Achilles,EntityMetaCodeGenTest.should_fail_building_abstract_class,"    @Test
    public void should_fail_building_abstract_class() throws Exception {
        setExec(aptUtils -> {
            final String className = TestEntityWithAbstractClass.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            final EntityMetaCodeGen builder = new EntityMetaCodeGen(aptUtils);
            final List<FieldParser.FieldMetaSignature> parsingResults = getTypeParsingResults(aptUtils, typeElement, context);
            builder.buildEntityMeta(EntityType.TABLE, typeElement, context, parsingResults, emptyList());

        });
        failTestWithMessage(
                ""Bean type 'info.archinnov.achilles.internals.sample_classes.parser.entity.TestEntityWithAbstractClass' should not be abstract"",
                TestEntityWithAbstractClass.class);
    }
",non-flaky,5
136545,doanduyhai_Achilles,EntityMetaCodeGenTest.should_fail_building_class_with_no_public_constructor,"    @Test
    public void should_fail_building_class_with_no_public_constructor() throws Exception {
        setExec(aptUtils -> {
            final String className = TestEntityWithNoPublicConstructor.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            final EntityMetaCodeGen builder = new EntityMetaCodeGen(aptUtils);
            final List<FieldParser.FieldMetaSignature> parsingResults = getTypeParsingResults(aptUtils, typeElement, context);
            builder.buildEntityMeta(EntityType.TABLE, typeElement, context, parsingResults, emptyList());
        });
        failTestWithMessage(
                ""Bean type 'info.archinnov.achilles.internals.sample_classes.parser.entity.TestEntityWithNoPublicConstructor' "" +
                        ""should have either a public no-args constructor or ONE custom constructor with annotation @EntityCreator"",
                TestEntityWithNoPublicConstructor.class);
    }
",non-flaky,5
136546,doanduyhai_Achilles,EntityMetaCodeGenTest.should_fail_building_class_with_duplicated_cql_name,"    @Test
    public void should_fail_building_class_with_duplicated_cql_name() throws Exception {
        setExec(aptUtils -> {
            final String className = TestEntityWithDuplicateCQLColumn.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            final EntityMetaCodeGen builder = new EntityMetaCodeGen(aptUtils);
            final List<FieldParser.FieldMetaSignature> parsingResults = getTypeParsingResults(aptUtils, typeElement, context);
            builder.buildEntityMeta(EntityType.TABLE, typeElement, context, parsingResults, emptyList());
        });
        failTestWithMessage(
                ""The class 'info.archinnov.achilles.internals.sample_classes.parser.entity.TestEntityWithDuplicateCQLColumn' already contains a cql column with name 'value'"",
                TestEntityWithDuplicateCQLColumn.class);
    }
",non-flaky,5
136547,doanduyhai_Achilles,EntityMetaCodeGenTest.should_fail_building_class_with_no_partition_key,"    @Test
    public void should_fail_building_class_with_no_partition_key() throws Exception {
        setExec(aptUtils -> {
            final String className = TestEntityWithNoPartitionKey.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            final EntityMetaCodeGen builder = new EntityMetaCodeGen(aptUtils);
            final List<FieldParser.FieldMetaSignature> parsingResults = getTypeParsingResults(aptUtils, typeElement, context);
            builder.buildEntityMeta(EntityType.TABLE, typeElement, context, parsingResults, emptyList());
        });
        failTestWithMessage(
                ""The class 'info.archinnov.achilles.internals.sample_classes.parser.entity.TestEntityWithNoPartitionKey' should have at least 1 partition key (@PartitionKey)"",
                TestEntityWithNoPartitionKey.class);
    }
",non-flaky,5
136548,doanduyhai_Achilles,EntityMetaCodeGenTest.should_fail_building_class_with_static_without_clustering,"    @Test
    public void should_fail_building_class_with_static_without_clustering() throws Exception {
        setExec(aptUtils -> {
            final String className = TestEntityWithStaticWithoutClustering.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            final EntityMetaCodeGen builder = new EntityMetaCodeGen(aptUtils);
            final List<FieldParser.FieldMetaSignature> parsingResults = getTypeParsingResults(aptUtils, typeElement, context);
            builder.buildEntityMeta(EntityType.TABLE, typeElement, context, parsingResults, emptyList());
        });
        failTestWithMessage(
                ""The class 'info.archinnov.achilles.internals.sample_classes.parser.entity.TestEntityWithStaticWithoutClustering' cannot have static columns without at least 1 clustering column"",
                TestEntityWithStaticWithoutClustering.class);
    }
",non-flaky,5
136549,doanduyhai_Achilles,EntityMetaCodeGenTest.should_fail_building_class_with_wrong_computed_column,"    @Test
    public void should_fail_building_class_with_wrong_computed_column() throws Exception {
        setExec(aptUtils -> {
            final String className = TestEntityWithWrongComputed.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            final EntityMetaCodeGen builder = new EntityMetaCodeGen(aptUtils);
            final List<FieldParser.FieldMetaSignature> parsingResults = getTypeParsingResults(aptUtils, typeElement, context);
            builder.buildEntityMeta(EntityType.TABLE, typeElement, context, parsingResults, emptyList());
        });
        failTestWithMessage(
                ""Target field 'one' in @Computed annotation of field 'value' of class 'info.archinnov.achilles.internals.sample_classes.parser.entity.TestEntityWithWrongComputed' does not exist"",
                TestEntityWithWrongComputed.class);
    }
",non-flaky,5
136550,doanduyhai_Achilles,EntityMetaCodeGenTest.should_fail_building_class_with_wrong_composite_partition_key_order,"    @Test
    public void should_fail_building_class_with_wrong_composite_partition_key_order() throws Exception {
        setExec(aptUtils -> {
            final String className = TestEntityWithWrongCompositePartitionKey.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            final EntityMetaCodeGen builder = new EntityMetaCodeGen(aptUtils);
            final List<FieldParser.FieldMetaSignature> parsingResults = getTypeParsingResults(aptUtils, typeElement, context);
            builder.buildEntityMeta(EntityType.TABLE, typeElement, context, parsingResults, emptyList());
        });
        failTestWithMessage(
                ""The @PartitionKey ordering is wrong in class 'info.archinnov.achilles.internals.sample_classes.parser.entity.TestEntityWithWrongCompositePartitionKey'"",
                TestEntityWithWrongCompositePartitionKey.class);
    }
",non-flaky,5
136551,doanduyhai_Achilles,EntityMetaCodeGenTest.should_fail_building_class_with_wrong_clustering_column_order,"    @Test
    public void should_fail_building_class_with_wrong_clustering_column_order() throws Exception {
        setExec(aptUtils -> {
            final String className = TestEntityWithWrongClusteringColumns.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            final EntityMetaCodeGen builder = new EntityMetaCodeGen(aptUtils);
            final List<FieldParser.FieldMetaSignature> parsingResults = getTypeParsingResults(aptUtils, typeElement, context);
            builder.buildEntityMeta(EntityType.TABLE, typeElement, context, parsingResults, emptyList());
        });
        failTestWithMessage(
                ""The @ClusteringColumn ordering is wrong in class 'info.archinnov.achilles.internals.sample_classes.parser.entity.TestEntityWithWrongClusteringColumns'"",
                TestEntityWithWrongClusteringColumns.class);
    }
",non-flaky,5
136552,doanduyhai_Achilles,EntityMetaCodeGenTest.should_fail_building_child_class_with_shadowed_field,"    @Test
    public void should_fail_building_child_class_with_shadowed_field() throws Exception {
        setExec(aptUtils -> {
            final String className = TestEntityAsChildShadowingVariable.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            final EntityMetaCodeGen builder = new EntityMetaCodeGen(aptUtils);
            final List<FieldParser.FieldMetaSignature> parsingResults = getTypeParsingResults(aptUtils, typeElement, context);
            builder.buildEntityMeta(EntityType.TABLE, typeElement, context, parsingResults, emptyList());
        });
        failTestWithMessage(
                ""The class 'info.archinnov.achilles.internals.sample_classes.parser.entity.TestEntityAsChildShadowingVariable' already contains a field with name 'value'"",
                TestEntityAsChildShadowingVariable.class);
    }
",non-flaky,5
136553,doanduyhai_Achilles,EntityMetaCodeGenTest.should_fail_building_view_meta_with_entity_annotation,"    @Test
    public void should_fail_building_view_meta_with_entity_annotation() throws Exception {
        setExec(aptUtils -> {
            final String className = TestViewWithEntityAnnotation.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            final EntityMetaCodeGen builder = new EntityMetaCodeGen(aptUtils);
            final List<FieldParser.FieldMetaSignature> parsingResults = getTypeParsingResults(aptUtils, typeElement, context);
            builder.buildEntityMeta(EntityType.VIEW, typeElement, context, parsingResults, emptyList());

        });
        failTestWithMessage(
                ""Cannot have both @Table and @MaterializedView on the class 'info.archinnov.achilles.internals.sample_classes.parser.view.TestViewWithEntityAnnotation'"",
                TestViewWithEntityAnnotation.class);
    }
",non-flaky,5
136554,doanduyhai_Achilles,EntityMetaCodeGenTest.should_fail_building_view_meta_with_counter_column,"    @Test
    public void should_fail_building_view_meta_with_counter_column() throws Exception {
        setExec(aptUtils -> {
            final String className = TestViewCounter.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            final EntityMetaCodeGen builder = new EntityMetaCodeGen(aptUtils);
            final List<FieldParser.FieldMetaSignature> parsingResults = getTypeParsingResults(aptUtils, typeElement, context);
            builder.buildEntityMeta(EntityType.VIEW, typeElement, context, parsingResults, emptyList());

        });
        failTestWithMessage(
                ""The class 'info.archinnov.achilles.internals.sample_classes.parser.view.TestViewCounter' cannot have counter columns because it is a materialized view"",
                TestViewCounter.class);
    }
",non-flaky,5
136555,doanduyhai_Achilles,EntityMetaCodeGenTest.should_fail_building_view_meta_with_static_column,"    @Test
    public void should_fail_building_view_meta_with_static_column() throws Exception {
        setExec(aptUtils -> {
            final String className = TestViewStatic.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            final EntityMetaCodeGen builder = new EntityMetaCodeGen(aptUtils);
            final List<FieldParser.FieldMetaSignature> parsingResults = getTypeParsingResults(aptUtils, typeElement, context);
            builder.buildEntityMeta(EntityType.VIEW, typeElement, context, parsingResults, emptyList());

        });
        failTestWithMessage(
                ""The class 'info.archinnov.achilles.internals.sample_classes.parser.view.TestViewStatic' cannot have static columns because it is a materialized view"",
                TestViewStatic.class);
    }
",non-flaky,5
136556,doanduyhai_Achilles,EntityMetaCodeGenTest.should_fail_building_view_meta_without_view_annotation,"    @Test
    public void should_fail_building_view_meta_without_view_annotation() throws Exception {
        setExec(aptUtils -> {
            final String className = TestEntityWithSimplePartitionKey.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            final EntityMetaCodeGen builder = new EntityMetaCodeGen(aptUtils);
            final List<FieldParser.FieldMetaSignature> parsingResults = getTypeParsingResults(aptUtils, typeElement, context);
            builder.buildEntityMeta(EntityType.VIEW, typeElement, context, parsingResults, emptyList());

        });
        failTestWithMessage(
                ""Missing @MaterializedView annotation on entity class 'info.archinnov.achilles.internals.sample_classes.parser.entity.TestEntityWithSimplePartitionKey'"",
                TestEntityWithSimplePartitionKey.class);
    }
",non-flaky,5
136557,doanduyhai_Achilles,UDTMetaCodeGenTest.should_generate_udt_property_class,"    @Test
    public void should_generate_udt_property_class() throws Exception {
        setExec(aptUtils -> {
            final String className = TestUDT.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            final UDTMetaCodeGen builder = new UDTMetaCodeGen(aptUtils);

            final GlobalParsingContext globalContext = GlobalParsingContext.defaultContext();
            final EntityParsingContext context = new EntityParsingContext(typeElement,
                    ClassName.get(TestUDT.class), new LowerCaseNaming(), globalContext);
            final List<FieldMetaSignature> parsingResults = getTypeParsingResults(aptUtils, typeElement, globalContext);

            final TypeSpec typeSpec = builder.buildUDTClassProperty(typeElement, context, parsingResults, Collections.emptyList());

            assertThat(typeSpec.toString().trim()).isEqualTo(
                    readCodeBlockFromFile(""expected_code/udt_meta_builder/should_generate_udt_property_class.txt""));

        });
        launchTest(TestUDT.class);
    }
",non-flaky,5
136558,doanduyhai_Achilles,UDTMetaCodeGenTest.should_generate_udt_with_custom_constructor_property_class,"    @Test
    public void should_generate_udt_with_custom_constructor_property_class() throws Exception {
        setExec(aptUtils -> {
            final String className = TestUDTWithCustomConstructor.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            final UDTMetaCodeGen builder = new UDTMetaCodeGen(aptUtils);

            final GlobalParsingContext globalContext = GlobalParsingContext.defaultContext();
            final EntityParsingContext context = new EntityParsingContext(typeElement,
                    ClassName.get(TestUDT.class), new LowerCaseNaming(), globalContext);
            final List<AccessorsExclusionContext> exclusionContexts = Arrays.asList(
                    new AccessorsExclusionContext(""name"", false, true),
                    new AccessorsExclusionContext(""list"", false, true));
            final List<FieldMetaSignature> fieldMetaSignatures = getTypeParsingResults(aptUtils, typeElement, exclusionContexts, globalContext);

            final List<FieldMetaSignature> constructorInjectedFieldMetaSignatures = fieldMetaSignatures
                    .stream()
                    .filter(fieldMeta -> !fieldMeta.context.fieldName.equals(""date""))
                    .collect(Collectors.toList());

            final TypeSpec typeSpec = builder.buildUDTClassProperty(typeElement, context, fieldMetaSignatures, constructorInjectedFieldMetaSignatures);

            assertThat(typeSpec.toString().trim()).isEqualTo(
                    readCodeBlockFromFile(""expected_code/udt_meta_builder/should_generate_udt_with_custom_constructor_property_class.txt""));

        });
        launchTest(TestUDTWithCustomConstructor.class);
    }
",non-flaky,5
136559,doanduyhai_Achilles,OverridingOptionalFinalTest.should_get_first_present_value,"    @Test
    public void should_get_first_present_value() throws Exception {
        //When
        final String actual = OverridingOptional
                .from(Optional.<String>empty())
                .andThen(Optional.<String>empty())
                .andThen(Optional.ofNullable(""value""))
                .defaultValue(""default"")
                .get();

        //Then
        assertThat(actual).isEqualTo(""value"");
    }
",non-flaky,5
136560,doanduyhai_Achilles,OverridingOptionalFinalTest.should_get_first_value,"    @Test
    public void should_get_first_value() throws Exception {
        //When
        final String actual = OverridingOptional
                .from(Optional.ofNullable(""first""))
                .andThen(Optional.<String>empty())
                .andThen(Optional.ofNullable(""value""))
                .defaultValue(""default"")
                .get();

        //Then
        assertThat(actual).isEqualTo(""first"");
    }
",non-flaky,5
148803,microsoft_botbuilder-java,HeroCardTest.testToAttachment,"    @Test
    public void testToAttachment() {
        Attachment attachment = getCard().toAttachment();
        Assert.assertNotNull(attachment);
        Assert.assertEquals(""application/vnd.microsoft.card.hero"", attachment.getContentType());
    }
",non-flaky,5
148804,microsoft_botbuilder-java,AudioCardTest.testToAttachment,"    @Test
    public void testToAttachment() {
        Attachment attachment = getCard().toAttachment();
        Assert.assertNotNull(attachment);
        Assert.assertEquals(""application/vnd.microsoft.card.audio"", attachment.getContentType());
    }
",non-flaky,5
148805,microsoft_botbuilder-java,ActivityTest.GetConversationReference,"    @Test
    public void GetConversationReference() {
        Activity activity = createActivity();
        ConversationReference conversationReference = activity.getConversationReference();

        Assert.assertEquals(activity.getId(), conversationReference.getActivityId());
        Assert.assertEquals(activity.getFrom().getId(), conversationReference.getUser().getId());
        Assert.assertEquals(activity.getRecipient().getId(), conversationReference.getBot().getId());
        Assert.assertEquals(activity.getConversation().getId(), conversationReference.getConversation().getId());
        Assert.assertEquals(activity.getLocale(), conversationReference.getLocale());
        Assert.assertEquals(activity.getChannelId(), conversationReference.getChannelId());
        Assert.assertEquals(activity.getServiceUrl(), conversationReference.getServiceUrl());

        activity.setType(ActivityTypes.CONVERSATION_UPDATE);
        conversationReference = activity.getConversationReference();
        Assert.assertNull(conversationReference.getActivityId());

    }
",non-flaky,5
148806,microsoft_botbuilder-java,ActivityTest.GetReplyConversationReference,"    @Test
    public void GetReplyConversationReference() {
        Activity activity = createActivity();

        ResourceResponse reply = new ResourceResponse();
        reply.setId(""1234"");

        ConversationReference conversationReference = activity.getReplyConversationReference(reply);

        Assert.assertEquals(reply.getId(), conversationReference.getActivityId());
        Assert.assertEquals(activity.getFrom().getId(), conversationReference.getUser().getId());
        Assert.assertEquals(activity.getRecipient().getId(), conversationReference.getBot().getId());
        Assert.assertEquals(activity.getConversation().getId(), conversationReference.getConversation().getId());
        Assert.assertEquals(activity.getLocale(), conversationReference.getLocale());
        Assert.assertEquals(activity.getChannelId(), conversationReference.getChannelId());
        Assert.assertEquals(activity.getServiceUrl(), conversationReference.getServiceUrl());
    }
",non-flaky,5
148807,microsoft_botbuilder-java,ActivityTest.ApplyConversationReference_isIncoming,"    @Test
    public void ApplyConversationReference_isIncoming() {
        Activity activity = createActivity();

        ConversationReference conversationReference = new ConversationReference();
        conversationReference.setChannelId(""cr_123"");
        conversationReference.setServiceUrl(""cr_serviceUrl"");
        ConversationAccount conversation = new ConversationAccount();
        conversation.setId(""cr_456"");
        conversationReference.setConversation(conversation);
        ChannelAccount userAccount = new ChannelAccount();
        userAccount.setId(""cr_abc"");
        conversationReference.setUser(userAccount);
        ChannelAccount botAccount = new ChannelAccount();
        botAccount.setId(""cr_def"");
        conversationReference.setBot(botAccount);
        conversationReference.setActivityId(""cr_12345"");
        // Intentionally oddly-cased to check that it isn't defaulted somewhere, but
        // tests stay in English
        conversationReference.setLocale(""en-uS"");

        activity.applyConversationReference(conversationReference, true);

        Assert.assertEquals(conversationReference.getChannelId(), activity.getChannelId());
        Assert.assertEquals(conversationReference.getLocale(), activity.getLocale());
        Assert.assertEquals(conversationReference.getServiceUrl(), activity.getServiceUrl());
        Assert.assertEquals(conversationReference.getConversation().getId(), activity.getConversation().getId());

        Assert.assertEquals(conversationReference.getUser().getId(), activity.getFrom().getId());
        Assert.assertEquals(conversationReference.getBot().getId(), activity.getRecipient().getId());
        Assert.assertEquals(conversationReference.getActivityId(), activity.getId());
    }
",non-flaky,5
148808,microsoft_botbuilder-java,ActivityTest.ApplyConversationReference,"    @Test
    public void ApplyConversationReference() {
        Activity activity = createActivity();

        ConversationReference conversationReference = new ConversationReference();
        conversationReference.setChannelId(""123"");
        conversationReference.setServiceUrl(""serviceUrl"");
        ConversationAccount conversation = new ConversationAccount();
        conversation.setId(""456"");
        conversationReference.setConversation(conversation);
        ChannelAccount userAccount = new ChannelAccount();
        userAccount.setId(""abc"");
        conversationReference.setUser(userAccount);
        ChannelAccount botAccount = new ChannelAccount();
        botAccount.setId(""def"");
        conversationReference.setBot(botAccount);
        conversationReference.setActivityId(""12345"");
        // Intentionally oddly-cased to check that it isn't defaulted somewhere, but
        // tests stay in English
        conversationReference.setLocale(""en-uS"");

        activity.applyConversationReference(conversationReference, false);

        Assert.assertEquals(conversationReference.getChannelId(), activity.getChannelId());
        Assert.assertEquals(conversationReference.getLocale(), activity.getLocale());
        Assert.assertEquals(conversationReference.getServiceUrl(), activity.getServiceUrl());
        Assert.assertEquals(conversationReference.getConversation().getId(), activity.getConversation().getId());

        Assert.assertEquals(conversationReference.getBot().getId(), activity.getFrom().getId());
        Assert.assertEquals(conversationReference.getUser().getId(), activity.getRecipient().getId());
        Assert.assertEquals(conversationReference.getActivityId(), activity.getReplyToId());
    }
",non-flaky,5
148809,microsoft_botbuilder-java,ActivityTest.ApplyConversationReferenceOverload,"    @Test
    public void ApplyConversationReferenceOverload() {
        Activity activity = createActivity();

        ConversationReference conversationReference = new ConversationReference();
        conversationReference.setChannelId(""123"");
        conversationReference.setServiceUrl(""serviceUrl"");
        ConversationAccount conversation = new ConversationAccount();
        conversation.setId(""456"");
        conversationReference.setConversation(conversation);
        ChannelAccount userAccount = new ChannelAccount();
        userAccount.setId(""abc"");
        conversationReference.setUser(userAccount);
        ChannelAccount botAccount = new ChannelAccount();
        botAccount.setId(""def"");
        conversationReference.setBot(botAccount);
        conversationReference.setActivityId(""12345"");
        // Intentionally oddly-cased to check that it isn't defaulted somewhere, but
        // tests stay in English
        conversationReference.setLocale(""en-uS"");

        activity.applyConversationReference(conversationReference);

        Assert.assertEquals(conversationReference.getChannelId(), activity.getChannelId());
        Assert.assertEquals(conversationReference.getLocale(), activity.getLocale());
        Assert.assertEquals(conversationReference.getServiceUrl(), activity.getServiceUrl());
        Assert.assertEquals(conversationReference.getConversation().getId(), activity.getConversation().getId());

        Assert.assertEquals(conversationReference.getBot().getId(), activity.getFrom().getId());
        Assert.assertEquals(conversationReference.getUser().getId(), activity.getRecipient().getId());
        Assert.assertEquals(conversationReference.getActivityId(), activity.getReplyToId());
    }
",non-flaky,5
148810,microsoft_botbuilder-java,ActivityTest.ApplyConversationReferenceOverloadAlternatePaths,"    @Test
    public void ApplyConversationReferenceOverloadAlternatePaths() {
        Activity activity = createActivity();

        ConversationReference conversationReference = new ConversationReference();
        conversationReference.setChannelId(""123"");
        conversationReference.setServiceUrl(""serviceUrl"");
        ConversationAccount conversation = new ConversationAccount();
        conversation.setId(""456"");
        conversationReference.setConversation(conversation);
        ChannelAccount userAccount = new ChannelAccount();
        userAccount.setId(""abc"");
        conversationReference.setUser(userAccount);
        ChannelAccount botAccount = new ChannelAccount();
        botAccount.setId(""def"");
        conversationReference.setBot(botAccount);
        conversationReference.setActivityId(null);
        conversationReference.setLocale(null);

        activity.applyConversationReference(conversationReference, false);

        Assert.assertEquals(conversationReference.getChannelId(), activity.getChannelId());
        Assert.assertEquals(""en-uS"", activity.getLocale());
        Assert.assertEquals(conversationReference.getServiceUrl(), activity.getServiceUrl());
        Assert.assertEquals(conversationReference.getConversation().getId(), activity.getConversation().getId());

        Assert.assertEquals(conversationReference.getBot().getId(), activity.getFrom().getId());
        Assert.assertEquals(conversationReference.getUser().getId(), activity.getRecipient().getId());
        Assert.assertEquals(conversationReference.getActivityId(), activity.getReplyToId());

        activity.applyConversationReference(conversationReference, true);

        Assert.assertEquals(conversationReference.getChannelId(), activity.getChannelId());
        Assert.assertEquals(""en-uS"", activity.getLocale());
        Assert.assertEquals(conversationReference.getServiceUrl(), activity.getServiceUrl());
        Assert.assertEquals(conversationReference.getConversation().getId(), activity.getConversation().getId());

        Assert.assertEquals(conversationReference.getUser().getId(), activity.getFrom().getId());
        Assert.assertEquals(conversationReference.getBot().getId(), activity.getRecipient().getId());
        Assert.assertEquals(conversationReference.getActivityId(), activity.getReplyToId());
    }
",non-flaky,5
148811,microsoft_botbuilder-java,ActivityTest.CreateTraceAllowsNullRecipient,"    @Test
    public void CreateTraceAllowsNullRecipient() {
        Activity activity = createActivity();
        activity.setRecipient(null);
        Activity trace = activity.createTrace(""test"");

        Assert.assertNull(trace.getFrom().getId());
    }
",non-flaky,5
148812,microsoft_botbuilder-java,ActivityTest.DeserializeActivity,"    @Test
    public void DeserializeActivity() throws IOException {
        ObjectMapper objectMapper = new ObjectMapper();
        objectMapper.findAndRegisterModules();
        Activity activity = objectMapper.readValue(this.serializedActivity, Activity.class);

        Assert.assertNotNull(activity.getTimestamp());
        Assert.assertEquals(""b18a1c99-7a29-4801-ac0c-579f2c36d52c"", activity.getConversation().getId());
        Assert.assertNotNull(activity.getValue());
    }
",non-flaky,5
148813,microsoft_botbuilder-java,ActivityTest.DeserializeActivityWithDifferentTimeZone,"    @Test
    public void DeserializeActivityWithDifferentTimeZone() throws IOException {
        ObjectMapper objectMapper = new ObjectMapper();
        objectMapper.findAndRegisterModules();
        Activity activity = objectMapper.readValue(this.serializedActivityWithDifferentTimeZone, Activity.class);

        Assert.assertNotNull(activity.getTimestamp());
        Assert.assertEquals(""b18a1c99-7a29-4801-ac0c-579f2c36d52c"", activity.getConversation().getId());
        Assert.assertNotNull(activity.getValue());
    }
",non-flaky,5
148814,microsoft_botbuilder-java,ActivityTest.GetInformationForMicrosoftTeams,"    @Test
    public void GetInformationForMicrosoftTeams() throws JsonProcessingException, IOException {
        ObjectMapper objectMapper = new ObjectMapper();
        objectMapper.findAndRegisterModules();
        Activity activity = objectMapper.readValue(ActivityTest.serializedActivityFromTeams, Activity.class);
        Assert.assertEquals(""19:123cb42aa5a0a7e56f83@thread.skype"", activity.teamsGetChannelId());
        Assert.assertEquals(""19:104f2cb42aa5a0a7e56f83@thread.skype"", activity.teamsGetTeamId());
        Assert.assertEquals(true, activity.isTeamsActivity());

        activity = objectMapper.readValue(ActivityTest.serializedActivityFromTeamsWithoutTeamsChannelIdorTeamId,
                Activity.class);

        Assert.assertEquals(""channel_id"", activity.teamsGetChannelId());
        Assert.assertEquals(""team_id"", activity.teamsGetTeamId());

        TeamsChannelData teamsChannelData = activity.getChannelData(TeamsChannelData.class);
        Assert.assertEquals(""channel_id"", teamsChannelData.getChannel().getId());
        Assert.assertEquals(""channel_name"", teamsChannelData.getChannel().getName());
        Assert.assertEquals(""team_id"", teamsChannelData.getTeam().getId());
        Assert.assertEquals(""team_name"", teamsChannelData.getTeam().getName());
        Assert.assertEquals(""aad_groupid"", teamsChannelData.getTeam().getAadGroupId());
        Assert.assertEquals(true, teamsChannelData.getNotification().getAlert());
        Assert.assertEquals(""teamMemberAdded"", teamsChannelData.getEventType());
        Assert.assertEquals(""tenant_id"", teamsChannelData.getTenant().getId());
    }
",non-flaky,5
148815,microsoft_botbuilder-java,ActivityTest.GetTeamsChannelIdBadChannelData,"    @Test
    public void GetTeamsChannelIdBadChannelData() {
        Activity activity = new Activity();
        activity.setChannelData(""badChannelData"");
        String channelId = activity.teamsGetChannelId();
        Assert.assertNull(channelId);
    }
",non-flaky,5
148816,microsoft_botbuilder-java,ActivityTest.GetTeamsTeamIdBadChannelData,"    @Test
    public void GetTeamsTeamIdBadChannelData() {
        Activity activity = new Activity();
        activity.setChannelData(""badChannelData"");
        String channelId = activity.teamsGetTeamId();
        Assert.assertNull(channelId);
    }
",non-flaky,5
148817,microsoft_botbuilder-java,ActivityTest.GetTeamsTeamIdNullChannelData,"    @Test
    public void GetTeamsTeamIdNullChannelData() {
        Activity activity = new Activity();
        String channelId = activity.teamsGetTeamId();
        Assert.assertNull(channelId);
    }
",non-flaky,5
148818,microsoft_botbuilder-java,ActivityTest.GetTeamsGetInfo,"    @Test
    public void GetTeamsGetInfo() throws JsonProcessingException, IOException {
        ObjectMapper objectMapper = new ObjectMapper();
        objectMapper.findAndRegisterModules();

        Activity activity = objectMapper.readValue(
            ActivityTest.serializedActivityFromTeamsWithoutTeamsChannelIdorTeamId, Activity.class);

        TeamInfo teamsInfo = activity.teamsGetTeamInfo();
        Assert.assertNotNull(teamsInfo);
    }
",non-flaky,5
148819,microsoft_botbuilder-java,ActivityTest.GetTeamsGetInfoBadChannelData,"    @Test
    public void GetTeamsGetInfoBadChannelData() {
        Activity activity = new Activity();
        activity.setChannelData(""badChannelData"");
        TeamInfo teamInfo = activity.teamsGetTeamInfo();
        Assert.assertNull(teamInfo);
    }
",non-flaky,5
148820,microsoft_botbuilder-java,ActivityTest.TeamsNotifyUser,"    @Test
    public void TeamsNotifyUser() throws JsonProcessingException, IOException {
        ObjectMapper objectMapper = new ObjectMapper();
        objectMapper.findAndRegisterModules();
        Activity activity = objectMapper.readValue(
            ActivityTest.serializedActivityFromTeamsWithoutNotificationTeamsChannelIdOrTeamId, Activity.class);

        TeamsChannelData channelData = activity.teamsGetChannelData();
        Assert.assertNull(channelData.getNotification());
        activity.teamsNotifyUser();
        Assert.assertNotNull(activity.teamsGetChannelData().getNotification());
    }
",non-flaky,5
148821,microsoft_botbuilder-java,ActivityTest.TeamsNotifyUserBadChannelData,"    @Test
    public void TeamsNotifyUserBadChannelData() throws JsonProcessingException, IOException {
        Activity activity = new Activity();
        activity.setChannelData(""badChannelData"");

        TeamsChannelData channelData = activity.teamsGetChannelData();
        Assert.assertNull(channelData);
        activity.teamsNotifyUser();
        Assert.assertNotNull(activity.teamsGetChannelData().getNotification());
    }
",non-flaky,5
148822,microsoft_botbuilder-java,ActivityTest.TeamsNotifyUserAlertInMeeting,"    @Test
    public void TeamsNotifyUserAlertInMeeting() throws JsonProcessingException, IOException {
        ObjectMapper objectMapper = new ObjectMapper();
        objectMapper.findAndRegisterModules();
        Activity activity = objectMapper.readValue(
            ActivityTest.serializedActivityFromTeamsWithoutNotificationTeamsChannelIdOrTeamId, Activity.class);

        TeamsChannelData channelData = activity.teamsGetChannelData();
        Assert.assertNull(channelData.getNotification());
        activity.teamsNotifyUser(true, ""externalresourceURL"");
        Assert.assertNotNull(activity.teamsGetChannelData().getNotification());
        Assert.assertEquals(activity.teamsGetChannelData().getNotification().getExternalResourceUrl(),
                            ""externalresourceURL"");
        Assert.assertTrue(activity.teamsGetChannelData().getNotification().getAlertInMeeting());
    }
",non-flaky,5
148823,microsoft_botbuilder-java,ActivityTest.TeamsNotifyUserAlertInMeetingBadChannelData,"    @Test
    public void TeamsNotifyUserAlertInMeetingBadChannelData() throws JsonProcessingException, IOException {
        Activity activity = new Activity();
        activity.setChannelData(""badChannelData"");

        Assert.assertNull(activity.teamsGetChannelData());
        activity.teamsNotifyUser(true, ""externalresourceURL"");
        Assert.assertNotNull(activity.teamsGetChannelData().getNotification());
        Assert.assertEquals(activity.teamsGetChannelData().getNotification().getExternalResourceUrl(),
                            ""externalresourceURL"");
        Assert.assertTrue(activity.teamsGetChannelData().getNotification().getAlertInMeeting());
    }
",non-flaky,5
148824,microsoft_botbuilder-java,ActivityTest.TeamsGetMeetingInfoNull,"    @Test
    public void TeamsGetMeetingInfoNull() throws JsonProcessingException, IOException {
        ObjectMapper objectMapper = new ObjectMapper();
        objectMapper.findAndRegisterModules();
        Activity activity = objectMapper.readValue(
            ActivityTest.serializedActivityFromTeamsWithoutNotificationTeamsChannelIdOrTeamId, Activity.class);

        TeamsMeetingInfo meetingInfo = activity.teamsGetMeetingInfo();
        Assert.assertNull(meetingInfo);
    }
",non-flaky,5
148825,microsoft_botbuilder-java,ActivityTest.TeamsGetMeetingInfo,"    @Test
    public void TeamsGetMeetingInfo() throws JsonProcessingException, IOException {
        Activity activity = new Activity();
        TeamsChannelData channelData = new TeamsChannelData();
        TeamsMeetingInfo meeting = new TeamsMeetingInfo();
        meeting.setId(""meetingId"");
        channelData.setMeeting(meeting);
        activity.setChannelData(channelData);

        TeamsMeetingInfo meetingInfo = activity.teamsGetMeetingInfo();
        Assert.assertNotNull(meetingInfo);
        Assert.assertEquals(meetingInfo.getId(), ""meetingId"");
    }
",non-flaky,5
148826,microsoft_botbuilder-java,ActivityTest.TeamsGetMeetingInfoBadChannelData,"    @Test
    public void TeamsGetMeetingInfoBadChannelData() throws JsonProcessingException, IOException {
        Activity activity = new Activity();
        activity.setChannelData(""badChannelData"");

        TeamsMeetingInfo meetingInfo = activity.teamsGetMeetingInfo();
        Assert.assertNull(meetingInfo);
    }
",non-flaky,5
148827,microsoft_botbuilder-java,ActivityTest.CreateMessageActivity,"    @Test
    public void CreateMessageActivity() {
        Activity activity = Activity.createMessageActivity();

        Assert.assertEquals(activity.getType(), ActivityTypes.MESSAGE);
    }
",non-flaky,5
148828,microsoft_botbuilder-java,ActivityTest.CreateContactRelationUpdateActivity,"    @Test
    public void CreateContactRelationUpdateActivity() {
        Activity activity = Activity.createContactRelationUpdateActivity();

        Assert.assertEquals(activity.getType(), ActivityTypes.CONTACT_RELATION_UPDATE);
    }
",non-flaky,5
148829,microsoft_botbuilder-java,ActivityTest.CreateConversationUpdateActivity,"    @Test
    public void CreateConversationUpdateActivity() {
        Activity activity = Activity.createConversationUpdateActivity();

        Assert.assertEquals(activity.getType(), ActivityTypes.CONVERSATION_UPDATE);
    }
",non-flaky,5
148830,microsoft_botbuilder-java,ActivityTest.CreateTypingActivity,"    @Test
    public void CreateTypingActivity() {
        Activity activity = Activity.createTypingActivity();

        Assert.assertEquals(activity.getType(), ActivityTypes.TYPING);
    }
",non-flaky,5
148831,microsoft_botbuilder-java,ActivityTest.CreateHandoffActivity,"    @Test
    public void CreateHandoffActivity() {
        Activity activity = Activity.createHandoffActivity();

        Assert.assertEquals(activity.getType(), ActivityTypes.HANDOFF);
    }
",non-flaky,5
148832,microsoft_botbuilder-java,ActivityTest.CreateEndOfConversationActivity,"    @Test
    public void CreateEndOfConversationActivity() {
        Activity activity = Activity.createEndOfConversationActivity();

        Assert.assertEquals(activity.getType(), ActivityTypes.END_OF_CONVERSATION);
    }
",non-flaky,5
148833,microsoft_botbuilder-java,ActivityTest.CreateEventActivity,"    @Test
    public void CreateEventActivity() {
        Activity activity = Activity.createEventActivity();

        Assert.assertEquals(activity.getType(), ActivityTypes.EVENT);
    }
",non-flaky,5
148834,microsoft_botbuilder-java,ActivityTest.CreateInvokeActivity,"    @Test
    public void CreateInvokeActivity() {
        Activity activity = Activity.createInvokeActivity();

        Assert.assertEquals(activity.getType(), ActivityTypes.INVOKE);
    }
",non-flaky,5
148835,microsoft_botbuilder-java,ActivityTest.CreateTraceActivity,"    @Test
    public void CreateTraceActivity() {
        String name = ""test-activity"";
        String valueType = ""string"";
        String value = ""test-value"";
        String label = ""test-label"";

        Activity activity = Activity.createTraceActivity(name, valueType, value, label);

        Assert.assertEquals(activity.getType(), ActivityTypes.TRACE);
        Assert.assertEquals(activity.getName(), name);
        Assert.assertEquals(activity.getValueType(), valueType);
        Assert.assertEquals(activity.getValue(), value);
        Assert.assertEquals(activity.getLabel(), label);

        Activity secondActivity = Activity.createTraceActivity(name);
        Assert.assertEquals(secondActivity.getType(), ActivityTypes.TRACE);
        Assert.assertEquals(secondActivity.getName(), name);

        Activity thirdActivity = Activity.createTraceActivity(name, null, value, label);
        Assert.assertEquals(thirdActivity.getType(), ActivityTypes.TRACE);
        Assert.assertEquals(thirdActivity.getName(), name);

        Assert.assertTrue(thirdActivity.isType(ActivityTypes.TRACE));
    }
",non-flaky,5
148836,microsoft_botbuilder-java,ActivityTest.CreateTraceActivityWithoutValueType,"    @Test
    public void CreateTraceActivityWithoutValueType() {
        String name = ""test-activity"";
        String value = ""test-value"";
        String label = ""test-label"";

        Activity activity = Activity.createTraceActivity(name, null, value, label);

        Assert.assertEquals(activity.getType(), ActivityTypes.TRACE);
        Assert.assertEquals(activity.getValueType(), value.getClass().getTypeName());
        Assert.assertEquals(activity.getLabel(), label);
    }
",non-flaky,5
148837,microsoft_botbuilder-java,ActivityTest.CreateReply,"    @Test
    public void CreateReply() {
        Activity activity = createActivity();

        String text = ""test-reply"";
        String locale = ""en-us"";

        Activity reply = activity.createReply(text, locale);

        Assert.assertEquals(reply.getType(), ActivityTypes.MESSAGE);
        Assert.assertEquals(reply.getText(), text);
        Assert.assertEquals(reply.getLocale(), locale);

        activity.setFrom(null);
        activity.setRecipient(null);
        activity.setConversation(null);
        Activity reply2 = activity.createReply(text);
        Assert.assertEquals(reply2.getType(), ActivityTypes.MESSAGE);
        Assert.assertEquals(reply2.getText(), text);
        Assert.assertEquals(reply2.getLocale(), ""en-uS"");
        Assert.assertTrue(reply2.getFrom() != null);
        Assert.assertTrue(reply2.getRecipient() != null);
        Assert.assertTrue(reply2.getConversation() != null);
    }
",non-flaky,5
148838,microsoft_botbuilder-java,ActivityTest.CreateReplyWithoutArguments,"    @Test
    public void CreateReplyWithoutArguments() {
        Activity activity = createActivity();

        Activity reply = activity.createReply();

        Assert.assertEquals(reply.getType(), ActivityTypes.MESSAGE);
        Assert.assertEquals(reply.getText(), """");
        Assert.assertEquals(reply.getLocale(), activity.getLocale());
    }
",non-flaky,5
148839,microsoft_botbuilder-java,ActivityTest.HasContentIsFalseWhenActivityTextHasNoContent,"    @Test
    public void HasContentIsFalseWhenActivityTextHasNoContent() {
        Activity activity = createActivity();

        boolean result = activity.hasContent();

        Assert.assertEquals(result, false);
    }
",non-flaky,5
148840,microsoft_botbuilder-java,ActivityTest.HasContentIsTrueWhenActivityTextHasContent,"    @Test
    public void HasContentIsTrueWhenActivityTextHasContent() {
        Activity activity = createActivity();

        activity.setText(""test-text"");

        boolean result = activity.hasContent();

        Assert.assertEquals(result, true);
    }
",non-flaky,5
148841,microsoft_botbuilder-java,ActivityTest.HasContentIsTrueWhenActivitySummaryContent,"    @Test
    public void HasContentIsTrueWhenActivitySummaryContent() {
        Activity activity = createActivity();

        activity.setText(null);
        activity.setSummary(""test-summary"");

        boolean result = activity.hasContent();

        Assert.assertEquals(result, true);
    }
",non-flaky,5
148842,microsoft_botbuilder-java,ActivityTest.HasContentIsTrueWhenActivityAttachmentsHaveContent,"    @Test
    public void HasContentIsTrueWhenActivityAttachmentsHaveContent() {
        Activity activity = createActivity();
        ArrayList<Attachment> attachments = new ArrayList<>();
        attachments.add(CreateAttachment());

        activity.setText(null);
        activity.setSummary(null);
        activity.setAttachments(attachments);

        boolean result = activity.hasContent();

        Assert.assertEquals(result, true);
    }
",non-flaky,5
148843,microsoft_botbuilder-java,ActivityTest.HasContentIsTrueWhenActivityChannelDataHasContent,"    @Test
    public void HasContentIsTrueWhenActivityChannelDataHasContent() {
        Activity activity = createActivity();

        activity.setText(null);
        activity.setSummary(null);
        activity.setChannelData(""test-channelData"");

        boolean result = activity.hasContent();

        Assert.assertEquals(result, true);
    }
",non-flaky,5
148844,microsoft_botbuilder-java,ActivityTest.GetMentions,"    @Test
    public void GetMentions() {
        ArrayList<Entity> mentions = new ArrayList<Entity>();

        Entity mentionEntity = new Entity();
        mentionEntity.setType(""mention"");
        mentions.add(mentionEntity);
        Entity reactionEntity = new Entity();
        reactionEntity.setType(""reaction"");
        mentions.add(reactionEntity);

        Activity activity = createActivity();

        activity.setEntities(mentions);

        List<Mention> mentionsResult = activity.getMentions();

        Assert.assertEquals(mentionsResult.size(), 1);
        Assert.assertEquals(mentionsResult.get(0).getType(), ""mention"");
    }
",non-flaky,5
148845,microsoft_botbuilder-java,ActivityTest.GetMentionsNull,"    @Test
    public void GetMentionsNull() {
        Activity activity = createActivity();
        activity.setEntities(null);
        Assert.assertTrue(activity.getMentions() != null);
    }
",non-flaky,5
148846,microsoft_botbuilder-java,ActivityTest.CreateTraceForConversationUpdateActivity,"    @Test
    public void CreateTraceForConversationUpdateActivity() {
        Activity activity = createActivity();
        activity.setType(ActivityTypes.CONVERSATION_UPDATE);
        Activity trace = activity.createTrace(""test"");
        Assert.assertNull(trace.getReplyToId());
    }
",non-flaky,5
148847,microsoft_botbuilder-java,ActivityTest.CreateReplyForConversationUpdateActivity,"    @Test
    public void CreateReplyForConversationUpdateActivity() {
        Activity activity = createActivity();
        activity.setType(ActivityTypes.CONVERSATION_UPDATE);
        Activity reply = activity.createReply(""test"");
        Assert.assertNull(reply.getReplyToId());
    }
",non-flaky,5
148848,microsoft_botbuilder-java,ActivityTest.CreateTrace,"    @Test
    public void CreateTrace() {
        Activity activity = createActivity();

        String name = ""test-activity"";
        String value = ""test-value"";
        String valueType = ""string"";
        String label = ""test-label"";

        Activity trace = activity.createTrace(name, value, valueType, label);

        Assert.assertEquals(trace.getType(), ActivityTypes.TRACE);
        Assert.assertEquals(trace.getName(), name);
        Assert.assertEquals(trace.getValue(), value);
        Assert.assertEquals(trace.getValueType(), valueType);
        Assert.assertEquals(trace.getLabel(), label);

        Activity secondActivity = createActivity();
        secondActivity.setRecipient(null);
        secondActivity.setFrom(null);
        Activity secondTrace = secondActivity.createTrace(name, value, null, label);
        Assert.assertEquals(secondTrace.getType(), ActivityTypes.TRACE);
        Assert.assertEquals(secondTrace.getName(), name);
        Assert.assertEquals(secondTrace.getValue(), value);
        Assert.assertEquals(secondTrace.getValueType(), value.getClass().getTypeName());
        Assert.assertEquals(secondTrace.getLabel(), label);
        Assert.assertTrue(secondTrace.getRecipient() != null);
        Assert.assertTrue(secondTrace.getFrom() != null);
    }
",non-flaky,5
148849,microsoft_botbuilder-java,ActivityTest.IsFromStreamingConnection,"    @Test
    public void IsFromStreamingConnection() {
        ArrayList<String> nonStreaming = new ArrayList<>();
        nonStreaming.add(""http://yayay.com"");
        nonStreaming.add(""https://yayay.com"");
        nonStreaming.add(""HTTP://yayay.com"");
        nonStreaming.add(""HTTPS://yayay.com"");

        ArrayList<String> streaming = new ArrayList<>();
        streaming.add(""urn:botframework:WebSocket:wss://beep.com"");
        streaming.add(""urn:botframework:WebSocket:http://beep.com"");
        streaming.add(""URN:botframework:WebSocket:wss://beep.com"");
        streaming.add(""URN:botframework:WebSocket:http://beep.com"");

        Activity activity = createActivity();
        activity.setServiceUrl(null);

        Assert.assertFalse(activity.isFromStreamingConnection());

        nonStreaming.forEach(s ->
        {
            activity.setServiceUrl(s);
            Assert.assertFalse(activity.isFromStreamingConnection());
        });

        streaming.forEach(s ->
        {
            activity.setServiceUrl(s);
            Assert.assertTrue(activity.isFromStreamingConnection());
        });
    }
",non-flaky,5
148850,microsoft_botbuilder-java,ActivityTest.ActivityCloneTest,"    @Test
    public void ActivityCloneTest() throws JsonProcessingException {
        Activity activity = new Activity(ActivityTypes.MESSAGE);
        activity.setAction(""TestAction"");

        Attachment attachment = new Attachment();
        attachment.setContentType(""testContentType"");
        attachment.setContentUrl(""testContentUrl"");
        attachment.setContent(""testContent"");
        attachment.setName(""testName"");
        attachment.setThumbnailUrl(""testThumbnailUrl"");
        attachment.setProperties(""testProperty"", getTestNode());
        activity.setAttachment(attachment);

        activity.setCallerId(""testCallerId"");
        activity.setChannelData(""testChannelData"");
        activity.setCode(EndOfConversationCodes.BOT_TIMED_OUT);

        ConversationAccount conversation = new ConversationAccount(""testConversation"");
        activity.setConversation(conversation);

        activity.setDeliveryMode(""testDeliveryMode"");

        List<Entity> entityList = new ArrayList<Entity>();
        Entity entity1 = new Entity();
        entity1.setType(""testEntity"");
        entityList.add(entity1);
        activity.setEntities(entityList);

        LocalDateTime expiration = LocalDateTime.now();
        activity.setExpiration(expiration);

        ChannelAccount fromChannel = new ChannelAccount(""fromChannel"");
        activity.setFrom(fromChannel);

        activity.setHistoryDisclosed(true);
        activity.setId(""testId"");
        activity.setImportance(""testImportance"");
        activity.setInputHint(InputHints.ACCEPTING_INPUT);
        activity.setLabel(""testLabel"");

        List<String> listen = new ArrayList<String>();
        listen.add(""listen1"");
        listen.add(""listen2"");
        activity.setListenFor(listen);

        activity.setLocalTimeZone(""testLocalTimeZone"");
        OffsetDateTime offsetDateTime = OffsetDateTime.now();
        activity.setLocalTimestamp(offsetDateTime);
        activity.setLocale(""testLocale"");

        List<ChannelAccount> membersAdded = new ArrayList<ChannelAccount>();
        ChannelAccount firstMember = new ChannelAccount(""firstMember"");
        ChannelAccount secondMember = new ChannelAccount(""secondMember"");
        membersAdded.add(firstMember);
        membersAdded.add(secondMember);
        activity.setMembersAdded(membersAdded);

        List<ChannelAccount> membersRemoved = new ArrayList<ChannelAccount>();
        ChannelAccount firstMemberRemoved = new ChannelAccount(""firstMember"");
        ChannelAccount secondMemberRemoved = new ChannelAccount(""secondMember"");
        membersRemoved.add(firstMemberRemoved);
        membersRemoved.add(secondMemberRemoved);
        activity.setMembersRemoved(membersRemoved);

        List<Mention> mentions = new ArrayList<Mention>();
        Mention firstMention = new Mention();
        firstMention.setText(""testTest"");
        firstMention.setMentioned(firstMember);
        Mention secondMention = new Mention();
        secondMention.setText(""testTest"");
        secondMention.setMentioned(firstMember);
        mentions.add(secondMention);
        activity.setMentions(mentions);

        activity.setName(""testName"");
        activity.setProperties(""testProperty"", getTestNode());

        List<MessageReaction> reactionsAdded = new ArrayList<MessageReaction>();
        MessageReaction firstReaction = new MessageReaction();
        firstReaction.setType(""testType"");
        reactionsAdded.add(firstReaction);
        MessageReaction secondReaction = new MessageReaction();
        secondReaction.setType(""testType"");
        reactionsAdded.add(secondReaction);
        activity.setReactionsAdded(reactionsAdded);

        List<MessageReaction> reactionsRemoved = new ArrayList<MessageReaction>();
        MessageReaction firstReactionRemoved = new MessageReaction();
        firstReactionRemoved.setType(""testType"");
        reactionsRemoved.add(firstReactionRemoved);
        MessageReaction secondReactionRemoved = new MessageReaction();
        secondReactionRemoved.setType(""testType"");
        reactionsRemoved.add(secondReactionRemoved);
        activity.setReactionsRemoved(reactionsRemoved);

        ChannelAccount recipientRemoved = new ChannelAccount();
        recipientRemoved.setId(""testRecipient"");
        activity.setRecipient(recipientRemoved);

        ConversationReference relatesToReference = new ConversationReference();
        relatesToReference.setActivityId(""testActivityId"");
        activity.setRelatesTo(relatesToReference);

        activity.setReplyToId(""testReplyToId"");
        activity.setServiceUrl(""testServiceUrl"");
        activity.setText(""testText"");
        activity.setTextFormat(TextFormatTypes.MARKDOWN);

        List<TextHighlight> textHighlights = new ArrayList<TextHighlight>();
        TextHighlight firstTextHighlight = new TextHighlight();
        firstTextHighlight.setText(""testText"");
        textHighlights.add(firstTextHighlight);
        TextHighlight secondTextHighlight = new TextHighlight();
        secondTextHighlight.setText(""testText"");
        textHighlights.add(secondTextHighlight);
        activity.setTextHighlights(textHighlights);

        OffsetDateTime timestamp = OffsetDateTime.now();
        activity.setTimestamp(timestamp);

        activity.setTopicName(""testTopicName"");
        activity.setType(""testType"");
        activity.setValue(""testValue"");
        activity.setValueType(""testValueType"");

        Activity clonedActivity = Activity.clone(activity);

        Assert.assertEquals(activity.getAction(), clonedActivity.getAction());
        Assert.assertEquals(activity.getCallerId(), clonedActivity.getCallerId());
        Assert.assertEquals(activity.getChannelData(), clonedActivity.getChannelData());
        Assert.assertEquals(activity.getDeliveryMode(), clonedActivity.getDeliveryMode());
        Assert.assertEquals(activity.getId(), clonedActivity.getId());
        Assert.assertEquals(activity.getImportance(), clonedActivity.getImportance());
        Assert.assertEquals(activity.getLabel(), clonedActivity.getLabel());
        Assert.assertEquals(activity.getLocalTimezone(), clonedActivity.getLocalTimezone());
        Assert.assertEquals(activity.getLocale(), clonedActivity.getLocale());
        Assert.assertEquals(activity.getName(), clonedActivity.getName());
        Assert.assertEquals(activity.getReplyToId(), clonedActivity.getReplyToId());
        Assert.assertEquals(activity.getServiceUrl(), clonedActivity.getServiceUrl());
        Assert.assertEquals(activity.getSpeak(), clonedActivity.getSpeak());
        Assert.assertEquals(activity.getSummary(), clonedActivity.getSummary());
        Assert.assertEquals(activity.getText(), clonedActivity.getText());
        Assert.assertEquals(activity.getTopicName(), clonedActivity.getTopicName());
        Assert.assertEquals(activity.getType(), clonedActivity.getType());
        Assert.assertEquals(activity.getValue(), clonedActivity.getValue());
        Assert.assertEquals(activity.getValueType(), clonedActivity.getValueType());
        Assert.assertEquals(activity.getAttachmentLayout(), clonedActivity.getAttachmentLayout());
        Assert.assertEquals(activity.getAttachments().get(0).getName(),
                            clonedActivity.getAttachments().get(0).getName());
        Assert.assertEquals(activity.getChannelData(ChannelAccount.class).getId(),
                            clonedActivity.getChannelData(ChannelAccount.class).getId());
        Assert.assertEquals(activity.getCode(), clonedActivity.getCode());
        Assert.assertEquals(activity.getConversation().getName(), clonedActivity.getConversation().getName());
        Assert.assertEquals(activity.getConversationReference().getChannelId(),
                            clonedActivity.getConversationReference().getChannelId());
        Assert.assertEquals(activity.getEntities().get(0).getType(), clonedActivity.getEntities().get(0).getType());
        Assert.assertEquals(activity.getExpiration(), clonedActivity.getExpiration());
        Assert.assertEquals(activity.getFrom().getId(), clonedActivity.getFrom().getId());
        Assert.assertEquals(activity.getInputHint(), clonedActivity.getInputHint());
        Assert.assertEquals(activity.getListenFor(), clonedActivity.getListenFor());
        Assert.assertEquals(activity.getLocalTimestamp(), clonedActivity.getLocalTimestamp());
        Assert.assertEquals(activity.getMembersAdded().get(0).getId(), clonedActivity.getMembersAdded().get(0).getId());
        Assert.assertEquals(activity.getMembersRemoved().get(0).getId(),
                            clonedActivity.getMembersRemoved().get(0).getId());
        Assert.assertEquals(activity.getMentions().get(0).getText(), clonedActivity.getMentions().get(0).getText());
        Assert.assertEquals(activity.getProperties(), clonedActivity.getProperties());
        Assert.assertEquals(activity.getReactionsAdded().get(0).getType(),
                            clonedActivity.getReactionsAdded().get(0).getType());
        Assert.assertEquals(activity.getReactionsRemoved().get(0).getType(),
                            clonedActivity.getReactionsRemoved().get(0).getType());
        Assert.assertEquals(activity.getRecipient().getId(), clonedActivity.getRecipient().getId());
        Assert.assertEquals(activity.getRelatesTo().getActivityId(), clonedActivity.getRelatesTo().getActivityId());
        // add activity.getReplyConversationReference(reply)
        Assert.assertEquals(activity.getSuggestedActions(), clonedActivity.getSuggestedActions());
        Assert.assertEquals(activity.getTextFormat(), clonedActivity.getTextFormat());
        Assert.assertEquals(activity.getTextHighlights(), clonedActivity.getTextHighlights());
        Assert.assertEquals(activity.getTimestamp(), clonedActivity.getTimestamp());
    }
",non-flaky,5
148851,microsoft_botbuilder-java,ActivityTest.EnsureCloneAddsIdIfMissing,"    @Test
    public void EnsureCloneAddsIdIfMissing() {
        Activity testActivity = new Activity(ActivityTypes.COMMAND);
        Assert.assertTrue(testActivity.getId() == null);
        Activity clonedActivity = Activity.clone(testActivity);
        Assert.assertTrue(clonedActivity.getId() != null);
    }
",non-flaky,5
148852,microsoft_botbuilder-java,ActivityTest.TryGetChannelData,"    @Test
    public void TryGetChannelData() {
        Activity activity = createActivity();
        ResultPair<TeamsChannelData> channelData = activity.tryGetChannelData(
            TeamsChannelData.class
        );

        activity.setChannelData(new TeamsChannelData());
        channelData = activity.tryGetChannelData(
            TeamsChannelData.class
        );
        Assert.assertTrue(channelData.getLeft());

        activity.setChannelData(null);
        Assert.assertNull(activity.teamsGetChannelData());
    }
",non-flaky,5
148853,microsoft_botbuilder-java,ActivityTest.TryGetChannelDataBadChannelData,"    @Test
    public void TryGetChannelDataBadChannelData() {
        Activity activity = createActivity();
        activity.setChannelData(""badChannelData"");
        ResultPair<TeamsChannelData> channelData = activity.tryGetChannelData(
            TeamsChannelData.class
        );
        Assert.assertFalse(channelData.getLeft());
        Assert.assertNull(channelData.getRight());
    }
",non-flaky,5
148854,microsoft_botbuilder-java,ActivityTest.RemoveRecipientMention,"    @Test
    public void RemoveRecipientMention() {
        Activity activity = createActivity();
        activity.setText(""<at>firstName</at> lastName\n"");
        String expectedStrippedName = ""lastName"";

        List<Mention> mentionList = new ArrayList<Mention>();
        Mention mention = new Mention();
        ChannelAccount channelAccount = new ChannelAccount();
        channelAccount.setId(activity.getRecipient().getId());
        channelAccount.setName(""firstName"");
        mention.setMentioned(channelAccount);
        mentionList.add(mention);
        activity.setMentions(mentionList);

        String strippedActivityText = activity.removeRecipientMention();
        Assert.assertEquals(strippedActivityText, expectedStrippedName);
    }
",non-flaky,5
148855,microsoft_botbuilder-java,ActivityTest.RemoveRecipientMentionImmutable,"    @Test
    public void RemoveRecipientMentionImmutable() {
        Activity activity = createActivity();
        activity.setText(""<at>firstName</at> lastName\n"");
        String expectedStrippedName = ""lastName"";

        List<Mention> mentionList = new ArrayList<Mention>();
        Mention mention = new Mention();
        ChannelAccount channelAccount = new ChannelAccount();
        channelAccount.setId(activity.getRecipient().getId());
        channelAccount.setName(""firstName"");
        mention.setMentioned(channelAccount);
        mentionList.add(mention);
        activity.setMentions(mentionList);

        String strippedActivityText = Activity.removeRecipientMentionImmutable(activity);
        Assert.assertEquals(strippedActivityText, expectedStrippedName);
    }
",non-flaky,5
148856,microsoft_botbuilder-java,ActivityTest.RemoveRecipientMentionNoRecipient,"    @Test
    public void RemoveRecipientMentionNoRecipient() {
        Activity activity = createActivity();
        activity.setText(""<at>firstName</at> lastName\n"");
        String expectedStrippedName = ""<at>firstName</at> lastName\n"";

        List<Mention> mentionList = new ArrayList<Mention>();
        Mention mention = new Mention();
        ChannelAccount channelAccount = new ChannelAccount();
        channelAccount.setId(activity.getRecipient().getId());
        channelAccount.setName(""firstName"");
        mention.setMentioned(channelAccount);
        mentionList.add(mention);
        activity.setMentions(mentionList);
        activity.setRecipient(null);

        String strippedActivityText = activity.removeRecipientMention();
        Assert.assertEquals(strippedActivityText, expectedStrippedName);
    }
",non-flaky,5
148857,microsoft_botbuilder-java,ActivityTest.RemoveRecipientMentionImmutableNoRecipient,"    @Test
    public void RemoveRecipientMentionImmutableNoRecipient() {
        Activity activity = createActivity();
        activity.setText(""<at>firstName</at> lastName\n"");
        String expectedStrippedName = ""<at>firstName</at> lastName\n"";

        List<Mention> mentionList = new ArrayList<Mention>();
        Mention mention = new Mention();
        ChannelAccount channelAccount = new ChannelAccount();
        channelAccount.setId(activity.getRecipient().getId());
        channelAccount.setName(""firstName"");
        mention.setMentioned(channelAccount);
        mentionList.add(mention);
        activity.setMentions(mentionList);
        activity.setRecipient(null);

        String strippedActivityText = Activity.removeRecipientMentionImmutable(activity);
        Assert.assertEquals(strippedActivityText, expectedStrippedName);
    }
",non-flaky,5
148858,microsoft_botbuilder-java,ActivityTest.RemoveRecipientMentionText,"    @Test
    public void RemoveRecipientMentionText() {
        Activity activity = createActivity();
        activity.setText(""<at>firstName</at> lastName\n"");
        String expectedStrippedName = ""<at>firstName</at>"";

        List<Mention> mentionList = new ArrayList<Mention>();
        Mention mention = new Mention();
        mention.setText(""lastName"");
        ChannelAccount channelAccount = new ChannelAccount();
        channelAccount.setId(activity.getRecipient().getId());
        channelAccount.setName(""firstName"");
        mention.setMentioned(channelAccount);
        mentionList.add(mention);
        activity.setMentions(mentionList);

        String strippedActivityText = activity.removeRecipientMention();
        Assert.assertEquals(strippedActivityText, expectedStrippedName);
    }
",non-flaky,5
148859,microsoft_botbuilder-java,ActivityTest.RemoveRecipientMentionTextNoId,"    @Test
    public void RemoveRecipientMentionTextNoId() {
        Activity activity = createActivity();
        activity.setText(""<at>firstName</at> lastName\n"");
        String expectedStrippedName = ""<at>firstName</at> lastName\n"";

        List<Mention> mentionList = new ArrayList<Mention>();
        Mention mention = new Mention();
        mention.setText(""lastName"");
        ChannelAccount channelAccount = new ChannelAccount();
        channelAccount.setId(activity.getRecipient().getId());
        channelAccount.setName(""firstName"");
        mention.setMentioned(channelAccount);
        mentionList.add(mention);
        activity.setMentions(mentionList);

        String strippedActivityText = Activity.removeMentionTextImmutable(activity, null);
        Assert.assertEquals(strippedActivityText, expectedStrippedName);
    }
",non-flaky,5
148860,microsoft_botbuilder-java,ActivityTest.RemoveRecipientMentionTextNoText,"    @Test
    public void RemoveRecipientMentionTextNoText() {
        Activity activity = createActivity();
        activity.setText("""");
        String expectedStrippedName = """";

        List<Mention> mentionList = new ArrayList<Mention>();
        Mention mention = new Mention();
        mention.setText(""lastName"");
        ChannelAccount channelAccount = new ChannelAccount();
        channelAccount.setId(activity.getRecipient().getId());
        channelAccount.setName(""firstName"");
        mention.setMentioned(channelAccount);
        mentionList.add(mention);
        activity.setMentions(mentionList);

        String strippedActivityText = Activity.removeMentionTextImmutable(activity, ""lastName"");
        Assert.assertEquals(strippedActivityText, expectedStrippedName);
    }
",non-flaky,5
148861,microsoft_botbuilder-java,ActivityTest.isActivity,"    @Test
    public void IsActivity() {
        class MyActivity extends Activity {
            @Override
            public boolean isActivity(String activityType) {
                return super.isActivity(activityType);
            }
",non-flaky,5
148862,microsoft_botbuilder-java,ActivityTest.isActivity,"    @Test
    public void IsActivityNoType() {
        class MyActivity extends Activity {
            @Override
            public boolean isActivity(String activityType) {
                return super.isActivity(activityType);
            }
",non-flaky,5
148863,microsoft_botbuilder-java,ActivityTest.isActivity,"    @Test
    public void IsActivityExtendedType() {
        class MyActivity extends Activity {
            @Override
            public boolean isActivity(String activityType) {
                return super.isActivity(activityType);
            }
",non-flaky,5
148864,microsoft_botbuilder-java,ActivityTest.isActivity,"    @Test
    public void IsActivityExtendedTypeNoMatch() {
        class MyActivity extends Activity {
            @Override
            public boolean isActivity(String activityType) {
                return super.isActivity(activityType);
            }
",non-flaky,5
148865,microsoft_botbuilder-java,ActivityTest.isActivity,"    @Test
    public void IsActivityNoMatch() {
        class MyActivity extends Activity {
            @Override
            public boolean isActivity(String activityType) {
                return super.isActivity(activityType);
            }
",non-flaky,5
148866,microsoft_botbuilder-java,ActivityTest.isActivity,"    @Test
    public void IsActivityShorterTypeName() {
        class MyActivity extends Activity {
            @Override
            public boolean isActivity(String activityType) {
                return super.isActivity(activityType);
            }
",non-flaky,5
148867,microsoft_botbuilder-java,MediaCardTest.TestPropertySetterGetter,"    @Test
    public void TestPropertySetterGetter() {
        MediaCard mediaCard = new MediaCard();
        mediaCard.setAspect(""aspect"");
        mediaCard.setAutoloop(true);
        mediaCard.setAutostart(true);

        List<CardAction> buttons = new ArrayList<CardAction>();
        CardAction cardAction1 = new CardAction(ActionTypes.CALL, ""test1"");
        CardAction cardAction2 = new CardAction(ActionTypes.DOWNLOAD_FILE, ""test2"");
        buttons.add(cardAction1);
        buttons.add(cardAction2);
        mediaCard.setButtons(buttons);

        mediaCard.setDuration(""duration"");

        ThumbnailUrl thumbnailUrl = new ThumbnailUrl();
        thumbnailUrl.setAlt(""alt"");
        thumbnailUrl.setUrl(""testUrl"");
        mediaCard.setImage(thumbnailUrl);

        mediaCard.setShareable(true);
        mediaCard.setSubtitle(""subTitle"");
        mediaCard.setText(""text"");
        mediaCard.setTitle(""title"");
        mediaCard.setValue(""value"");

        Assert.assertEquals(mediaCard.getAspect(), ""aspect"");
        Assert.assertEquals(mediaCard.getAutoloop(), true);
        Assert.assertEquals(mediaCard.getAutostart(), true);
        Assert.assertEquals(mediaCard.getButtons().size(), 2);
        Assert.assertEquals(mediaCard.getButtons().get(0).getType(), ActionTypes.CALL);
        Assert.assertEquals(mediaCard.getButtons().get(0).getTitle(), ""test1"");
        Assert.assertEquals(mediaCard.getButtons().get(1).getType(), ActionTypes.DOWNLOAD_FILE);
        Assert.assertEquals(mediaCard.getButtons().get(1).getTitle(), ""test2"");
        Assert.assertEquals(mediaCard.getDuration(), ""duration"");
        Assert.assertEquals(mediaCard.getImage().getUrl(), ""testUrl"");
        Assert.assertEquals(mediaCard.getImage().getAlt(), ""alt"");
        Assert.assertEquals(mediaCard.getShareable(), true);
        Assert.assertEquals(mediaCard.getSubtitle(), ""subTitle"");
        Assert.assertEquals(mediaCard.getText(), ""text"");
        Assert.assertEquals(mediaCard.getTitle(), ""title"");
        Assert.assertEquals(mediaCard.getValue(), ""value"");
    }
",non-flaky,5
148868,microsoft_botbuilder-java,ReceiptCardTest.testToAttachment,"    @Test
    public void testToAttachment() {
        Attachment attachment = getCard().toAttachment();
        Assert.assertNotNull(attachment);
        Assert.assertEquals(""application/vnd.microsoft.card.receipt"", attachment.getContentType());
    }
",non-flaky,5
148869,microsoft_botbuilder-java,SigninCardTest.testToAttachment,"    @Test
    public void testToAttachment() {
        Attachment attachment = getCard().toAttachment();
        Assert.assertNotNull(attachment);
        Assert.assertEquals(""application/vnd.microsoft.card.signin"", attachment.getContentType());
    }
",non-flaky,5
148870,microsoft_botbuilder-java,MessageActionsPayloadTest.TestMessageActionPayloadConstructor,"    @Test
    public void TestMessageActionPayloadConstructor(){
        MessageActionsPayload messageActionsPayload = new MessageActionsPayload();
        Assert.assertNotNull(messageActionsPayload);
    }
",non-flaky,5
148871,microsoft_botbuilder-java,MessageActionsPayloadTest.TestGetId,"    @Test
    public void TestGetId(){
        String id = ""testId"";
        MessageActionsPayload messageActionsPayload = new MessageActionsPayload();
        messageActionsPayload.setId(id);
        String result = messageActionsPayload.getId();

        Assert.assertEquals(result, id);
    }
",non-flaky,5
148872,microsoft_botbuilder-java,MessageActionsPayloadTest.TestGetReplyToId,"    @Test
    public void TestGetReplyToId(){
        String replyToId = ""testReplyToId"";
        MessageActionsPayload messageActionsPayload = new MessageActionsPayload();
        messageActionsPayload.setReplyToId(replyToId);
        String result = messageActionsPayload.getReplyToId();

        Assert.assertEquals(result, replyToId);
    }
",non-flaky,5
148873,microsoft_botbuilder-java,MessageActionsPayloadTest.TestGetMessageType,"    @Test
    public void TestGetMessageType(){
        String messageType = ""testMessageType"";
        MessageActionsPayload messageActionsPayload = new MessageActionsPayload();
        messageActionsPayload.setMessageType(messageType);
        String result = messageActionsPayload.getMessageType();

        Assert.assertEquals(result, messageType);
    }
",non-flaky,5
148874,microsoft_botbuilder-java,MessageActionsPayloadTest.TestGetCreatedDateTime,"    @Test
    public void TestGetCreatedDateTime(){
        String createdDateTime = ""2000-01-01"";
        MessageActionsPayload messageActionsPayload = new MessageActionsPayload();
        messageActionsPayload.setCreatedDateTime(createdDateTime);
        String result = messageActionsPayload.getCreatedDateTime();

        Assert.assertEquals(result, createdDateTime);
    }
",non-flaky,5
148875,microsoft_botbuilder-java,MessageActionsPayloadTest.TestGetLastModifiedDateTime,"    @Test
    public void TestGetLastModifiedDateTime(){
        String lastModifiedDateTime = ""2000-01-01"";
        MessageActionsPayload messageActionsPayload = new MessageActionsPayload();
        messageActionsPayload.setLastModifiedDateTime(lastModifiedDateTime);
        String result = messageActionsPayload.getLastModifiedDateTime();

        Assert.assertEquals(result, lastModifiedDateTime);
    }
",non-flaky,5
148876,microsoft_botbuilder-java,MessageActionsPayloadTest.TestGetDeleted,"    @Test
    public void TestGetDeleted(){
        Boolean deleted = false;
        MessageActionsPayload messageActionsPayload = new MessageActionsPayload();
        messageActionsPayload.setDeleted(deleted);
        Boolean result = messageActionsPayload.getDeleted();

        Assert.assertEquals(result, deleted);
    }
",non-flaky,5
148877,microsoft_botbuilder-java,MessageActionsPayloadTest.TestGetSubject,"    @Test
    public void TestGetSubject(){
        String subject = ""testSubject"";
        MessageActionsPayload messageActionsPayload = new MessageActionsPayload();
        messageActionsPayload.setSubject(subject);
        String result = messageActionsPayload.getSubject();

        Assert.assertEquals(result, subject);
    }
",non-flaky,5
148878,microsoft_botbuilder-java,MessageActionsPayloadTest.TestGetSummary,"    @Test
    public void TestGetSummary(){
        String summary = ""testSummary"";
        MessageActionsPayload messageActionsPayload = new MessageActionsPayload();
        messageActionsPayload.setSummary(summary);
        String result = messageActionsPayload.getSummary();

        Assert.assertEquals(result, summary);
    }
",non-flaky,5
148879,microsoft_botbuilder-java,MessageActionsPayloadTest.TestGetImportance,"    @Test
    public void TestGetImportance(){
        String importance = ""normal"";
        MessageActionsPayload messageActionsPayload = new MessageActionsPayload();
        messageActionsPayload.setImportance(importance);
        String result = messageActionsPayload.getImportance();

        Assert.assertEquals(result, importance);
    }
",non-flaky,5
148880,microsoft_botbuilder-java,MessageActionsPayloadTest.TestGetLinkToMessage,"    @Test
    public void TestGetLinkToMessage(){
        String linkToMessage = ""https://teams.microsoft.com/l/message/testing-id"";
        MessageActionsPayload messageActionsPayload = new MessageActionsPayload();
        messageActionsPayload.setLinkToMessage(linkToMessage);
        String result = messageActionsPayload.getLinkToMessage();

        Assert.assertEquals(result, linkToMessage);
    }
",non-flaky,5
148881,microsoft_botbuilder-java,MessageActionsPayloadTest.TestGetLocale,"    @Test
    public void TestGetLocale(){
        String locale = ""US"";
        MessageActionsPayload messageActionsPayload = new MessageActionsPayload();
        messageActionsPayload.setLocale(locale);
        String result = messageActionsPayload.getLocale();

        Assert.assertEquals(result, locale);
    }
",non-flaky,5
148882,microsoft_botbuilder-java,MessageActionsPayloadTest.TestGetFrom,"    @Test
    public void TestGetFrom(){
        MessageActionsPayloadFrom from = new MessageActionsPayloadFrom();
        MessageActionsPayload messageActionsPayload = new MessageActionsPayload();
        messageActionsPayload.setFrom(from);
        MessageActionsPayloadFrom result = messageActionsPayload.getFrom();

        Assert.assertEquals(result, from);
    }
",non-flaky,5
148883,microsoft_botbuilder-java,MessageActionsPayloadTest.TestGetBody,"    @Test
    public void TestGetBody(){
        MessageActionsPayloadBody body = new MessageActionsPayloadBody();
        MessageActionsPayload messageActionsPayload = new MessageActionsPayload();
        messageActionsPayload.setBody(body);
        MessageActionsPayloadBody result = messageActionsPayload.getBody();

        Assert.assertEquals(result, body);
    }
",non-flaky,5
148884,microsoft_botbuilder-java,MessageActionsPayloadTest.TestGetAttachmentLayout,"    @Test
    public void TestGetAttachmentLayout(){
        String attachmentLayout = ""testAttachmentLayout"";
        MessageActionsPayload messageActionsPayload = new MessageActionsPayload();
        messageActionsPayload.setAttachmentLayout(attachmentLayout);
        String result = messageActionsPayload.getAttachmentLayout();

        Assert.assertEquals(result, attachmentLayout);
    }
",non-flaky,5
148885,microsoft_botbuilder-java,MessageActionsPayloadTest.TestGetAttachments,"    @Test
    public void TestGetAttachments(){
        List<MessageActionsPayloadAttachment> attachments = new ArrayList<MessageActionsPayloadAttachment>();
        MessageActionsPayload messageActionsPayload = new MessageActionsPayload();
        messageActionsPayload.setAttachments(attachments);
        List<MessageActionsPayloadAttachment> result = messageActionsPayload.getAttachments();

        Assert.assertEquals(result, attachments);
    }
",non-flaky,5
148886,microsoft_botbuilder-java,MessageActionsPayloadTest.TestGetMentions,"    @Test
    public void TestGetMentions(){
        List<MessageActionsPayloadMention> mentions = new ArrayList<MessageActionsPayloadMention>();
        MessageActionsPayload messageActionsPayload = new MessageActionsPayload();
        messageActionsPayload.setMentions(mentions);
        List<MessageActionsPayloadMention> result = messageActionsPayload.getMentions();

        Assert.assertEquals(result, mentions);
    }
",non-flaky,5
148887,microsoft_botbuilder-java,MessageActionsPayloadTest.TestGetReactions,"    @Test
    public void TestGetReactions(){
        List<MessageActionsPayloadReaction> reactions = new ArrayList<MessageActionsPayloadReaction>();
        MessageActionsPayload messageActionsPayload = new MessageActionsPayload();
        messageActionsPayload.setReactions(reactions);
        List<MessageActionsPayloadReaction> result = messageActionsPayload.getReactions();

        Assert.assertEquals(result, reactions);
    }
",non-flaky,5
148888,microsoft_botbuilder-java,OAuthCardTest.testToAttachment,"    @Test
    public void testToAttachment() {
        Attachment attachment = getCard().toAttachment();
        Assert.assertNotNull(attachment);
        Assert.assertEquals(""application/vnd.microsoft.card.oauth"", attachment.getContentType());
    }
",non-flaky,5
148889,microsoft_botbuilder-java,EntitySchemaValidationTest.EntityTests_GeoCoordinatesSerializationDeserializationTest,"    @Test
    public void EntityTests_GeoCoordinatesSerializationDeserializationTest() {
        GeoCoordinates geoCoordinates = new GeoCoordinates();
        geoCoordinates.setLatitude(22.00);
        geoCoordinates.setLongitude(23.00);

        Assert.assertEquals(""GeoCoordinates"", geoCoordinates.getType());

        Entity deserializedEntity = new Entity().setAs(geoCoordinates);
        Assert.assertEquals(deserializedEntity.getType(), geoCoordinates.getType());

        GeoCoordinates geoDeserialized = deserializedEntity.getAs(GeoCoordinates.class);
        Assert.assertEquals(geoCoordinates.getType(), geoDeserialized.getType());
        Assert.assertEquals(
            geoCoordinates.getLatitude(), geoDeserialized.getLatitude(), Double.MAX_VALUE
        );
        Assert.assertEquals(
            geoCoordinates.getLongitude(), geoDeserialized.getLongitude(), Double.MAX_VALUE
        );
    }
",non-flaky,5
148890,microsoft_botbuilder-java,EntitySchemaValidationTest.EntityTests_MentionSerializationDeserializationTest,"    @Test
    public void EntityTests_MentionSerializationDeserializationTest() {
        Mention mentionEntity = new Mention();
        mentionEntity.setText(""TESTTEST"");

        Assert.assertEquals(""mention"", mentionEntity.getType());

        Entity deserializedEntity = new Entity().setAs(mentionEntity);
        Assert.assertEquals(deserializedEntity.getType(), mentionEntity.getType());
        Assert.assertEquals(
            deserializedEntity.getProperties().get(""text"").textValue(), mentionEntity.getText()
        );

        Mention mentionDeserialized = deserializedEntity.getAs(Mention.class);
        Assert.assertEquals(mentionEntity.getType(), mentionDeserialized.getType());
        Assert.assertEquals(
            deserializedEntity.getProperties().get(""text"").textValue(), mentionEntity.getText()
        );
    }
",non-flaky,5
148891,microsoft_botbuilder-java,EntitySchemaValidationTest.EntityTests_PlaceSerializationDeserializationTest,"    @Test
    public void EntityTests_PlaceSerializationDeserializationTest() {
        Place placeEntity = new Place();
        placeEntity.setName(""TESTTEST"");

        Assert.assertEquals(""Place"", placeEntity.getType());

        Entity deserializedEntity = new Entity().setAs(placeEntity);
        Assert.assertEquals(deserializedEntity.getType(), placeEntity.getType());

        Place placeDeserialized = deserializedEntity.getAs(Place.class);
        Assert.assertEquals(placeEntity.getType(), placeDeserialized.getType());
    }
",non-flaky,5
148892,microsoft_botbuilder-java,ThumbnailCardTest.testToAttachment,"    @Test
    public void testToAttachment() {
        Attachment attachment = getCard().toAttachment();
        Assert.assertNotNull(attachment);
        Assert.assertEquals(""application/vnd.microsoft.card.thumbnail"", attachment.getContentType());
    }
",non-flaky,5
148893,microsoft_botbuilder-java,AnimationCardTest.testToAttachment,"    @Test
    public void testToAttachment() {
        Attachment attachment = getCard().toAttachment();
        Assert.assertNotNull(attachment);
        Assert.assertEquals(""application/vnd.microsoft.card.animation"", attachment.getContentType());
    }
",non-flaky,5
148894,microsoft_botbuilder-java,CardActionTest.TestImplicitConversation,"    @Test
    public void TestImplicitConversation() {
        SuggestedActions actions = new SuggestedActions(
            new CardAction[] { new CardAction(""x""), new CardAction(""y""), new CardAction(""z"") }
        );

        Assert.assertEquals(""x"", actions.getActions().get(0).getTitle());
        Assert.assertEquals(""x"", actions.getActions().get(0).getValue());
        Assert.assertEquals(""y"", actions.getActions().get(1).getTitle());
        Assert.assertEquals(""y"", actions.getActions().get(1).getValue());
        Assert.assertEquals(""z"", actions.getActions().get(2).getTitle());
        Assert.assertEquals(""z"", actions.getActions().get(2).getValue());
    }
",non-flaky,5
148895,microsoft_botbuilder-java,CardActionTest.TestClone,"    @Test
    public void TestClone() {

        CardAction cardAction =  new CardAction();
        cardAction.setChannelData(""channelData"");
        cardAction.setDisplayText(""displayTest"");
        cardAction.setImage(""image"");
        cardAction.setImageAltText(""imageAltText"");
        cardAction.setText(""text"");
        cardAction.setTitle(""title"");
        cardAction.setType(ActionTypes.CALL);
        cardAction.setValue(""value"");

        CardAction newCardAction = CardAction.clone(cardAction);

        Assert.assertEquals(cardAction.getChannelData(), newCardAction.getChannelData());
        Assert.assertEquals(cardAction.getDisplayText(), newCardAction.getDisplayText());
        Assert.assertEquals(cardAction.getImage(), newCardAction.getImage());
        Assert.assertEquals(cardAction.getImageAltText(), newCardAction.getImageAltText());
        Assert.assertEquals(cardAction.getText(), newCardAction.getText());
        Assert.assertEquals(cardAction.getTitle(), newCardAction.getTitle());
        Assert.assertEquals(cardAction.getType(), newCardAction.getType());
        Assert.assertEquals(cardAction.getValue(), newCardAction.getValue());
    }
",non-flaky,5
148896,microsoft_botbuilder-java,CardActionTest.TestCloneNull,"    @Test
    public void TestCloneNull() {
        CardAction newCardAction = CardAction.clone(null);
        Assert.assertNull(newCardAction);
    }
",non-flaky,5
148897,microsoft_botbuilder-java,CardActionTest.TestConstructorTwoParams,"    @Test
    public void TestConstructorTwoParams() {
        CardAction cardAction =  new CardAction(ActionTypes.CALL, ""title"");
        Assert.assertEquals(cardAction.getType(), ActionTypes.CALL);
        Assert.assertEquals(cardAction.getTitle(), ""title"");
    }
",non-flaky,5
148898,microsoft_botbuilder-java,CardActionTest.TestConstructorThreeParams,"    @Test
    public void TestConstructorThreeParams() {
        CardAction cardAction =  new CardAction(ActionTypes.CALL, ""title"", ""value"");
        Assert.assertEquals(cardAction.getType(), ActionTypes.CALL);
        Assert.assertEquals(cardAction.getTitle(), ""title"");
        Assert.assertEquals(cardAction.getValue(), ""value"");
    }
",non-flaky,5
148899,microsoft_botbuilder-java,SerializationTest.testGetAs,"    @Test
    public void testGetAs() {
        Activity activity = createActivity();
        JsonNode activityNode = Serialization.objectToTree(activity);
        Activity resultActivity = Serialization.getAs(activityNode, Activity.class);
        Assert.assertEquals(activity.getId(), resultActivity.getId());
        Assert.assertEquals(activity.getFrom().getId(), resultActivity.getFrom().getId());
        Assert.assertEquals(activity.getConversation().getId(), resultActivity.getConversation().getId());
    }
",non-flaky,5
148900,microsoft_botbuilder-java,SerializationTest.testGetAsNull,"    @Test
    public void testGetAsNull() {
        Activity resultActivity = Serialization.getAs(null, Activity.class);
        Assert.assertNull(resultActivity);
    }
",non-flaky,5
148901,microsoft_botbuilder-java,SerializationTest.testClone,"    @Test
    public void testClone() {
        Activity activity = createActivity();
        Activity resultActivity = (Activity) Serialization.clone((Object) activity);
        Assert.assertEquals(activity.getId(), resultActivity.getId());
        Assert.assertEquals(activity.getFrom().getId(), resultActivity.getFrom().getId());
        Assert.assertEquals(activity.getConversation().getId(), resultActivity.getConversation().getId());
    }
",non-flaky,5
148902,microsoft_botbuilder-java,SerializationTest.testCloneNull,"    @Test
    public void testCloneNull() {
        Activity resultActivity = (Activity) Serialization.clone((Object) null);
        Assert.assertNull(resultActivity);
    }
",non-flaky,5
150113,apache_hive,TestHplsqlUdf.testEvaluateWithoutRun,"  @Test
  public void testEvaluateWithoutRun() throws HiveException {
    // init udf
    Udf udf = new Udf();
    ObjectInspector[] initArguments = {queryOI, argOI};
    udf.initialize(initArguments);
    //set arguments
    DeferredObject queryObj = new DeferredJavaObject(""hello(:1)"");
      DeferredObject argObj = new DeferredJavaObject(""name"");
      DeferredObject[] argumentsObj = {queryObj, argObj};
      
      // init exec and set parameters, included
      udf.initExec(argumentsObj);
      udf.setParameters(argumentsObj);
      
      // checking var exists and its value is right
      Var var = udf.exec.findVariable("":1"");
      Assert.assertNotNull(var);
      String val = (String) var.value;
      Assert.assertEquals(val, ""name"");
  }
",non-flaky,5
150114,apache_hive,TestHplsqlOffline.testCreateTableDb2,"  @Test
  public void testCreateTableDb2() throws Exception {
    run(""create_table_db2"");
  }
",non-flaky,5
150115,apache_hive,TestHplsqlOffline.testCreateTableMssql,"  @Test
  public void testCreateTableMssql() throws Exception {
    run(""create_table_mssql"");
  }
",non-flaky,5
150116,apache_hive,TestHplsqlOffline.testCreateTableMssql2,"  @Test
  public void testCreateTableMssql2() throws Exception {
    run(""create_table_mssql2"");
  }
",non-flaky,5
150117,apache_hive,TestHplsqlOffline.testCreateTableMysql,"  @Test
  public void testCreateTableMysql() throws Exception {
    run(""create_table_mysql"");
  }
",non-flaky,5
150118,apache_hive,TestHplsqlOffline.testCreateTableOra,"  @Test
  public void testCreateTableOra() throws Exception {
    run(""create_table_ora"");
  }
",non-flaky,5
150119,apache_hive,TestHplsqlOffline.testCreateTableOra2,"  @Test
  public void testCreateTableOra2() throws Exception {
    run(""create_table_ora2"");
  }
",non-flaky,5
150120,apache_hive,TestHplsqlOffline.testCreateTablePg,"  @Test
  public void testCreateTablePg() throws Exception {
    run(""create_table_pg"");
  }
",non-flaky,5
150121,apache_hive,TestHplsqlOffline.testCreateTableTd,"  @Test
  public void testCreateTableTd() throws Exception {
    run(""create_table_td"");
  }
",non-flaky,5
150122,apache_hive,TestHplsqlOffline.testDeleteAll,"  @Test
  public void testDeleteAll() throws Exception {
    run(""delete_all"");
  }
",non-flaky,5
150123,apache_hive,TestHplsqlOffline.testInsertMysql,"  @Test
  public void testInsertMysql() throws Exception {
    run(""insert_mysql"");
  }
",non-flaky,5
150124,apache_hive,TestHplsqlOffline.testSelect,"  @Test
  public void testSelect() throws Exception {
    run(""select"");
  }
",non-flaky,5
150125,apache_hive,TestHplsqlOffline.testSelectDb2,"  @Test
  public void testSelectDb2() throws Exception {
    run(""select_db2"");
  }
",non-flaky,5
150126,apache_hive,TestHplsqlOffline.testSelectTeradata,"  @Test
  public void testSelectTeradata() throws Exception {
    run(""select_teradata"");
  }
",non-flaky,5
150127,apache_hive,TestHplsqlOffline.testUpdate,"  @Test
  public void testUpdate() throws Exception {
    run(""update"");
  }
",non-flaky,5
150128,apache_hive,TestHplsqlLocal.testAdd,"  @Test
  public void testAdd() throws Exception {
    run(""add"");
  }
",non-flaky,5
150129,apache_hive,TestHplsqlLocal.testAssign,"  @Test
  public void testAssign() throws Exception {
    run(""assign"");
  }
",non-flaky,5
150130,apache_hive,TestHplsqlLocal.testBool,"  @Test
  public void testBool() throws Exception {
    run(""bool"");
  }
",non-flaky,5
150131,apache_hive,TestHplsqlLocal.testBoolExpr,"  @Test
  public void testBoolExpr() throws Exception {
    run(""bool_expr"");
  }
",non-flaky,5
150132,apache_hive,TestHplsqlLocal.testBreak,"  @Test
  public void testBreak() throws Exception {
    run(""break"");
  }
",non-flaky,5
150133,apache_hive,TestHplsqlLocal.testCase,"  @Test
  public void testCase() throws Exception {
    run(""case"");
  }
",non-flaky,5
150134,apache_hive,TestHplsqlLocal.testCast,"  @Test
  public void testCast() throws Exception {
    run(""cast"");
  }
",non-flaky,5
150135,apache_hive,TestHplsqlLocal.testCast2,"  @Test
  public void testCast2() throws Exception {
    run(""cast2"");
  }
",non-flaky,5
150136,apache_hive,TestHplsqlLocal.testChar,"  @Test
  public void testChar() throws Exception {
    run(""char"");
  }
",non-flaky,5
150137,apache_hive,TestHplsqlLocal.testCoalesce,"  @Test
  public void testCoalesce() throws Exception {
    run(""coalesce"");
  }
",non-flaky,5
150138,apache_hive,TestHplsqlLocal.testConcat,"  @Test
  public void testConcat() throws Exception {
    run(""concat"");
  }
",non-flaky,5
150139,apache_hive,TestHplsqlLocal.testCreateFunction,"  @Test
  public void testCreateFunction() throws Exception {
    run(""create_function"");
  }
",non-flaky,5
150140,apache_hive,TestHplsqlLocal.testCreateFunction2,"  @Test
  public void testCreateFunction2() throws Exception {
    run(""create_function2"");
  }
",non-flaky,5
150141,apache_hive,TestHplsqlLocal.testCreateFunction3,"  @Test
  public void testCreateFunction3() throws Exception {
    run(""create_function3"");
  }
",non-flaky,5
150142,apache_hive,TestHplsqlLocal.testCreateFunction4,"  @Test
  public void testCreateFunction4() throws Exception {
    run(""create_function4"");
  }
",non-flaky,5
150143,apache_hive,TestHplsqlLocal.testCreateFunction5,"  @Test
  public void testCreateFunction5() throws Exception {
    run(""create_function5"");
  }
",non-flaky,5
150144,apache_hive,TestHplsqlLocal.testCreatePackage,"  @Test
  public void testCreatePackage() throws Exception {
    run(""create_package"");
  }
",non-flaky,5
150145,apache_hive,TestHplsqlLocal.testCreatePackage2,"  @Test
  public void testCreatePackage2() throws Exception {
    run(""create_package2"");
  }
",non-flaky,5
150146,apache_hive,TestHplsqlLocal.testCreatePackage3,"  @Test
  public void testCreatePackage3() throws Exception {
    run(""create_package3"");
  }
",non-flaky,5
150147,apache_hive,TestHplsqlLocal.testCreateProcedure,"  @Test
  public void testCreateProcedure() throws Exception {
    run(""create_procedure"");
  }
",non-flaky,5
150148,apache_hive,TestHplsqlLocal.testCreateProcedure2,"  @Test
  public void testCreateProcedure2() throws Exception {
    run(""create_procedure2"");
  }
",non-flaky,5
150149,apache_hive,TestHplsqlLocal.testCreateProcedure3,"  @Test
  public void testCreateProcedure3() throws Exception {
    run(""create_procedure3"");
  }
",non-flaky,5
150150,apache_hive,TestHplsqlLocal.testCreateProcedure4,"  @Test
  public void testCreateProcedure4() throws Exception {
    run(""create_procedure4"");
  }
",non-flaky,5
150151,apache_hive,TestHplsqlLocal.testCreateProcedureNoParams,"  @Test
  public void testCreateProcedureNoParams() throws Exception {
    run(""create_procedure_no_params"");
  }
",non-flaky,5
150152,apache_hive,TestHplsqlLocal.testDatatypes,"  @Test
  public void testDatatypes() throws Exception {
    run(""datatypes"");
  }
",non-flaky,5
150153,apache_hive,TestHplsqlLocal.testDate,"  @Test
  public void testDate() throws Exception {
    run(""date"");
  }
",non-flaky,5
150154,apache_hive,TestHplsqlLocal.testDbmsOutput,"  @Test
  public void testDbmsOutput() throws Exception {
    run(""dbms_output"");
  }
",non-flaky,5
150155,apache_hive,TestHplsqlLocal.testDeclare,"  @Test
  public void testDeclare() throws Exception {
    run(""declare"");
  }
",non-flaky,5
150156,apache_hive,TestHplsqlLocal.testDeclare2,"  @Test
  public void testDeclare2() throws Exception {
    run(""declare2"");
  }
",non-flaky,5
150157,apache_hive,TestHplsqlLocal.testDeclare3,"  @Test
  public void testDeclare3() throws Exception {
    run(""declare3"");
  }
",non-flaky,5
150158,apache_hive,TestHplsqlLocal.testDeclareCondition,"  @Test
  public void testDeclareCondition() throws Exception {
    run(""declare_condition"");
  }
",non-flaky,5
150159,apache_hive,TestHplsqlLocal.testDeclareCondition2,"  @Test
  public void testDeclareCondition2() throws Exception {
    run(""declare_condition2"");
  }
",non-flaky,5
150160,apache_hive,TestHplsqlLocal.testDecode,"  @Test
  public void testDecode() throws Exception {
    run(""decode"");
  }
",non-flaky,5
150161,apache_hive,TestHplsqlLocal.testEqual,"  @Test
  public void testEqual() throws Exception {
    run(""equal"");
  }
",non-flaky,5
150162,apache_hive,TestHplsqlLocal.testException,"  @Test
  public void testException() throws Exception {
    run(""exception"");
  }
",non-flaky,5
150163,apache_hive,TestHplsqlLocal.testExceptionDivideByZero,"  @Test
  public void testExceptionDivideByZero() throws Exception {
    run(""exception_divide_by_zero"");
  }
",non-flaky,5
150164,apache_hive,TestHplsqlLocal.testExit,"  @Test
  public void testExit() throws Exception {
    run(""exit"");
  }
",non-flaky,5
150165,apache_hive,TestHplsqlLocal.testExpr,"  @Test
  public void testExpr() throws Exception {
    run(""expr"");
  }
",non-flaky,5
150166,apache_hive,TestHplsqlLocal.testFloat,"  @Test
  public void testFloat() throws Exception {
    run(""float"");
  }
",non-flaky,5
150167,apache_hive,TestHplsqlLocal.testForRange,"  @Test
  public void testForRange() throws Exception {
    run(""for_range"");
  }
",non-flaky,5
150168,apache_hive,TestHplsqlLocal.testIf,"  @Test
  public void testIf() throws Exception {
    run(""if"");
  }
",non-flaky,5
150169,apache_hive,TestHplsqlLocal.testIf2,"  @Test
  public void testIf2() throws Exception {
    run(""if2"");
  }
",non-flaky,5
150170,apache_hive,TestHplsqlLocal.testIf3Bteq,"  @Test
  public void testIf3Bteq() throws Exception {
    run(""if3_bteq"");
  }
",non-flaky,5
150171,apache_hive,TestHplsqlLocal.testInclude,"  @Test
  public void testInclude() throws Exception {
    run(""include"");
  }
",non-flaky,5
150172,apache_hive,TestHplsqlLocal.testInstr,"  @Test
  public void testInstr() throws Exception {
    run(""instr"");
  }
",non-flaky,5
150173,apache_hive,TestHplsqlLocal.testInterval,"  @Test
  public void testInterval() throws Exception {
    run(""interval"");
  }
",non-flaky,5
150174,apache_hive,TestHplsqlLocal.testLang,"  @Test
  public void testLang() throws Exception {
    run(""lang"");
  }
",non-flaky,5
150175,apache_hive,TestHplsqlLocal.testLeave,"  @Test
  public void testLeave() throws Exception {
    run(""leave"");
  }
",non-flaky,5
150176,apache_hive,TestHplsqlLocal.testLength,"  @Test
  public void testLength() throws Exception {
    run(""length"");
  }
",non-flaky,5
150177,apache_hive,TestHplsqlLocal.testLen,"  @Test
  public void testLen() throws Exception {
    run(""len"");
  }
",non-flaky,5
150178,apache_hive,TestHplsqlLocal.testLower,"  @Test
  public void testLower() throws Exception {
    run(""lower"");
  }
",non-flaky,5
150179,apache_hive,TestHplsqlLocal.testMultDiv,"  @Test
  public void testMultDiv() throws Exception {
    run(""mult_div"");
  }
",non-flaky,5
150180,apache_hive,TestHplsqlLocal.testNvl,"  @Test
  public void testNvl() throws Exception {
    run(""nvl"");
  }
",non-flaky,5
150181,apache_hive,TestHplsqlLocal.testNvl2,"  @Test
  public void testNvl2() throws Exception {
    run(""nvl2"");
  }
",non-flaky,5
150182,apache_hive,TestHplsqlLocal.testPrint,"  @Test
  public void testPrint() throws Exception {
    run(""print"");
  }
",non-flaky,5
150183,apache_hive,TestHplsqlLocal.testReplace,"  @Test
  public void testReplace() throws Exception {
    run(""replace"");
  }
",non-flaky,5
150184,apache_hive,TestHplsqlLocal.testReturn,"  @Test
  public void testReturn() throws Exception {
    run(""return"");
  }
",non-flaky,5
150185,apache_hive,TestHplsqlLocal.testSetError,"  @Test
  public void testSetError() throws Exception {
    run(""seterror"");
  }
",non-flaky,5
150186,apache_hive,TestHplsqlLocal.testSub,"  @Test
  public void testSub() throws Exception {
    run(""sub"");
  }
",non-flaky,5
150187,apache_hive,TestHplsqlLocal.testSubstring,"  @Test
  public void testSubstring() throws Exception {
    run(""substring"");
  }
",non-flaky,5
150188,apache_hive,TestHplsqlLocal.testSubstr,"  @Test
  public void testSubstr() throws Exception {
    run(""substr"");
  }
",non-flaky,5
150189,apache_hive,TestHplsqlLocal.testTimestampIso,"  @Test
  public void testTimestampIso() throws Exception {
    run(""timestamp_iso"");
  }
",non-flaky,5
150190,apache_hive,TestHplsqlLocal.testTimestamp,"  @Test
  public void testTimestamp() throws Exception {
    run(""timestamp"");
  }
",non-flaky,5
150191,apache_hive,TestHplsqlLocal.testToChar,"  @Test
  public void testToChar() throws Exception {
    run(""to_char"");
  }
",non-flaky,5
150192,apache_hive,TestHplsqlLocal.testToTimestamp,"  @Test
  public void testToTimestamp() throws Exception {
    run(""to_timestamp"");
  }
",non-flaky,5
150193,apache_hive,TestHplsqlLocal.testTrim,"  @Test
  public void testTrim() throws Exception {
    run(""trim"");
  }
",non-flaky,5
150194,apache_hive,TestHplsqlLocal.testTwoPipes,"  @Test
  public void testTwoPipes() throws Exception {
    run(""twopipes"");
  }
",non-flaky,5
150195,apache_hive,TestHplsqlLocal.testUpper,"  @Test
  public void testUpper() throws Exception {
    run(""upper"");
  }
",non-flaky,5
150196,apache_hive,TestHplsqlLocal.testValuesInto,"  @Test
  public void testValuesInto() throws Exception {
    run(""values_into"");
  }
",non-flaky,5
150197,apache_hive,TestHplsqlLocal.testVarScope,"  @Test
  public void testVarScope() throws Exception {
    run(""var_scope"");
  }
",non-flaky,5
150198,apache_hive,TestHplsqlLocal.testVarScope2,"  @Test
  public void testVarScope2() throws Exception {
    run(""var_scope2"");
  }
",non-flaky,5
150199,apache_hive,TestHplsqlLocal.testWhile,"  @Test
  public void testWhile() throws Exception {
    run(""while"");
  }
",non-flaky,5
150200,apache_hive,TestJavaDataModel.testGetDoesNotReturnNull,"  @Test
  public void testGetDoesNotReturnNull() throws Exception {
    JavaDataModel model = JavaDataModel.get();
    assertNotNull(model);
  }
",non-flaky,5
150201,apache_hive,TestJavaDataModel.testGetModelForSystemWhenSetTo32,"  @Test
  public void testGetModelForSystemWhenSetTo32() throws Exception {
    System.setProperty(DATA_MODEL_PROPERTY, ""32"");
    assertSame(JavaDataModel.JAVA32, JavaDataModel.getModelForSystem());
  }
",non-flaky,5
150202,apache_hive,TestJavaDataModel.testGetModelForSystemWhenSetTo64,"  @Test
  public void testGetModelForSystemWhenSetTo64() throws Exception {
    System.setProperty(DATA_MODEL_PROPERTY, ""64"");
    assertSame(JavaDataModel.JAVA64, JavaDataModel.getModelForSystem());
  }
",non-flaky,5
150203,apache_hive,TestJavaDataModel.testGetModelForSystemWhenSetToUnknown,"  @Test
  public void testGetModelForSystemWhenSetToUnknown() throws Exception {
    System.setProperty(DATA_MODEL_PROPERTY, ""unknown"");
    assertSame(JavaDataModel.JAVA64, JavaDataModel.getModelForSystem());
  }
",non-flaky,5
150204,apache_hive,TestJavaDataModel.testGetModelForSystemWhenUndefined,"  @Test
  public void testGetModelForSystemWhenUndefined() throws Exception {
    System.clearProperty(DATA_MODEL_PROPERTY);
    assertSame(JavaDataModel.JAVA64, JavaDataModel.getModelForSystem());
  }
",non-flaky,5
150205,apache_hive,TestBytesColumnVector.testSmallBufferReuse,"  @Test
  public void testSmallBufferReuse() {
    BytesColumnVector col = new BytesColumnVector();
    int smallWriteSize = 1024;
    int largeWriteSize = 1024 * 1024 * 2;

    int rowIdx = 0;
    int bytesWrittenToBytes1 = 0;
    col.reset();

    // Initial write (small value)
    byte[] bytes1 = writeToBytesColumnVector(rowIdx, col, smallWriteSize, (byte) 1);
    bytesWrittenToBytes1 += smallWriteSize;

    // Write a large value. This should use a different byte buffer
    rowIdx++;
    byte[] bytes2 = writeToBytesColumnVector(rowIdx, col, largeWriteSize, (byte) 2);
    assertFalse(bytes1 == bytes2);

    // Another small write. smallBuffer should be re-used for this write
    rowIdx++;
    byte[] bytes3 = writeToBytesColumnVector(rowIdx, col, smallWriteSize, (byte) 1);
    bytesWrittenToBytes1 += smallWriteSize;
    assertTrue(bytes1 == bytes3);

    // Write another large value. This should use a different byte buffer
    rowIdx++;
    byte[] bytes4 = writeToBytesColumnVector(rowIdx, col, largeWriteSize, (byte) 3);
    assertFalse(bytes1 == bytes4);
    assertFalse(bytes2 == bytes4);

    // Eventually enough small writes should result in another buffer getting created
    boolean gotNewBuffer = false;
    // Test is dependent on getting a new buffer within 1MB.
    // This may need to change as the implementation changes.
    for (int i = 0; i < 1024; ++i) {
      rowIdx++;
      byte[] currBytes = writeToBytesColumnVector(rowIdx, col, smallWriteSize, (byte) 1);
      if (currBytes == bytes1) {
        bytesWrittenToBytes1 += smallWriteSize;
      } else {
        gotNewBuffer = true;
        break;
      }
    }

    assertTrue(gotNewBuffer);

    // All small writes to the first buffer should be in contiguous memory
    for (int i = 0; i < bytesWrittenToBytes1; ++i) {
      assertEquals((byte) 1, bytes1[i]);
    }
  }
",non-flaky,5
150206,apache_hive,TestListColumnVector.testFlatten,"  @Test
  public void testFlatten() throws Exception {
    LongColumnVector col1 = new LongColumnVector(10);
    ListColumnVector vector = new ListColumnVector(10, col1);
    vector.init();

    // TEST - repeating NULL & no selection
    col1.isRepeating = true;
    vector.isRepeating = true;
    vector.noNulls = false;
    vector.isNull[0] = true;
    vector.childCount = 0;
    for(int i=0; i < 10; ++i) {
      col1.vector[i] = i + 3;
      vector.offsets[i] = i;
      vector.lengths[i] = 10 + i;
    }
    vector.flatten(false, null, 10);
    // make sure the vector was flattened
    assertFalse(vector.isRepeating);
    assertFalse(vector.noNulls);
    // child isn't flattened, because parent is repeating null
    assertTrue(col1.isRepeating);
    assertTrue(col1.noNulls);
    for(int i=0; i < 10; ++i) {
      assertTrue(""isNull at "" + i, vector.isNull[i]);
    }
    for(int i=0; i < 10; ++i) {
      StringBuilder buf = new StringBuilder();
      vector.stringifyValue(buf, i);
      assertEquals(""null"", buf.toString());
    }
    vector.unFlatten();
    assertTrue(col1.isRepeating);
    assertTrue(vector.isRepeating);

    // TEST - repeating NULL & selection
    Arrays.fill(vector.isNull, 1, 10, false);
    int[] sel = new int[]{3, 5, 7};
    vector.flatten(true, sel, 3);
    for(int i=1; i < 10; i++) {
      assertEquals(""failure at "" + i,
          i == 3 || i == 5 || i == 7, vector.isNull[i]);
    }
    vector.unFlatten();

    // TEST - repeating non-NULL & no-selection
    vector.noNulls = true;
    vector.isRepeating = true;
    vector.offsets[0] = 0;
    vector.lengths[0] = 3;
    vector.childCount = 3;
    vector.flatten(false, null, 10);
    // make sure the vector was flattened
    assertFalse(vector.isRepeating);
    assertFalse(vector.noNulls);
    assertFalse(col1.isRepeating);
    assertFalse(col1.noNulls);
    for(int i=0; i < 10; ++i) {
      assertEquals(""offset at "" + i, 0, vector.offsets[i]);
      assertEquals(""length at "" + i, 3, vector.lengths[i]);
    }
    for(int i=0; i < 10; ++i) {
      StringBuilder buf = new StringBuilder();
      vector.stringifyValue(buf, i);
      assertEquals(""[3, 3, 3]"", buf.toString());
    }
    vector.unFlatten();
    assertTrue(col1.isRepeating);
    assertTrue(col1.noNulls);
    assertTrue(vector.isRepeating);
    assertTrue(vector.noNulls);

    // TEST - repeating non-NULL & selection
    Arrays.fill(vector.offsets, 1, 10, -1);
    Arrays.fill(vector.lengths, 1, 10, -1);
    Arrays.fill(col1.vector, 1, 10, -1);
    vector.flatten(true, sel, 3);
    for(int i=1; i < 10; i++) {
      if (i == 3 || i == 5 || i == 7) {
        assertEquals(""failure at "" + i, 0, vector.offsets[i]);
        assertEquals(""failure at "" + i, 3, vector.lengths[i]);
      } else {
        assertEquals(""failure at "" + i, -1, vector.offsets[i]);
        assertEquals(""failure at "" + i, -1, vector.lengths[i]);
      }
    }
    for(int i=0; i < 3; ++i) {
      assertEquals(""failure at "" + i, 3, col1.vector[i]);
    }
    for(int i=3; i < 10; ++i) {
      assertEquals(""failure at "" + i, -1, col1.vector[i]);
    }
    vector.unFlatten();

    // TEST - reset
    vector.reset();
    assertFalse(col1.isRepeating);
    assertTrue(col1.noNulls);
    assertFalse(vector.isRepeating);
    assertTrue(vector.noNulls);
    assertEquals(0, vector.childCount);
  }
",non-flaky,5
150207,apache_hive,TestListColumnVector.testSet,"  @Test
  public void testSet() throws Exception {
    LongColumnVector input1 = new LongColumnVector(10);
    ListColumnVector input = new ListColumnVector(10, input1);
    input.init();
    LongColumnVector output1 = new LongColumnVector(30);
    ListColumnVector output = new ListColumnVector(10, output1);
    output.init();
    input.noNulls = false;
    input.isNull[6] = true;
    input.childCount = 11;
    Arrays.fill(output1.vector, -1);
    for(int i=0; i < 10; ++i) {
      input1.vector[i] = 10 * i;
      input.offsets[i] = i;
      input.lengths[i] = 2;
      output.offsets[i] = i + 2;
      output.lengths[i] = 3;
    }
    output.childCount = 30;

    // copy a null
    output.isNull[3] = false;
    output.setElement(3, 6, input);
    assertEquals(30, output.childCount);
    StringBuilder buf = new StringBuilder();
    output.stringifyValue(buf, 3);
    assertEquals(""null"", buf.toString());

    // copy a value
    output.isNull[3] = false;
    output.setElement(3, 5, input);
    assertEquals(30, output.offsets[3]);
    assertEquals(2, output.lengths[3]);
    assertEquals(32, output.childCount);
    buf = new StringBuilder();
    output.stringifyValue(buf, 3);
    assertEquals(""[50, 60]"", buf.toString());

    // overwrite a value
    output.isNull[3] = false;
    output.setElement(3, 4, input);
    assertEquals(34, output.childCount);
    assertEquals(34, output1.vector.length);
    assertEquals(50, output1.vector[30]);
    assertEquals(60, output1.vector[31]);
    buf = new StringBuilder();
    output.stringifyValue(buf, 3);
    assertEquals(""[40, 50]"", buf.toString());

    input.reset();
    assertEquals(false, input1.isRepeating);
    assertEquals(true, input.noNulls);
    output.reset();
    assertEquals(0, output.childCount);

    input.isRepeating = true;
    input.offsets[0] = 0;
    input.lengths[0] = 10;
    output.setElement(2, 7, input);
    assertEquals(10, output.childCount);
    buf = new StringBuilder();
    output.stringifyValue(buf, 2);
    assertEquals(""[0, 10, 20, 30, 40, 50, 60, 70, 80, 90]"", buf.toString());
  }
",non-flaky,5
150208,apache_hive,TestStringExpr.test,"  @Test
  public void test() throws Exception {
    StringExpr.Finder pattern = compile(""pattern"");
    assertNotNull(pattern);

    StringExpr.Finder patternOneChar = compile(""g"");
    assertNotNull(patternOneChar);

    StringExpr.Finder patternZero = compile("""");
    assertNotNull(patternZero);

    String input1 = ""string that contains a patterN..."";
    String input2 = ""string that contains a pattern..."";
    String input3 = ""pattern at the start of a string"";
    String input4 = ""string that ends with a pattern"";

    assertEquals(""Testing invalid match"", -1, find(pattern, input1));
    assertEquals(""Testing valid match"", 23, find(pattern, input2));
    assertEquals(""Testing single-character match"", 5, find(patternOneChar, input1));
    assertEquals(""Testing zero-length pattern"", 0, find(patternZero, input1));
    assertEquals(""Testing match at start of string"", 0, find(pattern, input3));
    assertEquals(""Testing match at end of string"", 24, find(pattern, input4));
  }
",non-flaky,5
150209,apache_hive,TestUnionColumnVector.testFlatten,"  @Test
  public void testFlatten() throws Exception {
    LongColumnVector col1 = new LongColumnVector(10);
    LongColumnVector col2 = new LongColumnVector(10);
    UnionColumnVector vector = new UnionColumnVector(10, col1, col2);
    vector.init();
    col1.isRepeating = true;
    for(int i=0; i < 10; ++i) {
      vector.tags[i] = i % 2;
      col1.vector[i] = i;
      col2.vector[i] = 2 * i;
    }
    vector.flatten(false, null, 10);
    assertFalse(col1.isRepeating);
    for(int i=0; i < 10; ++i) {
      assertEquals(i % 2, vector.tags[i]);
      assertEquals(""col1 at "" + i, 0, col1.vector[i]);
      assertEquals(""col2 at "" + i, 2 * i, col2.vector[i]);
    }
    vector.unFlatten();
    assertTrue(col1.isRepeating);
    for(int i=0; i < 10; ++i) {
      StringBuilder buf = new StringBuilder();
      vector.stringifyValue(buf, i);
      assertEquals(""{\""tag\"": "" + (i % 2) + "", \""value\"": "" +
          (i % 2 == 0 ? 0 : 2 * i) + ""}"", buf.toString());
    }
    vector.reset();
    assertFalse(col1.isRepeating);
  }
",non-flaky,5
150210,apache_hive,TestUnionColumnVector.testSet,"  @Test
  public void testSet() throws Exception {
    LongColumnVector input1 = new LongColumnVector(10);
    LongColumnVector input2 = new LongColumnVector(10);
    UnionColumnVector input = new UnionColumnVector(10, input1, input2);
    input.init();
    LongColumnVector output1 = new LongColumnVector(10);
    LongColumnVector output2 = new LongColumnVector(10);
    UnionColumnVector output = new UnionColumnVector(10, output1, output2);
    output.init();
    input1.isRepeating = true;
    for(int i=0; i < 10; ++i) {
      input.tags[i] = i % 2;
      input1.vector[i] = i + 1;
      input2.vector[i] = i + 2;
    }
    output.setElement(3, 4, input);
    StringBuilder buf = new StringBuilder();
    output.stringifyValue(buf, 3);
    assertEquals(""{\""tag\"": 0, \""value\"": 1}"", buf.toString());
    input.noNulls = false;
    input.isNull[5] = true;
    output.setElement(3, 5, input);
    buf = new StringBuilder();
    output.stringifyValue(buf, 3);
    assertEquals(""null"", buf.toString());
    input.reset();
    assertEquals(false, input1.isRepeating);
    assertEquals(true, input.noNulls);
  }
",non-flaky,5
150211,apache_hive,TestStructColumnVector.testFlatten,"  @Test
  public void testFlatten() throws Exception {
    LongColumnVector col1 = new LongColumnVector(10);
    LongColumnVector col2 = new LongColumnVector(10);
    StructColumnVector vector = new StructColumnVector(10, col1, col2);
    vector.init();
    col1.isRepeating = true;
    for(int i=0; i < 10; ++i) {
      col1.vector[i] = i;
      col2.vector[i] = 2 * i;
    }
    vector.flatten(false, null, 10);
    assertFalse(col1.isRepeating);
    for(int i=0; i < 10; ++i) {
      assertEquals(""col1 at "" + i, 0, col1.vector[i]);
      assertEquals(""col2 at "" + i, 2 * i, col2.vector[i]);
    }
    vector.unFlatten();
    assertTrue(col1.isRepeating);
    for(int i=0; i < 10; ++i) {
      StringBuilder buf = new StringBuilder();
      vector.stringifyValue(buf, i);
      assertEquals(""[0, "" + (2 * i) + ""]"", buf.toString());
    }
    vector.reset();
    assertFalse(col1.isRepeating);
  }
",non-flaky,5
150212,apache_hive,TestStructColumnVector.testSet,"  @Test
  public void testSet() throws Exception {
    LongColumnVector input1 = new LongColumnVector(10);
    LongColumnVector input2 = new LongColumnVector(10);
    StructColumnVector input = new StructColumnVector(10, input1, input2);
    input.init();
    LongColumnVector output1 = new LongColumnVector(10);
    LongColumnVector output2 = new LongColumnVector(10);
    StructColumnVector output = new StructColumnVector(10, output1, output2);
    output.init();
    input1.isRepeating = true;
    input2.noNulls = false;
    input2.isNull[5] = true;
    input.noNulls = false;
    input.isNull[6] = true;
    for(int i=0; i < 10; ++i) {
      input1.vector[i] = i + 1;
      input2.vector[i] = i + 2;
    }
    output.isNull[3] = false;
    output.setElement(3, 6, input);
    StringBuilder buf = new StringBuilder();
    output.stringifyValue(buf, 3);
    assertEquals(""null"", buf.toString());
    output.isNull[3] = false;
    output.setElement(3, 5, input);
    buf = new StringBuilder();
    output.stringifyValue(buf, 3);
    assertEquals(""[1, null]"", buf.toString());
    output.isNull[3] = false;
    output.setElement(3, 4, input);
    buf = new StringBuilder();
    output.stringifyValue(buf, 3);
    assertEquals(""[1, 6]"", buf.toString());
    input.reset();
    assertEquals(false, input1.isRepeating);
    assertEquals(true, input.noNulls);
  }
",non-flaky,5
159489,realm_realm-java,RealmProcessorTest.compileSimpleFile,"    @Test
    public void compileSimpleFile() {
        ASSERT.about(javaSource())
                .that(simpleModel)
                .compilesWithoutError();
    }
",non-flaky,5
159490,realm_realm-java,RealmProcessorTest.compileProcessedSimpleFile,"    @Test
    public void compileProcessedSimpleFile() throws Exception {
        ASSERT.about(javaSource())
                .that(simpleModel)
                .processedWith(new RealmProcessor())
                .compilesWithoutError();
    }
",non-flaky,5
159491,realm_realm-java,RealmProcessorTest.compileProcessedEmptyFile,"    @Test
    public void compileProcessedEmptyFile() throws Exception {
        ASSERT.about(javaSource())
                .that(emptyModel)
                .processedWith(new RealmProcessor())
                .failsToCompile();
    }
",non-flaky,5
159492,realm_realm-java,RealmProcessorTest.compileSimpleProxyFile,"    // Disabled because it does not seem to find the generated interface file @Test
    public void compileSimpleProxyFile() throws Exception {
        ASSERT.about(javaSource())
                .that(simpleProxy)
                .compilesWithoutError();
    }
",non-flaky,5
159493,realm_realm-java,RealmProcessorTest.compareProcessedSimpleFile,"    @Test
    public void compareProcessedSimpleFile() throws Exception {
        ASSERT.about(javaSource())
                .that(simpleModel)
                .processedWith(new RealmProcessor())
                .compilesWithoutError()
                .and()
                .generatesSources(simpleProxy);
    }
",non-flaky,5
159494,realm_realm-java,RealmProcessorTest.compileProcessedNullTypesFile,"    @Test
    public void compileProcessedNullTypesFile() throws Exception {
        ASSERT.about(javaSource())
                .that(nullTypesModel)
                .processedWith(new RealmProcessor())
                .compilesWithoutError();
    }
",non-flaky,5
159495,realm_realm-java,RealmProcessorTest.compareProcessedNullTypesFile,"    @Test
    public void compareProcessedNullTypesFile() throws Exception {
        ASSERT.about(javaSource())
                .that(nullTypesModel)
                .processedWith(new RealmProcessor())
                .compilesWithoutError()
                .and()
                .generatesSources(nullTypesProxy);
    }
",non-flaky,5
159496,realm_realm-java,RealmProcessorTest.compileAllTypesFile,"    @Test
    public void compileAllTypesFile() {
        ASSERT.about(javaSource())
                .that(allTypesModel)
                .compilesWithoutError();
    }
",non-flaky,5
159497,realm_realm-java,RealmProcessorTest.compileProcessedAllTypesFile,"    @Test
    public void compileProcessedAllTypesFile() throws Exception {
        ASSERT.about(javaSource())
                .that(allTypesModel)
                .processedWith(new RealmProcessor())
                .compilesWithoutError();
    }
",non-flaky,5
159498,realm_realm-java,RealmProcessorTest.compileAllTypesProxyFile,"    @Test
    public void compileAllTypesProxyFile() throws Exception {
        ASSERT.about(javaSource())
                .that(allTypesModel)
                .compilesWithoutError();
    }
",non-flaky,5
159499,realm_realm-java,RealmProcessorTest.compareProcessedAllTypesFile,"    @Test
    public void compareProcessedAllTypesFile() throws Exception {
        ASSERT.about(javaSource())
                .that(allTypesModel)
                .processedWith(new RealmProcessor())
                .compilesWithoutError()
                .and()
                .generatesSources(allTypesDefaultMediator, allTypesDefaultModule,
                        allTypesDefaultMediator, allTypesProxy);
    }
",non-flaky,5
159500,realm_realm-java,RealmProcessorTest.compileAppModuleCustomClasses,"    @Test
    public void compileAppModuleCustomClasses() throws Exception {
        ASSERT.about(javaSources())
                .that(Arrays.asList(allTypesModel, JavaFileObjects.forResource(""some/test/AppModuleCustomClasses.java"")))
                .processedWith(new RealmProcessor())
                .compilesWithoutError();
    }
",non-flaky,5
159501,realm_realm-java,RealmProcessorTest.compileAppModuleAllClasses,"    @Test
    public void compileAppModuleAllClasses() throws Exception {
        ASSERT.about(javaSources())
                .that(Arrays.asList(allTypesModel, JavaFileObjects.forResource(""some/test/AppModuleAllClasses.java"")))
                .processedWith(new RealmProcessor())
                .compilesWithoutError();
    }
",non-flaky,5
159502,realm_realm-java,RealmProcessorTest.compileLibraryModulesAllClasses,"    @Test
    public void compileLibraryModulesAllClasses() throws Exception {
        ASSERT.about(javaSources())
                .that(Arrays.asList(allTypesModel, JavaFileObjects.forResource(""some/test/LibraryModuleAllClasses.java"")))
                .processedWith(new RealmProcessor())
                .compilesWithoutError();
    }
",non-flaky,5
159503,realm_realm-java,RealmProcessorTest.compileLibraryModulesCustomClasses,"    @Test
    public void compileLibraryModulesCustomClasses() throws Exception {
        ASSERT.about(javaSources())
                .that(Arrays.asList(allTypesModel, JavaFileObjects.forResource(""some/test/LibraryModuleCustomClasses.java"")))
                .processedWith(new RealmProcessor())
                .compilesWithoutError();
    }
",non-flaky,5
159504,realm_realm-java,RealmProcessorTest.compileAppModuleMixedParametersFail,"    @Test
    public void compileAppModuleMixedParametersFail() throws Exception {
        ASSERT.about(javaSources())
                .that(Arrays.asList(allTypesModel, JavaFileObjects.forResource(
                    ""some/test/InvalidAllTypesModuleMixedParameters.java"")))
                .processedWith(new RealmProcessor())
                .failsToCompile();
    }
",non-flaky,5
159505,realm_realm-java,RealmProcessorTest.compileAppModuleWrongTypeFail,"    @Test
    public void compileAppModuleWrongTypeFail() throws Exception {
        ASSERT.about(javaSources())
                .that(Arrays.asList(allTypesModel, JavaFileObjects.forResource(
                    ""some/test/InvalidAllTypesModuleWrongType.java"")))
                .processedWith(new RealmProcessor())
                .failsToCompile();
    }
",non-flaky,5
159506,realm_realm-java,RealmProcessorTest.compileLibraryModuleMixedParametersFail,"    @Test
    public void compileLibraryModuleMixedParametersFail() throws Exception {
        ASSERT.about(javaSources())
                .that(Arrays.asList(allTypesModel, JavaFileObjects.forResource(""some/test/InvalidLibraryModuleMixedParameters.java"")))
                .processedWith(new RealmProcessor())
                .failsToCompile();
    }
",non-flaky,5
159507,realm_realm-java,RealmProcessorTest.compileLibraryModuleWrongTypeFail,"    @Test
    public void compileLibraryModuleWrongTypeFail() throws Exception {
        ASSERT.about(javaSources())
                .that(Arrays.asList(allTypesModel, JavaFileObjects.forResource(""some/test/InvalidLibraryModuleWrongType.java"")))
                .processedWith(new RealmProcessor())
                .failsToCompile();
    }
",non-flaky,5
159508,realm_realm-java,RealmProcessorTest.compileBooleanFile,"    @Test
    public void compileBooleanFile() {
        ASSERT.about(javaSource())
                .that(booleansModel)
                .compilesWithoutError();
    }
",non-flaky,5
159509,realm_realm-java,RealmProcessorTest.compileProcessedBooleansFile,"    @Test
    public void compileProcessedBooleansFile() throws Exception {
        ASSERT.about(javaSource())
                .that(booleansModel)
                .processedWith(new RealmProcessor())
                .compilesWithoutError();
    }
",non-flaky,5
159510,realm_realm-java,RealmProcessorTest.compileBooleansProxyFile,"    @Test
    public void compileBooleansProxyFile() throws Exception {
        ASSERT.about(javaSource())
                .that(booleansModel)
                .compilesWithoutError();
    }
",non-flaky,5
159511,realm_realm-java,RealmProcessorTest.compareProcessedBooleansFile,"    @Test
    public void compareProcessedBooleansFile() throws Exception {
        ASSERT.about(javaSource())
                .that(booleansModel)
                .processedWith(new RealmProcessor())
                .compilesWithoutError()
                .and()
                .generatesSources(booleansProxy);
    }
",non-flaky,5
159512,realm_realm-java,RealmProcessorTest.compileMissingGenericType,"    @Test
    public void compileMissingGenericType() {
        ASSERT.about(javaSource())
                .that(missingGenericTypeModel)
                .processedWith(new RealmProcessor())
                .failsToCompile();
    }
",non-flaky,5
159513,realm_realm-java,RealmProcessorTest.compileFieldNamesFiles,"    // Disabled because it does not seem to find the generated Interface file @Test
    public void compileFieldNamesFiles() {
        ASSERT.about(javaSource())
                .that(fieldNamesModel)
                .processedWith(new RealmProcessor())
                .compilesWithoutError();
    }
",non-flaky,5
159514,realm_realm-java,RealmProcessorTest.compileCustomAccessor,"    @Test
    public void compileCustomAccessor() {
        ASSERT.about(javaSource())
                .that(customAccessorModel)
                .processedWith(new RealmProcessor())
                .failsToCompile();
    }
",non-flaky,5
159515,realm_realm-java,RealmProcessorTest.compileIndexTypes,"    @Test
    public void compileIndexTypes() throws IOException {
        final String[] validIndexFieldTypes = {""byte"", ""short"", ""int"", ""long"", ""boolean"", ""String"", ""java.util.Date"",
                ""Byte"", ""Short"", ""Integer"", ""Long"", ""Boolean""};

        for (String fieldType : validIndexFieldTypes) {
            TestRealmObjectFileObject javaFileObject =
                    TestRealmObjectFileObject.getSingleFieldInstance(""ValidIndexType"", ""Index"", fieldType, ""testField"");
            ASSERT.about(javaSource())
                    .that(javaFileObject)
                    .processedWith(new RealmProcessor())
                    .compilesWithoutError();
        }
    }
",non-flaky,5
159516,realm_realm-java,RealmProcessorTest.compileInvalidIndexTypes,"    @Test
    public void compileInvalidIndexTypes() throws IOException {
        final String[] invalidIndexFieldTypes = {""float"", ""double"", ""byte[]"", ""Simple"", ""RealmList"", ""Float"", ""Double""};

        for (String fieldType : invalidIndexFieldTypes) {
            TestRealmObjectFileObject javaFileObject = TestRealmObjectFileObject.getSingleFieldInstance(
                    ""InvalidIndexType"", ""Index"", fieldType, ""testField"");
            ASSERT.about(javaSource())
                    .that(javaFileObject)
                    .processedWith(new RealmProcessor())
                    .failsToCompile();
        }
    }
",non-flaky,5
159517,realm_realm-java,RealmProcessorTest.compilePrimaryKeyTypes,"    @Test
    public void compilePrimaryKeyTypes() throws IOException {
        final String[] validPrimaryKeyFieldTypes = {""byte"", ""short"", ""int"", ""long"", ""String"", ""Byte"", ""Short"", ""Integer"", ""Long""};

        for (String fieldType : validPrimaryKeyFieldTypes) {
            TestRealmObjectFileObject javaFileObject = TestRealmObjectFileObject.getSingleFieldInstance(
                    ""ValidPrimaryKeyType"", ""PrimaryKey"", fieldType, ""testField"");
            ASSERT.about(javaSource())
                    .that(javaFileObject)
                    .processedWith(new RealmProcessor())
                    .compilesWithoutError();
        }
    }
",non-flaky,5
159518,realm_realm-java,RealmProcessorTest.compileInvalidPrimaryKeyTypes,"    @Test
    public void compileInvalidPrimaryKeyTypes() throws IOException {
        final String[] invalidPrimaryKeyFieldTypes = {""boolean"", ""java.util.Date"", ""Simple"", ""RealmList<Simple>"", ""Boolean""};

        for (String fieldType : invalidPrimaryKeyFieldTypes) {
            TestRealmObjectFileObject javaFileObject =
                    TestRealmObjectFileObject.getSingleFieldInstance(
                            ""InvalidPrimaryKeyType"", ""PrimaryKey"", fieldType, ""testField"");
            ASSERT.about(javaSource())
                    .that(javaFileObject)
                    .processedWith(new RealmProcessor())
                    .failsToCompile();
        }
    }
",non-flaky,5
159519,realm_realm-java,RealmProcessorTest.compileRequiredTypes,"    @Test
    public void compileRequiredTypes() throws IOException {
        final String[] validPrimaryKeyFieldTypes = {""Byte"", ""Short"", ""Integer"", ""Long"", ""String"",
                ""Float"", ""Double"", ""Boolean"", ""java.util.Date""};

        for (String fieldType : validPrimaryKeyFieldTypes) {
            TestRealmObjectFileObject javaFileObject = TestRealmObjectFileObject.getSingleFieldInstance(
                    ""ValidPrimaryKeyType"", ""Required"", fieldType, ""testField"");
            ASSERT.about(javaSource())
                    .that(javaFileObject)
                    .processedWith(new RealmProcessor())
                    .compilesWithoutError();
        }
    }
",non-flaky,5
159520,realm_realm-java,RealmProcessorTest.compileInvalidRequiredTypes,"    @Test
    public void compileInvalidRequiredTypes() throws IOException {
        final String[] validPrimaryKeyFieldTypes = {""byte"", ""short"", ""int"", ""long"", ""float"", ""double"",
                ""boolean"", ""RealmList<Simple>"", ""Simple""};

        for (String fieldType : validPrimaryKeyFieldTypes) {
            TestRealmObjectFileObject javaFileObject = TestRealmObjectFileObject.getSingleFieldInstance(
                    ""ValidPrimaryKeyType"", ""Required"", fieldType, ""testField"");
            ASSERT.about(javaSource())
                    .that(javaFileObject)
                    .processedWith(new RealmProcessor())
                    .failsToCompile();
        }
    }
",non-flaky,5
159521,realm_realm-java,RealmProcessorTest.compileConflictingFieldName,"    @Test
    public void compileConflictingFieldName() throws Exception {
        ASSERT.about(javaSource())
                .that(conflictingFieldNameModel)
                .processedWith(new RealmProcessor())
                .compilesWithoutError();
    }
",non-flaky,5
159522,realm_realm-java,RealmProcessorTest.failOnFinalFields,"    @Test
    public void failOnFinalFields() throws Exception {
        ASSERT.about(javaSource())
                .that(finalModel)
                .processedWith(new RealmProcessor())
                .failsToCompile();
    }
",non-flaky,5
159523,realm_realm-java,RealmProcessorTest.failOnTransientFields,"    @Test
    public void failOnTransientFields() throws Exception {
        ASSERT.about(javaSource())
                .that(transientModel)
                .processedWith(new RealmProcessor())
                .failsToCompile();
    }
",non-flaky,5
159524,realm_realm-java,RealmProcessorTest.failOnVolatileFields,"    @Test
    public void failOnVolatileFields() throws Exception {
        ASSERT.about(javaSource())
                .that(volatileModel)
                .processedWith(new RealmProcessor())
                .failsToCompile();
    }
",non-flaky,5
159525,realm_realm-java,RealmProcessorTest.failOnInvalidRealmModel_1,"    @Test
    public void failOnInvalidRealmModel_1() throws Exception {
        ASSERT.about(javaSource())
                .that(invalidRealmModelModel_1)
                .processedWith(new RealmProcessor())
                .failsToCompile();
    }
",non-flaky,5
159526,realm_realm-java,RealmProcessorTest.failOnInvalidRealmModel_2,"    @Test
    public void failOnInvalidRealmModel_2() throws Exception {
        ASSERT.about(javaSource())
                .that(invalidRealmModelModel_2)
                .processedWith(new RealmProcessor())
                .failsToCompile();
    }
",non-flaky,5
159527,realm_realm-java,RealmProcessorTest.failOnInvalidRealmModel_3,"    @Test
    public void failOnInvalidRealmModel_3() throws Exception {
        ASSERT.about(javaSource())
                .that(invalidRealmModelModel_3)
                .processedWith(new RealmProcessor())
                .failsToCompile();
    }
",non-flaky,5
159528,realm_realm-java,RealmProcessorTest.validRealmModelUsingInheritance,"    @Test
    public void validRealmModelUsingInheritance() throws Exception {
        ASSERT.about(javaSource())
                .that(ValidModelPojo_ExtendingRealmObject)
                .processedWith(new RealmProcessor())
                .compilesWithoutError();
    }
",non-flaky,5
159529,realm_realm-java,RealmProcessorTest.canNotInheritRealmList,"    @Test
    public void canNotInheritRealmList() throws Exception {
        ASSERT.about(javaSource())
                .that(UseExtendRealmList)
                .processedWith(new RealmProcessor())
                .failsToCompile();
    }
",non-flaky,5
159530,realm_realm-java,RealmProcessorTest.compileWithRealmModelFieldInReamlModel,"    @Test
    public void compileWithRealmModelFieldInReamlModel() {
        ASSERT.about(javaSource())
                .that(SimpleRealmModel)
                .processedWith(new RealmProcessor())
                .compilesWithoutError();
    }
",non-flaky,5
159531,realm_realm-java,RealmProcessorTest.compileWithInterfaceForList,"    @Test
    public void compileWithInterfaceForList() {
        ASSERT.about(javaSources())
                .that(Arrays.asList(JavaFileObjects.forResource(""some/test/InterfaceList.java""), customInterface))
                .processedWith(new RealmProcessor())
                .failsToCompile();
    }
",non-flaky,5
159532,realm_realm-java,RealmProcessorTest.compileWithInterfaceForObject,"    @Test
    public void compileWithInterfaceForObject() {
        ASSERT.about(javaSources())
                .that(Arrays.asList(JavaFileObjects.forResource(""some/test/InterfaceObjectReference.java""), customInterface))
                .processedWith(new RealmProcessor())
                .failsToCompile();
    }
",non-flaky,5
159533,realm_realm-java,RealmProcessorTest.compileBacklinks,"    @Test
    public void compileBacklinks() {
        ASSERT.about(javaSources())
            .that(Arrays.asList(backlinks, backlinksTarget))
            .processedWith(new RealmProcessor())
            .compilesWithoutError();
    }
",non-flaky,5
159534,realm_realm-java,RealmProcessorTest.failOnLinkingObjectsWithInvalidFieldType,"    @Test
    public void failOnLinkingObjectsWithInvalidFieldType() {
        ASSERT.about(javaSources())
            .that(Arrays.asList(backlinks, backlinksTarget, backlinksInvalidField))
            .processedWith(new RealmProcessor())
            .failsToCompile()
            .withErrorContaining(""Fields annotated with @LinkingObjects must be RealmResults"");
    }
",non-flaky,5
159535,realm_realm-java,RealmProcessorTest.failOnLinkingObjectsWithNonFinalField,"    @Test
    public void failOnLinkingObjectsWithNonFinalField() {
        ASSERT.about(javaSources())
            .that(Arrays.asList(backlinks, backlinksTarget, backlinksNonFinalField))
            .processedWith(new RealmProcessor())
            .failsToCompile()
            .withErrorContaining(""must be final"");
    }
",non-flaky,5
159536,realm_realm-java,RealmProcessorTest.failsOnLinkingObjectsWithLinkedFields,"    @Test
    public void failsOnLinkingObjectsWithLinkedFields() {
        ASSERT.about(javaSources())
            .that(Arrays.asList(backlinks, backlinksTarget, backlinksLinked))
            .processedWith(new RealmProcessor())
            .failsToCompile()
            .withErrorContaining(""The use of '.' to specify fields in referenced classes is not supported"");
    }
",non-flaky,5
159537,realm_realm-java,RealmProcessorTest.failsOnLinkingObjectsMissingFieldName,"    @Test
    public void failsOnLinkingObjectsMissingFieldName() {
        ASSERT.about(javaSources())
            .that(Arrays.asList(backlinks, backlinksTarget, backlinksMissingParam))
            .processedWith(new RealmProcessor())
            .failsToCompile()
            .withErrorContaining(""must have a parameter identifying the link target"");
    }
",non-flaky,5
159538,realm_realm-java,RealmProcessorTest.failsOnLinkingObjectsMissingGeneric,"    @Test
    public void failsOnLinkingObjectsMissingGeneric() {
        ASSERT.about(javaSources())
            .that(Arrays.asList(backlinks, backlinksTarget, backlinksMissingGeneric))
            .processedWith(new RealmProcessor())
            .failsToCompile()
            .withErrorContaining(""must specify a generic type"");
    }
",non-flaky,5
159539,realm_realm-java,RealmProcessorTest.failsOnLinkingObjectsWithRequiredFields,"    @Test
    public void failsOnLinkingObjectsWithRequiredFields() {
        ASSERT.about(javaSources())
            .that(Arrays.asList(backlinks, backlinksTarget, backlinksRequired))
            .processedWith(new RealmProcessor())
            .failsToCompile()
            .withErrorContaining(""cannot be @Required"");
    }
",non-flaky,5
159540,realm_realm-java,RealmProcessorTest.failsOnLinkingObjectsWithIgnoreFields,"    @Test
    public void failsOnLinkingObjectsWithIgnoreFields() {
        ASSERT.about(javaSources())
            .that(Arrays.asList(backlinks, backlinksTarget, backlinksIgnored))
            .processedWith(new RealmProcessor())
            .compilesWithoutError();
    }
",non-flaky,5
159541,realm_realm-java,RealmProcessorTest.failsOnLinkingObjectsFieldNotFound,"    @Test
    public void failsOnLinkingObjectsFieldNotFound() {
        ASSERT.about(javaSources())
            .that(Arrays.asList(backlinks, backlinksTarget, backlinksNotFound))
            .processedWith(new RealmProcessor())
            .failsToCompile()
            .withErrorContaining(""does not exist in class"");
    }
",non-flaky,5
159542,realm_realm-java,RealmProcessorTest.failsOnLinkingObjectsWithFieldWrongType,"    @Test
    public void failsOnLinkingObjectsWithFieldWrongType() {
        ASSERT.about(javaSources())
            .that(Arrays.asList(backlinks, backlinksTarget, backlinksWrongType))
            .processedWith(new RealmProcessor())
            .failsToCompile()
            .withErrorContaining(""instead of"");
    }
",non-flaky,5
159543,realm_realm-java,SortTest.sortMultiFailures,"    @Test
    public void sortMultiFailures() {
        RealmResults<AllTypes> allTypes = realm.where(AllTypes.class).findAll();

        // Zero fields specified.
        try {
            allTypes.sort(new String[]{}, new Sort[]{});
            fail();
        } catch (IllegalArgumentException ignored) {
        }

        // Number of fields and sorting orders don't match.
        try {
            allTypes.sort(new String[]{FIELD_STRING}, ORDER_ASC_ASC);
            fail();
        } catch (IllegalArgumentException ignored) {
        }

        // Null is not allowed.
        try {
            allTypes.sort(null, (Sort[]) null);
            fail();
        } catch (IllegalArgumentException ignored) {
        }
        try {
            allTypes.sort(new String[]{FIELD_STRING}, null);
            fail();
        } catch (IllegalArgumentException ignored) {
        }

        // Non-existing field name.
        try {
            allTypes.sort(new String[]{FIELD_STRING, ""dont-exist""}, ORDER_ASC_ASC);
            fail();
        } catch (IllegalArgumentException ignored) {
        }
    }
",non-flaky,5
159544,realm_realm-java,SortTest.sortRealmResultsTwoFields,"    @Test
    public void sortRealmResultsTwoFields() {
        RealmResults<AllTypes> results1 = realm.where(AllTypes.class).findAll().sort(ORDER_STRING_INT, ORDER_ASC_ASC);
        checkSortTwoFieldsStringAscendingIntAscending(results1);

        RealmResults<AllTypes> results2 = realm.where(AllTypes.class).findAll().sort(ORDER_INT_STRING, ORDER_ASC_ASC);
        checkSortTwoFieldsIntString(results2);

        RealmResults<AllTypes> results3 = realm.where(AllTypes.class).findAll().sort(ORDER_STRING_INT, ORDER_ASC_DES);
        checkSortTwoFieldsStringAscendingIntDescending(results3);

        RealmResults<AllTypes> results4 = realm.where(AllTypes.class).findAll().sort(ORDER_INT_STRING, ORDER_ASC_DES);
        checkSortTwoFieldsIntAscendingStringDescending(results4);
    }
",non-flaky,5
159545,realm_realm-java,SortTest.realmQuerySortTwoFields,"    @Test
    public void realmQuerySortTwoFields() {
        RealmResults<AllTypes> results1 = realm.where(AllTypes.class)
                .findAll().sort(ORDER_STRING_INT, ORDER_ASC_ASC);
        checkSortTwoFieldsStringAscendingIntAscending(results1);

        RealmResults<AllTypes> results2 = realm.where(AllTypes.class)
                .findAll().sort(ORDER_INT_STRING, ORDER_ASC_ASC);
        checkSortTwoFieldsIntString(results2);

        RealmResults<AllTypes> results3 = realm.where(AllTypes.class)
                .findAll().sort(ORDER_STRING_INT, ORDER_ASC_DES);
        checkSortTwoFieldsStringAscendingIntDescending(results3);

        RealmResults<AllTypes> results4 = realm.where(AllTypes.class)
                .findAll().sort(ORDER_INT_STRING, ORDER_ASC_DES);
        checkSortTwoFieldsIntAscendingStringDescending(results4);
    }
",non-flaky,5
159546,realm_realm-java,SortTest.realmSortTwoFields,"    @Test
    public void realmSortTwoFields() {
        RealmResults<AllTypes> results1 = realm.where(AllTypes.class).findAll().
                sort(ORDER_STRING_INT, ORDER_ASC_ASC);
        checkSortTwoFieldsStringAscendingIntAscending(results1);

        RealmResults<AllTypes> results2 = realm.where(AllTypes.class).findAll().
                sort(ORDER_INT_STRING, ORDER_ASC_ASC);
        checkSortTwoFieldsIntString(results2);

        RealmResults<AllTypes> results3 = realm.where(AllTypes.class).findAll().
                sort(ORDER_STRING_INT, ORDER_ASC_DES);
        checkSortTwoFieldsStringAscendingIntDescending(results3);

        RealmResults<AllTypes> results4 = realm.where(AllTypes.class).findAll().
                sort(ORDER_INT_STRING, ORDER_ASC_DES);
        checkSortTwoFieldsIntAscendingStringDescending(results4);
    }
",non-flaky,5
159547,realm_realm-java,SortTest.realmSortMultiFailures,"    @Test
    public void realmSortMultiFailures() {
        RealmResults<AllTypes> allTypes = realm.where(AllTypes.class).findAll();

        // Zero fields specified.
        try {
            realm.where(AllTypes.class).findAll().sort(new String[]{}, new Sort[]{});
            fail();
        } catch (IllegalArgumentException ignored) {
        }

        // Number of fields and sorting orders don't match.
        try {
            realm.where(AllTypes.class).findAll().
                    sort(new String[]{FIELD_STRING}, ORDER_ASC_ASC);
            fail();
        } catch (IllegalArgumentException ignored) {
        }

        // Null is not allowed.
        try {
            realm.where(AllTypes.class).findAll().sort(null, (Sort[]) null);
            fail();
        } catch (IllegalArgumentException ignored) {
        }
        try {
            realm.where(AllTypes.class).findAll().sort(new String[]{FIELD_STRING}, null);
            fail();
        } catch (IllegalArgumentException ignored) {
        }

        // Non-existing field name.
        try {
            realm.where(AllTypes.class).findAll().
                    sort(new String[]{FIELD_STRING, ""dont-exist""}, ORDER_ASC_ASC);
            fail();
        } catch (IllegalArgumentException ignored) {
        }
    }
",non-flaky,5
159548,realm_realm-java,SortTest.run,"    @Test
    public void resorting() throws InterruptedException {
        final AtomicInteger changeListenerCalled = new AtomicInteger(4);

        final Realm realm = looperThread.getRealm();
        realm.setAutoRefresh(true);

        final Runnable endTest = new Runnable() {
            @Override
            public void run() {
                if (changeListenerCalled.decrementAndGet() == 0) {
                    realm.close();
                    looperThread.testComplete();
                }
            }
",non-flaky,5
159549,realm_realm-java,SortTest.sortingDates,"    @Test
    public void sortingDates() {
        final int TEST_SIZE = 10;

        populateDates(realm, TEST_SIZE);

        RealmResults<AllTypes> objectsAscending = realm.where(AllTypes.class).findAllSorted(AllTypes.FIELD_DATE, Sort.ASCENDING);
        assertEquals(TEST_SIZE, objectsAscending.size());
        int i = 0;
        for (AllTypes allTypes : objectsAscending) {
            assertEquals(new Date(i), allTypes.getColumnDate());
            i++;
        }

        RealmResults<AllTypes> objectsDescending = realm.where(AllTypes.class).findAllSorted(AllTypes.FIELD_DATE, Sort.DESCENDING);
        assertEquals(TEST_SIZE, objectsDescending.size());
        i = TEST_SIZE - 1;
        for (AllTypes allTypes : objectsDescending) {
            assertEquals(new Date(i), allTypes.getColumnDate());
            i--;
        }
    }
",non-flaky,5
159550,realm_realm-java,SortTest.run,"    @Test
    public void resortingDates() {
        final int TEST_SIZE = 10;
        final AtomicInteger changeListenerCalled = new AtomicInteger(2);

        final Realm realm = Realm.getInstance(looperThread.createConfiguration());
        realm.setAutoRefresh(true);
        populateDates(realm, TEST_SIZE);

        final Runnable endTest = new Runnable() {
            @Override
            public void run() {
                if (changeListenerCalled.decrementAndGet() == 0) {
                    realm.close();
                    looperThread.testComplete();
                }
            }
",non-flaky,5
159551,realm_realm-java,RealmInMemoryTest.inMemoryRealm,"    @Test
    public void inMemoryRealm() {
        testRealm.beginTransaction();
        Dog dog = testRealm.createObject(Dog.class);
        dog.setName(""DinoDog"");
        testRealm.commitTransaction();

        assertEquals(testRealm.where(Dog.class).count(), 1);
        assertEquals(testRealm.where(Dog.class).findFirst().getName(), ""DinoDog"");

        testRealm.close();
        // After all references to the in-mem-realm closed,
        // in-mem-realm with same identifier should create a fresh new instance.
        testRealm = Realm.getInstance(inMemConf);
        assertEquals(testRealm.where(Dog.class).count(), 0);
    }
",non-flaky,5
159552,realm_realm-java,RealmInMemoryTest.inMemoryRealmWithDifferentNames,"    @Test
    public void inMemoryRealmWithDifferentNames() {
        testRealm.beginTransaction();
        Dog dog = testRealm.createObject(Dog.class);
        dog.setName(""DinoDog"");
        testRealm.commitTransaction();

        // Creates the 2nd in-memory Realm with a different name. To make sure they are not affecting each other.
        RealmConfiguration inMemConf2 = configFactory.createConfigurationBuilder()
                .name(IDENTIFIER + ""2"")
                .inMemory()
                .build();
        Realm testRealm2 = Realm.getInstance(inMemConf2);
        testRealm2.beginTransaction();
        Dog dog2 = testRealm2.createObject(Dog.class);
        dog2.setName(""UFODog"");
        testRealm2.commitTransaction();

        assertEquals(testRealm.where(Dog.class).count(), 1);
        //noinspection ConstantConditions
        assertEquals(testRealm.where(Dog.class).findFirst().getName(), ""DinoDog"");
        assertEquals(testRealm2.where(Dog.class).count(), 1);
        //noinspection ConstantConditions
        assertEquals(testRealm2.where(Dog.class).findFirst().getName(), ""UFODog"");

        testRealm2.close();
    }
",non-flaky,5
159553,realm_realm-java,RealmInMemoryTest.delete,"    @Test
    public void delete() {
        RealmConfiguration configuration = testRealm.getConfiguration();
        try {
            Realm.deleteRealm(configuration);
            fail(""Realm.deleteRealm should fail with illegal state"");
        } catch (IllegalStateException ignored) {
        }

        // Nothing should happen when delete a closed in-mem-realm.
        testRealm.close();
        testRealm = null;
        assertTrue(Realm.deleteRealm(configuration));
    }
",non-flaky,5
159554,realm_realm-java,RealmInMemoryTest.writeCopyTo,"    @Test
    public void writeCopyTo() {
        byte[] key = TestHelper.getRandomKey();
        String fileName = IDENTIFIER + "".realm"";
        String encFileName = IDENTIFIER + "".enc.realm"";
        RealmConfiguration conf = configFactory.createConfigurationBuilder()
                .name(fileName)
                .build();
        RealmConfiguration encConf = configFactory.createConfigurationBuilder()
                .name(encFileName)
                .encryptionKey(key)
                .build();

        Realm.deleteRealm(conf);
        Realm.deleteRealm(encConf);

        testRealm.beginTransaction();
        Dog dog = testRealm.createObject(Dog.class);
        dog.setName(""DinoDog"");
        testRealm.commitTransaction();

        // Tests a normal Realm file.
        testRealm.writeCopyTo(new File(configFactory.getRoot(), fileName));
        Realm onDiskRealm = Realm.getInstance(conf);
        assertEquals(onDiskRealm.where(Dog.class).count(), 1);
        onDiskRealm.close();

        // Tests a encrypted Realm file.
        testRealm.writeEncryptedCopyTo(new File(configFactory.getRoot(), encFileName), key);
        onDiskRealm = Realm.getInstance(encConf);
        assertEquals(onDiskRealm.where(Dog.class).count(), 1);
        onDiskRealm.close();
        // Tests with a wrong key to see if it fails as expected.
        try {
            RealmConfiguration wrongKeyConf = configFactory.createConfigurationBuilder()
                    .name(encFileName)
                    .encryptionKey(TestHelper.getRandomKey(42))
                    .build();
            Realm.getInstance(wrongKeyConf);
            fail(""Realm.getInstance should fail with RealmFileException"");
        } catch (RealmFileException expected) {
            assertEquals(expected.getKind(), RealmFileException.Kind.ACCESS_ERROR);
        }
    }
",non-flaky,5
159555,realm_realm-java,RealmInMemoryTest.run,"    @Test
    public void multiThread() throws InterruptedException, ExecutionException {
        final CountDownLatch workerCommittedLatch = new CountDownLatch(1);
        final CountDownLatch workerClosedLatch = new CountDownLatch(1);
        final CountDownLatch realmInMainClosedLatch = new CountDownLatch(1);
        final AssertionFailedError threadError[] = new AssertionFailedError[1];

        // Step 2.
        Thread workerThread = new Thread(new Runnable() {
            @Override
            public void run() {
                Realm realm = Realm.getInstance(inMemConf);
                realm.beginTransaction();
                Dog dog = realm.createObject(Dog.class);
                dog.setName(""DinoDog"");
                realm.commitTransaction();

                try {
                    assertEquals(realm.where(Dog.class).count(), 1);
                } catch (AssertionFailedError afe) {
                    threadError[0] = afe;
                    realm.close();
                    return;
                }
                workerCommittedLatch.countDown();

                // Waits until Realm instance closed in main thread.
                try {
                    realmInMainClosedLatch.await(TestHelper.SHORT_WAIT_SECS, TimeUnit.SECONDS);
                } catch (InterruptedException e) {
                    threadError[0] = new AssertionFailedError(""Worker thread was interrupted."");
                    realm.close();
                    return;
                }

                realm.close();
                workerClosedLatch.countDown();
            }
",non-flaky,5
159556,realm_realm-java,NotificationsTest.call,"    @Test
    public void setAutoRefresh_failsOnNonLooperThread() throws ExecutionException, InterruptedException {
        ExecutorService executorService = Executors.newSingleThreadExecutor();
        Future<Boolean> future = executorService.submit(new Callable<Boolean>() {
            @Override
            public Boolean call() throws Exception {
                Realm realm = Realm.getInstance(realmConfig);
                boolean autoRefresh = realm.isAutoRefresh();
                assertFalse(autoRefresh);
                try {
                    realm.setAutoRefresh(true);
                    return false;
                } catch (IllegalStateException ignored) {
                    return true;
                } finally {
                    realm.close();
                }
            }
",non-flaky,5
159557,realm_realm-java,NotificationsTest.call,"    @Test
    public void setAutoRefresh_onHandlerThread() throws ExecutionException, InterruptedException {
        ExecutorService executorService = Executors.newSingleThreadExecutor();
        Future<Boolean> future = executorService.submit(new Callable<Boolean>() {
            @Override
            public Boolean call() throws Exception {
                Looper.prepare();
                Realm realm = Realm.getInstance(realmConfig);
                assertTrue(realm.isAutoRefresh());
                realm.setAutoRefresh(false);
                assertFalse(realm.isAutoRefresh());
                realm.setAutoRefresh(true);
                assertTrue(realm.isAutoRefresh());
                realm.close();
                return true;
            }
",non-flaky,5
159558,realm_realm-java,NotificationsTest.onChange,"    @Test
    public void removeChangeListener() throws InterruptedException, ExecutionException {
        final AtomicInteger counter = new AtomicInteger(0);
        RealmChangeListener<Realm> listener = new RealmChangeListener<Realm>() {
            @Override
            public void onChange(Realm object) {
                counter.incrementAndGet();
            }
",non-flaky,5
159559,realm_realm-java,NotificationsTest.onChange,"    @Test
    public void addChangeListener_duplicatedListener() {
        final AtomicInteger counter = new AtomicInteger(0);
        RealmChangeListener<Realm> listener = new RealmChangeListener<Realm>() {
            @Override
            public void onChange(Realm object) {
                counter.incrementAndGet();
            }
",non-flaky,5
159560,realm_realm-java,NotificationsTest.onChange,"    @Test
    public void notificationsNumber() throws InterruptedException, ExecutionException {
        final CountDownLatch isReady = new CountDownLatch(1);
        final CountDownLatch isRealmOpen = new CountDownLatch(1);
        final AtomicInteger counter = new AtomicInteger(0);
        final Looper[] looper = new Looper[1];
        final RealmChangeListener<Realm> listener = new RealmChangeListener<Realm>() {
            @Override
            public void onChange(Realm object) {
                counter.incrementAndGet();
            }
",non-flaky,5
159561,realm_realm-java,NotificationsTest.run,"    @Test
    public void closeClearingHandlerMessages() throws InterruptedException, TimeoutException, ExecutionException {
        final int TEST_SIZE = 10;
        final CountDownLatch backgroundLooperStarted = new CountDownLatch(1);
        final CountDownLatch addHandlerMessages = new CountDownLatch(1);

        ExecutorService executorService = Executors.newSingleThreadExecutor();
        Future<Boolean> future = executorService.submit(new Callable<Boolean>() {
            @Override
            public Boolean call() throws Exception {
                Looper.prepare(); // Fake background thread with a looper, eg. a IntentService.
                Realm realm = Realm.getInstance(realmConfig);
                backgroundLooperStarted.countDown();

                // Random operation in the client code.
                final RealmResults<Dog> dogs = realm.where(Dog.class).findAll();
                if (dogs.size() != 0) {
                    return false;
                }
                // Wait for main thread to add update messages.
                addHandlerMessages.await(TestHelper.VERY_SHORT_WAIT_SECS, TimeUnit.SECONDS);

                // Creates a Handler for the thread now. All message and references for the notification handler will be
                // cleared once we call close().
                Handler threadHandler = new Handler(Looper.myLooper());
                realm.close(); // Close native resources + associated handlers.

                // Looper now reads the update message from the main thread if the Handler was not
                // cleared. This will cause an IllegalStateException and should not happen.
                // If it works correctly. The looper will just block on an empty message queue.
                // This is normal behavior but is bad for testing, so we add a custom quit message
                // at the end so we can evaluate results faster.
                // 500 ms delay is to make sure the notification daemon thread gets time to send notification.
                threadHandler.postDelayed(new Runnable() {
                    @Override
                    public void run() {
                        TestHelper.quitLooperOrFail();
                    }
",non-flaky,5
159562,realm_realm-java,NotificationsTest.onChange,"    @Test
    public void globalListener_looperThread_triggeredByLocalCommit() {
        final AtomicInteger success = new AtomicInteger(0);
        Realm realm = looperThread.getRealm();
        realm.addChangeListener(new RealmChangeListener<Realm>() {
            @Override
            public void onChange(Realm object) {
                assertEquals(0, success.getAndIncrement());
                looperThread.testComplete();
            }
",non-flaky,5
159563,realm_realm-java,NotificationsTest.onChange,"    @Test
    public void globalListener_looperThread_triggeredByRemoteCommit() {
        final AtomicInteger success = new AtomicInteger(0);
        Realm realm = looperThread.getRealm();
        realm.addChangeListener(new RealmChangeListener<Realm>() {
            @Override
            public void onChange(Realm object) {
                assertEquals(1, success.get());
                looperThread.testComplete();
            }
",non-flaky,5
159564,realm_realm-java,NotificationsTest.onChange,"    @Test
    public void emptyCommitTriggerChangeListener() {
        final RealmChangeListener<Realm> listener = new RealmChangeListener<Realm>() {
            @Override
            public void onChange(Realm object) {
                looperThread.testComplete();
            }
",non-flaky,5
159565,realm_realm-java,NotificationsTest.onChange,"    @Test
    public void addRemoveListenerConcurrency() {
        final Realm realm = looperThread.getRealm();
        final AtomicInteger counter1 = new AtomicInteger(0);
        final AtomicInteger counter2 = new AtomicInteger(0);
        final AtomicInteger counter3 = new AtomicInteger(0);

        // At least we need 2 listeners existing in the list to make sure
        // the iterator.next get called.

        // This one will be added when listener2's onChange called.
        final RealmChangeListener<Realm> listener1 = new RealmChangeListener<Realm>() {
            @Override
            public void onChange(Realm object) {
                // Step 7: Last listener called. Should only be called once.
                counter1.incrementAndGet();

                // after listener2.onChange
                // Since duplicated entries will be ignored, we still have:
                // [listener2, listener1].
                assertEquals(1, counter1.get());
                assertEquals(2, counter2.get());
                assertEquals(1, counter3.get());
                looperThread.testComplete();
            }
",non-flaky,5
159566,realm_realm-java,NotificationsTest.onChange,"    @Test
    public void realmNotificationOrder() {
        // Tests that global notifications are called in the order they are added
        // Test both ways to check accidental ordering from unordered collections.
        final AtomicInteger listenerACalled = new AtomicInteger(0);
        final AtomicInteger listenerBCalled = new AtomicInteger(0);
        final Realm realm = looperThread.getRealm();

        final RealmChangeListener<Realm> listenerA = new RealmChangeListener<Realm>() {

            @Override
            public void onChange(Realm object) {
                int called = listenerACalled.incrementAndGet();
                if (called == 2) {
                    assertEquals(2, listenerBCalled.get());
                    looperThread.testComplete();
                }
            }
",non-flaky,5
159567,realm_realm-java,NotificationsTest.onChange,"    @Test
    public void doNotUseClosedHandler() throws InterruptedException {
        final CountDownLatch handlerNotified = new CountDownLatch(1);
        final CountDownLatch backgroundThread1Started = new CountDownLatch(1);
        final CountDownLatch backgroundThread2Closed = new CountDownLatch(1);

        // Creates Handler on Thread1 by opening a Realm instance.
        new Thread(""thread1"") {

            @Override
            public void run() {
                Looper.prepare();
                final Realm realm = Realm.getInstance(realmConfig);
                RealmChangeListener<Realm> listener = new RealmChangeListener<Realm>() {
                    @Override
                    public void onChange(Realm object) {
                        realm.close();
                        handlerNotified.countDown();
                    }
",non-flaky,5
159568,realm_realm-java,NotificationsTest.run,"    @Test
    public void looperThreadQuitsLooperEarly() throws InterruptedException {
        final CountDownLatch backgroundLooperStartedAndStopped = new CountDownLatch(1);
        final CountDownLatch mainThreadCommitCompleted = new CountDownLatch(1);
        final CountDownLatch backgroundThreadStopped = new CountDownLatch(1);

        // Starts background looper and let it hang.
        ExecutorService executorService = Executors.newSingleThreadExecutor();
        //noinspection unused
        final Future<?> future = executorService.submit(new Runnable() {
            @Override
            public void run() {
                Looper.prepare(); // Fake background thread with a looper, eg. a IntentService.

                Realm realm = Realm.getInstance(realmConfig);
                realm.setAutoRefresh(false);
                TestHelper.quitLooperOrFail();
                backgroundLooperStartedAndStopped.countDown();
                // This will prevent backgroundThreadStopped from being called.
                TestHelper.awaitOrFail(mainThreadCommitCompleted);
                realm.close();
                backgroundThreadStopped.countDown();
            }
",non-flaky,5
159569,realm_realm-java,NotificationsTest.onChange,"    @Test
    public void handlerThreadShouldReceiveNotification() throws ExecutionException, InterruptedException {
        final AssertionFailedError[] assertionFailedErrors = new AssertionFailedError[1];
        final CountDownLatch backgroundThreadReady = new CountDownLatch(1);
        final CountDownLatch numberOfInvocation = new CountDownLatch(1);

        HandlerThread handlerThread = new HandlerThread(""handlerThread"");
        handlerThread.start();
        Handler handler = new Handler(handlerThread.getLooper());
        handler.post(new Runnable() {
            @Override
            public void run() {
                try {
                    assertEquals(""handlerThread"", Thread.currentThread().getName());
                } catch (AssertionFailedError e) {
                    assertionFailedErrors[0] = e;
                }
                final Realm backgroundRealm = Realm.getInstance(realmConfig);
                backgroundRealm.addChangeListener(new RealmChangeListener<Realm>() {
                    @Override
                    public void onChange(Realm object) {
                        backgroundRealm.close();
                        numberOfInvocation.countDown();
                    }
",non-flaky,5
159570,realm_realm-java,NotificationsTest.run,"    @Test
    public void nonLooperThreadShouldNotifyLooperThreadAboutCommit() {
        final CountDownLatch mainThreadReady = new CountDownLatch(1);
        final CountDownLatch backgroundThreadClosed = new CountDownLatch(1);
        final CountDownLatch numberOfInvocation = new CountDownLatch(1);
        Thread thread = new Thread() {
            @Override
            public void run() {
                TestHelper.awaitOrFail(mainThreadReady);
                Realm realm = Realm.getInstance(realmConfig);
                realm.beginTransaction();
                realm.createObject(AllTypes.class);
                realm.commitTransaction();
                realm.close();
                backgroundThreadClosed.countDown();
            }
",non-flaky,5
159571,realm_realm-java,NotificationsTest.onChange,"    @Test
    public void asyncRealmResultsShouldNotBlockBackgroundCommitNotification() {
        final Realm realm = looperThread.getRealm();
        final RealmResults<Dog> dogs = realm.where(Dog.class).findAllAsync();
        final AtomicBoolean resultsListenerDone = new AtomicBoolean(false);
        final AtomicBoolean realmListenerDone = new AtomicBoolean(false);

        looperThread.keepStrongReference(dogs);
        assertTrue(dogs.load());
        assertEquals(0, dogs.size());
        dogs.addChangeListener(new RealmChangeListener<RealmResults<Dog>>() {
            @Override
            public void onChange(RealmResults<Dog> results) {
                if (dogs.size() == 2) {
                    // Results has the latest changes.
                    resultsListenerDone.set(true);
                    if (realmListenerDone.get()) {
                        looperThread.testComplete();
                    }
                }
            }
",non-flaky,5
159572,realm_realm-java,NotificationsTest.onChange,"    @Test
    public void asyncRealmObjectShouldNotBlockBackgroundCommitNotification() {
        final AtomicInteger numberOfRealmCallbackInvocation = new AtomicInteger(0);
        final CountDownLatch signalClosedRealm = new CountDownLatch(1);
        final Realm realm = looperThread.getRealm();
        realm.addChangeListener(new RealmChangeListener<Realm>() {
            @Override
            public void onChange(final Realm realm) {
                switch (numberOfRealmCallbackInvocation.incrementAndGet()) {
                    case 1: {
                        // First commit.
                        Dog dog = realm.where(Dog.class).findFirstAsync();
                        assertTrue(dog.load());
                        dog.addChangeListener(new RealmChangeListener<Dog>() {
                            @Override
                            public void onChange(Dog dog) {
                            }
",non-flaky,5
159573,realm_realm-java,NotificationsTest.execute,"    @Test
    public void realmListener_realmResultShouldBeSynced() {
        final Realm realm = looperThread.getRealm();
        final RealmResults<AllTypes> results = realm.where(AllTypes.class).findAll();
        assertEquals(1, results.size());

        realm.executeTransactionAsync(new Realm.Transaction() {
            @Override
            public void execute(Realm realm) {
                AllTypes allTypes = realm.where(AllTypes.class).findFirst();
                assertNotNull(allTypes);
                allTypes.deleteFromRealm();
                assertEquals(0, realm.where(AllTypes.class).count());
            }
",non-flaky,5
159574,realm_realm-java,NotificationsTest.execute,"    @Test
    public void accessingSyncRealmResultInsideAsyncResultListener() {
        final Realm realm = looperThread.getRealm();
        final AtomicInteger asyncResultCallback = new AtomicInteger(0);

        final RealmResults<AllTypes> syncResults = realm.where(AllTypes.class).findAll();

        RealmResults<AllTypes> results = realm.where(AllTypes.class).findAllAsync();
        looperThread.keepStrongReference(results);
        results.addChangeListener(new RealmChangeListener<RealmResults<AllTypes>>() {
            @Override
            public void onChange(RealmResults<AllTypes> results) {
                switch (asyncResultCallback.incrementAndGet()) {
                    case 1:
                        // Called when first async query completes.
                        assertEquals(0, results.size());
                        realm.executeTransactionAsync(new Realm.Transaction() {
                            @Override
                            public void execute(Realm realm) {
                                realm.createObject(AllTypes.class);
                            }
",non-flaky,5
159575,realm_realm-java,NotificationsTest.onChange,"    @Test
    public void accessingSyncRealmResultsInsideAnotherResultListener() {
        final Realm realm = looperThread.getRealm();
        final RealmResults<AllTypes> syncResults1 = realm.where(AllTypes.class).findAll();
        final RealmResults<AllTypes> syncResults2 = realm.where(AllTypes.class).findAll();

        looperThread.keepStrongReference(syncResults1);
        syncResults1.addChangeListener(new RealmChangeListener<RealmResults<AllTypes>>() {
            @Override
            public void onChange(RealmResults<AllTypes> element) {
                assertEquals(1, syncResults1.size());
                assertEquals(1, syncResults2.size()); // If syncResults2 is not in sync yet, this will fail.
                looperThread.testComplete();
            }
",non-flaky,5
159576,realm_realm-java,NotificationsTest.onChange,"    @Test
    public void listenersNotAllowedOnIntentServiceThreads() {
        final Realm realm = looperThread.getRealm();
        realm.beginTransaction();
        AllTypes obj = realm.createObject(AllTypes.class);
        realm.commitTransaction();
        RealmResults<AllTypes> results = realm.where(AllTypes.class).findAll();

        // Global listener
        try {
            realm.addChangeListener(new RealmChangeListener<Realm>() {
                @Override
                public void onChange(Realm element) {

                }
",non-flaky,5
159577,realm_realm-java,NotificationsTest.onChange,"    @Test
    public void listenersNotAllowedOnNonLooperThreads() {
        realm = Realm.getInstance(realmConfig);
        realm.beginTransaction();
        AllTypes obj = realm.createObject(AllTypes.class);
        realm.commitTransaction();
        RealmResults<AllTypes> results = realm.where(AllTypes.class).findAll();

        // Global listener
        try {
            realm.addChangeListener(new RealmChangeListener<Realm>() {
                @Override
                public void onChange(Realm element) {

                }
",non-flaky,5
159578,realm_realm-java,JNINativeTest.nativeExceptions,"    @Test
    public void nativeExceptions() {
        long maxExceptionNumber = TestUtil.getMaxExceptionNumber();
        for (long i = 0; i < maxExceptionNumber; i++) {
            String expect = TestUtil.getExpectedMessage(i);
            try {
                TestUtil.testThrowExceptions(i);
            } catch (Throwable throwable) {
                assertEquals(""Exception kind: "" + i, expect, throwable.toString());
            }
        }
    }
",non-flaky,5
159579,realm_realm-java,JNITableInsertTest.testShouldThrowExceptionWhenColumnNameIsTooLong,"    @Test
    public void testShouldThrowExceptionWhenColumnNameIsTooLong() {

        Table table = new Table();
        try {
            table.addColumn(RealmFieldType.STRING, ""THIS STRING HAS 64 CHARACTERS, ""
                    + ""LONGER THAN THE MAX 63 CHARACTERS"");
            fail(""Too long name"");
        } catch (IllegalArgumentException e) {
        }
    }
",non-flaky,5
159580,realm_realm-java,JNITableInsertTest.testWhenColumnNameIsExactly63CharLong,"    @Test
    public void testWhenColumnNameIsExactly63CharLong() {

        Table table = new Table();
        table.addColumn(RealmFieldType.STRING, ""THIS STRING HAS 63 CHARACTERS PERFECT FOR THE MAX 63 CHARACTERS"");
    }
",non-flaky,5
159581,realm_realm-java,JNITableInsertTest.testGenericAddOnTable,"    @Test
    public void testGenericAddOnTable() {
        for (int i = 0; i < value.size(); i++) {
            for (int j = 0; j < value.size(); j++) {

                Table t = new Table();

                // If the objects matches no exception will be thrown.
                if (value.get(i).getClass().equals(value.get(j).getClass())) {
                    assertTrue(true);

                } else {
                    // Adds column.
                    t.addColumn(TestHelper.getColumnType(value.get(j)), value.get(j).getClass().getSimpleName());
                    // Adds value.
                    try {
                        t.add(value.get(i));
                        fail(""No matching type"");
                    } catch (IllegalArgumentException e) {
                    }
                }
            }
        }
    }
",non-flaky,5
159582,realm_realm-java,JNITableTest.tableToString,"    @Test
    public void tableToString() {
        Table t = new Table();

        t.addColumn(RealmFieldType.STRING, ""stringCol"");
        t.addColumn(RealmFieldType.INTEGER, ""intCol"");
        t.addColumn(RealmFieldType.BOOLEAN, ""boolCol"");

        t.add(""s1"", 1, true);
        t.add(""s2"", 2, false);

        String expected = ""The Table contains 3 columns: stringCol, intCol, boolCol. And 2 rows."";
        assertEquals(expected, t.toString());
    }
",non-flaky,5
159583,realm_realm-java,JNITableTest.rowOperationsOnZeroRow,"    @Test
    public void rowOperationsOnZeroRow(){

        Table t = new Table();
        // Removes rows without columns.
        try { t.moveLastOver(0);  fail(""No rows in table""); } catch (ArrayIndexOutOfBoundsException ignored) {}
        try { t.moveLastOver(10); fail(""No rows in table""); } catch (ArrayIndexOutOfBoundsException ignored) {}

        // Column added, remove rows again.
        t.addColumn(RealmFieldType.STRING, """");
        try { t.moveLastOver(0);  fail(""No rows in table""); } catch (ArrayIndexOutOfBoundsException ignored) {}
        try { t.moveLastOver(10); fail(""No rows in table""); } catch (ArrayIndexOutOfBoundsException ignored) {}

    }
",non-flaky,5
159584,realm_realm-java,JNITableTest.zeroColOperations,"    @Test
    public void zeroColOperations() {
        Table tableZeroCols = new Table();

        // Adds rows.
        try { tableZeroCols.add(""val"");         fail(""No columns in table""); } catch (IndexOutOfBoundsException ignored) {}
        try { tableZeroCols.addEmptyRow();      fail(""No columns in table""); } catch (IndexOutOfBoundsException ignored) {}
        try { tableZeroCols.addEmptyRows(10);   fail(""No columns in table""); } catch (IndexOutOfBoundsException ignored) {}


        // Col operations
        try { tableZeroCols.removeColumn(0);                fail(""No columns in table""); } catch (ArrayIndexOutOfBoundsException ignored) {}
        try { tableZeroCols.renameColumn(0, ""newName"");     fail(""No columns in table""); } catch (ArrayIndexOutOfBoundsException ignored) {}
        try { tableZeroCols.removeColumn(10);               fail(""No columns in table""); } catch (ArrayIndexOutOfBoundsException ignored) {}
        try { tableZeroCols.renameColumn(10, ""newName"");    fail(""No columns in table""); } catch (ArrayIndexOutOfBoundsException ignored) {}
    }
",non-flaky,5
159585,realm_realm-java,JNITableTest.findFirstNonExisting,"    @Test
    public void findFirstNonExisting() {
        Table t = TestHelper.getTableWithAllColumnTypes();
        t.add(new byte[]{1, 2, 3}, true, new Date(1384423149761L), 4.5d, 5.7f, 100, ""string"");

        assertEquals(-1, t.findFirstBoolean(1, false));
        // FIXME: reenable when core implements find_first_timestamp(): assertEquals(-1, t.findFirstDate(2, new Date(138442314986l)));
        assertEquals(-1, t.findFirstDouble(3, 1.0d));
        assertEquals(-1, t.findFirstFloat(4, 1.0f));
        assertEquals(-1, t.findFirstLong(5, 50));
    }
",non-flaky,5
159586,realm_realm-java,JNITableTest.findFirst,"    @Test
    public void findFirst() {
        final int TEST_SIZE = 10;
        Table t = TestHelper.getTableWithAllColumnTypes();
        for (int i = 0; i < TEST_SIZE; i++) {
            t.add(new byte[]{1,2,3}, true, new Date(i), (double)i, (float)i, i, ""string "" + i);
        }
        t.add(new byte[]{1, 2, 3}, true, new Date(TEST_SIZE), (double) TEST_SIZE, (float) TEST_SIZE, TEST_SIZE, """");

        assertEquals(0, t.findFirstBoolean(1, true));
        for (int i = 0; i < TEST_SIZE; i++) {
            assertEquals(i, t.findFirstDate(2, new Date(i)));
            assertEquals(i, t.findFirstDouble(3, (double) i));
            assertEquals(i, t.findFirstFloat(4, (float) i));
            assertEquals(i, t.findFirstLong(5, i));
        }

        try {
            t.findFirstString(6, null);
            fail();
        } catch (IllegalArgumentException ignored) {}

        try {
            t.findFirstDate(2, null);
            fail();
        } catch (IllegalArgumentException ignored) {}
    }
",non-flaky,5
159587,realm_realm-java,JNITableTest.getValuesFromNonExistingColumn,"    @Test
    public void getValuesFromNonExistingColumn() {
        Table t = TestHelper.getTableWithAllColumnTypes();
        t.addEmptyRows(10);

        try { t.getBinaryByteArray(-1, 0);          fail(""Column is less than 0""); } catch (ArrayIndexOutOfBoundsException ignored) { }
        try { t.getBinaryByteArray(-10, 0);         fail(""Column is less than 0""); } catch (ArrayIndexOutOfBoundsException ignored) { }
        try { t.getBinaryByteArray(9, 0);           fail(""Column does not exist""); } catch (ArrayIndexOutOfBoundsException ignored) { }

        try { t.getBoolean(-1, 0);                  fail(""Column is less than 0""); } catch (ArrayIndexOutOfBoundsException ignored) { }
        try { t.getBoolean(-10, 0);                 fail(""Column is less than 0""); } catch (ArrayIndexOutOfBoundsException ignored) { }
        try { t.getBoolean(9, 0);                   fail(""Column does not exist""); } catch (ArrayIndexOutOfBoundsException ignored) { }

        try { t.getDate(-1, 0);                     fail(""Column is less than 0""); } catch (ArrayIndexOutOfBoundsException ignored) { }
        try { t.getDate(-10, 0);                    fail(""Column is less than 0""); } catch (ArrayIndexOutOfBoundsException ignored) { }
        try { t.getDate(9, 0);                      fail(""Column does not exist""); } catch (ArrayIndexOutOfBoundsException ignored) { }

        try { t.getDouble(-1, 0);                   fail(""Column is less than 0""); } catch (ArrayIndexOutOfBoundsException ignored) { }
        try { t.getDouble(-10, 0);                  fail(""Column is less than 0""); } catch (ArrayIndexOutOfBoundsException ignored) { }
        try { t.getDouble(9, 0);                    fail(""Column does not exist""); } catch (ArrayIndexOutOfBoundsException ignored) { }

        try { t.getFloat(-1, 0);                    fail(""Column is less than 0""); } catch (ArrayIndexOutOfBoundsException ignored) { }
        try { t.getFloat(-10, 0);                   fail(""Column is less than 0""); } catch (ArrayIndexOutOfBoundsException ignored) { }
        try { t.getFloat(9, 0);                     fail(""Column does not exist""); } catch (ArrayIndexOutOfBoundsException ignored) { }

        try { t.getLong(-1, 0);                     fail(""Column is less than 0""); } catch (ArrayIndexOutOfBoundsException ignored) { }
        try { t.getLong(-10, 0);                    fail(""Column is less than 0""); } catch (ArrayIndexOutOfBoundsException ignored) { }
        try { t.getLong(9, 0);                      fail(""Column does not exist""); } catch (ArrayIndexOutOfBoundsException ignored) { }

        try { t.getString(-1, 0);                   fail(""Column is less than 0""); } catch (ArrayIndexOutOfBoundsException ignored) { }
        try { t.getString(-10, 0);                  fail(""Column is less than 0""); } catch (ArrayIndexOutOfBoundsException ignored) { }
        try { t.getString(9, 0);                    fail(""Column does not exist""); } catch (ArrayIndexOutOfBoundsException ignored) { }
    }
",non-flaky,5
159588,realm_realm-java,JNITableTest.getNonExistingColumn,"    @Test
    public void getNonExistingColumn() {
        Table t = new Table();
        t.addColumn(RealmFieldType.INTEGER, ""int"");

        assertEquals(-1, t.getColumnIndex(""non-existing column""));
        try { t.getColumnIndex(null); fail(""column name null""); } catch (IllegalArgumentException ignored) { }
    }
",non-flaky,5
160320,triplea-game_triplea,AbstractPropertyReaderTestCase.setupPropertyReader,"    @BeforeEach
    public void setupPropertyReader() throws Exception {
      propertyReader = newSingletonPropertyReader(PRESENT_PROPERTY_VALUE);
    }
",non-flaky,5
160321,triplea-game_triplea,AbstractPropertyReaderTestCase.shouldThrowExceptionWhenKeyIsNull,"    @Test
    public void shouldThrowExceptionWhenKeyIsNull() {
      assertThrows(NullPointerException.class, () -> propertyReader.readProperty(null));
    }
",non-flaky,5
160322,triplea-game_triplea,AbstractPropertyReaderTestCase.shouldThrowExceptionWhenKeyIsEmptyOrOnlyWhitespace,"    @Test
    public void shouldThrowExceptionWhenKeyIsEmptyOrOnlyWhitespace() {
      assertThrows(IllegalArgumentException.class, () -> propertyReader.readProperty(""""));
      assertThrows(IllegalArgumentException.class, () -> propertyReader.readProperty(""    ""));
    }
",non-flaky,5
160323,triplea-game_triplea,AbstractPropertyReaderTestCase.shouldReturnValueWhenKeyIsPresent,"    @Test
    public void shouldReturnValueWhenKeyIsPresent() {
      assertThat(propertyReader.readProperty(PRESENT_PROPERTY_KEY), is(PRESENT_PROPERTY_VALUE));
    }
",non-flaky,5
160324,triplea-game_triplea,AbstractPropertyReaderTestCase.shouldReturnTrimmedValueWhenKeyIsPresentAndValueHasLeadingAndTrailingWhitespace,"    @Test
    public void shouldReturnTrimmedValueWhenKeyIsPresentAndValueHasLeadingAndTrailingWhitespace()
        throws Exception {
      final PropertyReader propertyReader =
          newSingletonPropertyReader(""  "" + PRESENT_PROPERTY_VALUE + ""  "");

      assertThat(propertyReader.readProperty(PRESENT_PROPERTY_KEY), is(PRESENT_PROPERTY_VALUE));
    }
",non-flaky,5
160325,triplea-game_triplea,AbstractPropertyReaderTestCase.shouldReturnEmptyWhenKeyIsAbsent,"    @Test
    public void shouldReturnEmptyWhenKeyIsAbsent() {
      assertThat(propertyReader.readProperty(ABSENT_PROPERTY_KEY), is(emptyString()));
    }
",non-flaky,5
160326,triplea-game_triplea,AbstractPropertyReaderTestCase.shouldReturnValueWhenKeyIsPresent,"    @Test
    public void shouldReturnValueWhenKeyIsPresent() throws Exception {
      final String value = ""value"";
      final PropertyReader propertyReader = newSingletonPropertyReader(value);

      assertThat(
          propertyReader.readPropertyOrDefault(PRESENT_PROPERTY_KEY, ""defaultValue""), is(value));
    }
",non-flaky,5
160327,triplea-game_triplea,AbstractPropertyReaderTestCase.shouldReturnDefaultValueWhenKeyIsAbsent,"    @Test
    public void shouldReturnDefaultValueWhenKeyIsAbsent() throws Exception {
      final String defaultValue = ""defaultValue"";
      final PropertyReader propertyReader = newEmptyPropertyReader();

      assertThat(
          propertyReader.readPropertyOrDefault(ABSENT_PROPERTY_KEY, defaultValue),
          is(defaultValue));
    }
",non-flaky,5
160328,triplea-game_triplea,AbstractPropertyReaderTestCase.shouldReturnValueWhenKeyIsPresent,"    @Test
    public void shouldReturnValueWhenKeyIsPresent() throws Exception {
      final boolean value = true;
      final PropertyReader propertyReader = newSingletonPropertyReader(String.valueOf(value));

      assertThat(
          propertyReader.readBooleanPropertyOrDefault(PRESENT_PROPERTY_KEY, false), is(value));
    }
",non-flaky,5
160329,triplea-game_triplea,AbstractPropertyReaderTestCase.shouldReturnDefaultValueWhenKeyIsAbsent,"    @Test
    public void shouldReturnDefaultValueWhenKeyIsAbsent() throws Exception {
      final boolean defaultValue = true;
      final PropertyReader propertyReader = newEmptyPropertyReader();

      assertThat(
          propertyReader.readBooleanPropertyOrDefault(ABSENT_PROPERTY_KEY, defaultValue),
          is(defaultValue));
    }
",non-flaky,5
160330,triplea-game_triplea,AbstractPropertyReaderTestCase.shouldReturnValueWhenKeyIsPresent,"    @Test
    public void shouldReturnValueWhenKeyIsPresent() throws Exception {
      final int value = 42;
      final PropertyReader propertyReader = newSingletonPropertyReader(String.valueOf(value));

      assertThat(propertyReader.readIntegerPropertyOrDefault(PRESENT_PROPERTY_KEY, -1), is(value));
    }
",non-flaky,5
160331,triplea-game_triplea,AbstractPropertyReaderTestCase.shouldReturnDefaultValueWhenKeyIsPresentAndValueIsNotAnInteger,"    @Test
    public void shouldReturnDefaultValueWhenKeyIsPresentAndValueIsNotAnInteger() throws Exception {
      final int defaultValue = 777;
      final PropertyReader propertyReader = newSingletonPropertyReader(""other"");

      assertThat(
          propertyReader.readIntegerPropertyOrDefault(PRESENT_PROPERTY_KEY, defaultValue),
          is(defaultValue));
    }
",non-flaky,5
160332,triplea-game_triplea,AbstractPropertyReaderTestCase.shouldReturnDefaultValueWhenKeyIsAbsent,"    @Test
    public void shouldReturnDefaultValueWhenKeyIsAbsent() throws Exception {
      final int defaultValue = 777;
      final PropertyReader propertyReader = newEmptyPropertyReader();

      assertThat(
          propertyReader.readIntegerPropertyOrDefault(ABSENT_PROPERTY_KEY, defaultValue),
          is(defaultValue));
    }
",non-flaky,5
160333,triplea-game_triplea,MessengerIntegrationTest.connectionRemoved,"  @Test
          public void connectionRemoved(final INode to) {
            serverCount.decrementAndGet();
          }
",non-flaky,5
160334,triplea-game_triplea,MessengerIntegrationTest.messageReceived,"  @Test
    public void messageReceived(final Serializable msg, final INode from) {
      synchronized (lock) {
        messages.add(msg);
        senders.add(from);
        lock.notifyAll();
      }
    }
",non-flaky,5
160335,triplea-game_triplea,ChatIntegrationTest.updatePlayerList,"  @Test
    public void updatePlayerList(final Collection<ChatParticipant> players) {
      playerCount.set(players.size());
    }
",non-flaky,5
160336,triplea-game_triplea,RemoteMessengerTest.increment,"  @Test
    public int increment(final int testVal) {
      senderNode = MessageContext.getSender();
      return testVal + 1;
    }
",non-flaky,5
160337,triplea-game_triplea,ChannelMessengerTest.testNoParams,"  @Test
    public void testNoParams() {
      incrementCount();
    }
",non-flaky,5
160338,triplea-game_triplea,ThreadPoolTest.run,"  @Test
    public void run() {
      Interruptibles.sleep(0L, 1);
      done = true;
    }
",non-flaky,5
160339,triplea-game_triplea,LobbyLoginValidatorTest.createLobbyLoginValidator,"    @BeforeEach
    public void createLobbyLoginValidator() throws Exception {
      lobbyLoginValidator =
          new LobbyLoginValidator(
              databaseDao,
              new RsaAuthenticator(TestSecurityUtils.loadRsaKeyPair()),
              () -> bcryptSalt,
              failedLoginThrottle,
              tempPasswordVerification,
              new AllowLoginRules(databaseDao),
              AllowCreateUserRules.builder()
                  .userDao(userDao)
                  .nameValidator(PlayerNameValidation::validate)
                  .emailValidator(PlayerEmailValidation::validate)
                  .build());
    }
",non-flaky,5
160340,triplea-game_triplea,LobbyLoginValidatorTest.givenNoBans,"    @BeforeEach
    public void givenNoBans() {
      givenNoMacIsBanned();
      givenNoUsernameIsBanned();
      when(databaseDao.getBadWordDao()).thenReturn(badWordDao);
    }
",non-flaky,5
162345,epimorphics_appbase,TestUnionSource.testUnionSource,"    @Test
    public void testUnionSource() {
        DatasetAccessor accessor = source.getAccessor();
        accessor.add(TEST + ""g1"", createGraph(""graph1""));
        accessor.add(TEST + ""g2"", createGraph(""graph2""));
        TestUtil.testArray(checkGraphs(), new String[]{""graph1"", ""graph2""});
        
        accessor.putModel(TEST + ""g1"", createGraph(""graph1-b""));
        TestUtil.testArray(checkGraphs(), new String[]{""graph1-b"", ""graph2""});
        
        accessor.deleteModel(TEST + ""g2"");
        TestUtil.testArray(checkGraphs(), new String[]{""graph1-b""});
    }
",non-flaky,5
162346,epimorphics_appbase,TestSearch.testAll,"    @Test
    public void testAll() {
        assertEquals(5, numMatches(""'Somerset'""));
        
        // These options would require per-predicate text index
//        assertEquals(2, numMatches(""(rdfs:label 'Somerset')""));
//        assertEquals(3, numMatches(""(eg:label   'Somerset')""));
    }
",non-flaky,5
162347,epimorphics_appbase,TestSource.testWrapper,"    @Test
    public void testWrapper() {
        // Labels
        assertEquals(""Pref label"", getNode(""test:i1"").getLabel());
        assertEquals(""Alt label"",  getNode(""test:i2"").getLabel());
        assertEquals(""rdfs label"", getNode(""test:i3"").getLabel());
        assertEquals(""name"",       getNode(""test:i4"").getLabel());
        
        WNode node = getNode(""test:i5"");
        assertEquals(""en label"", node.getLabel(""en""));
        assertEquals(""plain label"", node.getLabel(""cy""));
        
        assertTrue(node.isResource());
        assertTrue(node.isURIResource());
        assertFalse(node.isLiteral());
        assertEquals(""http://www.epimorphics.com/vocabs/test/i5"", node.getURI());
        
        DatasetGraph dsg = source.constructViews(""?uri skos:altLabel ?rdfs_label"", TEST_NS + ""i1"", TEST_NS + ""i2"");
        checkLabel(dsg, ""i1"", ""Alt label"");
        checkLabel(dsg, ""i2"", ""Alt label"");
        
        // General accessors
        WNode test = getNode(""test:test"");
        WNode v = test.getPropertyValue(""test:num"");
        assertNotNull(v);
        assertTrue(v.isLiteral());
        assertTrue(v.isNumber());
        assertEquals(42, v.asInt());
        
        v = test.getPropertyValue(""test:float"");
        assertNotNull(v);
        assertTrue(v.isLiteral());
        assertTrue(v.isNumber());
        assertEquals(3.14, v.asFloat(), 0.001);
        
        v = test.getPropertyValue(""test:string"");
        assertNotNull(v);
        assertEquals(""a string"", v.getLabel());
        
        List<WNode> values = test.listPropertyValues(""test:resource"");
        assertEquals(2, values.size());
        TestUtil.testArray(values, new WNode[]{ getNode(""test:i1""), getNode(""test:i2"")});
        
        // Lists
        WNode list = test.getPropertyValue(""test:list"");
        assertNotNull(list);
        assertTrue(list.isList());
        List<WNode> elts = list.asList();
        assertEquals(3,  elts.size());
        assertEquals(1, elts.get(0).asInt());
        assertEquals(3, elts.get(1).asInt());
        assertEquals(5, elts.get(2).asInt());
        
        // Connections
        checkConnections( getNode(""test:c"").listInLinks(""test:p""), new String[]{""test:a"", ""test:b""} );
        checkConnections( getNode(""test:d"").listInLinks(""test:p""), new String[]{""test:c""} );
        checkConnections( getNode(""test:c"").listInLinks(""test:q""), new String[]{""test:f""} );

        checkConnections( getNode(""test:c"").connectedNodes(""test:p / test:p""), new String[]{""test:e""} );
        
        List<PropertyValue> connections = getNode(""test:c"").listInLinks();
        assertEquals(2, connections.size());
        assertEquals(getNode(""test:p""), connections.get(0).getProp());
        checkConnections( connections.get(0).getValues(), new String[]{""test:a"", ""test:b""});
        assertEquals(getNode(""test:q""), connections.get(1).getProp());
        checkConnections( connections.get(1).getValues(), new String[]{""test:f""});
        
        // Text search
        List<WNode> matches = source.search(""aa"");
        assertEquals(1, matches.size());
        checkConnections(matches, new String[]{""test:a""});

        matches = source.search(""label"");
        assertEquals(4, matches.size());
        checkConnections(matches, new String[]{""test:i1"", ""test:i2"", ""test:i3"", ""test:i5""});
        
        matches = source.search(""label"", 2);
        assertEquals(2, matches.size());
        
        matches = source.search(""pref"");
        assertEquals(1, matches.size());
        checkConnections(matches, new String[]{""test:i1""});
    }
",non-flaky,5
162348,epimorphics_appbase,TestSource.testQuery,"    @Test
    public void testQuery() {
        List<Literal> literals = SQueryUtil.selectLiteralVar(""x"", ""SELECT ?x WHERE {test:i1 ?p ?x}"", ssource, app.getPrefixes());
        TestUtil.testArray(literals, new Literal[]{
                ResourceFactory.createPlainLiteral(""name""),
                ResourceFactory.createPlainLiteral(""rdfs label""),
                ResourceFactory.createPlainLiteral(""Alt label""),
                ResourceFactory.createPlainLiteral(""Pref label""),
        });
        
        List<Resource> resources = SQueryUtil.selectResourceVar(""x"", ""SELECT ?x WHERE {test:i1 ?p ?x}"", ssource, app.getPrefixes());
        TestUtil.testArray(resources, new Resource[]{
                ResourceFactory.createResource(TEST_NS + ""Sample"")
        });
    }
",non-flaky,5
162349,epimorphics_appbase,TestSource.testStreamableSelect,"    @Test
    public void testStreamableSelect() {
        String query = PrefixUtils.expandQuery(""SELECT ?x WHERE {test:i1 a ?x}"", app.getPrefixes());
        ClosableResultSet results = ssource.streamableSelect(query);
        try {
            assertTrue(results.hasNext());
            assertEquals(ResourceFactory.createResource(TEST_NS + ""Sample""), results.next().getResource(""x""));
            assertFalse(results.hasNext());
        } finally {
            results.close();
        }
    }
",non-flaky,5
162350,epimorphics_appbase,TestSource.testUpdate,"    @Test
    public void testUpdate() {
        assertTrue( ssource.isUpdateable() );
        String update = """" +
        		""PREFIX test: <http://www.epimorphics.com/vocabs/test/> \n"" +
        		""DELETE {?x test:string ?s}\n"" +
        		""INSERT {?x test:string 'new string'}\n"" +
        		""WHERE {?x test:num 42}"";
        ssource.update( UpdateFactory.create(update) );
        
        WNode v = getNode(""test:test"").getPropertyValue(""test:string"");
        assertNotNull(v);
        assertEquals(""new string"", v.getLabel());
    }
",non-flaky,5
162351,epimorphics_appbase,TestRemoteSource.testRemoveSource,"    @Ignore @Test
    public void testRemoveSource() {
        setup();
        
        assertTrue(source.isUpdateable());
        
        DatasetAccessor accessor = source.getAccessor();
        for (int i = 0; i < 2; i++) {
            Model m = ModelFactory.createDefaultModel();
            m.createResource(TEST + ""i"" + i)
                .addProperty(RDFS.label, ""In graph "" + i);
            accessor.putModel(TEST + ""g"" + i, m);
        }
        
        checkLabels(new String[]{""In graph 0"", ""In graph 1""});

        String update = """" +
                ""PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> \n"" +
                ""WITH <http://localhost/test/def#g1> \n"" +
                ""DELETE {?x rdfs:label ?l} \n"" +
                ""INSERT {?x rdfs:label 'new string'} \n"" +
                ""WHERE {?x rdfs:label ?l}"";
        source.update( UpdateFactory.create(update) );

        checkLabels(new String[]{""In graph 0"", ""new string""});

        // clean up
        for (int i = 0; i < 2; i++) {
            accessor.deleteModel(TEST + ""g"" + i);
        }
        app.shutdown();
    }
",non-flaky,5
162352,epimorphics_appbase,TestJsonActions.testSimpleActions,"    @Test
    public void testSimpleActions() {
        ActionExecution ae = runAction(""messageThrice"", ""message=test"");
        ProgressMonitorReporter pm = ae.getMonitor();
        assertTrue(pm.succeeded());
        assertEquals(5, pm.getMessages().size());
        assertTrue( pm.getMessages().get(0).getMessage().startsWith(""messageThrice"") );
        assertEquals( ""test"", pm.getMessages().get(1).getMessage() );
        
        pm = runAction(""helloThrice"", """").getMonitor();
        assertTrue(pm.succeeded());
        assertEquals(5, pm.getMessages().size());
        assertTrue( pm.getMessages().get(0).getMessage().startsWith(""helloThrice"") );
        assertEquals( ""Hello"", pm.getMessages().get(1).getMessage() );
        
    }
",non-flaky,5
162353,epimorphics_appbase,TestJsonActions.testErrorHander,"    @Test
    public void testErrorHander() throws InterruptedException {
        ActionExecution ae = runAction(""testErrorHandler"", """");
//        dumpState(ae);
        ProgressMonitorReporter pm = ae.getMonitor();
        assertFalse(pm.succeeded());
        assertEquals(2, pm.getMessages().size());
        assertTrue( pm.getMessages().get(0).getMessage().contains(""Forcing error from CreateErrorAction"") );
        assertEquals( ""Error detected"", pm.getMessages().get(1).getMessage() );

        ae = runAction(""testErrorTimeout"", """");
        Thread.sleep(10);  // Allow time out processing to complete, more robust way?
        pm = ae.getMonitor();
        assertFalse(pm.succeeded());
        List<ProgressMessage> messages = pm.getMessages();
        assertEquals( ""Timeout detected"", messages.get(messages.size() - 1).getMessage());
    }
",non-flaky,5
162354,epimorphics_appbase,TestJsonActions.testCompoundflows,"    @Test
    public void testCompoundflows() throws InterruptedException {
        ActionExecution ae = runAction(""sequenceTest"", """");
        ProgressMonitorReporter pm = ae.getMonitor();
        assertTrue(pm.succeeded());
        List<ProgressMessage> messages = pm.getMessages();
        assertEquals(3, messages.size());
        assertEquals(""sequence 1"", messages.get(0).getMessage());
        assertEquals(""sequence 2"", messages.get(1).getMessage());
        assertEquals(""sequence 3"", messages.get(2).getMessage());

        ae = runAction(""parTest"", """");
//        dumpState(ae);
        pm = ae.getMonitor();
        assertTrue(pm.succeeded());
        messages = pm.getMessages();
        assertEquals(3, messages.size());
        for (int i = 0; i < 3; i++) {
            assertTrue( messages.get(i).getMessage().matches(""par [123]"") );
        }
    }
",non-flaky,5
162355,epimorphics_appbase,TestJsonActions.testEvents,"    @Test
    public void testEvents() {
        RecordingAction.reset();
        List<ActionExecution> aes = new ArrayList<>();
        aes.addAll( am.fireEvent(""test/foo"", createParams("""")) );
        aes.addAll( am.fireEvent(""miss/foo"", createParams("""")) );
        aes.addAll( am.fireEvent(""test/bar"", createParams("""")) );
        for (ActionExecution ae : aes) {
            ae.waitForCompletion();
        }
        List<String> firings = RecordingAction.getMessages();
        assertEquals(2, firings.size());
        assertTrue( 
                (firings.get(0).equals(""fired - test/foo"") && firings.get(1).equals(""fired - test/bar"")) 
                ||
                (firings.get(0).equals(""fired - test/bar"") && firings.get(1).equals(""fired - test/foo"")) 
                );
        RecordingAction.reset();
    }
",non-flaky,5
162356,epimorphics_appbase,TestStatusReports.testJSONSerialize,"    @Test
    public void testJSONSerialize() throws IOException {
        SimpleProgressMonitor monitor = new SimpleProgressMonitor();
        monitor.setState(TaskState.Running);
        monitor.setProgress(42);
//        monitor.setSuccess(true);
        monitor.report(""message 1"");
        monitor.report(""message 2"");
        
        ByteArrayOutputStream bos = new ByteArrayOutputStream();
        JSFullWriter out = new JSFullWriter(bos);
        monitor.writeTo(out);
        out.finishOutput();
        
        String serialization = bos.toString();
//        System.out.println(serialization);
        
        JsonObject object = JSON.parse( serialization );
        assertEquals( 42,       object.get(""progress"").getAsNumber().value().intValue());
        assertEquals(""Running"", object.get(""state"").getAsString().value());
        assertEquals( true,     object.get(""succeeded"").getAsBoolean().value());
        JsonArray messages = object.get(""messages"").getAsArray();
        assertEquals( 2,        messages.size());
        JsonObject m = messages.get(1).getAsObject();
        assertEquals( ""message 2"",   m.get(""message"").getAsString().value());
    }
",non-flaky,5
162357,epimorphics_appbase,TestActionManager.testSimpleActions,"    @Test
    public void testSimpleActions() throws InterruptedException, ExecutionException {
        Action action = new DummyAction();
        ActionManager am = new ActionManager();
        am.register(action);
        
        ActionManager.ActionExecution ae1 = am.runAction(action, createParams(""message=Test message,count=2""));
        ActionManager.ActionExecution ae2 = am.runAction(action, createParams(""message=Test message long,count=50""));

        assertEquals(2, am.listActiveExecutions().size());
        
        ae1.waitForCompletion();
//        dumpState(ae1);
        List<ProgressMessage> messages = ae1.getMonitor().getMessages();
        assertEquals(4, messages.size());
        assertTrue(messages.get(messages.size() - 1).toString().endsWith(""finished""));
        assertEquals(1, am.listActiveExecutions().size());
        assertTrue(ae1.getMonitor().succeeded());
        
        ae2.waitForCompletion();
//        dumpState(ae2);
        Thread.sleep(10);  // Allow ActionManager to see the timeout and update the action state list, more robust method?
        messages = ae2.getMonitor().getMessages();
        assertTrue(messages.size() < 50);
        assertTrue(messages.get(messages.size() - 1).toString().endsWith(""timeout""));
        assertFalse(ae2.getMonitor().succeeded());
            
        assertEquals(0, am.listActiveExecutions().size());

        assertEquals(ae1, am.getExecution(ae1.getId()));
        assertEquals(ae2, am.getExecution(ae2.getId()));
    }
",non-flaky,5
162358,epimorphics_appbase,TestBasicRender.testRender,"    @Test
    public void testRender() {
        ClientResponse response = getResponse(BASE_URL + ""/test?arg=foo"", ""text/hml"");
        assertEquals(200, response.getStatus());
        
        // Should do testing base on HTML structure if can figure the right library for that, in the meantime ...
        List<String> paras = findmatches(response.getEntity(String.class), ""<p>([^<]*)</p>"");
        assertEquals(""Hello there from myapp (parameter = This is a string)"", paras.get(0));
        assertEquals(""Query param arg = foo"", paras.get(1));
        assertEquals(""Component1.prop1 = Component 1 name"", paras.get(2));
        assertEquals(""Library plugin: Hello from lib plugin - myplugin in application myapp"", paras.get(3));
    }
",non-flaky,5
162359,epimorphics_appbase,TestConfig.testConfig,"    @Test
    public void testConfig() {
        App app = AppConfig.getApp();
        assertEquals(""This is a string"", app.getParam(""stringParam""));
        assertEquals(new Long(42), app.getParam(""intParam""));
        
        TrialBean component1 = app.getComponentAs(""component1"", TrialBean.class);
        assertEquals(""name 1"", component1.getProp1());
        assertEquals(1, component1.getProplong());
        assertEquals(true, component1.isProp());
        
        TrialBean component2 = app.getComponentAs(""component2"", TrialBean.class);
        assertEquals(""name 2"", component2.getProp1());
        assertEquals(component1, component2.getRef());
        assertEquals(false, component2.isProp());
        
        TrialBean component3 = app.getComponentAs(""component3"", TrialBean.class);
        List<Object> xref = component3.getXref();
        assertNotNull(xref);
        assertEquals(2, xref.size());
        assertEquals(component1, xref.get(0));
        assertEquals(component2, xref.get(1));
    }
",non-flaky,5
162556,open-telemetry_opentelemetry-java-instrumentation,MuzzlePluginTest.compareTo,"  @Test
    public int compareTo(Version o) {
      return toString().compareTo(o.toString());
    }
",non-flaky,5
162557,open-telemetry_opentelemetry-java-instrumentation,AbstractHttpClientTest.complete,"@TestInstance(TestInstance.Lifecycle.PER_CLASS)
    public void complete(int status) {
      complete(() -> status, null);
    }
",non-flaky,5
162558,open-telemetry_opentelemetry-java-instrumentation,PortAllocatorTest.testSimple,"  @Test
  public void testSimple() {
    PortAllocator portAllocator = getPortAllocator((port) -> true);
    int next = PortAllocator.RANGE_MIN + 1;
    for (int i = 0; i < 1000; i++) {
      Assertions.assertEquals(next, portAllocator.getPort());
      next++;
      if (next % PortAllocator.CHUNK_SIZE == 0) {
        next++;
      }
    }
    Assertions.assertEquals(next, portAllocator.getPorts(10));
    Assertions.assertEquals(12101, portAllocator.getPorts(PortAllocator.CHUNK_SIZE - 1));
    assertThatThrownBy(() -> portAllocator.getPorts(PortAllocator.CHUNK_SIZE + 1))
        .isInstanceOf(IllegalStateException.class);
  }
",non-flaky,5
162559,open-telemetry_opentelemetry-java-instrumentation,PortAllocatorTest.testEven,"  @Test
  public void testEven() {
    PortAllocator portAllocator = getPortAllocator((port) -> port % 2 == 0);
    int next = PortAllocator.RANGE_MIN + 2;
    for (int i = 0; i < 1000; i++) {
      Assertions.assertEquals(next, portAllocator.getPort());
      next += 2;
      if (next % PortAllocator.CHUNK_SIZE == 0) {
        next += 2;
      }
    }
    assertThatThrownBy(() -> portAllocator.getPorts(2)).isInstanceOf(IllegalStateException.class);
  }
",non-flaky,5
162560,open-telemetry_opentelemetry-java-instrumentation,FieldMapperTest.shouldMapNestedField,"  @Test
  public void shouldMapNestedField() {

    // given
    AwsSdkRequest awsSdkRequest = UpdateTable;
    MethodHandleFactory methodHandleFactory = new MethodHandleFactory();
    Serializer serializer = mock(Serializer.class);
    FieldMapper underTest = new FieldMapper(serializer, methodHandleFactory);
    UpdateTableRequest sdkRequest =
        UpdateTableRequest.builder()
            .provisionedThroughput(
                ProvisionedThroughput.builder()
                    .readCapacityUnits(55L)
                    .writeCapacityUnits(77L)
                    .build())
            .build();
    given(serializer.serialize(55L)).willReturn(""55"");
    given(serializer.serialize(77L)).willReturn(""77"");

    Span span = mock(Span.class);
    // when
    underTest.mapToAttributes(sdkRequest, awsSdkRequest, span);
    // then
    verify(span).setAttribute(""aws.dynamodb.provisioned_throughput.read_capacity_units"", ""55"");
    verify(span).setAttribute(""aws.dynamodb.provisioned_throughput.write_capacity_units"", ""77"");
    verifyNoMoreInteractions(span);
  }
",non-flaky,5
162561,open-telemetry_opentelemetry-java-instrumentation,FieldMapperTest.shouldMapRequestFieldsOnly,"  @Test
  public void shouldMapRequestFieldsOnly() {

    // given
    AwsSdkRequest awsSdkRequest = BatchWriteItem;
    MethodHandleFactory methodHandleFactory = new MethodHandleFactory();
    Serializer serializer = mock(Serializer.class);
    FieldMapper underTest = new FieldMapper(serializer, methodHandleFactory);
    Map<String, Collection<WriteRequest>> items = new HashMap();
    BatchWriteItemRequest sdkRequest = BatchWriteItemRequest.builder().requestItems(items).build();
    given(serializer.serialize(items)).willReturn(""firstTable,secondTable"");

    Span span = mock(Span.class);
    // when
    underTest.mapToAttributes(sdkRequest, awsSdkRequest, span);
    // then
    verify(span).setAttribute(""aws.dynamodb.table_names"", ""firstTable,secondTable"");
    verifyNoMoreInteractions(span);
  }
",non-flaky,5
162562,open-telemetry_opentelemetry-java-instrumentation,FieldMapperTest.shouldMapResponseFieldsOnly,"  @Test
  public void shouldMapResponseFieldsOnly() {

    // given
    AwsSdkRequest awsSdkRequest = BatchWriteItem;
    MethodHandleFactory methodHandleFactory = new MethodHandleFactory();
    Serializer serializer = mock(Serializer.class);
    FieldMapper underTest = new FieldMapper(serializer, methodHandleFactory);
    Map<String, Collection<ItemCollectionMetrics>> items = new HashMap();
    BatchWriteItemResponse sdkResponse =
        BatchWriteItemResponse.builder()
            .consumedCapacity(ConsumedCapacity.builder().build())
            .itemCollectionMetrics(items)
            .build();
    given(serializer.serialize(sdkResponse.consumedCapacity())).willReturn(""consumedCapacity"");
    given(serializer.serialize(items)).willReturn(""itemCollectionMetrics"");

    Span span = mock(Span.class);
    // when
    underTest.mapToAttributes(sdkResponse, awsSdkRequest, span);
    // then
    verify(span).setAttribute(""aws.dynamodb.consumed_capacity"", ""consumedCapacity"");
    verify(span).setAttribute(""aws.dynamodb.item_collection_metrics"", ""itemCollectionMetrics"");
    verifyNoMoreInteractions(span);
  }
",non-flaky,5
162563,open-telemetry_opentelemetry-java-instrumentation,SerializerTest.shouldSerializeSimpleString,"  @Test
  public void shouldSerializeSimpleString() {
    // given
    // when
    String serialized = new Serializer().serialize(""simpleString"");
    // then
    assertThat(serialized).isEqualTo(""simpleString"");
  }
",non-flaky,5
162564,open-telemetry_opentelemetry-java-instrumentation,SerializerTest.shouldSerializeSdkPojo,"  @Test
  public void shouldSerializeSdkPojo() {
    // given
    SdkPojo sdkPojo =
        ProvisionedThroughput.builder().readCapacityUnits(1L).writeCapacityUnits(2L).build();
    // when
    String serialized = new Serializer().serialize(sdkPojo);
    // then
    assertThat(serialized).isEqualTo(""{\""ReadCapacityUnits\"":1,\""WriteCapacityUnits\"":2}"");
  }
",non-flaky,5
162565,open-telemetry_opentelemetry-java-instrumentation,SerializerTest.shouldSerializeCollection,"  @Test
  public void shouldSerializeCollection() {
    // given
    List<String> collection = Arrays.asList(""one"", ""two"", ""three"");
    // when
    String serialized = new Serializer().serialize(collection);
    // then
    assertThat(serialized).isEqualTo(""[one,two,three]"");
  }
",non-flaky,5
162566,open-telemetry_opentelemetry-java-instrumentation,SerializerTest.shouldSerializeEmptyCollectionAsNull,"  @Test
  public void shouldSerializeEmptyCollectionAsNull() {
    // given
    List<String> collection = Collections.emptyList();
    // when
    String serialized = new Serializer().serialize(collection);
    // then
    assertThat(serialized).isNull();
  }
",non-flaky,5
162567,open-telemetry_opentelemetry-java-instrumentation,SerializerTest.shouldSerializeMapAsKeyCollection,"  @Test
  public void shouldSerializeMapAsKeyCollection() {
    // given
    Map<String, Object> map = new HashMap<>();
    map.put(""uno"", 1L);
    map.put(""dos"", new LinkedHashMap<>());
    map.put(""tres"", ""cuatro"");
    // when
    String serialized = new Serializer().serialize(map);
    // then
    assertThat(serialized).isEqualTo(""[uno,dos,tres]"");
  }
",non-flaky,5
162568,open-telemetry_opentelemetry-java-instrumentation,ServletContextPathTest.shouldAddSlashBetweenContextAndSpanName,"  @Test
  public void shouldAddSlashBetweenContextAndSpanName() {
    Context contextWithEmptyPath = ServletContextPath.init(Context.root(), p -> p, """");
    Context contextWithPath = ServletContextPath.init(Context.root(), p -> p, ""/context"");

    assertThat(ServletContextPath.prepend(contextWithEmptyPath, ""spanName"")).isEqualTo(""spanName"");
    assertThat(ServletContextPath.prepend(contextWithPath, ""spanName""))
        .isEqualTo(""/context/spanName"");
  }
",non-flaky,5
162569,open-telemetry_opentelemetry-java-instrumentation,ServletContextPathTest.shouldNotResultInDuplicateSlash,"  @Test
  public void shouldNotResultInDuplicateSlash() {
    Context contextWithEmptyPath = ServletContextPath.init(Context.root(), p -> p, """");
    Context contextWithPath = ServletContextPath.init(Context.root(), p -> p, ""/context"");

    assertThat(ServletContextPath.prepend(contextWithEmptyPath, ""/spanName""))
        .isEqualTo(""/spanName"");
    assertThat(ServletContextPath.prepend(contextWithPath, ""/spanName""))
        .isEqualTo(""/context/spanName"");
  }
",non-flaky,5
162570,open-telemetry_opentelemetry-java-instrumentation,ServletContextPathTest.shouldIgnoreEmptySpanName,"  @Test
  public void shouldIgnoreEmptySpanName() {
    Context contextWithEmptyPath = ServletContextPath.init(Context.root(), p -> p, """");
    Context contextWithPath = ServletContextPath.init(Context.root(), p -> p, ""/context"");

    assertThat(ServletContextPath.prepend(contextWithEmptyPath, """")).isEqualTo("""");
    assertThat(ServletContextPath.prepend(contextWithPath, """")).isEqualTo(""/context"");

    assertThat(ServletContextPath.prepend(contextWithEmptyPath, null)).isEqualTo(null);
    assertThat(ServletContextPath.prepend(contextWithPath, null)).isEqualTo(""/context"");
  }
",non-flaky,5
162571,open-telemetry_opentelemetry-java-instrumentation,OpenTelemetryAutoConfigurationTest.customOpenTelemetry,"  @TestConfiguration
    public OpenTelemetry customOpenTelemetry() {
      return OpenTelemetry.noop();
    }
",non-flaky,5
162572,open-telemetry_opentelemetry-java-instrumentation,WithSpanAspectTest.getParameterNames,"  @BeforeEach
          public String[] getParameterNames(Method method) {
            return new String[] {""discoveredName"", null, ""parameter"", ""nullAttribute"", ""notTraced""};
          }
",non-flaky,5
162573,open-telemetry_opentelemetry-java-instrumentation,AbstractOpenTelemetryHandlerMappingFilterTest.testSuccess,"  @Test
  public void testSuccess() {
    AggregatedHttpResponse response = client.get(getAddress(""hello/world"")).aggregate().join();

    assertThat(response.status().code()).isEqualTo(200);
    assertThat(response.contentUtf8()).isEqualTo(""hello world"");

    testing.waitAndAssertTraces(
        trace ->
            trace.hasSpansSatisfyingExactly(
                span -> span.hasName(""/hello/{name}"").hasKind(SpanKind.SERVER).hasNoParent(),
                span ->
                    span.hasName(""HelloController.hello"")
                        .hasKind(SpanKind.INTERNAL)
                        .hasParent(trace.getSpan(0))));
  }
",non-flaky,5
162574,open-telemetry_opentelemetry-java-instrumentation,ParentContextExtractorTest.shouldUseHttpIfAwsParentNotSampled,"  @Test
  public void shouldUseHttpIfAwsParentNotSampled() {
    // given
    Map<String, String> headers =
        ImmutableMap.of(
            ""X-b3-traceId"",
            ""4fd0b6131f19f39af59518d127b0cafe"",
            ""x-b3-spanid"",
            ""0000000000000123"",
            ""X-B3-Sampled"",
            ""true"");
    environmentVariables.set(
        ""_X_AMZN_TRACE_ID"",
        ""Root=1-8a3c60f7-d188f8fa79d48a391a778fa6;Parent=0000000000000456;Sampled=0"");

    // when
    Context context = ParentContextExtractor.extract(headers, INSTRUMENTER);
    // then
    Span span = Span.fromContext(context);
    SpanContext spanContext = span.getSpanContext();
    assertThat(spanContext.isValid()).isTrue();
    assertThat(spanContext.isValid()).isTrue();
    assertThat(spanContext.getSpanId()).isEqualTo(""0000000000000123"");
    assertThat(spanContext.getTraceId()).isEqualTo(""4fd0b6131f19f39af59518d127b0cafe"");
  }
",non-flaky,5
162575,open-telemetry_opentelemetry-java-instrumentation,ParentContextExtractorTest.shouldPreferAwsParentHeaderIfValidAndSampled,"  @Test
  public void shouldPreferAwsParentHeaderIfValidAndSampled() {
    // given
    Map<String, String> headers =
        ImmutableMap.of(
            ""X-b3-traceId"",
            ""4fd0b6131f19f39af59518d127b0cafe"",
            ""x-b3-spanid"",
            ""0000000000000456"",
            ""X-B3-Sampled"",
            ""true"");
    environmentVariables.set(
        ""_X_AMZN_TRACE_ID"",
        ""Root=1-8a3c60f7-d188f8fa79d48a391a778fa6;Parent=0000000000000456;Sampled=1"");

    // when
    Context context = ParentContextExtractor.extract(headers, INSTRUMENTER);
    // then
    Span span = Span.fromContext(context);
    SpanContext spanContext = span.getSpanContext();
    assertThat(spanContext.isValid()).isTrue();
    assertThat(spanContext.isValid()).isTrue();
    assertThat(spanContext.getSpanId()).isEqualTo(""0000000000000456"");
    assertThat(spanContext.getTraceId()).isEqualTo(""8a3c60f7d188f8fa79d48a391a778fa6"");
  }
",non-flaky,5
162576,open-telemetry_opentelemetry-java-instrumentation,ParentContextExtractorTest.shouldExtractCaseInsensitiveHeaders,"  @Test
  public void shouldExtractCaseInsensitiveHeaders() {
    // given
    Map<String, String> headers =
        ImmutableMap.of(
            ""X-b3-traceId"",
            ""4fd0b6131f19f39af59518d127b0cafe"",
            ""x-b3-spanid"",
            ""0000000000000456"",
            ""X-B3-Sampled"",
            ""true"");

    // when
    Context context = ParentContextExtractor.extract(headers, INSTRUMENTER);
    // then
    Span span = Span.fromContext(context);
    SpanContext spanContext = span.getSpanContext();
    assertThat(spanContext.isValid()).isTrue();
    assertThat(spanContext.isValid()).isTrue();
    assertThat(spanContext.getSpanId()).isEqualTo(""0000000000000456"");
    assertThat(spanContext.getTraceId()).isEqualTo(""4fd0b6131f19f39af59518d127b0cafe"");
  }
",non-flaky,5
162577,open-telemetry_opentelemetry-java-instrumentation,HeadersFactoryTest.shouldReadHeadersFromStream,"  @Test
  public void shouldReadHeadersFromStream() {
    // given
    String json =
        ""{""
            + ""\""headers\"" : {""
            + ""\""X-B3-TraceId\"": \""4fd0b6131f19f39af59518d127b0cafe\"", \""X-B3-SpanId\"": \""0000000000000456\"", \""X-B3-Sampled\"": \""true\""""
            + ""},""
            + ""\""body\"" : \""hello\""""
            + ""}"";
    InputStream inputStream = new ByteArrayInputStream(json.getBytes(Charset.defaultCharset()));
    // when
    Map<String, String> headers = HeadersFactory.ofStream(inputStream);
    // then
    assertThat(headers).isNotNull();
    assertThat(headers.size()).isEqualTo(3);
    assertThat(headers)
        .containsOnly(
            entry(""X-B3-TraceId"", ""4fd0b6131f19f39af59518d127b0cafe""),
            entry(""X-B3-SpanId"", ""0000000000000456""),
            entry(""X-B3-Sampled"", ""true""));
  }
",non-flaky,5
162578,open-telemetry_opentelemetry-java-instrumentation,HeadersFactoryTest.shouldReturnNullIfNoHeadersInStream,"  @Test
  public void shouldReturnNullIfNoHeadersInStream() {
    // given
    String json = ""{\""something\"" : \""else\""}"";
    InputStream inputStream = new ByteArrayInputStream(json.getBytes(Charset.defaultCharset()));
    // when
    Map<String, String> headers = HeadersFactory.ofStream(inputStream); // then
    assertThat(headers).isNull();
  }
",non-flaky,5
162579,open-telemetry_opentelemetry-java-instrumentation,ApiGatewayProxyRequestTest.shouldCreateNoopRequestIfNoPropagatorsSet,"  @Test
  public void shouldCreateNoopRequestIfNoPropagatorsSet() throws IOException {
    // given
    InputStream mock = mock(InputStream.class);
    GlobalOpenTelemetry.set(OpenTelemetry.noop());
    // when
    ApiGatewayProxyRequest created = ApiGatewayProxyRequest.forStream(mock);
    // then
    assertThat(created.freshStream()).isEqualTo(mock);
    assertThat(created.getHeaders()).isEmpty();
  }
",non-flaky,5
162580,open-telemetry_opentelemetry-java-instrumentation,ApiGatewayProxyRequestTest.shouldCreateNoopRequestIfXrayPropagatorsSet,"  @Test
  public void shouldCreateNoopRequestIfXrayPropagatorsSet() throws IOException {
    // given
    InputStream mock = mock(InputStream.class);
    GlobalOpenTelemetry.set(
        OpenTelemetry.propagating(ContextPropagators.create(AwsXrayPropagator.getInstance())));
    // when
    ApiGatewayProxyRequest created = ApiGatewayProxyRequest.forStream(mock);
    // then
    assertThat(created.freshStream()).isEqualTo(mock);
    assertThat(created.getHeaders()).isEmpty();
  }
",non-flaky,5
162581,open-telemetry_opentelemetry-java-instrumentation,ApiGatewayProxyRequestTest.shouldUseStreamMarkingIfHttpPropagatorsSet,"  @Test
  public void shouldUseStreamMarkingIfHttpPropagatorsSet() throws IOException {
    // given
    InputStream mock = mock(InputStream.class);
    given(mock.markSupported()).willReturn(true);
    GlobalOpenTelemetry.set(
        OpenTelemetry.propagating(ContextPropagators.create(B3Propagator.injectingSingleHeader())));
    // when
    ApiGatewayProxyRequest created = ApiGatewayProxyRequest.forStream(mock);
    // then
    assertThat(created.freshStream()).isEqualTo(mock);
    then(mock).should(atLeastOnce()).mark(Integer.MAX_VALUE);
    then(mock).should().reset();
  }
",non-flaky,5
162582,open-telemetry_opentelemetry-java-instrumentation,ApiGatewayProxyRequestTest.shouldUseCopyIfMarkingNotAvailableAndHttpPropagatorsSet,"  @Test
  public void shouldUseCopyIfMarkingNotAvailableAndHttpPropagatorsSet() throws IOException {
    // given
    InputStream mock = mock(InputStream.class);
    given(mock.markSupported()).willReturn(false);
    given(mock.read(any(byte[].class))).willReturn(-1);
    GlobalOpenTelemetry.set(
        OpenTelemetry.propagating(ContextPropagators.create(B3Propagator.injectingSingleHeader())));
    // when
    ApiGatewayProxyRequest created = ApiGatewayProxyRequest.forStream(mock);
    // then
    assertThat(created.freshStream()).isInstanceOf(ByteArrayInputStream.class);
    then(mock).should(never()).mark(any(Integer.class));
    then(mock).should(never()).reset();
    then(mock).should().read(any());
  }
",non-flaky,5
162583,open-telemetry_opentelemetry-java-instrumentation,ProcessMetricsTest.test,"  @Test
  public void test() {
    ProcessMetrics.registerObservers();

    waitAndAssertMetrics(
        metric ->
            metric
                .hasName(""runtime.java.memory"")
                .hasUnit(""bytes"")
                .hasLongGauge()
                .points()
                .anySatisfy(point -> assertThat(point.getValue()).isPositive()),
        metric ->
            metric
                .hasName(""runtime.java.cpu_time"")
                .hasUnit(""seconds"")
                .hasDoubleGauge()
                .points()
                .anySatisfy(point -> assertThat(point.getValue()).isPositive()));
  }
",non-flaky,5
162584,open-telemetry_opentelemetry-java-instrumentation,SystemMetricsTest.test,"  @Test
  public void test() {
    SystemMetrics.registerObservers();

    waitAndAssertMetrics(
        metric ->
            metric
                .hasName(""system.memory.usage"")
                .hasUnit(""By"")
                .hasLongGauge()
                .points()
                .anySatisfy(point -> assertThat(point.getValue()).isPositive()),
        metric ->
            metric
                .hasName(""system.memory.utilization"")
                .hasUnit(""1"")
                .hasDoubleGauge()
                .points()
                .anySatisfy(point -> assertThat(point.getValue()).isPositive()),
        metric -> metric.hasName(""system.network.io"").hasUnit(""By"").hasLongGauge(),
        metric -> metric.hasName(""system.network.packets"").hasUnit(""packets"").hasLongGauge(),
        metric -> metric.hasName(""system.network.errors"").hasUnit(""errors"").hasLongGauge(),
        metric -> metric.hasName(""system.disk.operations"").hasUnit(""operations"").hasLongGauge());
  }
",non-flaky,5
162585,open-telemetry_opentelemetry-java-instrumentation,AbstractArquillianRestTest.testHelloRequest,"  @ParameterizedTest
  public void testHelloRequest(String path, String className) {
    AggregatedHttpResponse response = client.get(url.resolve(path).toString()).aggregate().join();

    assertThat(response.status().code()).isEqualTo(200);
    assertThat(response.contentUtf8()).isEqualTo(""hello"");

    testing.waitAndAssertTraces(
        trace ->
            trace.hasSpansSatisfyingExactly(
                span ->
                    span.hasName(getContextRoot() + path).hasKind(SpanKind.SERVER).hasNoParent(),
                span -> span.hasName(className + "".hello"").hasParent(trace.getSpan(0))));
  }
",non-flaky,5
162586,open-telemetry_opentelemetry-java-instrumentation,AbstractQuartzTest.execute,"  @Test
    public void execute(JobExecutionContext context) {
      GlobalOpenTelemetry.getTracer(""jobtracer"").spanBuilder(""child"").startSpan().end();
    }
",non-flaky,5
162587,open-telemetry_opentelemetry-java-instrumentation,AbstractArquillianJaxWsTest.testHelloRequest,"  @ParameterizedTest
  public void testHelloRequest(String service) {
    String soapMessage =
        ""<soapenv:Envelope xmlns:soapenv=\""http://schemas.xmlsoap.org/soap/envelope/\"" xmlns:hel=\""http://opentelemetry.io/test/hello-web-service\"">""
            + ""   <soapenv:Header/>""
            + ""   <soapenv:Body>""
            + ""      <hel:helloRequest>""
            + ""         <name>Test</name>""
            + ""      </hel:helloRequest>""
            + ""   </soapenv:Body>""
            + ""</soapenv:Envelope>"";

    AggregatedHttpResponse response =
        client.post(getAddress(service), soapMessage).aggregate().join();
    Document doc = Jsoup.parse(response.contentUtf8());

    assertThat(response.status().code()).isEqualTo(200);
    assertThat(doc.selectFirst(""message"").text()).isEqualTo(""Hello Test"");

    String methodName = ""hello"";
    testing.waitAndAssertTraces(
        trace ->
            trace.hasSpansSatisfyingExactly(
                span -> assertServerSpan(span, serverSpanName(service, methodName)).hasNoParent(),
                span -> assertHandlerSpan(span, service, methodName).hasParent(trace.getSpan(0)),
                span ->
                    assertAnnotationHandlerSpan(span, service, methodName)
                        .hasParent(trace.getSpan(1))));
  }
",non-flaky,5
162588,open-telemetry_opentelemetry-java-instrumentation,CamelPropagationUtilTest.shouldExtractHttpParentForHttpEndpoint,"  @Test
  public void shouldExtractHttpParentForHttpEndpoint() throws Exception {

    // given
    Endpoint endpoint = new HttpEndpoint("""", new HttpComponent(), URI.create(""""));
    Map<String, Object> exchangeHeaders =
        Collections.singletonMap(
            ""uber-trace-id"", ""1f7f8dab3f0043b1b9cf0a75caf57510:a13825abcb764bd3:0:1"");

    // when
    Context parent = CamelPropagationUtil.extractParent(exchangeHeaders, endpoint);

    // then
    Span parentSpan = Span.fromContext(parent);
    SpanContext parentSpanContext = parentSpan.getSpanContext();
    assertThat(parentSpanContext.getTraceId()).isEqualTo(""1f7f8dab3f0043b1b9cf0a75caf57510"");
    assertThat(parentSpanContext.getSpanId()).isEqualTo(""a13825abcb764bd3"");
  }
",non-flaky,5
162589,open-telemetry_opentelemetry-java-instrumentation,CamelPropagationUtilTest.shouldNotFailExtractingNullHttpParentForHttpEndpoint,"  @Test
  public void shouldNotFailExtractingNullHttpParentForHttpEndpoint() throws Exception {

    // given
    Endpoint endpoint = new HttpEndpoint("""", new HttpComponent(), URI.create(""""));
    Map<String, Object> exchangeHeaders = Collections.singletonMap(""uber-trace-id"", null);

    // when
    Context parent = CamelPropagationUtil.extractParent(exchangeHeaders, endpoint);

    // then
    Span parentSpan = Span.fromContext(parent);
    SpanContext parentSpanContext = parentSpan.getSpanContext();
    assertThat(parentSpanContext.isValid()).isEqualTo(false);
  }
",non-flaky,5
162590,open-telemetry_opentelemetry-java-instrumentation,CamelPropagationUtilTest.shouldNotFailExtractingNullAwsParentForSqsEndpoint,"  @Test
  public void shouldNotFailExtractingNullAwsParentForSqsEndpoint() {

    // given
    Endpoint endpoint = new SqsEndpoint("""", new SqsComponent(), new SqsConfiguration());
    Map<String, Object> exchangeHeaders = Collections.singletonMap(""AWSTraceHeader"", null);

    // when
    Context parent = CamelPropagationUtil.extractParent(exchangeHeaders, endpoint);

    // then
    Span parentSpan = Span.fromContext(parent);
    SpanContext parentSpanContext = parentSpan.getSpanContext();
    assertThat(parentSpanContext.isValid()).isEqualTo(false);
  }
",non-flaky,5
162591,open-telemetry_opentelemetry-java-instrumentation,CamelPropagationUtilTest.shouldExtractAwsParentForSqsEndpoint,"  @Test
  public void shouldExtractAwsParentForSqsEndpoint() {

    // given
    Endpoint endpoint = new SqsEndpoint("""", new SqsComponent(), new SqsConfiguration());
    Map<String, Object> exchangeHeaders =
        Collections.singletonMap(
            ""AWSTraceHeader"",
            ""Root=1-5759e988-bd862e3fe1be46a994272793;Parent=53995c3f42cd8ad8;Sampled=1\n"");

    // when
    Context parent = CamelPropagationUtil.extractParent(exchangeHeaders, endpoint);

    // then
    Span parentSpan = Span.fromContext(parent);
    SpanContext parentSpanContext = parentSpan.getSpanContext();
    assertThat(parentSpanContext.getTraceId()).isEqualTo(""5759e988bd862e3fe1be46a994272793"");
    assertThat(parentSpanContext.getSpanId()).isEqualTo(""53995c3f42cd8ad8"");
  }
",non-flaky,5
162592,open-telemetry_opentelemetry-java-instrumentation,ReferenceCollectorTest.methodBodyCreatesReferences,"  @Test
  public void methodBodyCreatesReferences() {
    ReferenceCollector collector = new ReferenceCollector((String s) -> false);

    collector.collectReferencesFromAdvice(MethodBodyAdvice.class.getName());
    collector.prune();
    Map<String, ClassRef> references = collector.getReferences();

    assertThat(references)
        .containsOnlyKeys(
            MethodBodyAdvice.A.class.getName(),
            MethodBodyAdvice.B.class.getName(),
            MethodBodyAdvice.SomeInterface.class.getName(),
            MethodBodyAdvice.SomeImplementation.class.getName());

    ClassRef refB = references.get(MethodBodyAdvice.B.class.getName());
    ClassRef refA = references.get(MethodBodyAdvice.A.class.getName());

    // interface flags
    assertThat(refB.getFlags()).contains(ManifestationFlag.NON_INTERFACE);
    assertThat(references.get(MethodBodyAdvice.SomeInterface.class.getName()).getFlags())
        .contains(ManifestationFlag.INTERFACE);

    // class access flags
    assertThat(refA.getFlags()).contains(PACKAGE_OR_HIGHER);
    assertThat(refB.getFlags()).contains(PACKAGE_OR_HIGHER);

    // method refs
    assertMethod(
        refB,
        ""method"",
        ""(Ljava/lang/String;)Ljava/lang/String;"",
        PROTECTED_OR_HIGHER,
        OwnershipFlag.NON_STATIC);
    assertMethod(
        refB, ""methodWithPrimitives"", ""(Z)V"", PROTECTED_OR_HIGHER, OwnershipFlag.NON_STATIC);
    assertMethod(refB, ""staticMethod"", ""()V"", PROTECTED_OR_HIGHER, OwnershipFlag.STATIC);
    assertMethod(
        refB,
        ""methodWithArrays"",
        ""([Ljava/lang/String;)[Ljava/lang/Object;"",
        PROTECTED_OR_HIGHER,
        OwnershipFlag.NON_STATIC);

    // field refs
    assertThat(refB.getFields()).isEmpty();
    assertThat(refA.getFields()).hasSize(2);
    assertField(refA, ""publicB"", PACKAGE_OR_HIGHER, OwnershipFlag.NON_STATIC);
    assertField(refA, ""staticB"", PACKAGE_OR_HIGHER, OwnershipFlag.STATIC);
  }
",non-flaky,5
162593,open-telemetry_opentelemetry-java-instrumentation,ReferenceCollectorTest.protectedRefTest,"  @Test
  public void protectedRefTest() {
    ReferenceCollector collector = new ReferenceCollector(s -> false);
    collector.collectReferencesFromAdvice(MethodBodyAdvice.B2.class.getName());
    collector.prune();
    Map<String, ClassRef> references = collector.getReferences();

    assertMethod(
        references.get(MethodBodyAdvice.B.class.getName()),
        ""protectedMethod"",
        ""()V"",
        PROTECTED_OR_HIGHER,
        OwnershipFlag.NON_STATIC);
  }
",non-flaky,5
162594,open-telemetry_opentelemetry-java-instrumentation,ReferenceCollectorTest.ldcCreatesReferences,"  @Test
  public void ldcCreatesReferences() {
    ReferenceCollector collector = new ReferenceCollector(s -> false);
    collector.collectReferencesFromAdvice(LdcAdvice.class.getName());
    collector.prune();
    Map<String, ClassRef> references = collector.getReferences();

    assertThat(references).containsKey(MethodBodyAdvice.A.class.getName());
  }
",non-flaky,5
162595,open-telemetry_opentelemetry-java-instrumentation,ReferenceCollectorTest.instanceofCreatesReferences,"  @Test
  public void instanceofCreatesReferences() {
    ReferenceCollector collector = new ReferenceCollector(s -> false);
    collector.collectReferencesFromAdvice(TestClasses.InstanceofAdvice.class.getName());
    collector.prune();
    Map<String, ClassRef> references = collector.getReferences();

    assertThat(references).containsKey(MethodBodyAdvice.A.class.getName());
  }
",non-flaky,5
162596,open-telemetry_opentelemetry-java-instrumentation,ReferenceCollectorTest.invokedynamicCreatesReferences,"  @Test
  public void invokedynamicCreatesReferences() {
    ReferenceCollector collector = new ReferenceCollector(s -> false);
    collector.collectReferencesFromAdvice(TestClasses.InvokeDynamicAdvice.class.getName());
    collector.prune();
    Map<String, ClassRef> references = collector.getReferences();

    assertThat(references).containsKey(""muzzle.TestClasses$MethodBodyAdvice$SomeImplementation"");
    assertMethod(
        references.get(""muzzle.TestClasses$MethodBodyAdvice$SomeImplementation""),
        ""someMethod"",
        ""()V"",
        PROTECTED_OR_HIGHER,
        OwnershipFlag.NON_STATIC);
    assertThat(references).containsKey(""muzzle.TestClasses$MethodBodyAdvice$B"");
    assertMethod(
        references.get(""muzzle.TestClasses$MethodBodyAdvice$B""),
        ""staticMethod"",
        ""()V"",
        PROTECTED_OR_HIGHER,
        OwnershipFlag.STATIC);
    assertThat(references).containsKey(""muzzle.TestClasses$MethodBodyAdvice$A"");
    assertMethod(
        references.get(""muzzle.TestClasses$MethodBodyAdvice$A""),
        ""<init>"",
        ""()V"",
        PROTECTED_OR_HIGHER,
        OwnershipFlag.NON_STATIC);
  }
",non-flaky,5
162597,open-telemetry_opentelemetry-java-instrumentation,ReferenceCollectorTest.shouldCreateReferencesForHelperClasses,"  @Test
  public void shouldCreateReferencesForHelperClasses() {
    ReferenceCollector collector = new ReferenceCollector(s -> false);
    collector.collectReferencesFromAdvice(HelperAdvice.class.getName());
    Map<String, ClassRef> references = collector.getReferences();

    assertThat(references)
        .containsOnlyKeys(
            TestHelperClasses.Helper.class.getName(),
            TestHelperClasses.HelperSuperClass.class.getName(),
            TestHelperClasses.HelperInterface.class.getName());

    ClassRef helperSuperClass = references.get(TestHelperClasses.HelperSuperClass.class.getName());
    assertThat(helperSuperClass.getFlags()).contains(ManifestationFlag.ABSTRACT);
    assertHelperSuperClassMethod(helperSuperClass, true);
    assertMethod(
        helperSuperClass,
        ""finalMethod"",
        ""()Ljava/lang/String;"",
        VisibilityFlag.PUBLIC,
        OwnershipFlag.NON_STATIC,
        ManifestationFlag.FINAL);

    ClassRef helperInterface = references.get(TestHelperClasses.HelperInterface.class.getName());
    assertThat(helperInterface.getFlags()).contains(ManifestationFlag.ABSTRACT);
    assertHelperInterfaceMethod(helperInterface, true);

    ClassRef helperClass = references.get(TestHelperClasses.Helper.class.getName());
    assertThat(helperClass.getFlags()).contains(ManifestationFlag.NON_FINAL);
    assertHelperSuperClassMethod(helperClass, false);
    assertHelperInterfaceMethod(helperClass, false);
  }
",non-flaky,5
162598,open-telemetry_opentelemetry-java-instrumentation,ReferenceCollectorTest.shouldCollectFieldDeclarationReferences,"  @Test
  public void shouldCollectFieldDeclarationReferences() {
    ReferenceCollector collector =
        new ReferenceCollector(s -> s.equals(DeclaredFieldTestClass.Helper.class.getName()));
    collector.collectReferencesFromAdvice(DeclaredFieldTestClass.Advice.class.getName());
    collector.prune();
    Map<String, ClassRef> references = collector.getReferences();

    ClassRef helperClass = references.get(DeclaredFieldTestClass.Helper.class.getName());
    FieldRef superField = findField(helperClass, ""superField"");
    assertThat(superField).isNotNull();
    assertThat(superField.isDeclared()).isFalse();

    FieldRef field = findField(helperClass, ""helperField"");
    assertThat(field).isNotNull();
    assertThat(field.isDeclared()).isTrue();

    ClassRef libraryBaseClass =
        references.get(DeclaredFieldTestClass.LibraryBaseClass.class.getName());
    assertThat(libraryBaseClass.getFields()).isEmpty();
  }
",non-flaky,5
162599,open-telemetry_opentelemetry-java-instrumentation,ReferenceCollectorTest.shouldFindAllHelperClasses,"  @Test
  public void shouldFindAllHelperClasses() {
    ReferenceCollector collector = new ReferenceCollector(s -> false);
    collector.collectReferencesFromAdvice(HelperAdvice.class.getName());
    collector.prune();
    List<String> helperClasses = collector.getSortedHelperClasses();

    assertThat(helperClasses)
        .containsSubsequence(
            Arrays.asList(
                TestHelperClasses.HelperInterface.class.getName(),
                TestHelperClasses.Helper.class.getName()));
    assertThat(helperClasses)
        .containsSubsequence(
            Arrays.asList(
                TestHelperClasses.HelperSuperClass.class.getName(),
                TestHelperClasses.Helper.class.getName()));
  }
",non-flaky,5
162600,open-telemetry_opentelemetry-java-instrumentation,ReferenceCollectorTest.shouldCorrectlyFindHelperClassesFromMultipleAdviceClasses,"  @Test
  public void shouldCorrectlyFindHelperClassesFromMultipleAdviceClasses() {
    ReferenceCollector collector = new ReferenceCollector(s -> false);
    collector.collectReferencesFromAdvice(HelperAdvice.class.getName());
    collector.collectReferencesFromAdvice(TestClasses.HelperOtherAdvice.class.getName());
    collector.prune();
    List<String> helperClasses = collector.getSortedHelperClasses();

    assertThat(helperClasses)
        .containsSubsequence(
            Arrays.asList(
                TestHelperClasses.HelperInterface.class.getName(),
                TestHelperClasses.Helper.class.getName()));
    assertThat(helperClasses)
        .containsSubsequence(
            Arrays.asList(
                TestHelperClasses.HelperSuperClass.class.getName(),
                TestHelperClasses.Helper.class.getName()));
    assertThat(helperClasses)
        .containsSubsequence(
            Arrays.asList(
                OtherTestHelperClasses.TestEnum.class.getName(),
                OtherTestHelperClasses.TestEnum.class.getName() + ""$1""));

    assertThat(helperClasses)
        .containsExactlyInAnyOrder(
            TestHelperClasses.HelperSuperClass.class.getName(),
            TestHelperClasses.HelperInterface.class.getName(),
            TestHelperClasses.Helper.class.getName(),
            OtherTestHelperClasses.Bar.class.getName(),
            OtherTestHelperClasses.Foo.class.getName(),
            OtherTestHelperClasses.TestEnum.class.getName(),
            OtherTestHelperClasses.TestEnum.class.getName() + ""$1"",
            OtherTestHelperClasses.class.getName() + ""$1"");
  }
",non-flaky,5
162601,open-telemetry_opentelemetry-java-instrumentation,ReferenceCollectorTest.shouldCorrectlyFindExternalInstrumentationClasses,"  @Test
  public void shouldCorrectlyFindExternalInstrumentationClasses() {
    ReferenceCollector collector =
        new ReferenceCollector(s -> s.startsWith(""external.instrumentation""));
    collector.collectReferencesFromAdvice(
        TestClasses.ExternalInstrumentationAdvice.class.getName());
    collector.prune();

    Map<String, ClassRef> references = collector.getReferences();
    assertThat(references.get(""external.NotInstrumentation"")).isNotNull();

    List<String> helperClasses = collector.getSortedHelperClasses();
    assertThat(helperClasses).containsExactly(ExternalHelper.class.getName());
  }
",non-flaky,5
162602,open-telemetry_opentelemetry-java-instrumentation,ReferenceCollectorTest.shouldCollectHelperClassesFromResourceFile,"  @ParameterizedTest
  public void shouldCollectHelperClassesFromResourceFile(
      @SuppressWarnings(""unused"") String desc, String resource) {
    ReferenceCollector collector = new ReferenceCollector(s -> false);
    collector.collectReferencesFromResource(HelperResource.create(resource, resource));
    collector.prune();

    List<String> helperClasses = collector.getSortedHelperClasses();
    assertThat(helperClasses)
        .containsSubsequence(
            Arrays.asList(
                TestHelperClasses.HelperInterface.class.getName(),
                TestHelperClasses.Helper.class.getName()));
    assertThat(helperClasses)
        .containsSubsequence(
            Arrays.asList(
                TestHelperClasses.HelperSuperClass.class.getName(),
                TestHelperClasses.Helper.class.getName()));
  }
",non-flaky,5
162603,open-telemetry_opentelemetry-java-instrumentation,ReferenceCollectorTest.shouldIgnoreArbitraryResourceFile,"  @Test
  public void shouldIgnoreArbitraryResourceFile() {
    ReferenceCollector collector = new ReferenceCollector(s -> false);
    collector.collectReferencesFromResource(
        HelperResource.create(""application.properties"", ""application.properties""));
    collector.prune();

    assertThat(collector.getReferences()).isEmpty();
    assertThat(collector.getSortedHelperClasses()).isEmpty();
  }
",non-flaky,5
162604,open-telemetry_opentelemetry-java-instrumentation,ReferenceCollectorTest.shouldCollectVirtualFields,"  @Test
  public void shouldCollectVirtualFields() {
    ReferenceCollector collector = new ReferenceCollector(s -> false);
    collector.collectReferencesFromAdvice(VirtualFieldTestClasses.ValidAdvice.class.getName());
    collector.prune();

    VirtualFieldMappings virtualFieldMappings = collector.getVirtualFieldMappings();
    assertThat(virtualFieldMappings.entrySet())
        .containsExactlyInAnyOrder(
            entry(VirtualFieldTestClasses.Key1.class.getName(), Context.class.getName()),
            entry(VirtualFieldTestClasses.Key2.class.getName(), Context.class.getName()));
  }
",non-flaky,5
162605,open-telemetry_opentelemetry-java-instrumentation,ReferenceCollectorTest.shouldCollectMultipleVirtualFieldsForSingleClass,"  @Test
  public void shouldCollectMultipleVirtualFieldsForSingleClass() {
    ReferenceCollector collector = new ReferenceCollector(s -> false);
    collector.collectReferencesFromAdvice(
        VirtualFieldTestClasses.TwoVirtualFieldsInTheSameClassAdvice.class.getName());
    collector.prune();

    VirtualFieldMappings virtualFieldMappings = collector.getVirtualFieldMappings();
    assertThat(virtualFieldMappings.entrySet())
        .containsExactlyInAnyOrder(
            entry(VirtualFieldTestClasses.Key1.class.getName(), Context.class.getName()),
            entry(VirtualFieldTestClasses.Key1.class.getName(), State.class.getName()));
  }
",non-flaky,5
162606,open-telemetry_opentelemetry-java-instrumentation,ReferenceCollectorTest.shouldNotCollectVirtualFieldsForInvalidScenario,"  @ParameterizedTest(name = ""{0}"")
  public void shouldNotCollectVirtualFieldsForInvalidScenario(
      @SuppressWarnings(""unused"") String desc, String adviceClassName) {
    ReferenceCollector collector = new ReferenceCollector(s -> false);

    Assertions.assertThatExceptionOfType(MuzzleCompilationException.class)
        .isThrownBy(
            () -> {
              collector.collectReferencesFromAdvice(adviceClassName);
              collector.prune();
            });
  }
",non-flaky,5
162607,open-telemetry_opentelemetry-java-instrumentation,ReferenceCollectorTest.shouldCollectArrayVirtualField,"  @Test
  public void shouldCollectArrayVirtualField() {
    ReferenceCollector collector = new ReferenceCollector(s -> false);
    collector.collectReferencesFromAdvice(
        VirtualFieldTestClasses.UsingArrayAsFieldAdvice.class.getName());
    collector.prune();

    VirtualFieldMappings virtualFieldMappings = collector.getVirtualFieldMappings();
    assertThat(virtualFieldMappings.entrySet())
        .containsExactly(
            entry(
                VirtualFieldTestClasses.Key1.class.getName(),
                Type.getType(Context[].class).getClassName()));
  }
",non-flaky,5
162608,open-telemetry_opentelemetry-java-instrumentation,MethodCacheTest.getItemFromCache,"  @Test
  public void getItemFromCache() throws Exception {
    Cache<Method, String> cache = new MethodCache<>();
    Method key = TestClass.class.getDeclaredMethod(""method"");
    String value = ""Value"";

    cache.put(key, value);

    assertThat(cache.get(key)).isEqualTo(""Value"");
  }
",non-flaky,5
162609,open-telemetry_opentelemetry-java-instrumentation,SpringBootIntegrationTest.extensionsAreLoadedFromJar,"  @Test
  public void extensionsAreLoadedFromJar() throws IOException, InterruptedException {
    startTarget(""/opentelemetry-extensions.jar"");

    testAndVerify();

    stopTarget();
  }
",non-flaky,5
162610,open-telemetry_opentelemetry-java-instrumentation,SpringBootIntegrationTest.extensionsAreLoadedFromFolder,"  @Test
  public void extensionsAreLoadedFromFolder() throws IOException, InterruptedException {
    startTarget(""/"");

    testAndVerify();

    stopTarget();
  }
",non-flaky,5
162611,open-telemetry_opentelemetry-java-instrumentation,SpringBootIntegrationTest.extensionsAreLoadedFromJavaagent,"  @Test
  public void extensionsAreLoadedFromJavaagent() throws IOException, InterruptedException {
    startTargetWithExtendedAgent();

    testAndVerify();

    stopTarget();
  }
",non-flaky,5
162612,open-telemetry_opentelemetry-java-instrumentation,SpringBootSmokeTest.springBootSmokeTestOnJDK,"  @Test
  public void springBootSmokeTestOnJDK() throws IOException, InterruptedException {
    startTarget(8);
    String url = String.format(""http://localhost:%d/greeting"", target.getMappedPort(8080));
    Request request = new Request.Builder().url(url).get().build();

    String currentAgentVersion =
        (String)
            new JarFile(agentPath)
                .getManifest()
                .getMainAttributes()
                .get(Attributes.Name.IMPLEMENTATION_VERSION);

    Response response = client.newCall(request).execute();
    System.out.println(response.headers().toString());

    Collection<ExportTraceServiceRequest> traces = waitForTraces();

    Assertions.assertNotNull(response.header(""X-server-id""));
    Assertions.assertEquals(1, response.headers(""X-server-id"").size());
    Assertions.assertTrue(TraceId.isValid(response.header(""X-server-id"")));
    Assertions.assertEquals(""Hi!"", response.body().string());
    Assertions.assertEquals(1, countSpansByName(traces, ""/greeting""));
    Assertions.assertEquals(0, countSpansByName(traces, ""WebController.greeting""));
    Assertions.assertEquals(1, countSpansByName(traces, ""WebController.withSpan""));
    Assertions.assertEquals(2, countSpansByAttributeValue(traces, ""custom"", ""demo""));
    Assertions.assertNotEquals(
        0, countResourcesByValue(traces, ""telemetry.auto.version"", currentAgentVersion));
    Assertions.assertNotEquals(0, countResourcesByValue(traces, ""custom.resource"", ""demo""));

    stopTarget();
  }
",non-flaky,5
162613,open-telemetry_opentelemetry-java-instrumentation,PatchLoggerTest.equals,"  @Test
    public boolean equals(Object obj) {
      if (obj == this) {
        return true;
      }
      if (!(obj instanceof MethodSignature)) {
        return false;
      }
      MethodSignature other = (MethodSignature) obj;
      return Objects.equals(name, other.name)
          && Objects.equals(parameterTypes, other.parameterTypes)
          && Objects.equals(returnType, other.returnType);
    }
",non-flaky,5
162614,open-telemetry_opentelemetry-java-instrumentation,HttpServerTracerTest.extractForwarded,"  @Test
  public void extractForwarded() {
    assertEquals(""1.1.1.1"", HttpServerTracer.extractForwarded(""for=1.1.1.1""));
  }
",non-flaky,5
162615,open-telemetry_opentelemetry-java-instrumentation,HttpServerTracerTest.extractForwardedIpv6,"  @Test
  public void extractForwardedIpv6() {
    assertEquals(
        ""1111:1111:1111:1111:1111:1111:1111:1111"",
        HttpServerTracer.extractForwarded(""for=\""[1111:1111:1111:1111:1111:1111:1111:1111]\""""));
  }
",non-flaky,5
162616,open-telemetry_opentelemetry-java-instrumentation,HttpServerTracerTest.extractForwardedWithPort,"  @Test
  public void extractForwardedWithPort() {
    assertEquals(""1.1.1.1"", HttpServerTracer.extractForwarded(""for=\""1.1.1.1:2222\""""));
  }
",non-flaky,5
162617,open-telemetry_opentelemetry-java-instrumentation,HttpServerTracerTest.extractForwardedIpv6WithPort,"  @Test
  public void extractForwardedIpv6WithPort() {
    assertEquals(
        ""1111:1111:1111:1111:1111:1111:1111:1111"",
        HttpServerTracer.extractForwarded(
            ""for=\""[1111:1111:1111:1111:1111:1111:1111:1111]:2222\""""));
  }
",non-flaky,5
162618,open-telemetry_opentelemetry-java-instrumentation,HttpServerTracerTest.extractForwardedCaps,"  @Test
  public void extractForwardedCaps() {
    assertEquals(""1.1.1.1"", HttpServerTracer.extractForwarded(""For=1.1.1.1""));
  }
",non-flaky,5
162619,open-telemetry_opentelemetry-java-instrumentation,HttpServerTracerTest.extractForwardedMalformed,"  @Test
  public void extractForwardedMalformed() {
    assertNull(HttpServerTracer.extractForwarded(""for=;for=1.1.1.1""));
  }
",non-flaky,5
162620,open-telemetry_opentelemetry-java-instrumentation,HttpServerTracerTest.extractForwardedEmpty,"  @Test
  public void extractForwardedEmpty() {
    assertNull(HttpServerTracer.extractForwarded(""""));
  }
",non-flaky,5
162621,open-telemetry_opentelemetry-java-instrumentation,HttpServerTracerTest.extractForwardedEmptyValue,"  @Test
  public void extractForwardedEmptyValue() {
    assertNull(HttpServerTracer.extractForwarded(""for=""));
  }
",non-flaky,5
162622,open-telemetry_opentelemetry-java-instrumentation,HttpServerTracerTest.extractForwardedEmptyValueWithSemicolon,"  @Test
  public void extractForwardedEmptyValueWithSemicolon() {
    assertNull(HttpServerTracer.extractForwarded(""for=;""));
  }
",non-flaky,5
162623,open-telemetry_opentelemetry-java-instrumentation,HttpServerTracerTest.extractForwardedNoFor,"  @Test
  public void extractForwardedNoFor() {
    assertNull(HttpServerTracer.extractForwarded(""by=1.1.1.1;test=1.1.1.1""));
  }
",non-flaky,5
162624,open-telemetry_opentelemetry-java-instrumentation,HttpServerTracerTest.extractForwardedMultiple,"  @Test
  public void extractForwardedMultiple() {
    assertEquals(""1.1.1.1"", HttpServerTracer.extractForwarded(""for=1.1.1.1;for=1.2.3.4""));
  }
",non-flaky,5
162625,open-telemetry_opentelemetry-java-instrumentation,HttpServerTracerTest.extractForwardedMultipleIpV6,"  @Test
  public void extractForwardedMultipleIpV6() {
    assertEquals(
        ""1111:1111:1111:1111:1111:1111:1111:1111"",
        HttpServerTracer.extractForwarded(
            ""for=\""[1111:1111:1111:1111:1111:1111:1111:1111]\"";for=1.2.3.4""));
  }
",non-flaky,5
162626,open-telemetry_opentelemetry-java-instrumentation,HttpServerTracerTest.extractForwardedMultipleWithPort,"  @Test
  public void extractForwardedMultipleWithPort() {
    assertEquals(""1.1.1.1"", HttpServerTracer.extractForwarded(""for=\""1.1.1.1:2222\"";for=1.2.3.4""));
  }
",non-flaky,5
162627,open-telemetry_opentelemetry-java-instrumentation,HttpServerTracerTest.extractForwardedMultipleIpV6WithPort,"  @Test
  public void extractForwardedMultipleIpV6WithPort() {
    assertEquals(
        ""1111:1111:1111:1111:1111:1111:1111:1111"",
        HttpServerTracer.extractForwarded(
            ""for=\""[1111:1111:1111:1111:1111:1111:1111:1111]:2222\"";for=1.2.3.4""));
  }
",non-flaky,5
162628,open-telemetry_opentelemetry-java-instrumentation,HttpServerTracerTest.extractForwardedMixedSplitter,"  @Test
  public void extractForwardedMixedSplitter() {
    assertEquals(
        ""1.1.1.1"",
        HttpServerTracer.extractForwarded(""test=abcd; by=1.2.3.4, for=1.1.1.1;for=1.2.3.4""));
  }
",non-flaky,5
162629,open-telemetry_opentelemetry-java-instrumentation,HttpServerTracerTest.extractForwardedMixedSplitterIpv6,"  @Test
  public void extractForwardedMixedSplitterIpv6() {
    assertEquals(
        ""1111:1111:1111:1111:1111:1111:1111:1111"",
        HttpServerTracer.extractForwarded(
            ""test=abcd; by=1.2.3.4, for=\""[1111:1111:1111:1111:1111:1111:1111:1111]\"";for=1.2.3.4""));
  }
",non-flaky,5
162630,open-telemetry_opentelemetry-java-instrumentation,HttpServerTracerTest.extractForwardedMixedSplitterWithPort,"  @Test
  public void extractForwardedMixedSplitterWithPort() {
    assertEquals(
        ""1.1.1.1"",
        HttpServerTracer.extractForwarded(
            ""test=abcd; by=1.2.3.4, for=\""1.1.1.1:2222\"";for=1.2.3.4""));
  }
",non-flaky,5
162631,open-telemetry_opentelemetry-java-instrumentation,HttpServerTracerTest.extractForwardedMixedSplitterIpv6WithPort,"  @Test
  public void extractForwardedMixedSplitterIpv6WithPort() {
    assertEquals(
        ""1111:1111:1111:1111:1111:1111:1111:1111"",
        HttpServerTracer.extractForwarded(
            ""test=abcd; by=1.2.3.4, for=\""[1111:1111:1111:1111:1111:1111:1111:1111]:2222\"";for=1.2.3.4""));
  }
",non-flaky,5
162632,open-telemetry_opentelemetry-java-instrumentation,HttpServerTracerTest.extractForwardedFor,"  @Test
  public void extractForwardedFor() {
    assertEquals(""1.1.1.1"", HttpServerTracer.extractForwardedFor(""1.1.1.1""));
  }
",non-flaky,5
162633,open-telemetry_opentelemetry-java-instrumentation,HttpServerTracerTest.extractForwardedForIpv6,"  @Test
  public void extractForwardedForIpv6() {
    assertEquals(
        ""1111:1111:1111:1111:1111:1111:1111:1111"",
        HttpServerTracer.extractForwardedFor(""\""[1111:1111:1111:1111:1111:1111:1111:1111]\""""));
  }
",non-flaky,5
162634,open-telemetry_opentelemetry-java-instrumentation,HttpServerTracerTest.extractForwardedForIpv6Unquoted,"  @Test
  public void extractForwardedForIpv6Unquoted() {
    assertEquals(
        ""1111:1111:1111:1111:1111:1111:1111:1111"",
        HttpServerTracer.extractForwardedFor(""[1111:1111:1111:1111:1111:1111:1111:1111]""));
  }
",non-flaky,5
162635,open-telemetry_opentelemetry-java-instrumentation,HttpServerTracerTest.extractForwardedForIpv6Unbracketed,"  @Test
  public void extractForwardedForIpv6Unbracketed() {
    assertEquals(
        ""1111:1111:1111:1111:1111:1111:1111:1111"",
        HttpServerTracer.extractForwardedFor(""1111:1111:1111:1111:1111:1111:1111:1111""));
  }
",non-flaky,5
162636,open-telemetry_opentelemetry-java-instrumentation,HttpServerTracerTest.extractForwardedForWithPort,"  @Test
  public void extractForwardedForWithPort() {
    assertEquals(""1.1.1.1"", HttpServerTracer.extractForwardedFor(""1.1.1.1:2222""));
  }
",non-flaky,5
162637,open-telemetry_opentelemetry-java-instrumentation,HttpServerTracerTest.extractForwardedForIpv6WithPort,"  @Test
  public void extractForwardedForIpv6WithPort() {
    assertEquals(
        ""1111:1111:1111:1111:1111:1111:1111:1111"",
        HttpServerTracer.extractForwardedFor(""\""[1111:1111:1111:1111:1111:1111:1111:1111]:2222\""""));
  }
",non-flaky,5
162638,open-telemetry_opentelemetry-java-instrumentation,HttpServerTracerTest.extractForwardedForIpv6UnquotedWithPort,"  @Test
  public void extractForwardedForIpv6UnquotedWithPort() {
    assertEquals(
        ""1111:1111:1111:1111:1111:1111:1111:1111"",
        HttpServerTracer.extractForwardedFor(""[1111:1111:1111:1111:1111:1111:1111:1111]:2222""));
  }
",non-flaky,5
162639,open-telemetry_opentelemetry-java-instrumentation,HttpServerTracerTest.extractForwardedForEmpty,"  @Test
  public void extractForwardedForEmpty() {
    assertNull(HttpServerTracer.extractForwardedFor(""""));
  }
",non-flaky,5
162640,open-telemetry_opentelemetry-java-instrumentation,HttpServerTracerTest.extractForwardedForMultiple,"  @Test
  public void extractForwardedForMultiple() {
    assertEquals(""1.1.1.1"", HttpServerTracer.extractForwardedFor(""1.1.1.1,1.2.3.4""));
  }
",non-flaky,5
162641,open-telemetry_opentelemetry-java-instrumentation,HttpServerTracerTest.extractForwardedForMultipleIpv6,"  @Test
  public void extractForwardedForMultipleIpv6() {
    assertEquals(
        ""1111:1111:1111:1111:1111:1111:1111:1111"",
        HttpServerTracer.extractForwardedFor(
            ""\""[1111:1111:1111:1111:1111:1111:1111:1111]\"",1.2.3.4""));
  }
",non-flaky,5
162642,open-telemetry_opentelemetry-java-instrumentation,HttpServerTracerTest.extractForwardedForMultipleIpv6Unquoted,"  @Test
  public void extractForwardedForMultipleIpv6Unquoted() {
    assertEquals(
        ""1111:1111:1111:1111:1111:1111:1111:1111"",
        HttpServerTracer.extractForwardedFor(""[1111:1111:1111:1111:1111:1111:1111:1111],1.2.3.4""));
  }
",non-flaky,5
162643,open-telemetry_opentelemetry-java-instrumentation,HttpServerTracerTest.extractForwardedForMultipleIpv6Unbracketed,"  @Test
  public void extractForwardedForMultipleIpv6Unbracketed() {
    assertEquals(
        ""1111:1111:1111:1111:1111:1111:1111:1111"",
        HttpServerTracer.extractForwardedFor(""1111:1111:1111:1111:1111:1111:1111:1111,1.2.3.4""));
  }
",non-flaky,5
162644,open-telemetry_opentelemetry-java-instrumentation,HttpServerTracerTest.extractForwardedForMultipleWithPort,"  @Test
  public void extractForwardedForMultipleWithPort() {
    assertEquals(""1.1.1.1"", HttpServerTracer.extractForwardedFor(""1.1.1.1:2222,1.2.3.4""));
  }
",non-flaky,5
162645,open-telemetry_opentelemetry-java-instrumentation,HttpServerTracerTest.extractForwardedForMultipleIpv6WithPort,"  @Test
  public void extractForwardedForMultipleIpv6WithPort() {
    assertEquals(
        ""1111:1111:1111:1111:1111:1111:1111:1111"",
        HttpServerTracer.extractForwardedFor(
            ""\""[1111:1111:1111:1111:1111:1111:1111:1111]:2222\"",1.2.3.4""));
  }
",non-flaky,5
162646,open-telemetry_opentelemetry-java-instrumentation,HttpServerTracerTest.extractForwardedForMultipleIpv6UnquotedWithPort,"  @Test
  public void extractForwardedForMultipleIpv6UnquotedWithPort() {
    assertEquals(
        ""1111:1111:1111:1111:1111:1111:1111:1111"",
        HttpServerTracer.extractForwardedFor(
            ""[1111:1111:1111:1111:1111:1111:1111:1111]:2222,1.2.3.4""));
  }
",non-flaky,5
162647,open-telemetry_opentelemetry-java-instrumentation,NetServerAttributesExtractorTest.doesNotSetDuplicateAttributes,"  @Test
  public void doesNotSetDuplicateAttributes() {
    // given
    Map<String, String> request = new HashMap<>();
    request.put(""transport"", ""TCP"");
    request.put(""peerName"", ""1.2.3.4"");
    request.put(""peerIp"", ""1.2.3.4"");
    request.put(""peerPort"", ""123"");

    Map<String, String> response = new HashMap<>();
    response.put(""peerName"", ""4.3.2.1"");
    response.put(""peerPort"", ""42"");
    response.put(""peerIp"", ""4.3.2.1"");

    TestNetServerAttributesExtractor extractor = new TestNetServerAttributesExtractor();

    // when
    AttributesBuilder startAttributes = Attributes.builder();
    extractor.onStart(startAttributes, request);

    AttributesBuilder endAttributes = Attributes.builder();
    extractor.onEnd(endAttributes, request, response, null);

    // then
    assertThat(startAttributes.build())
        .containsOnly(
            entry(SemanticAttributes.NET_TRANSPORT, ""TCP""),
            entry(SemanticAttributes.NET_PEER_PORT, 123L),
            entry(SemanticAttributes.NET_PEER_IP, ""1.2.3.4""));

    assertThat(endAttributes.build()).isEmpty();
  }
",non-flaky,5
162648,open-telemetry_opentelemetry-java-instrumentation,NetServerAttributesExtractorTest.doesNotSetNegativePort,"  @Test
  public void doesNotSetNegativePort() {
    // given
    Map<String, String> request = new HashMap<>();
    request.put(""peerPort"", ""-42"");

    Map<String, String> response = new HashMap<>();
    response.put(""peerPort"", ""-1"");

    TestNetServerAttributesExtractor extractor = new TestNetServerAttributesExtractor();

    // when
    AttributesBuilder startAttributes = Attributes.builder();
    extractor.onStart(startAttributes, request);

    AttributesBuilder endAttributes = Attributes.builder();
    extractor.onEnd(endAttributes, request, response, null);

    // then
    assertThat(startAttributes.build()).isEmpty();
    assertThat(endAttributes.build()).isEmpty();
  }
",non-flaky,5
162649,open-telemetry_opentelemetry-java-instrumentation,NetClientAttributesExtractorTest.doesNotSetDuplicateAttributes,"  @Test
  public void doesNotSetDuplicateAttributes() {
    // given
    Map<String, String> request = new HashMap<>();
    request.put(""transport"", ""TCP"");
    request.put(""peerName"", ""1.2.3.4"");
    request.put(""peerIp"", ""1.2.3.4"");
    request.put(""peerPort"", ""123"");

    Map<String, String> response = new HashMap<>();
    response.put(""peerName"", ""4.3.2.1"");
    response.put(""peerPort"", ""42"");
    response.put(""peerIp"", ""4.3.2.1"");

    TestNetClientAttributesExtractor extractor = new TestNetClientAttributesExtractor();

    // when
    AttributesBuilder startAttributes = Attributes.builder();
    extractor.onStart(startAttributes, request);

    AttributesBuilder endAttributes = Attributes.builder();
    extractor.onEnd(endAttributes, request, response, null);

    // then
    assertThat(startAttributes.build()).isEmpty();

    assertThat(endAttributes.build())
        .containsOnly(
            entry(SemanticAttributes.NET_PEER_PORT, 42L),
            entry(SemanticAttributes.NET_PEER_IP, ""4.3.2.1""));
  }
",non-flaky,5
162650,open-telemetry_opentelemetry-java-instrumentation,NetClientAttributesExtractorTest.doesNotSetNegativePort,"  @Test
  public void doesNotSetNegativePort() {
    // given
    Map<String, String> request = new HashMap<>();
    request.put(""peerPort"", ""-42"");

    Map<String, String> response = new HashMap<>();
    response.put(""peerPort"", ""-1"");

    TestNetClientAttributesExtractor extractor = new TestNetClientAttributesExtractor();

    // when
    AttributesBuilder startAttributes = Attributes.builder();
    extractor.onStart(startAttributes, request);

    AttributesBuilder endAttributes = Attributes.builder();
    extractor.onEnd(endAttributes, request, response, null);

    // then
    assertThat(startAttributes.build()).isEmpty();
    assertThat(endAttributes.build()).isEmpty();
  }
",non-flaky,5
162651,open-telemetry_opentelemetry-java-instrumentation,PropagatorBasedSpanLinksExtractorTest.keys,"  @Test
    public Iterable<String> keys(Map<String, String> carrier) {
      return carrier.keySet();
    }
",non-flaky,5
162652,open-telemetry_opentelemetry-java-instrumentation,SpanSuppressionStrategyTest.serverSpan,"  @Test
  public void serverSpan() {
    // SpanKey.SERVER will never be passed to SpanSuppressionStrategy.from(), it cannot be
    // automatically determined by te builder - thus it does not make any sense to test it (for now)
    SpanSuppressionStrategy strategy = SpanSuppressionStrategy.from(emptySet());

    Context context = strategy.storeInContext(Context.root(), SpanKind.SERVER, SPAN);

    assertThat(strategy.shouldSuppress(context, SpanKind.SERVER)).isTrue();
    Stream.of(SpanKind.CLIENT, SpanKind.CONSUMER, SpanKind.PRODUCER)
        .forEach(spanKind -> assertThat(strategy.shouldSuppress(context, spanKind)).isFalse());

    verifySpanKey(SpanKey.SERVER, context);
  }
",non-flaky,5
162653,open-telemetry_opentelemetry-java-instrumentation,SpanSuppressionStrategyTest.consumerSpan,"  @ParameterizedTest
  public void consumerSpan(SpanKey spanKey) {
    SpanSuppressionStrategy strategy = SpanSuppressionStrategy.from(singleton(spanKey));

    verifyNoSuppression(strategy, Context.root());

    Context context = strategy.storeInContext(Context.root(), SpanKind.CONSUMER, SPAN);

    assertThat(strategy.shouldSuppress(context, SpanKind.SERVER)).isFalse();
    Stream.of(SpanKind.CLIENT, SpanKind.CONSUMER, SpanKind.PRODUCER)
        .forEach(spanKind -> assertThat(strategy.shouldSuppress(context, spanKind)).isTrue());

    verifySpanKey(spanKey, context);
  }
",non-flaky,5
162654,open-telemetry_opentelemetry-java-instrumentation,SpanSuppressionStrategyTest.clientSpan,"  @ParameterizedTest
  public void clientSpan(SpanKey spanKey) {
    SpanSuppressionStrategy strategy = SpanSuppressionStrategy.from(singleton(spanKey));

    verifyNoSuppression(strategy, Context.root());

    Context context = strategy.storeInContext(Context.root(), SpanKind.CLIENT, SPAN);

    assertThat(strategy.shouldSuppress(context, SpanKind.SERVER)).isFalse();
    Stream.of(SpanKind.CLIENT, SpanKind.CONSUMER, SpanKind.PRODUCER)
        .forEach(spanKind -> assertThat(strategy.shouldSuppress(context, spanKind)).isTrue());

    verifySpanKey(spanKey, context);
  }
",non-flaky,5
162655,open-telemetry_opentelemetry-java-instrumentation,SpanSuppressionStrategyTest.producerSpan,"  @Test
  public void producerSpan() {
    SpanSuppressionStrategy strategy = SpanSuppressionStrategy.from(singleton(SpanKey.PRODUCER));

    verifyNoSuppression(strategy, Context.root());

    Context context = strategy.storeInContext(Context.root(), SpanKind.PRODUCER, SPAN);

    assertThat(strategy.shouldSuppress(context, SpanKind.SERVER)).isFalse();
    Stream.of(SpanKind.CLIENT, SpanKind.CONSUMER, SpanKind.PRODUCER)
        .forEach(spanKind -> assertThat(strategy.shouldSuppress(context, spanKind)).isTrue());

    verifySpanKey(SpanKey.PRODUCER, context);
  }
",non-flaky,5
176832,OryxProject_oryx,RandomManagerTest.testRandomState,"  @Test
  public void testRandomState() {
    // Really, a test that the random generator state is reset in tests
    RandomGenerator generator = RandomManager.getRandom();
    assertEquals(1553355631, generator.nextInt());
    assertNotEquals(1553355631, generator.nextInt());
  }
",non-flaky,5
176833,OryxProject_oryx,RandomManagerRandomTest.testRandomState,"  @Test
  public void testRandomState() {
    RandomGenerator generator = RandomManager.getRandom();
    double unseededValue = generator.nextDouble();
    RandomManager.useTestSeed();
    double seededValue = generator.nextDouble();
    assertNotEquals(unseededValue, seededValue);
    assertEquals(seededValue, RandomManager.getRandom().nextDouble());
  }
",non-flaky,5
176834,OryxProject_oryx,LinearSystemSolverTest.testSolveFToD,"  @Test
  public void testSolveFToD() {
    RealMatrix a = new Array2DRowRealMatrix(new double[][] {
        {1.3, -2.0, 3.0},
        {2.0, 0.0, 5.0},
        {0.0, -1.5, 5.5},
    });
    Solver solver = new LinearSystemSolver().getSolver(a);
    assertNotNull(solver);
    double[] y = solver.solveFToD(new float[] {1.0f, 2.0f, 6.5f});
    assertArrayEquals(
        new double[] {-1.9560439560439564,0.002197802197802894,1.1824175824175824}, y);
  }
",non-flaky,5
176835,OryxProject_oryx,LinearSystemSolverTest.testSolveDToD,"  @Test
  public void testSolveDToD() {
    RealMatrix a = new Array2DRowRealMatrix(new double[][] {
        {1.3, -2.0, 3.0},
        {2.0, 0.0, 5.0},
        {0.0, -1.5, 5.5},
    });
    Solver solver = new LinearSystemSolver().getSolver(a);
    assertNotNull(solver);
    double[] y = solver.solveDToD(new double[]{1.0, 2.0, 6.5});
    assertArrayEquals(
        new double[] {-1.9560439560439564,0.002197802197802894,1.1824175824175824}, y);
  }
",non-flaky,5
176836,OryxProject_oryx,LinearSystemSolverTest.testIsNonSingular,"  @Test
  public void testIsNonSingular() {
    RealMatrix nonSingular = new Array2DRowRealMatrix(new double[][] {
        {1.3, -2.0, 3.0},
        {2.0, 0.0, 5.0},
        {0.0, -1.5, 5.5},
    });
    assertTrue(new LinearSystemSolver().isNonSingular(nonSingular));
    RealMatrix singular = new Array2DRowRealMatrix(new double[][] {
        {1.3, -2.0, 3.0},
        {2.6, -4.0, 6.0},
        {0.0, -1.5, 5.5},
    });
    assertFalse(new LinearSystemSolver().isNonSingular(singular));
  }
",non-flaky,5
176837,OryxProject_oryx,LinearSystemSolverTest.testApparentRank,"  @Test
  public void testApparentRank() {
    RealMatrix nearSingular = new Array2DRowRealMatrix(new double[][] {
        {1.31, -2.0, 3.0},
        {2.6, -4.01, 6.01},
        {0.0, -1.5, 5.5},
    });
    try {
      new LinearSystemSolver().getSolver(nearSingular);
    } catch (SingularMatrixSolverException smse) {
      assertEquals(2, smse.getApparentRank());
    }
  }
",non-flaky,5
176838,OryxProject_oryx,VectorMathTest.testDotFF,"  @Test
  public void testDotFF() {
    assertEquals(5.35, VectorMath.dot(VEC1, VEC2), FLOAT_EPSILON);
  }
",non-flaky,5
176839,OryxProject_oryx,VectorMathTest.testDotDF,"  @Test
  public void testDotDF() {
    assertEquals(5.35, VectorMath.dot(VEC1D, VEC2), FLOAT_EPSILON);
  }
",non-flaky,5
176840,OryxProject_oryx,VectorMathTest.testToFloats,"  @Test
  public void testToFloats() {
    assertArrayEquals(new float[] {1.2f}, VectorMath.toFloats(1.2), FLOAT_EPSILON);
  }
",non-flaky,5
176841,OryxProject_oryx,VectorMathTest.testToDoubles,"  @Test
  public void testToDoubles() {
    assertArrayEquals(new double[] {1.2}, VectorMath.toDoubles(1.2f), FLOAT_EPSILON);
  }
",non-flaky,5
176842,OryxProject_oryx,VectorMathTest.testParseVector,"  @Test
  public void testParseVector() {
    assertArrayEquals(
        new double[] {-1.0, 2.01, 3.5},
        VectorMath.parseVector(new String[] {""-1.0"", ""2.01"", ""3.5""}));
  }
",non-flaky,5
176843,OryxProject_oryx,VectorMathTest.testSmall,"  @Test
  public void testSmall() {
    float[] a = { 1.0e-24f };
    assertEquals(1.0e-24 * 1.0e-24, VectorMath.dot(a, a));
  }
",non-flaky,5
176844,OryxProject_oryx,VectorMathTest.testBig,"  @Test
  public void testBig() {
    float[] a = { 1.0e20f };
    assertEquals((double) 1.0e20f * (double) 1.0e20f, VectorMath.dot(a, a));
  }
",non-flaky,5
176845,OryxProject_oryx,VectorMathTest.testNorm,"  @Test
  public void testNorm() {
    assertEquals(0.0, VectorMath.norm(new float[] {0.0f}), FLOAT_EPSILON);
    assertEquals(3.674234614174767, VectorMath.norm(VEC1), FLOAT_EPSILON);
    assertEquals(10.72800074571213, VectorMath.norm(VEC2), FLOAT_EPSILON);
  }
",non-flaky,5
176846,OryxProject_oryx,VectorMathTest.testTransposeTimesSelf,"  @Test
  public void testTransposeTimesSelf() {
    Map<Integer,float[]> a = new HashMap<>();
    a.put(-1, new float[] {1.3f, -2.0f, 3.0f});
    a.put(1, new float[] {2.0f, 0.0f, 5.0f});
    a.put(3, new float[] {0.0f, -1.5f, 5.5f});
    RealMatrix ata = VectorMath.transposeTimesSelf(a.values());
    RealMatrix expected = new Array2DRowRealMatrix(new double[][] {
        {5.69, -2.6, 13.9},
        {-2.6, 6.25, -14.25},
        {13.9, -14.25, 64.25}
    });
    for (int row = 0; row < 3; row++) {
      for (int col = 0; col < 3; col++) {
        assertEquals(expected.getEntry(row, col), ata.getEntry(row, col), FLOAT_EPSILON);
      }
    }
  }
",non-flaky,5
176847,OryxProject_oryx,DoubleWeightedMeanTest.testNone,"  @Test
  public void testNone() {
    DoubleWeightedMean mean = new DoubleWeightedMean();
    assertEquals(0, mean.getN());
    assertTrue(Double.isNaN(mean.getResult()));
  }
",non-flaky,5
176848,OryxProject_oryx,DoubleWeightedMeanTest.testOne,"  @Test
  public void testOne() {
    DoubleWeightedMean mean = new DoubleWeightedMean();
    mean.increment(1.5);
    assertEquals(1, mean.getN());
    assertEquals(1.5, mean.getResult());
    assertEquals(""1.5"", mean.toString());
  }
",non-flaky,5
176849,OryxProject_oryx,DoubleWeightedMeanTest.testWeighted,"  @Test
  public void testWeighted() {
    DoubleWeightedMean mean = new DoubleWeightedMean();
    mean.increment(0.2, 4.0);
    mean.increment(-0.1, 2.0);
    assertEquals(2, mean.getN());
    assertEquals(0.1, mean.getResult());
  }
",non-flaky,5
176850,OryxProject_oryx,DoubleWeightedMeanTest.testNegative,"  @Test
  public void testNegative() {
    DoubleWeightedMean mean = new DoubleWeightedMean();
    mean.increment(-0.1, 2.1);
    mean.increment(0.1, 2.1);
    assertEquals(2, mean.getN());
    assertEquals(0.0, mean.getResult());
  }
",non-flaky,5
176851,OryxProject_oryx,DoubleWeightedMeanTest.testComplex,"  @Test
  public void testComplex() {
    DoubleWeightedMean mean = new DoubleWeightedMean();
    for (int i = 1; i <= 5; i++) {
      mean.increment(1.0 / (i + 1), i);
    }
    assertEquals(5, mean.getN());
    assertEquals((1.0/2.0 + 2.0/3.0 + 3.0/4.0 + 4.0/5.0 + 5.0/6.0) / 15.0, mean.getResult());
  }
",non-flaky,5
176852,OryxProject_oryx,DoubleWeightedMeanTest.testCopyEquals,"  @Test
  public void testCopyEquals() {
    DoubleWeightedMean mean = new DoubleWeightedMean();
    mean.increment(0.2, 4.0);
    mean.increment(-0.1, 2.0);
    DoubleWeightedMean copy = mean.copy();
    assertEquals(copy, mean);
    assertEquals(copy.hashCode(), mean.hashCode());
    DoubleWeightedMean zero = new DoubleWeightedMean();
    mean.clear();
    assertEquals(zero, mean);
  }
",non-flaky,5
176853,OryxProject_oryx,JVMUtilsTest.close,"  @Test
  public void testShutdownHook() {
    // Can't really test this except to verify that no exception is thrown now or at shutdown
    JVMUtils.closeAtShutdown(new Closeable() {
      @Override
      public void close() {
        // do nothing
      }
",non-flaky,5
176854,OryxProject_oryx,JVMUtilsTest.testUsedMemory,"  @Test
  public void testUsedMemory() {
    // Reasonable guess
    assertTrue(JVMUtils.getUsedMemory() >= 1L << 20);
  }
",non-flaky,5
176855,OryxProject_oryx,LoggingTest.doRun,"  @Test(expected = IllegalStateException.class)
  public void testLoggingRunnableException() {
    new LoggingRunnable() {
      @Override
      public void doRun() throws IOException {
        throw buildIOE();
      }
",non-flaky,5
176856,OryxProject_oryx,LoggingTest.doCall,"  @Test
  public void testLoggingCallable() {
    Integer result = new LoggingCallable<Integer>() {
      @Override
      public Integer doCall() {
        return 3;
      }
",non-flaky,5
176857,OryxProject_oryx,LoggingTest.doCall,"  @Test(expected = IllegalStateException.class)
  public void testLoggingCallableException() {
    new LoggingCallable<Void>() {
      @Override
      public Void doCall() throws IOException {
        throw buildIOE();
      }
",non-flaky,5
176858,OryxProject_oryx,LoggingTest.doCall,"  @Test(expected = IllegalStateException.class)
  public void testLoggingVoidCallableException() {
    new LoggingVoidCallable() {
      @Override
      public void doCall() throws IOException {
        throw buildIOE();
      }
",non-flaky,5
176859,OryxProject_oryx,LangUtilsTest.testHashDouble,"  @Test
  public void testHashDouble() {
    for (int i = 0; i < 1000; i++) {
      assertEquals(Double.valueOf(i).hashCode(), LangUtils.hashDouble(i));
    }
  }
",non-flaky,5
176860,OryxProject_oryx,AutoLockTest.testClose,"  @Test
  public void testClose() {
    ReentrantLock lock = new ReentrantLock();
    assertFalse(lock.isHeldByCurrentThread());
    AutoLock al = new AutoLock(lock);
    assertTrue(lock.isHeldByCurrentThread());
    al.close();
    assertFalse(lock.isHeldByCurrentThread());
  }
",non-flaky,5
176861,OryxProject_oryx,AutoLockTest.testAutoClose,"  @Test
  public void testAutoClose() {
    ReentrantLock lock = new ReentrantLock();
    assertFalse(lock.isHeldByCurrentThread());
    try (AutoLock al = new AutoLock(lock)) {
      assertTrue(lock.isHeldByCurrentThread());
    }
    assertFalse(lock.isHeldByCurrentThread());
  }
",non-flaky,5
176862,OryxProject_oryx,ClassUtilsTest.testLoadClass,"  @Test
  public void testLoadClass() {
    assertSame(ArrayList.class, ClassUtils.loadClass(ArrayList.class.getName()));
  }
",non-flaky,5
176863,OryxProject_oryx,ClassUtilsTest.testLoadClass2,"  @Test
  public void testLoadClass2() {
    assertSame(ArrayList.class, ClassUtils.loadClass(ArrayList.class.getName(), List.class));
  }
",non-flaky,5
176864,OryxProject_oryx,ClassUtilsTest.testLoadInstanceOf,"  @Test
  public void testLoadInstanceOf() {
    assertTrue(ClassUtils.loadInstanceOf(HashSet.class) instanceof HashSet);
  }
",non-flaky,5
176865,OryxProject_oryx,ClassUtilsTest.testLoadInstanceOf2,"  @Test
  public void testLoadInstanceOf2() {
    assertTrue(ClassUtils.loadInstanceOf(HashSet.class.getName(), Set.class) instanceof HashSet);
  }
",non-flaky,5
176866,OryxProject_oryx,ClassUtilsTest.testInstantiateWithArgs,"  @Test
  public void testInstantiateWithArgs() {
    Number n = ClassUtils.loadInstanceOf(Integer.class.getName(),
        Number.class,
        new Class<?>[]{int.class},
        new Object[]{3});
    assertEquals(3, n.intValue());
  }
",non-flaky,5
176867,OryxProject_oryx,ClassUtilsTest.testNoSuchMethod,"  @Test(expected = IllegalArgumentException.class)
  public void testNoSuchMethod() {
    ClassUtils.loadInstanceOf(Long.class.getName(), Long.class);
  }
",non-flaky,5
176868,OryxProject_oryx,ClassUtilsTest.tesInvocationException,"  @Test(expected = IllegalStateException.class)
  public void tesInvocationException() {
    ClassUtils.loadInstanceOf(String.class.getName(),
                              String.class,
                              new Class<?>[] { char[].class },
                              new Object[] { null });
  }
",non-flaky,5
176869,OryxProject_oryx,ClassUtilsTest.testExists,"  @Test
  public void testExists() {
    assertTrue(ClassUtils.classExists(""java.lang.String""));
    assertTrue(ClassUtils.classExists(""com.cloudera.oryx.common.lang.ClassUtils""));
    assertFalse(ClassUtils.classExists(""java.Foo""));
  }
",non-flaky,5
176870,OryxProject_oryx,PMMLUtilsTest.testSkeleton,"  @Test
  public void testSkeleton() {
    PMML pmml = PMMLUtils.buildSkeletonPMML();
    assertEquals(""Oryx"", pmml.getHeader().getApplication().getName());
    assertNotNull(pmml.getHeader().getTimestamp());
  }
",non-flaky,5
176871,OryxProject_oryx,PMMLUtilsTest.testReadWrite,"  @Test
  public void testReadWrite() throws Exception {
    Path tempModelFile = Files.createTempFile(getTempDir(), ""model"", "".pmml.gz"");
    PMML model = buildDummyModel();
    PMMLUtils.write(model, tempModelFile);
    assertTrue(Files.exists(tempModelFile));
    PMML model2 = PMMLUtils.read(tempModelFile);
    List<Model> models = model2.getModels();
    assertEquals(1, models.size());
    assertTrue(models.get(0) instanceof TreeModel);
    TreeModel treeModel = (TreeModel) models.get(0);
    assertEquals(123.0, treeModel.getNode().getRecordCount().doubleValue());
    assertEquals(MiningFunctionType.CLASSIFICATION, treeModel.getFunctionName());
  }
",non-flaky,5
176872,OryxProject_oryx,PMMLUtilsTest.testToString,"  @Test
  public void testToString() throws Exception {
    PMML model = buildDummyModel();
    model.getHeader().setTimestamp(null);
    assertEquals(""<?xml version=\""1.0\"" encoding=\""UTF-8\"" standalone=\""yes\""?>\n"" +
                 ""<PMML version=\""4.2.1\"" xmlns=\""http://www.dmg.org/PMML-4_2\"">\n"" +
                 ""    <Header>\n"" +
                 ""        <Application name=\""Oryx\""/>\n"" +
                 ""    </Header>\n"" +
                 ""    <TreeModel functionName=\""classification\"">\n"" +
                 ""        <Node recordCount=\""123.0\""/>\n"" +
                 ""    </TreeModel>\n"" +
                 ""</PMML>\n"",
                 PMMLUtils.toString(model));
  }
",non-flaky,5
176873,OryxProject_oryx,PMMLUtilsTest.testFromString,"  @Test
  public void testFromString() throws Exception {
    PMML model = buildDummyModel();
    PMML model2 = PMMLUtils.fromString(PMMLUtils.toString(model));
    assertEquals(model.getHeader().getApplication().getName(),
                 model2.getHeader().getApplication().getName());
    assertEquals(model.getModels().get(0).getFunctionName(),
                 model2.getModels().get(0).getFunctionName());
  }
",non-flaky,5
176874,OryxProject_oryx,IOUtilsTest.testDeleteRecursively,"  @Test
  public void testDeleteRecursively() throws IOException {
    Path testDir = createTestDirs();
    IOUtils.deleteRecursively(testDir);
    assertFalse(Files.exists(testDir));
    assertFalse(Files.exists(testDir.resolve(""subFile1"")));
  }
",non-flaky,5
176875,OryxProject_oryx,IOUtilsTest.testListFiles,"  @Test
  public void testListFiles() throws IOException {
    Path testDir = createTestDirs();
    List<Path> files = IOUtils.listFiles(testDir, ""*"");
    assertEquals(2, files.size());
    assertTrue(files.contains(testDir.resolve(""subFile1"")));
    assertFalse(files.contains(testDir.resolve("".hidden"")));
    assertTrue(files.contains(testDir.resolve(""subDir1"")));
  }
",non-flaky,5
176876,OryxProject_oryx,IOUtilsTest.testListFiles2,"  @Test
  public void testListFiles2() throws IOException {
    Path testDir = createTestDirs();
    List<Path> files = IOUtils.listFiles(testDir, """");
    assertEquals(2, files.size());
    assertTrue(files.contains(testDir.resolve(""subFile1"")));
    assertFalse(files.contains(testDir.resolve("".hidden"")));
    assertTrue(files.contains(testDir.resolve(""subDir1"")));
  }
",non-flaky,5
176877,OryxProject_oryx,IOUtilsTest.testListSubdirs,"  @Test
  public void testListSubdirs() throws IOException {
    Path testDir = createTestDirs();
    List<Path> files = IOUtils.listFiles(testDir, ""*/*"");
    assertEquals(2, files.size());
    assertTrue(files.contains(testDir.resolve(""subDir1"").resolve(""subFile2"")));
    assertTrue(files.contains(testDir.resolve(""subDir1"").resolve(""subDir2"")));
  }
",non-flaky,5
176878,OryxProject_oryx,IOUtilsTest.testListSubdirs2,"  @Test
  public void testListSubdirs2() throws IOException {
    Path testDir = createTestDirs();
    List<Path> files = IOUtils.listFiles(testDir, ""*/subFile*"");
    assertEquals(1, files.size());
    assertTrue(files.contains(testDir.resolve(""subDir1"").resolve(""subFile2"")));
  }
",non-flaky,5
176879,OryxProject_oryx,IOUtilsTest.testOrder,"  @Test
  public void testOrder() throws IOException {
    Path testDir = createTestDirs();
    List<Path> files = IOUtils.listFiles(testDir, ""*/*"");
    assertEquals(testDir.resolve(""subDir1"").resolve(""subDir2""), files.get(0));
    assertEquals(testDir.resolve(""subDir1"").resolve(""subFile2""), files.get(1));
  }
",non-flaky,5
176880,OryxProject_oryx,IOUtilsTest.testReadLines,"  @Test
  public void testReadLines() throws IOException {
    Path tempDir = getTempDir();
    Path textFile = tempDir.resolve(""file.txt"");
    Files.write(textFile, Arrays.asList(""foo"", ""bar"", ""baz""), StandardCharsets.UTF_8);
    Iterator<String> it = IOUtils.readLines(textFile).iterator();
    assertTrue(it.hasNext());
    assertEquals(""foo"", it.next());
    assertTrue(it.hasNext());
    assertEquals(""bar"", it.next());
    assertTrue(it.hasNext());
    assertEquals(""baz"", it.next());
    assertFalse(it.hasNext());
  }
",non-flaky,5
176881,OryxProject_oryx,IOUtilsTest.testChooseFreePort,"  @Test
  public void testChooseFreePort() throws IOException {
    int freePort = IOUtils.chooseFreePort();
    assertTrue(freePort >= 1024 && freePort < 65536);
    try (ServerSocket socket = new ServerSocket(freePort, 0)) {
      assertEquals(freePort, socket.getLocalPort());
    }
  }
",non-flaky,5
176882,OryxProject_oryx,IOUtilsTest.testDistinctFreePorts,"  @Test
  public void testDistinctFreePorts() throws IOException {
    // This whole thing probably won't work unless successive calls really do return
    // different ports instead of reusing free ephemeral ports.
    Set<Integer> ports = new HashSet<>();
    for (int i = 0; i < 10; i++) {
      ports.add(IOUtils.chooseFreePort());
    }
    assertEquals(10, ports.size());
  }
",non-flaky,5
176883,OryxProject_oryx,ConfigUtilsTest.testDefaultConfig,"  @Test
  public void testDefaultConfig() {
    Config config = ConfigUtils.getDefault();
    assertEquals(""yarn-client"", config.getString(""oryx.batch.streaming.master""));
  }
",non-flaky,5
176884,OryxProject_oryx,ConfigUtilsTest.testSerialize,"  @Test
  public void testSerialize() {
    String serialized = ConfigUtils.serialize(ConfigUtils.getDefault());
    assertTrue(serialized.contains(""update-class""));
    Config deserialized = ConfigUtils.deserialize(serialized);
    assertEquals(
        ConfigUtils.getDefault().getString(""oryx.serving.api.port""),
        deserialized.getString(""oryx.serving.api.port""));
  }
",non-flaky,5
176885,OryxProject_oryx,ConfigUtilsTest.testOptionalString,"  @Test
  public void testOptionalString() {
    assertNull(ConfigUtils.getOptionalString(ConfigUtils.getDefault(), ""nonexistent""));
  }
",non-flaky,5
176886,OryxProject_oryx,ConfigUtilsTest.testOptionalStringList,"  @Test
  public void testOptionalStringList() {
    assertNull(ConfigUtils.getOptionalStringList(ConfigUtils.getDefault(), ""nonexistent""));
  }
",non-flaky,5
176887,OryxProject_oryx,ConfigUtilsTest.testOverlayOn,"  @Test
  public void testOverlayOn() {
    Map<String,Object> overlay = new HashMap<>();
    overlay.put(""foo"", ""bar"");
    Config config = ConfigUtils.overlayOn(overlay, ConfigUtils.getDefault());
    assertEquals(""bar"", config.getString(""foo""));
  }
",non-flaky,5
176888,OryxProject_oryx,ConfigUtilsTest.testSetPath,"  @Test
  public void testSetPath() throws Exception {
    Path cwd = Paths.get(""."");
    Map<String,Object> map = new HashMap<>();
    ConfigUtils.set(map, ""cwd"", cwd);
    ConfigUtils.set(map, ""temp"", Paths.get(""/tmp""));
    assertEquals(""\"""" + cwd.toRealPath(LinkOption.NOFOLLOW_LINKS).toUri() + ""\"""", map.get(""cwd""));
    assertEquals(""\""file:///tmp/\"""", map.get(""temp""));
  }
",non-flaky,5
176889,OryxProject_oryx,ConfigUtilsTest.testRedact,"  @Test
  public void testRedact() {
    String redacted = ConfigUtils.redact(""  password=foo \nPassword=foo\nPASSWORD = foo\n"" +
                                             "" the-password= foo \nThe-Password =foo"");
    assertFalse(redacted.contains(""foo""));
    assertTrue(redacted.contains(""*****""));
    assertTrue(redacted.contains(""password=""));
    assertTrue(redacted.contains(""Password=""));
    assertTrue(redacted.contains(""PASSWORD = ""));
    assertTrue(redacted.contains(""the-password= ""));
    assertTrue(redacted.contains(""The-Password =""));
  }
",non-flaky,5
176890,OryxProject_oryx,PairTest.testEquals,"  @Test
  public void testEquals() {
    assertEquals(new Pair<>(3.0, ""foo""), new Pair<>(3.0, ""foo""));
    assertEquals(new Pair<>(null, null), new Pair<>(null, null));
    assertFalse(new Pair<>(3.0, ""foo"").equals(new Pair<>(4.0, ""foo"")));
    assertNotEquals(new Pair<>(3.0, ""foo""), new Pair<>(""foo"", 3.0));
    assertNotEquals(""3.0,foo"", new Pair<>(3.0, ""foo""));
  }
",non-flaky,5
176891,OryxProject_oryx,PairTest.testHashCode,"  @Test
  public void testHashCode() {
    assertEquals(new Pair<>(3.0, ""foo"").hashCode(), new Pair<>(3.0, ""foo"").hashCode());
    assertEquals(new Pair<>(null, null).hashCode(), new Pair<>(null, null).hashCode());
  }
",non-flaky,5
176892,OryxProject_oryx,PairTest.testToString,"  @Test
  public void testToString() {
    assertEquals(""3.0,foo"", new Pair<>(3.0, ""foo"").toString());
  }
",non-flaky,5
176893,OryxProject_oryx,AndPredicateTest.testAnd,"  @Test
  public void testAnd() {
    NotContainsPredicate<String> a = new NotContainsPredicate<>(Arrays.asList(""foo""));
    NotContainsPredicate<String> b = new NotContainsPredicate<>(Arrays.asList(""bar"", ""baz""));
    AndPredicate<String> and = new AndPredicate<>(a, b);
    assertFalse(and.test(""foo""));
    assertFalse(and.test(""bar""));
    assertFalse(and.test(""baz""));
    assertTrue(and.test(""bing""));
  }
",non-flaky,5
176894,OryxProject_oryx,NotContainsPredicateTest.testPredicate,"  @Test
  public void testPredicate() {
    Collection<Integer> contains = Arrays.asList(1, 3, 5);
    NotContainsPredicate<Integer> predicate = new NotContainsPredicate<>(contains);
    assertTrue(predicate.test(2));
    assertFalse(predicate.test(5));
  }
",non-flaky,5
176895,OryxProject_oryx,PairComparatorsTest.testByFirst,"  @Test
  public void testByFirst() {
    List<Pair<Integer,String>> pairs = Arrays.asList(
        new Pair<>(3, ""foo""),
        new Pair<>(4, ""bing""),
        new Pair<>(1, ""baz""),
        new Pair<>(2, ""whizz"")
    );
    Collections.sort(pairs, PairComparators.<Integer>byFirst());
    assertEquals(1, pairs.get(0).getFirst().intValue());
    assertEquals(2, pairs.get(1).getFirst().intValue());
    assertEquals(""baz"", pairs.get(0).getSecond());
    assertEquals(""whizz"", pairs.get(1).getSecond());
  }
",non-flaky,5
176896,OryxProject_oryx,PairComparatorsTest.testBySecond,"  @Test
  public void testBySecond() {
    List<Pair<Integer,String>> pairs = Arrays.asList(
        new Pair<>(3, ""foo""),
        new Pair<>(4, ""bing""),
        new Pair<>(1, ""baz""),
        new Pair<>(2, ""whizz"")
    );
    Collections.sort(pairs, PairComparators.<String>bySecond());
    assertEquals(1, pairs.get(0).getFirst().intValue());
    assertEquals(4, pairs.get(1).getFirst().intValue());
    assertEquals(""baz"", pairs.get(0).getSecond());
    assertEquals(""bing"", pairs.get(1).getSecond());
  }
",non-flaky,5
176897,OryxProject_oryx,KeyOnlyBiPredicateTest.test,"  @Test
  public void testKeyOnly() {
    ObjObjMap<String,String> map = HashObjObjMaps.newMutableMap(
        new String[]{""foo"", ""bar"", ""baz""},
        new String[]{""1"", ""3"", ""4""}
    );
    map.removeIf(new KeyOnlyBiPredicate<String, String>(new Predicate<String>() {
      @Override
      public boolean test(String s) {
        return s.startsWith(""b"");
      }
",non-flaky,5
176898,OryxProject_oryx,TextUtilsTest.testParseJSON,"  @Test
  public void testParseJSON() throws Exception {
    assertArrayEquals(new String[] {""a"", ""1"", ""foo""},
                      TextUtils.parseJSONArray(""[\""a\"",\""1\"",\""foo\""]""));
    assertArrayEquals(new String[] {""a"", ""1"", ""foo"", """"},
                      TextUtils.parseJSONArray(""[\""a\"",\""1\"",\""foo\"",\""\""]""));
    assertArrayEquals(new String[] {""2.3""}, TextUtils.parseJSONArray(""[\""2.3\""]""));
    assertArrayEquals(new String[] {}, TextUtils.parseJSONArray(""[]""));
  }
",non-flaky,5
176899,OryxProject_oryx,TextUtilsTest.testParseDelimited,"  @Test
  public void testParseDelimited() throws Exception {
    assertArrayEquals(new String[] {""a"", ""1"", ""foo""}, TextUtils.parseDelimited(""a,1,foo"", ','));
    assertArrayEquals(new String[] {""a"", ""1"", ""foo"", """"},
                      TextUtils.parseDelimited(""a,1,foo,"", ','));
    assertArrayEquals(new String[] {""2.3""}, TextUtils.parseDelimited(""2.3"", ','));
    assertArrayEquals(new String[] {""\""a\""""}, TextUtils.parseDelimited(""\""\""\""a\""\""\"""", ','));
    assertArrayEquals(new String[] {""\"""", ""\""\""""},
                      TextUtils.parseDelimited(""\""\""\""\"" \""\""\""\""\""\"""", ' '));
    // Different from JSON, sort of:
    assertArrayEquals(new String[] {""""}, TextUtils.parseDelimited("""", ','));
    assertArrayEquals(new String[] {""a"", ""1,"", "",foo""},
                      TextUtils.parseDelimited(""a\t1,\t,foo"", '\t'));
    assertArrayEquals(new String[] {""a"", ""1"", ""foo"", """"},
                      TextUtils.parseDelimited(""a 1 foo "", ' '));
    assertArrayEquals(new String[] {""-1.0"", ""a\"" \""b""},
                      TextUtils.parseDelimited(""-1.0 a\""\\ \""b"", ' '));
    assertArrayEquals(new String[] {""-1.0"", ""a\""b\""c""},
                      TextUtils.parseDelimited(""-1.0 \""a\\\""b\\\""c\"""", ' '));

  }
",non-flaky,5
176900,OryxProject_oryx,TextUtilsTest.testParsePMMLDelimited,"  @Test
  public void testParsePMMLDelimited() {
    assertArrayEquals(new String[] {""1"", ""22"", ""3""}, TextUtils.parsePMMLDelimited(""1 22 3""));
    assertArrayEquals(new String[] {""ab"", ""a b"", ""with \""quotes\"" ""},
                      TextUtils.parsePMMLDelimited(""ab  \""a b\""   \""with \\\""quotes\\\"" \"" ""));
    assertArrayEquals(new String[] {""\"" \""""},
                      TextUtils.parsePMMLDelimited(""\""\\\"" \\\""\""""));
    assertArrayEquals(new String[] {"" c\"" d \""e "", "" c\"" d \""e ""},
                      TextUtils.parsePMMLDelimited("" \"" c\\\"" d \\\""e \"" \"" c\\\"" d \\\""e \"" ""));
  }
",non-flaky,5
176901,OryxProject_oryx,TextUtilsTest.testJoinDelimited,"  @Test
  public void testJoinDelimited() {
    assertEquals(""1,2,3"", TextUtils.joinDelimited(Arrays.asList(""1"", ""2"", ""3""), ','));
    assertEquals(""\""a,b\"""", TextUtils.joinDelimited(Arrays.asList(""a,b""), ','));
    assertEquals(""\""\""\""a\""\""\"""", TextUtils.joinDelimited(Arrays.asList(""\""a\""""), ','));
    assertEquals(""1 2 3"", TextUtils.joinDelimited(Arrays.asList(""1"", ""2"", ""3""), ' '));
    assertEquals(""\""1 \"" \""2 \"" 3"", TextUtils.joinDelimited(Arrays.asList(""1 "", ""2 "", ""3""), ' '));
    assertEquals(""\""\""\""a\""\""\"""", TextUtils.joinDelimited(Arrays.asList(""\""a\""""), ' '));
    assertEquals(""\""\""\""\"" \""\""\""\""\""\"""",
                 TextUtils.joinDelimited(Arrays.asList(""\"""", ""\""\""""), ' '));
    assertEquals("""", TextUtils.joinDelimited(Collections.emptyList(), '\t'));
  }
",non-flaky,5
176902,OryxProject_oryx,TextUtilsTest.testJoinPMMLDelimited,"  @Test
  public void testJoinPMMLDelimited() {
    assertEquals(""ab \""a b\"" \""with \\\""quotes\\\"" \"""",
                 TextUtils.joinPMMLDelimited(Arrays.asList(""ab"", ""a b"", ""with \""quotes\"" "")));
    assertEquals(""1 22 3"",
                 TextUtils.joinPMMLDelimited(Arrays.asList(""1"", ""22"", ""3"")));
    assertEquals(""\"" c\\\"" d \\\""e \"" \"" c\\\"" d \\\""e \"""",
                 TextUtils.joinPMMLDelimited(Arrays.asList("" c\"" d \""e "", "" c\"" d \""e "")));
  }
",non-flaky,5
176903,OryxProject_oryx,TextUtilsTest.testJoinPMMLDelimitedNumbers,"  @Test
  public void testJoinPMMLDelimitedNumbers() {
    assertEquals(""-1.0 2.01 3.5"",
                 TextUtils.joinPMMLDelimitedNumbers(Arrays.asList(-1.0, 2.01, 3.5)));
  }
",non-flaky,5
176904,OryxProject_oryx,TextUtilsTest.testJoinJSON,"  @Test
  public void testJoinJSON() {
    assertEquals(""[\""1\"",\""2\"",\""3\""]"", TextUtils.joinJSON(Arrays.asList(""1"", ""2"", ""3"")));
    assertEquals(""[\""1 \"",\""2 \"",\""3\""]"", TextUtils.joinJSON(Arrays.asList(""1 "", ""2 "", ""3"")));
    assertEquals(""[]"", TextUtils.joinJSON(Collections.emptyList()));
  }
",non-flaky,5
176905,OryxProject_oryx,TextUtilsTest.testJSONList,"  @Test
  public void testJSONList() {
    List<Object> list = new ArrayList<>();
    list.add(""foo"");
    list.add(2);
    assertEquals(""[\""A\"",[\""foo\"",2],\""B\""]"", TextUtils.joinJSON(Arrays.asList(""A"", list, ""B"")));
  }
",non-flaky,5
176906,OryxProject_oryx,TextUtilsTest.testJSONMap,"  @Test
  public void testJSONMap() {
    Map<Object,Object> map = new HashMap<>();
    map.put(1, ""bar"");
    map.put(""foo"", 2);
    assertEquals(""[\""A\"",{\""1\"":\""bar\"",\""foo\"":2},\""B\""]"",
                 TextUtils.joinJSON(Arrays.asList(""A"", map, ""B"")));
  }
",non-flaky,5
176907,OryxProject_oryx,RatingToTupleDoubleTest.testFunction,"  @Test
  public void testFunction() {
    Tuple2<Tuple2<Integer,Integer>,Double> tuple =
        new RatingToTupleDouble().call(new Rating(1, 2, 3.0));
    assertEquals(1, tuple._1()._1().intValue());
    assertEquals(2, tuple._1()._2().intValue());
    assertEquals(3.0, tuple._2().doubleValue());
  }
",non-flaky,5
176908,OryxProject_oryx,ALSHyperParamTuningIT.testHyperParameterTuning,"  @Test
  public void testHyperParameterTuning() throws Exception {
    Path tempDir = getTempDir();
    Path dataDir =  tempDir.resolve(""data"");
    Path modelDir = tempDir.resolve(""model"");

    Map<String,Object> overlayConfig = new HashMap<>();
    overlayConfig.put(""oryx.batch.update-class"", ALSUpdate.class.getName());
    ConfigUtils.set(overlayConfig, ""oryx.batch.storage.data-dir"", dataDir);
    ConfigUtils.set(overlayConfig, ""oryx.batch.storage.model-dir"", modelDir);
    overlayConfig.put(""oryx.batch.streaming.generation-interval-sec"", GEN_INTERVAL_SEC);
    overlayConfig.put(""oryx.batch.streaming.block-interval-sec"", BLOCK_INTERVAL_SEC);
    // Choose pairs of values where the best is predictable
    overlayConfig.put(""oryx.als.hyperparams.features"", ""[1,"" + TEST_FEATURES + ""]"");
    overlayConfig.put(""oryx.ml.eval.candidates"", 2);
    overlayConfig.put(""oryx.ml.eval.parallelism"", 2);
    Config config = ConfigUtils.overlayOn(overlayConfig, getConfig());

    startMessaging();

    startServerProduceConsumeTopics(config,
                                    new FeaturesALSDataGenerator(TEST_ELEMENTS,
                                                                 TEST_ELEMENTS,
                                                                 TEST_FEATURES),
                                    DATA_TO_WRITE,
                                    WRITE_INTERVAL_MSEC);

    List<Path> modelInstanceDirs = IOUtils.listFiles(modelDir, ""*"");

    checkIntervals(modelInstanceDirs.size(), DATA_TO_WRITE, WRITE_INTERVAL_MSEC, GEN_INTERVAL_SEC);

    Path latestModelDir = modelInstanceDirs.get(modelInstanceDirs.size() - 1);
    Path modelFile = latestModelDir.resolve(MLUpdate.MODEL_FILE_NAME);
    assertTrue(""No such model file: "" + modelFile, Files.exists(modelFile));

    PMML pmml = PMMLUtils.read(modelFile);
    assertEquals(8, pmml.getExtensions().size());
    assertNotNull(AppPMMLUtils.getExtensionValue(pmml, ""X""));
    assertNotNull(AppPMMLUtils.getExtensionValue(pmml, ""Y""));
    Map<String,Object> expected = new HashMap<>();
    expected.put(""features"", TEST_FEATURES);
    expected.put(""lambda"", 0.001);
    expected.put(""implicit"", true);
    expected.put(""alpha"", 1.0);
    checkExtensions(pmml, expected);
  }
",non-flaky,5
176909,OryxProject_oryx,ALSModelContentIT.testModelContent,"  @Test
  public void testModelContent() throws Exception {
    Path tempDir = getTempDir();
    Path modelDir = tempDir.resolve(""model"");

    Map<String,Object> overlayConfig = new HashMap<>();
    overlayConfig.put(""oryx.batch.update-class"", ALSUpdate.class.getName());
    ConfigUtils.set(overlayConfig, ""oryx.batch.storage.data-dir"", tempDir.resolve(""data""));
    ConfigUtils.set(overlayConfig, ""oryx.batch.storage.model-dir"", modelDir);
    overlayConfig.put(""oryx.batch.streaming.generation-interval-sec"", 10);
    overlayConfig.put(""oryx.batch.streaming.block-interval-sec"", 1);
    overlayConfig.put(""oryx.ml.eval.test-fraction"", 0);
    overlayConfig.put(""oryx.als.implicit"", false);
    overlayConfig.put(""oryx.als.hyperparams.lambda"", 0.0001);
    overlayConfig.put(""oryx.als.hyperparams.features"", 2);
    Config config = ConfigUtils.overlayOn(overlayConfig, getConfig());

    startMessaging();

    ModelContentDataGenerator generator = new ModelContentDataGenerator();
    List<Pair<String, String>> updates = startServerProduceConsumeTopics(
        config,
        generator,
        generator.getSentData().size(),
        20);

    Collection<String> modelUsers = null;
    Collection<String> modelItems = null;
    Map<String,Collection<String>> knownUsersItems = new HashMap<>();

    for (Pair<String, String> km : updates) {
      String type = km.getFirst();
      String value = km.getSecond();
      log.debug(""{} = {}"", type, value);

      if (""UP"".equals(type)) {

        List<?> update = MAPPER.readValue(value, List.class);
        if (""X"".equals(update.get(0).toString())) {
          String userID = update.get(1).toString();
          @SuppressWarnings(""unchecked"")
          Collection<String> userKnownItems = (Collection<String>) update.get(3);
          knownUsersItems.put(userID, new ArrayList<>(userKnownItems));
        }

      } else { // ""MODEL""

        PMML pmml = PMMLUtils.fromString(value);
        modelUsers = AppPMMLUtils.getExtensionContent(pmml, ""XIDs"");
        modelItems = AppPMMLUtils.getExtensionContent(pmml, ""YIDs"");

      }

    }

    assertContainsSame(Arrays.asList(""A0"", ""B1"", ""C2""), modelUsers);
    assertContainsSame(Arrays.asList(""A0"", ""B1"", ""C2"", ""D3""), modelItems);
    assertContainsSame(Arrays.asList(""A0"", ""B1"", ""C2"", ""D3""), knownUsersItems.get(""A0""));
    assertContainsSame(Arrays.asList(""C2"", ""D3""), knownUsersItems.get(""B1""));
    assertContainsSame(Arrays.asList(""D3""), knownUsersItems.get(""C2""));
  }
",non-flaky,5
176910,OryxProject_oryx,ALSUpdateIT.testALS,"  @Test
  public void testALS() throws Exception {
    Path tempDir = getTempDir();
    Path dataDir =  tempDir.resolve(""data"");
    Path modelDir = tempDir.resolve(""model"");

    Map<String,Object> overlayConfig = new HashMap<>();
    overlayConfig.put(""oryx.batch.update-class"", ALSUpdate.class.getName());
    ConfigUtils.set(overlayConfig, ""oryx.batch.storage.data-dir"", dataDir);
    ConfigUtils.set(overlayConfig, ""oryx.batch.storage.model-dir"", modelDir);
    overlayConfig.put(""oryx.batch.streaming.generation-interval-sec"", GEN_INTERVAL_SEC);
    overlayConfig.put(""oryx.batch.streaming.block-interval-sec"", BLOCK_INTERVAL_SEC);
    overlayConfig.put(""oryx.als.implicit"", false);
    overlayConfig.put(""oryx.als.hyperparams.lambda"", LAMBDA);
    overlayConfig.put(""oryx.als.hyperparams.features"", FEATURES);
    Config config = ConfigUtils.overlayOn(overlayConfig, getConfig());

    startMessaging();

    List<Pair<String,String>> updates = startServerProduceConsumeTopics(
        config,
        new RandomALSDataGenerator(NUM_USERS_ITEMS, NUM_USERS_ITEMS, 1, 5),
        DATA_TO_WRITE,
        WRITE_INTERVAL_MSEC);

    List<Path> modelInstanceDirs = IOUtils.listFiles(modelDir, ""*"");

    int generations = modelInstanceDirs.size();
    checkIntervals(generations, DATA_TO_WRITE, WRITE_INTERVAL_MSEC, GEN_INTERVAL_SEC);

    List<Collection<String>> userIDs = new ArrayList<>();
    userIDs.add(Collections.<String>emptySet()); // Add dummy empty set as prior value
    List<Collection<String>> productIDs = new ArrayList<>();
    productIDs.add(Collections.<String>emptySet()); // Add dummy empty set as prior value

    for (Path modelInstanceDir : modelInstanceDirs) {
      Path modelFile = modelInstanceDir.resolve(MLUpdate.MODEL_FILE_NAME);
      assertTrue(""Model file should exist: "" + modelFile, Files.exists(modelFile));
      assertTrue(""Model file should not be empty: "" + modelFile, Files.size(modelFile) > 0);
      PMMLUtils.read(modelFile); // Shouldn't throw exception
      Path xDir = modelInstanceDir.resolve(""X"");
      assertTrue(Files.exists(xDir));
      userIDs.add(checkFeatures(xDir, userIDs.get(userIDs.size() - 1)));
      Path yDir = modelInstanceDir.resolve(""Y"");
      assertTrue(Files.exists(yDir));
      productIDs.add(checkFeatures(yDir, productIDs.get(productIDs.size() - 1)));
    }
    // Remove dummy empty sets
    userIDs.remove(0);
    productIDs.remove(0);

    Collection<String> expectedUsers = null;
    Collection<String> expectedProducts = null;
    Collection<String> seenUsers = null;
    Collection<String> seenProducts = null;
    Collection<String> lastModelUsers = null;
    Collection<String> lastModelProducts = null;
    int whichGeneration = -1;
    for (Pair<String,String> km : updates) {

      String type = km.getFirst();
      String value = km.getSecond();

      log.debug(""{} = {}"", type, value);

      boolean isModel = ""MODEL"".equals(type);
      boolean isUpdate = ""UP"".equals(type);
      assertTrue(isModel || isUpdate);

      if (isUpdate) {

        assertNotNull(seenUsers);
        assertNotNull(seenProducts);

        List<?> update = MAPPER.readValue(value, List.class);
        // First field is X or Y, depending on whether it's a user or item vector
        String whichMatrixField = update.get(0).toString();
        boolean isUser = ""X"".equals(whichMatrixField);
        boolean isProduct = ""Y"".equals(whichMatrixField);
        // Next is user/item ID
        String id = update.get(1).toString();
        assertTrue(isUser || isProduct);
        if (isUser) {
          seenUsers.add(id);
        } else {
          seenProducts.add(id);
        }
        // Verify that feature vector are valid floats
        for (float f : MAPPER.convertValue(update.get(2), float[].class)) {
          assertTrue(!Float.isNaN(f) && !Float.isInfinite(f));
        }

        if (isUser) {
          // Only known-items for users exist now, not known users for items
          @SuppressWarnings(""unchecked"")
          Collection<String> knownUsersItems = (Collection<String>) update.get(3);
          assertFalse(knownUsersItems.isEmpty());
          for (String known : knownUsersItems) {
            int i = ALSUtilsTest.stringIDtoID(known);
            assertTrue(i >= 0 && i < NUM_USERS_ITEMS);
          }
        }

      } else {

        PMML pmml = PMMLUtils.fromString(value);

        checkHeader(pmml.getHeader());

        assertEquals(7, pmml.getExtensions().size());
        Map<String,Object> expected = new HashMap<>();
        expected.put(""features"", FEATURES);
        expected.put(""lambda"", LAMBDA);
        expected.put(""implicit"", false);
        checkExtensions(pmml, expected);

        // See if users/item sets seen in updates match what was expected from output
        assertContainsSame(expectedUsers, seenUsers);
        assertContainsSame(expectedProducts, seenProducts);

        // Also check key sets reported in model
        assertContainsSame(expectedUsers, lastModelUsers);
        assertContainsSame(expectedProducts, lastModelProducts);

        // Update for next round
        whichGeneration++;
        expectedUsers = userIDs.get(whichGeneration);
        expectedProducts = productIDs.get(whichGeneration);
        seenUsers = new HashSet<>();
        seenProducts = new HashSet<>();
        lastModelUsers = AppPMMLUtils.getExtensionContent(pmml, ""XIDs"");
        lastModelProducts = AppPMMLUtils.getExtensionContent(pmml, ""YIDs"");

      }
    }

  }
",non-flaky,5
176911,OryxProject_oryx,KMeansUpdateIT.testKMeans,"  @Test
  public void testKMeans() throws Exception {
    Path tempDir = getTempDir();
    Path dataDir = tempDir.resolve(""data"");
    Path modelDir = tempDir.resolve(""model"");

    Map<String,Object> overlayConfig = new HashMap<>();
    overlayConfig.put(""oryx.batch.update-class"", KMeansUpdate.class.getName());
    ConfigUtils.set(overlayConfig, ""oryx.batch.storage.data-dir"", dataDir);
    ConfigUtils.set(overlayConfig, ""oryx.batch.storage.model-dir"", modelDir);
    overlayConfig.put(""oryx.batch.streaming.generation-interval-sec"", GEN_INTERVAL_SEC);
    overlayConfig.put(""oryx.batch.streaming.block-interval-sec"", BLOCK_INTERVAL_SEC);
    overlayConfig.put(""oryx.kmeans.hyperparams.k"", NUM_CLUSTERS);
    overlayConfig.put(""oryx.kmeans.iterations"", 5);
    overlayConfig.put(""oryx.input-schema.num-features"", NUM_FEATURES);
    overlayConfig.put(""oryx.input-schema.categorical-features"", ""[]"");
    overlayConfig.put(""oryx.kmeans.evaluation-strategy"", EVALUATION_STRATEGY);

    Config config = ConfigUtils.overlayOn(overlayConfig, getConfig());

    startMessaging();

    List<Pair<String, String>> updates = startServerProduceConsumeTopics(
        config,
        new RandomKMeansDataGenerator(NUM_FEATURES),
        DATA_TO_WRITE,
        WRITE_INTERVAL_MSEC);

    List<Path> modelInstanceDirs = IOUtils.listFiles(modelDir, ""*"");

    int generations = modelInstanceDirs.size();
    checkIntervals(generations, DATA_TO_WRITE, WRITE_INTERVAL_MSEC, GEN_INTERVAL_SEC);

    for (Path modelInstanceDir : modelInstanceDirs) {
      Path modelFile = modelInstanceDir.resolve(MLUpdate.MODEL_FILE_NAME);
      assertTrue(""Model file should exist: "" + modelFile, Files.exists(modelFile));
      assertTrue(""Model file should not be empty: "" + modelFile, Files.size(modelFile) > 0);
      PMMLUtils.read(modelFile); // Shouldn't throw exception
    }

    InputSchema schema = new InputSchema(config);

    for (Pair<String,String> km : updates) {

      String type = km.getFirst();
      String value = km.getSecond();

      assertEquals(""MODEL"", type);

      PMML pmml = PMMLUtils.fromString(value);

      checkHeader(pmml.getHeader());

      checkDataDictionary(schema, pmml.getDataDictionary());

      Model rootModel = pmml.getModels().get(0);

      ClusteringModel clusteringModel = (ClusteringModel) rootModel;

      // Check if Basic hyperparameters match
      assertEquals(NUM_CLUSTERS, clusteringModel.getNumberOfClusters().intValue());
      assertEquals(NUM_CLUSTERS, clusteringModel.getClusters().size());
      assertEquals(NUM_FEATURES, clusteringModel.getClusteringFields().size());
      assertEquals(ComparisonMeasure.Kind.DISTANCE,
                   clusteringModel.getComparisonMeasure().getKind());
      assertEquals(NUM_FEATURES, clusteringModel.getClusters().get(0).getArray().getN().intValue());
      for (Cluster cluster : clusteringModel.getClusters()) {
        assertTrue(cluster.getSize() > 0);
      }
    }
  }
",non-flaky,5
176912,OryxProject_oryx,KMeansHyperParamTuningIT.testKMeans,"  @Test
  public void testKMeans() throws Exception {
    Path tempDir = getTempDir();
    Path dataDir = tempDir.resolve(""data"");
    Path modelDir = tempDir.resolve(""model"");

    Map<String,Object> overlayConfig = new HashMap<>();
    overlayConfig.put(""oryx.batch.update-class"", KMeansUpdate.class.getName());
    ConfigUtils.set(overlayConfig, ""oryx.batch.storage.data-dir"", dataDir);
    ConfigUtils.set(overlayConfig, ""oryx.batch.storage.model-dir"", modelDir);
    overlayConfig.put(""oryx.batch.streaming.generation-interval-sec"", GEN_INTERVAL_SEC);
    overlayConfig.put(""oryx.batch.streaming.block-interval-sec"", BLOCK_INTERVAL_SEC);
    overlayConfig.put(""oryx.kmeans.hyperparams.k"", ""[2,100]"");
    overlayConfig.put(""oryx.kmeans.iterations"", 20);
    overlayConfig.put(""oryx.kmeans.runs"", 20);
    overlayConfig.put(""oryx.input-schema.num-features"", NUM_FEATURES);
    overlayConfig.put(""oryx.input-schema.categorical-features"", ""[]"");
    overlayConfig.put(""oryx.ml.eval.candidates"", 3);
    overlayConfig.put(""oryx.ml.eval.parallelism"", 2);
    overlayConfig.put(""oryx.kmeans.evaluation-strategy"", EVALUATION_STRATEGY);

    Config config = ConfigUtils.overlayOn(overlayConfig, getConfig());

    startMessaging();

    startServerProduceConsumeTopics(
        config,
        new RandomKMeansDataGenerator(NUM_FEATURES),
        DATA_TO_WRITE,
        WRITE_INTERVAL_MSEC);

    List<Path> modelInstanceDirs = IOUtils.listFiles(modelDir, ""*"");

    checkIntervals(modelInstanceDirs.size(), DATA_TO_WRITE, WRITE_INTERVAL_MSEC, GEN_INTERVAL_SEC);

    Path latestModelDir = modelInstanceDirs.get(modelInstanceDirs.size() - 1);
    Path modelFile = latestModelDir.resolve(MLUpdate.MODEL_FILE_NAME);
    assertTrue(""No such model file: "" + modelFile, Files.exists(modelFile));

    PMML pmml = PMMLUtils.read(modelFile);
    Model rootModel = pmml.getModels().get(0);
    ClusteringModel clusteringModel = (ClusteringModel) rootModel;

    // Should have picked highest k
    assertEquals(100, clusteringModel.getNumberOfClusters().intValue());
  }
",non-flaky,5
176913,OryxProject_oryx,KMeansEvalIT.testFetchSampleEvalData,"  @Test
  public void testFetchSampleEvalData() {
    JavaRDD<Vector> evalData = SilhouetteCoefficient.fetchSampleData(getRddOfVectors());
    assertEquals(6, evalData.count());
  }
",non-flaky,5
176914,OryxProject_oryx,KMeansEvalIT.testDunnIndexForClustering,"  @Test
  public void testDunnIndexForClustering() {
    List<ClusterInfo> clusters = getClusters();
    DunnIndex dunnIndex = new DunnIndex(clusters);
    double eval = dunnIndex.evaluate(getRddOfVectors());
    log.info(""Dunn Index for {} clusters: {}"", clusters.size(), eval);
    assertEquals(1.7142857142857142, eval);
  }
",non-flaky,5
176915,OryxProject_oryx,KMeansEvalIT.testDaviesBouldinIndexForClustering,"  @Test
  public void testDaviesBouldinIndexForClustering() {
    List<ClusterInfo> clusters = getClusters();
    DaviesBouldinIndex daviesBouldinIndex = new DaviesBouldinIndex(clusters);
    double eval = daviesBouldinIndex.evaluate(getRddOfVectors());
    log.info(""Davies Bouldin Index for {} clusters: {}"", clusters.size(), eval);
    assertEquals(0.638888888888889, eval);
  }
",non-flaky,5
176916,OryxProject_oryx,KMeansEvalIT.testSilhouetteCoefficientForClustering,"  @Test
  public void testSilhouetteCoefficientForClustering() {
    List<ClusterInfo> clusters = getClusters();
    SilhouetteCoefficient silhouetteCoefficient = new SilhouetteCoefficient(clusters);
    double eval = silhouetteCoefficient.evaluate(getRddOfVectors());
    log.info(""Silhouette Coefficient for {} clusters: {}"", clusters.size(), eval);
    assertEquals(0.48484126984126985, eval);
  }
",non-flaky,5
176917,OryxProject_oryx,KMeansEvalIT.testComputeSilhouetteCoefficient,"  @Test
  public void testComputeSilhouetteCoefficient() {
    assertEquals(5.0, SilhouetteCoefficient.calcSilhouetteCoefficient(-0.8, 0.2));
    assertEquals(-1.25, SilhouetteCoefficient.calcSilhouetteCoefficient(0.8, -0.2));
    assertEquals(0.0, SilhouetteCoefficient.calcSilhouetteCoefficient(1.5, 1.5));
    assertEquals(1.0, SilhouetteCoefficient.calcSilhouetteCoefficient(1.5, Double.POSITIVE_INFINITY));
    assertEquals(-1.0, SilhouetteCoefficient.calcSilhouetteCoefficient(Double.POSITIVE_INFINITY, 1.5));
  }
",non-flaky,5
176918,OryxProject_oryx,RDFNumericHyperParamTuningIT.testRDF,"  @Test
  public void testRDF() throws Exception {
    Path tempDir = getTempDir();
    Path dataDir = tempDir.resolve(""data"");
    Path modelDir = tempDir.resolve(""model"");

    Map<String,Object> overlayConfig = new HashMap<>();
    overlayConfig.put(""oryx.batch.update-class"", RDFUpdate.class.getName());
    ConfigUtils.set(overlayConfig, ""oryx.batch.storage.data-dir"", dataDir);
    ConfigUtils.set(overlayConfig, ""oryx.batch.storage.model-dir"", modelDir);
    overlayConfig.put(""oryx.batch.streaming.generation-interval-sec"", GEN_INTERVAL_SEC);
    overlayConfig.put(""oryx.batch.streaming.block-interval-sec"", BLOCK_INTERVAL_SEC);
    overlayConfig.put(""oryx.rdf.num-trees"", NUM_TREES);
    // Low values like 1 are deliberately bad, won't work
    overlayConfig.put(""oryx.rdf.hyperparams.max-depth"", ""[1,"" + MAX_DEPTH + ""]"");
    overlayConfig.put(""oryx.rdf.hyperparams.max-split-candidates"", MAX_SPLIT_CANDIDATES);
    overlayConfig.put(""oryx.rdf.hyperparams.impurity"", IMPURITY);
    overlayConfig.put(""oryx.input-schema.num-features"", 5);
    overlayConfig.put(""oryx.input-schema.numeric-features"", ""[\""4\""]"");
    overlayConfig.put(""oryx.input-schema.id-features"", ""[\""0\""]"");
    overlayConfig.put(""oryx.input-schema.target-feature"", ""\""4\"""");
    overlayConfig.put(""oryx.ml.eval.candidates"", 2);
    overlayConfig.put(""oryx.ml.eval.parallelism"", 2);
    Config config = ConfigUtils.overlayOn(overlayConfig, getConfig());

    startMessaging();

    startServerProduceConsumeTopics(
        config,
        new RandomNumericRDFDataGenerator(3),
        DATA_TO_WRITE,
        WRITE_INTERVAL_MSEC);

    List<Path> modelInstanceDirs = IOUtils.listFiles(modelDir, ""*"");

    checkIntervals(modelInstanceDirs.size(), DATA_TO_WRITE, WRITE_INTERVAL_MSEC, GEN_INTERVAL_SEC);

    Path latestModelDir = modelInstanceDirs.get(modelInstanceDirs.size() - 1);
    Path modelFile = latestModelDir.resolve(MLUpdate.MODEL_FILE_NAME);
    assertTrue(""No such model file: "" + modelFile, Files.exists(modelFile));

    PMML pmml = PMMLUtils.read(modelFile);

    assertEquals(3, pmml.getExtensions().size());
    Map<String,Object> expected = new HashMap<>();
    expected.put(""maxSplitCandidates"", MAX_SPLIT_CANDIDATES);
    expected.put(""maxDepth"", MAX_DEPTH);
    expected.put(""impurity"", IMPURITY);
    checkExtensions(pmml, expected);

    Pair<DecisionForest,CategoricalValueEncodings> forestEncoding = RDFPMMLUtils.read(pmml);
    DecisionForest forest = forestEncoding.getFirst();
    CategoricalValueEncodings encoding = forestEncoding.getSecond();

    for (int f1 = 0; f1 <= 1; f1++) {
      for (int f2 = 0; f2 <= 1; f2++) {
        for (int f3 = 0; f3 <= 1; f3++) {
          NumericPrediction prediction = (NumericPrediction) forest.predict(new Example(null,
              null,
              CategoricalFeature.forEncoding(encoding.getValueEncodingMap(1).get(f1 == 1 ? ""A"" : ""B"")),
              CategoricalFeature.forEncoding(encoding.getValueEncodingMap(2).get(f2 == 1 ? ""A"" : ""B"")),
              CategoricalFeature.forEncoding(encoding.getValueEncodingMap(3).get(f3 == 1 ? ""A"" : ""B""))));
          int expectedCount = f1 + f2 + f3;
          if (expectedCount == 3) {
            // TODO this might be a bug in Spark RDF. The tree never creates a node for all
            // positive classes even though it should. Plenty of nodes, info gain, etc.
            assertEquals(2, Math.round(prediction.getPrediction()));
          } else {
            assertEquals(expectedCount, Math.round(prediction.getPrediction()));
          }
        }
      }
    }

  }
",non-flaky,5
176919,OryxProject_oryx,RDFUpdateIT.testRDF,"  @Test
  public void testRDF() throws Exception {
    Path tempDir = getTempDir();
    Path dataDir = tempDir.resolve(""data"");
    Path modelDir = tempDir.resolve(""model"");

    Map<String,Object> overlayConfig = new HashMap<>();
    overlayConfig.put(""oryx.batch.update-class"", RDFUpdate.class.getName());
    ConfigUtils.set(overlayConfig, ""oryx.batch.storage.data-dir"", dataDir);
    ConfigUtils.set(overlayConfig, ""oryx.batch.storage.model-dir"", modelDir);
    overlayConfig.put(""oryx.batch.streaming.generation-interval-sec"", GEN_INTERVAL_SEC);
    overlayConfig.put(""oryx.batch.streaming.block-interval-sec"", BLOCK_INTERVAL_SEC);
    overlayConfig.put(""oryx.rdf.num-trees"", NUM_TREES);
    overlayConfig.put(""oryx.rdf.hyperparams.max-depth"", MAX_DEPTH);
    overlayConfig.put(""oryx.rdf.hyperparams.max-split-candidates"", MAX_SPLIT_CANDIDATES);
    overlayConfig.put(""oryx.rdf.hyperparams.impurity"", IMPURITY);
    overlayConfig.put(""oryx.input-schema.num-features"", 5);
    overlayConfig.put(""oryx.input-schema.categorical-features"", ""[\""4\""]"");
    overlayConfig.put(""oryx.input-schema.id-features"", ""[\""0\""]"");
    overlayConfig.put(""oryx.input-schema.target-feature"", ""\""4\"""");

    Config config = ConfigUtils.overlayOn(overlayConfig, getConfig());

    startMessaging();

    List<Pair<String, String>> updates = startServerProduceConsumeTopics(
        config,
        new RandomCategoricalRDFDataGenerator(3),
        DATA_TO_WRITE,
        WRITE_INTERVAL_MSEC);

    List<Path> modelInstanceDirs = IOUtils.listFiles(modelDir, ""*"");

    int generations = modelInstanceDirs.size();
    checkIntervals(generations, DATA_TO_WRITE, WRITE_INTERVAL_MSEC, GEN_INTERVAL_SEC);

    for (Path modelInstanceDir : modelInstanceDirs) {
      Path modelFile = modelInstanceDir.resolve(MLUpdate.MODEL_FILE_NAME);
      assertTrue(""Model file should exist: "" + modelFile, Files.exists(modelFile));
      assertTrue(""Model file should not be empty: "" + modelFile, Files.size(modelFile) > 0);
      PMMLUtils.read(modelFile); // Shouldn't throw exception
    }

    InputSchema schema = new InputSchema(config);

    for (Pair<String,String> km : updates) {

      String type = km.getFirst();
      String value = km.getSecond();

      assertEquals(""MODEL"", type);

      PMML pmml = PMMLUtils.fromString(value);

      checkHeader(pmml.getHeader());

      assertEquals(3, pmml.getExtensions().size());
      Map<String,Object> expected = new HashMap<>();
      expected.put(""maxDepth"", MAX_DEPTH);
      expected.put(""maxSplitCandidates"", MAX_SPLIT_CANDIDATES);
      expected.put(""impurity"", IMPURITY);
      checkExtensions(pmml, expected);

      checkDataDictionary(schema, pmml.getDataDictionary());

      Model rootModel = pmml.getModels().get(0);
      if (rootModel instanceof TreeModel) {
        assertEquals(NUM_TREES, 1);
        TreeModel treeModel = (TreeModel) rootModel;
        checkTreeModel(treeModel);
      } else if (rootModel instanceof MiningModel) {
        MiningModel miningModel = (MiningModel) rootModel;
        Segmentation segmentation = miningModel.getSegmentation();
        if (schema.isClassification()) {
          assertEquals(MultipleModelMethodType.WEIGHTED_MAJORITY_VOTE,
                       segmentation.getMultipleModelMethod());
        } else {
          assertEquals(MultipleModelMethodType.WEIGHTED_AVERAGE,
                       segmentation.getMultipleModelMethod());
        }
        List<Segment> segments = segmentation.getSegments();
        assertEquals(NUM_TREES, segments.size());
        for (int i = 0; i < segments.size(); i++) {
          Segment segment = segments.get(i);
          assertEquals(Integer.toString(i), segment.getId());
          assertTrue(segment.getPredicate() instanceof True);
          assertEquals(1.0, segment.getWeight());
          assertTrue(segment.getModel() instanceof TreeModel);
          checkTreeModel((TreeModel) segment.getModel());
        }

      } else {
        fail(""Wrong model type: "" + rootModel.getClass());
        return;
      }

      if (schema.isClassification()) {
        assertEquals(MiningFunctionType.CLASSIFICATION, rootModel.getFunctionName());
      } else {
        assertEquals(MiningFunctionType.REGRESSION, rootModel.getFunctionName());
      }

      checkMiningSchema(schema, rootModel.getMiningSchema());

    }
  }
",non-flaky,5
176920,OryxProject_oryx,RDFCategoricalHyperParamTuningIT.testRDF,"  @Test
  public void testRDF() throws Exception {
    Path tempDir = getTempDir();
    Path dataDir = tempDir.resolve(""data"");
    Path modelDir = tempDir.resolve(""model"");

    Map<String,Object> overlayConfig = new HashMap<>();
    overlayConfig.put(""oryx.batch.update-class"", RDFUpdate.class.getName());
    ConfigUtils.set(overlayConfig, ""oryx.batch.storage.data-dir"", dataDir);
    ConfigUtils.set(overlayConfig, ""oryx.batch.storage.model-dir"", modelDir);
    overlayConfig.put(""oryx.batch.streaming.generation-interval-sec"", GEN_INTERVAL_SEC);
    overlayConfig.put(""oryx.batch.streaming.block-interval-sec"", BLOCK_INTERVAL_SEC);
    overlayConfig.put(""oryx.rdf.num-trees"", 10);
    overlayConfig.put(""oryx.rdf.hyperparams.max-depth"", MAX_DEPTH);
    // Low values like 1 are deliberately bad, won't work
    overlayConfig.put(""oryx.rdf.hyperparams.max-depth"", ""[1,"" + MAX_DEPTH + ""]"");
    overlayConfig.put(""oryx.rdf.hyperparams.max-split-candidates"", MAX_SPLIT_CANDIDATES);
    overlayConfig.put(""oryx.input-schema.num-features"", 5);
    overlayConfig.put(""oryx.input-schema.categorical-features"", ""[\""4\""]"");
    overlayConfig.put(""oryx.input-schema.id-features"", ""[\""0\""]"");
    overlayConfig.put(""oryx.input-schema.target-feature"", ""\""4\"""");
    overlayConfig.put(""oryx.ml.eval.candidates"", 3);
    overlayConfig.put(""oryx.ml.eval.parallelism"", 2);
    Config config = ConfigUtils.overlayOn(overlayConfig, getConfig());

    startMessaging();

    startServerProduceConsumeTopics(
        config,
        new RandomCategoricalRDFDataGenerator(3),
        DATA_TO_WRITE,
        WRITE_INTERVAL_MSEC);

    List<Path> modelInstanceDirs = IOUtils.listFiles(modelDir, ""*"");

    checkIntervals(modelInstanceDirs.size(), DATA_TO_WRITE, WRITE_INTERVAL_MSEC, GEN_INTERVAL_SEC);

    Path latestModelDir = modelInstanceDirs.get(modelInstanceDirs.size() - 1);
    Path modelFile = latestModelDir.resolve(MLUpdate.MODEL_FILE_NAME);
    assertTrue(""No such model file: "" + modelFile, Files.exists(modelFile));

    PMML pmml = PMMLUtils.read(modelFile);

    assertEquals(3, pmml.getExtensions().size());
    Map<String,Object> expected = new HashMap<>();
    expected.put(""maxSplitCandidates"", MAX_SPLIT_CANDIDATES);
    expected.put(""maxDepth"", MAX_DEPTH);
    expected.put(""impurity"", IMPURITY);
    checkExtensions(pmml, expected);

    Pair<DecisionForest,CategoricalValueEncodings> forestEncoding = RDFPMMLUtils.read(pmml);
    DecisionForest forest = forestEncoding.getFirst();
    CategoricalValueEncodings encoding = forestEncoding.getSecond();
    Map<String,Integer> targetEncoding = encoding.getValueEncodingMap(4);

    int[] zeroOne = { 0, 1 };
    for (int f1 : zeroOne) {
      for (int f2 : zeroOne) {
        for (int f3 : zeroOne) {
          CategoricalPrediction prediction =
              (CategoricalPrediction) forest.predict(new Example(null,
                                                                 null,
                                                                 NumericFeature.forValue(f1),
                                                                 NumericFeature.forValue(f2),
                                                                 NumericFeature.forValue(f3)));
          boolean expectedPositive = f1 == 1 && f2 == 1 && f3 == 1;
          assertEquals(targetEncoding.get(Boolean.toString(expectedPositive)).intValue(),
                       prediction.getMostProbableCategoryEncoding());
        }
      }
    }

  }
",non-flaky,5
176921,OryxProject_oryx,MultiRescorerProviderTest.testMultiRecommendRescorer,"  @Test
  public void testMultiRecommendRescorer() {
    RescorerProvider multi = new MultiRescorerProvider(
        new SimpleModRescorerProvider(2), new SimpleModRescorerProvider(3));
    
    Rescorer provider = multi.getRecommendRescorer(Collections.singletonList(""ABCDE""), null);
    assertNull(provider);

    Rescorer provider2 = multi.getRecommendRescorer(Collections.singletonList(""AB""), null);
    assertNotNull(provider2);
    assertFalse(provider2 instanceof MultiRescorer);
    assertTrue(provider2.isFiltered(""ABC""));
    assertFalse(provider2.isFiltered(""AB""));

    Rescorer provider3 = multi.getRecommendRescorer(Collections.singletonList(""ABCDEF""), null);
    assertNotNull(provider3);
    assertTrue(provider3 instanceof MultiRescorer);
    assertTrue(provider3.isFiltered(""ABC""));
    assertTrue(provider3.isFiltered(""AB""));
    assertFalse(provider3.isFiltered(""ABCDEFABCDEF""));
  }
",non-flaky,5
176922,OryxProject_oryx,MultiRescorerProviderTest.testMultiRecommendToAnonymousRescorer,"  @Test
  public void testMultiRecommendToAnonymousRescorer() {
    RescorerProvider multi = new MultiRescorerProvider(
        new SimpleModRescorerProvider(2), new SimpleModRescorerProvider(3));
    
    Rescorer provider = multi.getRecommendToAnonymousRescorer(
        Collections.singletonList(""ABCDE""), null);
    assertNull(provider);

    Rescorer provider2 =
        multi.getRecommendToAnonymousRescorer(Collections.singletonList(""AB""), null);
    assertNotNull(provider2);
    assertFalse(provider2 instanceof MultiRescorer);
    assertTrue(provider2.isFiltered(""ABC""));
    assertFalse(provider2.isFiltered(""AB""));

    Rescorer provider3 =
        multi.getRecommendToAnonymousRescorer(Collections.singletonList(""ABCDEF""), null);
    assertNotNull(provider3);
    assertTrue(provider3 instanceof MultiRescorer);
    assertTrue(provider3.isFiltered(""ABC""));
    assertTrue(provider3.isFiltered(""AB""));
    assertFalse(provider3.isFiltered(""ABCDEF""));
  }
",non-flaky,5
176923,OryxProject_oryx,MultiRescorerProviderTest.testMultiMostPopularItemsRescorer,"  @Test
  public void testMultiMostPopularItemsRescorer() {
    RescorerProvider multi = new MultiRescorerProvider(
        new SimpleModRescorerProvider(2), new SimpleModRescorerProvider(3));
    Rescorer provider = multi.getMostPopularItemsRescorer(null);
    assertNotNull(provider);
    assertTrue(provider instanceof MultiRescorer);
    assertTrue(provider.isFiltered(""ABC""));
    assertTrue(provider.isFiltered(""AB""));
    assertFalse(provider.isFiltered(""ABCDEF""));
  }
",non-flaky,5
176924,OryxProject_oryx,MultiRescorerProviderTest.testMultiMostActiveUsersRescorer,"  @Test
  public void testMultiMostActiveUsersRescorer() {
    RescorerProvider multi = new MultiRescorerProvider(
        new SimpleModRescorerProvider(2), new SimpleModRescorerProvider(3));
    Rescorer provider = multi.getMostActiveUsersRescorer(null);
    assertNotNull(provider);
    assertTrue(provider instanceof MultiRescorer);
    assertTrue(provider.isFiltered(""ABC""));
    assertTrue(provider.isFiltered(""AB""));
    assertFalse(provider.isFiltered(""ABCDEF""));
  }
",non-flaky,5
176925,OryxProject_oryx,MultiRescorerProviderTest.testMultiMostSimilarItemsRescorer,"  @Test
  public void testMultiMostSimilarItemsRescorer() {
    RescorerProvider multi = new MultiRescorerProvider(
        new SimpleModRescorerProvider(2), new SimpleModRescorerProvider(3));
    Rescorer provider = multi.getMostSimilarItemsRescorer(null);
    assertNotNull(provider);
    assertTrue(provider instanceof MultiRescorer);
    assertTrue(provider.isFiltered(""ABC""));
    assertTrue(provider.isFiltered(""ABCDE""));
    assertFalse(provider.isFiltered(""ABCDEFABCDEF""));
  }
",non-flaky,5
176926,OryxProject_oryx,AbstractRescorerProviderTest.testDefault,"  @Test
  public void testDefault() {
    RescorerProvider noop = new NullProvider1();
    assertNull(noop.getMostActiveUsersRescorer(null));
    assertNull(noop.getMostPopularItemsRescorer(null));
    assertNull(noop.getMostSimilarItemsRescorer(null));
    assertNull(noop.getRecommendRescorer(null, null));
    assertNull(noop.getRecommendToAnonymousRescorer(null, null));
  }
",non-flaky,5
176927,OryxProject_oryx,AbstractRescorerProviderTest.testLoad,"  @Test
  public void testLoad() {
    RescorerProvider provider = AbstractRescorerProvider.loadRescorerProviders(
        ""com.cloudera.oryx.app.als.NullProvider2"");
    assertTrue(provider instanceof NullProvider2);
    RescorerProvider multiProvider = AbstractRescorerProvider.loadRescorerProviders(
        ""com.cloudera.oryx.app.als.NullProvider1,com.cloudera.oryx.app.als.NullProvider2"");
    assertTrue(multiProvider instanceof MultiRescorerProvider);
  }
",non-flaky,5
176928,OryxProject_oryx,AbstractRescorerProviderTest.testNoClass,"  @Test(expected = IllegalArgumentException.class)
  public void testNoClass() {
    AbstractRescorerProvider.loadRescorerProviders(""noSuchClass"");
  }
",non-flaky,5
176929,OryxProject_oryx,AbstractRescorerProviderTest.testWrongClass,"  @Test(expected = ClassCastException.class)
  public void testWrongClass() {
    AbstractRescorerProvider.loadRescorerProviders(
        ""com.cloudera.oryx.app.als.AbstractRescorerProviderTest"");
  }
",non-flaky,5
176930,OryxProject_oryx,ServingLayerTest.testServingLayer,"  @Test
  public void testServingLayer() throws Exception {
    Map<String,Object> overlay = buildOverlay();
    Config config = ConfigUtils.overlayOn(overlay, ConfigUtils.getDefault());
    doTestServingLayer(config);
  }
",non-flaky,5
176931,OryxProject_oryx,ServingLayerTest.testServingLayerSecure,"  @Test
  public void testServingLayerSecure() throws Exception {
    Path keystoreFile = SecureAPIConfigIT.buildKeystoreFile();
    Map<String,Object> overlay = buildOverlay();
    overlay.put(""oryx.serving.api.keystore-file"", ""\"""" + keystoreFile + ""\"""");
    overlay.put(""oryx.serving.api.keystore-password"", ""oryxpass"");
    Config config = ConfigUtils.overlayOn(overlay, ConfigUtils.getDefault());
    try {
      doTestServingLayer(config);
    } finally {
      Files.delete(Paths.get(config.getString(""oryx.serving.api.keystore-file"")));
    }
  }
",non-flaky,5
